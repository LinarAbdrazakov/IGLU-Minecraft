{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *\n",
    "from wrappers_2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9fd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4, padding=0),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(64, 512, kernel_size=2, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import one_hot\n",
    "\n",
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        visual_features_dim = 512\n",
    "        target_features_dim = 9 * 11 * 11 \n",
    "        self.visual_encoder = VisualEncoder()\n",
    "        self.visual_encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AnnaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.target_encoder = nn.Sequential(\n",
    "            nn.Conv3d(7, 1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        policy_hidden_dim = 256 \n",
    "        self.policy_network = nn.Sequential(\n",
    "            nn.Linear(visual_features_dim + target_features_dim, policy_hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        self.action_head = nn.Linear(policy_hidden_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(policy_hidden_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.visual_encoder.cuda()\n",
    "            self.target_encoder.cuda()\n",
    "            self.policy_network.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs']\n",
    "        pov = obs['pov'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        target = one_hot(obs['target_grid'].long(), num_classes=7).permute(0, 4, 1, 2, 3).float()\n",
    "        if self.use_cuda:\n",
    "            pov.cuda()\n",
    "            target.cuda()\n",
    "            \n",
    "        visual_features = self.visual_encoder(pov)\n",
    "        target_features = self.target_encoder(target)\n",
    "        target_features = target_features.reshape(target_features.shape[0], -1)\n",
    "        features = torch.cat([visual_features, target_features], dim=1)\n",
    "        features = self.policy_network(features)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc09c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualObservationWrapper(ObsWrapper):\n",
    "    def __init__(self, env, include_target=False):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = {   \n",
    "            'pov': gym.spaces.Box(low=0, high=255, shape=(64, 64, 3)),\n",
    "            'inventory': gym.spaces.Box(low=0.0, high=20.0, shape=(6,)),\n",
    "            'compass': gym.spaces.Box(low=-180.0, high=180.0, shape=(1,))\n",
    "        }\n",
    "        if include_target:\n",
    "            self.observation_space['target_grid'] = \\\n",
    "                gym.spaces.Box(low=0, high=6, shape=(9, 11, 11))\n",
    "        self.observation_space = gym.spaces.Dict(self.observation_space)\n",
    "\n",
    "    def observation(self, obs, reward=None, done=None, info=None):\n",
    "        if info is not None:\n",
    "            if 'target_grid' in info:\n",
    "                target_grid = info['target_grid']\n",
    "                del info['target_grid']\n",
    "            else:\n",
    "                logger.error(f'info: {info}')\n",
    "                if hasattr(self.unwrapped, 'should_reset'):\n",
    "                    self.unwrapped.should_reset(True)\n",
    "                target_grid = self.env.unwrapped.tasks.current.target_grid\n",
    "        else:\n",
    "            target_grid = self.env.unwrapped.tasks.current.target_grid\n",
    "        return {\n",
    "            'pov': obs['pov'].astype(np.float32),\n",
    "            'inventory': obs['inventory'],\n",
    "            'compass': np.array([obs['compass']['angle'].item()]),\n",
    "            'target_grid': target_grid\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def reward(self, rew):\n",
    "        if rew == 0:\n",
    "            rew = -0.01\n",
    "        return rew\n",
    "    \n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C32']))\n",
    "    #env = PovOnlyWrapper(env)\n",
    "    env = VisualObservationWrapper(env, include_target=True)\n",
    "    env = SelectAndPlace(env)\n",
    "    env = Discretization(env, flat_action_space('human-level'))\n",
    "    env = RewardWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n",
      "2021-10-21 19:06:50,002\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-10-21 19:06:50,018\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to auto resume run with id ae052_00000 but id 0b57e_00000 is set.\n",
      "\u001b[2m\u001b[36m(pid=11962)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11962)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO MultiTask (C32) pretrained (AnnaCNN) (3 noops after placement) r: -0.01</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/0b57e_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/0b57e_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20211021_190651-0b57e_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11962)\u001b[0m 2021-10-21 19:06:53,424\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=11962)\u001b[0m 2021-10-21 19:06:53,424\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=11962)\u001b[0m 2021-10-21 19:06:59,380\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-07-57\n",
      "  done: false\n",
      "  episode_len_mean: 386.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.699999999999965\n",
      "  episode_reward_mean: -3.859999999999962\n",
      "  episode_reward_min: -4.019999999999959\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8657582283020018\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011295225388316114\n",
      "          policy_loss: 0.02128616985347536\n",
      "          total_loss: -0.0028004381391737196\n",
      "          vf_explained_var: 0.027744241058826447\n",
      "          vf_loss: 0.002311929318238981\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.709638554216866\n",
      "    ram_util_percent: 29.0867469879518\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386547732662845\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 55.24616689234228\n",
      "    mean_inference_ms: 1.7286968993378449\n",
      "    mean_raw_obs_processing_ms: 0.19295541913835676\n",
      "  time_since_restore: 58.157490491867065\n",
      "  time_this_iter_s: 58.157490491867065\n",
      "  time_total_s: 58.157490491867065\n",
      "  timers:\n",
      "    learn_throughput: 1236.911\n",
      "    learn_time_ms: 808.465\n",
      "    load_throughput: 38429.789\n",
      "    load_time_ms: 26.021\n",
      "    sample_throughput: 17.447\n",
      "    sample_time_ms: 57317.718\n",
      "    update_time_ms: 2.033\n",
      "  timestamp: 1634843277\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         58.1575</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">   -3.86</td><td style=\"text-align: right;\">                -3.7</td><td style=\"text-align: right;\">               -4.02</td><td style=\"text-align: right;\">               386</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-08-19\n",
      "  done: false\n",
      "  episode_len_mean: 395.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.699999999999965\n",
      "  episode_reward_mean: -4.257999999999958\n",
      "  episode_reward_min: -5.359999999999953\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 5\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.834957716200087\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011457809688480362\n",
      "          policy_loss: -0.01814782197276751\n",
      "          total_loss: 0.30453668755168717\n",
      "          vf_explained_var: 0.04040609300136566\n",
      "          vf_loss: 0.34874251517467203\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.521875\n",
      "    ram_util_percent: 31.084374999999998\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03842065336130525\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 44.500495637134875\n",
      "    mean_inference_ms: 1.7166595620875629\n",
      "    mean_raw_obs_processing_ms: 0.20568899540296792\n",
      "  time_since_restore: 80.50800848007202\n",
      "  time_this_iter_s: 22.350517988204956\n",
      "  time_total_s: 80.50800848007202\n",
      "  timers:\n",
      "    learn_throughput: 1171.141\n",
      "    learn_time_ms: 853.868\n",
      "    load_throughput: 36850.649\n",
      "    load_time_ms: 27.137\n",
      "    sample_throughput: 25.402\n",
      "    sample_time_ms: 39366.91\n",
      "    update_time_ms: 2.937\n",
      "  timestamp: 1634843299\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">          80.508</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">  -4.258</td><td style=\"text-align: right;\">                -3.7</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">             395.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-08-40\n",
      "  done: false\n",
      "  episode_len_mean: 401.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.699999999999965\n",
      "  episode_reward_mean: -4.224285714285672\n",
      "  episode_reward_min: -5.359999999999953\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 7\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8349191109339396\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015375480082855691\n",
      "          policy_loss: -0.06303459405899048\n",
      "          total_loss: -0.0801603024204572\n",
      "          vf_explained_var: 0.4462907612323761\n",
      "          vf_loss: 0.00814838793853091\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.02068965517241\n",
      "    ram_util_percent: 31.24827586206897\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03838424766875088\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.56929581629081\n",
      "    mean_inference_ms: 1.712232051879591\n",
      "    mean_raw_obs_processing_ms: 0.2056886492278592\n",
      "  time_since_restore: 100.86733555793762\n",
      "  time_this_iter_s: 20.3593270778656\n",
      "  time_total_s: 100.86733555793762\n",
      "  timers:\n",
      "    learn_throughput: 1181.894\n",
      "    learn_time_ms: 846.099\n",
      "    load_throughput: 40458.744\n",
      "    load_time_ms: 24.717\n",
      "    sample_throughput: 30.539\n",
      "    sample_time_ms: 32745.531\n",
      "    update_time_ms: 2.674\n",
      "  timestamp: 1634843320\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         100.867</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">-4.22429</td><td style=\"text-align: right;\">                -3.7</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">               401</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-09-01\n",
      "  done: false\n",
      "  episode_len_mean: 402.77777777777777\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.699999999999965\n",
      "  episode_reward_mean: -4.194444444444403\n",
      "  episode_reward_min: -5.359999999999953\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 9\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8013875934812758\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010863796409611836\n",
      "          policy_loss: -0.26396643395225206\n",
      "          total_loss: -0.03987264268928104\n",
      "          vf_explained_var: -0.14763697981834412\n",
      "          vf_loss: 0.2499349045360254\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.496774193548376\n",
      "    ram_util_percent: 30.941935483870967\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03826978490995239\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.69219086766483\n",
      "    mean_inference_ms: 1.707537113428471\n",
      "    mean_raw_obs_processing_ms: 0.204614966224538\n",
      "  time_since_restore: 121.89723014831543\n",
      "  time_this_iter_s: 21.029894590377808\n",
      "  time_total_s: 121.89723014831543\n",
      "  timers:\n",
      "    learn_throughput: 1193.873\n",
      "    learn_time_ms: 837.61\n",
      "    load_throughput: 40078.01\n",
      "    load_time_ms: 24.951\n",
      "    sample_throughput: 33.777\n",
      "    sample_time_ms: 29605.996\n",
      "    update_time_ms: 2.453\n",
      "  timestamp: 1634843341\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         121.897</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-4.19444</td><td style=\"text-align: right;\">                -3.7</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">           402.778</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-09-23\n",
      "  done: false\n",
      "  episode_len_mean: 397.1666666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.4099999999999713\n",
      "  episode_reward_mean: -6.288333333333313\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 12\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7516336679458617\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011317950294766217\n",
      "          policy_loss: -0.006284104453192817\n",
      "          total_loss: 0.023489785194396973\n",
      "          vf_explained_var: 0.458140105009079\n",
      "          vf_loss: 0.05502663639684518\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.981249999999996\n",
      "    ram_util_percent: 30.815625\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03812441352271475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 34.78186387482661\n",
      "    mean_inference_ms: 1.7022109699623194\n",
      "    mean_raw_obs_processing_ms: 0.2050764156606616\n",
      "  time_since_restore: 144.45219087600708\n",
      "  time_this_iter_s: 22.55496072769165\n",
      "  time_total_s: 144.45219087600708\n",
      "  timers:\n",
      "    learn_throughput: 1207.516\n",
      "    learn_time_ms: 828.146\n",
      "    load_throughput: 40454.711\n",
      "    load_time_ms: 24.719\n",
      "    sample_throughput: 35.673\n",
      "    sample_time_ms: 28032.022\n",
      "    update_time_ms: 2.327\n",
      "  timestamp: 1634843363\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         144.452</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">-6.28833</td><td style=\"text-align: right;\">               -3.41</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           397.167</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-09-46\n",
      "  done: false\n",
      "  episode_len_mean: 391.3333333333333\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.4099999999999713\n",
      "  episode_reward_mean: -5.766666666666643\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 15\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.784320574336582\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006212926283664485\n",
      "          policy_loss: -0.18536202195617887\n",
      "          total_loss: -0.20387611132529046\n",
      "          vf_explained_var: -0.06855765730142593\n",
      "          vf_loss: 0.008086532294853694\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.7516129032258\n",
      "    ram_util_percent: 30.751612903225812\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038005810619723426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.81501080871434\n",
      "    mean_inference_ms: 1.6982159766718994\n",
      "    mean_raw_obs_processing_ms: 0.20555934629160816\n",
      "  time_since_restore: 166.66357970237732\n",
      "  time_this_iter_s: 22.21138882637024\n",
      "  time_total_s: 166.66357970237732\n",
      "  timers:\n",
      "    learn_throughput: 1215.105\n",
      "    learn_time_ms: 822.974\n",
      "    load_throughput: 40416.314\n",
      "    load_time_ms: 24.742\n",
      "    sample_throughput: 37.141\n",
      "    sample_time_ms: 26924.116\n",
      "    update_time_ms: 2.255\n",
      "  timestamp: 1634843386\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         166.664</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">-5.76667</td><td style=\"text-align: right;\">               -3.41</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           391.333</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-10-09\n",
      "  done: false\n",
      "  episode_len_mean: 387.72222222222223\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.4099999999999713\n",
      "  episode_reward_mean: -5.729444444444417\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 18\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7888890716764663\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013232911376316705\n",
      "          policy_loss: -0.0374074989101953\n",
      "          total_loss: 0.24142726535598438\n",
      "          vf_explained_var: -0.06325794011354446\n",
      "          vf_loss: 0.304077071745673\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 7000\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 7000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.78235294117647\n",
      "    ram_util_percent: 30.794117647058822\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037930129933002626\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.401342836530276\n",
      "    mean_inference_ms: 1.6955453362264148\n",
      "    mean_raw_obs_processing_ms: 0.2065304005589969\n",
      "  time_since_restore: 190.13796639442444\n",
      "  time_this_iter_s: 23.47438669204712\n",
      "  time_total_s: 190.13796639442444\n",
      "  timers:\n",
      "    learn_throughput: 1219.013\n",
      "    learn_time_ms: 820.336\n",
      "    load_throughput: 40249.292\n",
      "    load_time_ms: 24.845\n",
      "    sample_throughput: 38.005\n",
      "    sample_time_ms: 26312.013\n",
      "    update_time_ms: 2.213\n",
      "  timestamp: 1634843409\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         190.138</td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\">-5.72944</td><td style=\"text-align: right;\">               -3.41</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           387.722</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-10-33\n",
      "  done: false\n",
      "  episode_len_mean: 385.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.4099999999999713\n",
      "  episode_reward_mean: -5.520999999999972\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 20\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7793382167816163\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005806164545941823\n",
      "          policy_loss: -0.20410702973604203\n",
      "          total_loss: -0.2224707822004954\n",
      "          vf_explained_var: -0.0941402018070221\n",
      "          vf_loss: 0.008268397745107197\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.72727272727273\n",
      "    ram_util_percent: 30.854545454545455\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037885860969341184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.64834825503042\n",
      "    mean_inference_ms: 1.6941211649400731\n",
      "    mean_raw_obs_processing_ms: 0.20669167494113316\n",
      "  time_since_restore: 213.55896282196045\n",
      "  time_this_iter_s: 23.42099642753601\n",
      "  time_total_s: 213.55896282196045\n",
      "  timers:\n",
      "    learn_throughput: 1220.629\n",
      "    learn_time_ms: 819.25\n",
      "    load_throughput: 39902.05\n",
      "    load_time_ms: 25.061\n",
      "    sample_throughput: 38.692\n",
      "    sample_time_ms: 25845.252\n",
      "    update_time_ms: 2.182\n",
      "  timestamp: 1634843433\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         213.559</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">  -5.521</td><td style=\"text-align: right;\">               -3.41</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">             385.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-10-56\n",
      "  done: false\n",
      "  episode_len_mean: 380.2173913043478\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2399999999999856\n",
      "  episode_reward_mean: -5.828260869565191\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 23\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7474695761998493\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009769350681929407\n",
      "          policy_loss: -0.0042065509491496615\n",
      "          total_loss: 0.194901561902629\n",
      "          vf_explained_var: 0.4173086881637573\n",
      "          vf_loss: 0.22462893807225756\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 9000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.241176470588236\n",
      "    ram_util_percent: 30.80588235294118\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037824147584082125\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.72208132821211\n",
      "    mean_inference_ms: 1.6922221257117285\n",
      "    mean_raw_obs_processing_ms: 0.20735858407666036\n",
      "  time_since_restore: 237.3485972881317\n",
      "  time_this_iter_s: 23.789634466171265\n",
      "  time_total_s: 237.3485972881317\n",
      "  timers:\n",
      "    learn_throughput: 1219.83\n",
      "    learn_time_ms: 819.786\n",
      "    load_throughput: 39788.703\n",
      "    load_time_ms: 25.133\n",
      "    sample_throughput: 39.182\n",
      "    sample_time_ms: 25521.89\n",
      "    update_time_ms: 2.16\n",
      "  timestamp: 1634843456\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         237.349</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">-5.82826</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           380.217</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-11-21\n",
      "  done: false\n",
      "  episode_len_mean: 378.11538461538464\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2399999999999856\n",
      "  episode_reward_mean: -5.573461538461511\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 26\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7589119328392875\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013169888188921538\n",
      "          policy_loss: -0.014173806541495852\n",
      "          total_loss: -0.028068392806582982\n",
      "          vf_explained_var: 0.2169017195701599\n",
      "          vf_loss: 0.011060554403552993\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.21428571428571\n",
      "    ram_util_percent: 30.822857142857142\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0377814775451191\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.98333352920337\n",
      "    mean_inference_ms: 1.6908349076342202\n",
      "    mean_raw_obs_processing_ms: 0.20792702594146656\n",
      "  time_since_restore: 261.4623234272003\n",
      "  time_this_iter_s: 24.113726139068604\n",
      "  time_total_s: 261.4623234272003\n",
      "  timers:\n",
      "    learn_throughput: 1219.628\n",
      "    learn_time_ms: 819.922\n",
      "    load_throughput: 39859.125\n",
      "    load_time_ms: 25.088\n",
      "    sample_throughput: 39.532\n",
      "    sample_time_ms: 25295.999\n",
      "    update_time_ms: 2.129\n",
      "  timestamp: 1634843481\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         261.462</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">-5.57346</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           378.115</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 11000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-11-43\n",
      "  done: false\n",
      "  episode_len_mean: 376.13793103448273\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2399999999999856\n",
      "  episode_reward_mean: -5.368275862068938\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 29\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7167169279522367\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013190728268114743\n",
      "          policy_loss: -0.007356235136588414\n",
      "          total_loss: -0.020772479308976067\n",
      "          vf_explained_var: 0.2638947665691376\n",
      "          vf_loss: 0.011112776365027659\n",
      "    num_agent_steps_sampled: 11000\n",
      "    num_agent_steps_trained: 11000\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 11000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.453125\n",
      "    ram_util_percent: 30.7875\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03774309378978763\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.363826608036334\n",
      "    mean_inference_ms: 1.6896440284188337\n",
      "    mean_raw_obs_processing_ms: 0.2084731248774903\n",
      "  time_since_restore: 284.00674629211426\n",
      "  time_this_iter_s: 22.54442286491394\n",
      "  time_total_s: 284.00674629211426\n",
      "  timers:\n",
      "    learn_throughput: 1221.098\n",
      "    learn_time_ms: 818.935\n",
      "    load_throughput: 40443.906\n",
      "    load_time_ms: 24.726\n",
      "    sample_throughput: 46.006\n",
      "    sample_time_ms: 21736.081\n",
      "    update_time_ms: 2.12\n",
      "  timestamp: 1634843503\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         284.007</td><td style=\"text-align: right;\">11000</td><td style=\"text-align: right;\">-5.36828</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           376.138</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-12-26\n",
      "  done: false\n",
      "  episode_len_mean: 372.4375\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2399999999999856\n",
      "  episode_reward_mean: -5.1806249999999725\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 32\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7508882549073963\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01112151950361359\n",
      "          policy_loss: 0.01755707586805026\n",
      "          total_loss: 0.0005017403927114275\n",
      "          vf_explained_var: -0.026238860562443733\n",
      "          vf_loss: 0.00822924212164556\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.91147540983606\n",
      "    ram_util_percent: 30.72622950819672\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03771558962403337\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.851529498344153\n",
      "    mean_inference_ms: 1.6887878373889724\n",
      "    mean_raw_obs_processing_ms: 0.35184002894033006\n",
      "  time_since_restore: 326.99193239212036\n",
      "  time_this_iter_s: 42.9851861000061\n",
      "  time_total_s: 326.99193239212036\n",
      "  timers:\n",
      "    learn_throughput: 1232.493\n",
      "    learn_time_ms: 811.363\n",
      "    load_throughput: 41052.127\n",
      "    load_time_ms: 24.359\n",
      "    sample_throughput: 42.003\n",
      "    sample_time_ms: 23807.68\n",
      "    update_time_ms: 1.928\n",
      "  timestamp: 1634843546\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         326.992</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-5.18062</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           372.438</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 13000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-12-50\n",
      "  done: false\n",
      "  episode_len_mean: 369.8285714285714\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2399999999999856\n",
      "  episode_reward_mean: -5.029714285714258\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 35\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7373414119084676\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014596404035734839\n",
      "          policy_loss: 0.02901813056733873\n",
      "          total_loss: 0.02188054554992252\n",
      "          vf_explained_var: 0.06825028359889984\n",
      "          vf_loss: 0.017316550846832493\n",
      "    num_agent_steps_sampled: 13000\n",
      "    num_agent_steps_trained: 13000\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 13000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.19428571428571\n",
      "    ram_util_percent: 31.354285714285716\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03769254818292175\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.416521717837274\n",
      "    mean_inference_ms: 1.688119252272457\n",
      "    mean_raw_obs_processing_ms: 0.4605945569045199\n",
      "  time_since_restore: 351.1043658256531\n",
      "  time_this_iter_s: 24.112433433532715\n",
      "  time_total_s: 351.1043658256531\n",
      "  timers:\n",
      "    learn_throughput: 1237.007\n",
      "    learn_time_ms: 808.403\n",
      "    load_throughput: 40357.126\n",
      "    load_time_ms: 24.779\n",
      "    sample_throughput: 41.347\n",
      "    sample_time_ms: 24185.645\n",
      "    update_time_ms: 1.908\n",
      "  timestamp: 1634843570\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         351.104</td><td style=\"text-align: right;\">13000</td><td style=\"text-align: right;\">-5.02971</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           369.829</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 14000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-13-13\n",
      "  done: false\n",
      "  episode_len_mean: 369.43243243243245\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2399999999999856\n",
      "  episode_reward_mean: -4.953783783783755\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 37\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7310907363891603\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011079240114774336\n",
      "          policy_loss: -0.0974456207619773\n",
      "          total_loss: -0.10927026917537054\n",
      "          vf_explained_var: 0.16954566538333893\n",
      "          vf_loss: 0.013270414553375708\n",
      "    num_agent_steps_sampled: 14000\n",
      "    num_agent_steps_trained: 14000\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.040625\n",
      "    ram_util_percent: 31.206249999999997\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037679214105673604\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.154363599155694\n",
      "    mean_inference_ms: 1.6877490725278421\n",
      "    mean_raw_obs_processing_ms: 0.5177594865831011\n",
      "  time_since_restore: 373.74112915992737\n",
      "  time_this_iter_s: 22.636763334274292\n",
      "  time_total_s: 373.74112915992737\n",
      "  timers:\n",
      "    learn_throughput: 1235.972\n",
      "    learn_time_ms: 809.08\n",
      "    load_throughput: 40371.499\n",
      "    load_time_ms: 24.77\n",
      "    sample_throughput: 41.075\n",
      "    sample_time_ms: 24345.619\n",
      "    update_time_ms: 1.925\n",
      "  timestamp: 1634843593\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         373.741</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\">-4.95378</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           369.432</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-13-37\n",
      "  done: false\n",
      "  episode_len_mean: 368.325\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2399999999999856\n",
      "  episode_reward_mean: -4.848249999999972\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 40\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.699818393919203\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013693973132897478\n",
      "          policy_loss: -0.162065526843071\n",
      "          total_loss: -0.17333738638295068\n",
      "          vf_explained_var: 0.44436919689178467\n",
      "          vf_loss: 0.01298752908801867\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_agent_steps_trained: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.59428571428572\n",
      "    ram_util_percent: 31.30285714285714\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037666976639917665\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.806110657951745\n",
      "    mean_inference_ms: 1.6873916541898997\n",
      "    mean_raw_obs_processing_ms: 0.5862843728165743\n",
      "  time_since_restore: 398.3316271305084\n",
      "  time_this_iter_s: 24.590497970581055\n",
      "  time_total_s: 398.3316271305084\n",
      "  timers:\n",
      "    learn_throughput: 1230.511\n",
      "    learn_time_ms: 812.67\n",
      "    load_throughput: 40450.107\n",
      "    load_time_ms: 24.722\n",
      "    sample_throughput: 40.741\n",
      "    sample_time_ms: 24545.598\n",
      "    update_time_ms: 1.946\n",
      "  timestamp: 1634843617\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         398.332</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">-4.84825</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           368.325</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-14-03\n",
      "  done: false\n",
      "  episode_len_mean: 365.0232558139535\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2399999999999856\n",
      "  episode_reward_mean: -4.733953488372065\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 43\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6675016005833943\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010699830811674834\n",
      "          policy_loss: -0.14293558282984628\n",
      "          total_loss: -0.15958038394649823\n",
      "          vf_explained_var: 0.7496100664138794\n",
      "          vf_loss: 0.0078902478937784\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.8864864864865\n",
      "    ram_util_percent: 31.327027027027025\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037654846120824904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.508444230659006\n",
      "    mean_inference_ms: 1.6871239308753552\n",
      "    mean_raw_obs_processing_ms: 0.6399712336495137\n",
      "  time_since_restore: 424.12532353401184\n",
      "  time_this_iter_s: 25.793696403503418\n",
      "  time_total_s: 424.12532353401184\n",
      "  timers:\n",
      "    learn_throughput: 1229.527\n",
      "    learn_time_ms: 813.321\n",
      "    load_throughput: 40317.48\n",
      "    load_time_ms: 24.803\n",
      "    sample_throughput: 40.156\n",
      "    sample_time_ms: 24903.087\n",
      "    update_time_ms: 1.947\n",
      "  timestamp: 1634843643\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         424.125</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-4.73395</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           365.023</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 17000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-14-27\n",
      "  done: false\n",
      "  episode_len_mean: 363.2391304347826\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2399999999999856\n",
      "  episode_reward_mean: -4.6454347826086675\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 46\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6294925848642987\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016362323851184878\n",
      "          policy_loss: -0.09309206696020232\n",
      "          total_loss: -0.11044523179945019\n",
      "          vf_explained_var: 0.8469886183738708\n",
      "          vf_loss: 0.005669295063449277\n",
      "    num_agent_steps_sampled: 17000\n",
      "    num_agent_steps_trained: 17000\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 17000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.932352941176475\n",
      "    ram_util_percent: 31.364705882352943\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03764492206948688\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.2433290376243\n",
      "    mean_inference_ms: 1.6868984946217898\n",
      "    mean_raw_obs_processing_ms: 0.6823208542843404\n",
      "  time_since_restore: 447.8722379207611\n",
      "  time_this_iter_s: 23.746914386749268\n",
      "  time_total_s: 447.8722379207611\n",
      "  timers:\n",
      "    learn_throughput: 1226.732\n",
      "    learn_time_ms: 815.174\n",
      "    load_throughput: 40505.461\n",
      "    load_time_ms: 24.688\n",
      "    sample_throughput: 40.115\n",
      "    sample_time_ms: 24928.604\n",
      "    update_time_ms: 1.953\n",
      "  timestamp: 1634843667\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         447.872</td><td style=\"text-align: right;\">17000</td><td style=\"text-align: right;\">-4.64543</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           363.239</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-14-53\n",
      "  done: false\n",
      "  episode_len_mean: 359.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2399999999999856\n",
      "  episode_reward_mean: -4.528999999999972\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 50\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6244274271859065\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012034335356499228\n",
      "          policy_loss: -0.05704166210359997\n",
      "          total_loss: -0.07224840819835662\n",
      "          vf_explained_var: 0.7505043745040894\n",
      "          vf_loss: 0.008630661361126436\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.105555555555554\n",
      "    ram_util_percent: 31.38333333333334\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03763200225488952\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.940073147559655\n",
      "    mean_inference_ms: 1.686641401244617\n",
      "    mean_raw_obs_processing_ms: 0.7262435156790252\n",
      "  time_since_restore: 473.35256814956665\n",
      "  time_this_iter_s: 25.480330228805542\n",
      "  time_total_s: 473.35256814956665\n",
      "  timers:\n",
      "    learn_throughput: 1223.62\n",
      "    learn_time_ms: 817.247\n",
      "    load_throughput: 40915.009\n",
      "    load_time_ms: 24.441\n",
      "    sample_throughput: 39.789\n",
      "    sample_time_ms: 25132.663\n",
      "    update_time_ms: 1.979\n",
      "  timestamp: 1634843693\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         473.353</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">  -4.529</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">             359.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 19000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-15-17\n",
      "  done: false\n",
      "  episode_len_mean: 357.47169811320754\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2399999999999856\n",
      "  episode_reward_mean: -4.453962264150916\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 53\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.611643279923333\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018196247434142974\n",
      "          policy_loss: -0.033908995075358286\n",
      "          total_loss: -0.051508699274725386\n",
      "          vf_explained_var: 0.8948479294776917\n",
      "          vf_loss: 0.004877480028274779\n",
      "    num_agent_steps_sampled: 19000\n",
      "    num_agent_steps_trained: 19000\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 19000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.82777777777777\n",
      "    ram_util_percent: 31.45555555555556\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037623700942065276\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.74161541996254\n",
      "    mean_inference_ms: 1.6864837194288498\n",
      "    mean_raw_obs_processing_ms: 0.7518076010093945\n",
      "  time_since_restore: 498.26593828201294\n",
      "  time_this_iter_s: 24.91337013244629\n",
      "  time_total_s: 498.26593828201294\n",
      "  timers:\n",
      "    learn_throughput: 1226.363\n",
      "    learn_time_ms: 815.419\n",
      "    load_throughput: 41247.159\n",
      "    load_time_ms: 24.244\n",
      "    sample_throughput: 39.609\n",
      "    sample_time_ms: 25247.063\n",
      "    update_time_ms: 1.969\n",
      "  timestamp: 1634843717\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 19\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         498.266</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">-4.45396</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           357.472</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-15-41\n",
      "  done: false\n",
      "  episode_len_mean: 357.9818181818182\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2399999999999856\n",
      "  episode_reward_mean: -4.427090909090881\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 55\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5799973752763536\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017503558094095636\n",
      "          policy_loss: -0.18049137517809868\n",
      "          total_loss: -0.19565252603756056\n",
      "          vf_explained_var: 0.7516571879386902\n",
      "          vf_loss: 0.007138114137988951\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.60303030303031\n",
      "    ram_util_percent: 31.515151515151516\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03761865513346346\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.617336183245317\n",
      "    mean_inference_ms: 1.6863865722559068\n",
      "    mean_raw_obs_processing_ms: 0.7655258487779261\n",
      "  time_since_restore: 521.3052976131439\n",
      "  time_this_iter_s: 23.03935933113098\n",
      "  time_total_s: 521.3052976131439\n",
      "  timers:\n",
      "    learn_throughput: 1227.286\n",
      "    learn_time_ms: 814.806\n",
      "    load_throughput: 41368.351\n",
      "    load_time_ms: 24.173\n",
      "    sample_throughput: 39.777\n",
      "    sample_time_ms: 25140.253\n",
      "    update_time_ms: 2.004\n",
      "  timestamp: 1634843741\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 20\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         521.305</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-4.42709</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           357.982</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-16-06\n",
      "  done: false\n",
      "  episode_len_mean: 357.01724137931035\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2399999999999856\n",
      "  episode_reward_mean: -4.373620689655144\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 58\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.558344923125373\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01770878237948765\n",
      "          policy_loss: -0.1374739609244797\n",
      "          total_loss: -0.15500223698715368\n",
      "          vf_explained_var: 0.8068132996559143\n",
      "          vf_loss: 0.004513414998331832\n",
      "    num_agent_steps_sampled: 21000\n",
      "    num_agent_steps_trained: 21000\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.70833333333334\n",
      "    ram_util_percent: 31.566666666666656\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037610894934911904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.447766574870425\n",
      "    mean_inference_ms: 1.6862562855988825\n",
      "    mean_raw_obs_processing_ms: 0.7820856695848164\n",
      "  time_since_restore: 546.7774157524109\n",
      "  time_this_iter_s: 25.472118139266968\n",
      "  time_total_s: 546.7774157524109\n",
      "  timers:\n",
      "    learn_throughput: 1226.081\n",
      "    learn_time_ms: 815.607\n",
      "    load_throughput: 40782.651\n",
      "    load_time_ms: 24.52\n",
      "    sample_throughput: 39.321\n",
      "    sample_time_ms: 25431.869\n",
      "    update_time_ms: 2.006\n",
      "  timestamp: 1634843766\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         546.777</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\">-4.37362</td><td style=\"text-align: right;\">               -2.24</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           357.017</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 22000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-16-49\n",
      "  done: false\n",
      "  episode_len_mean: 354.53225806451616\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -4.247741935483844\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 62\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.595480513572693\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013355610343311891\n",
      "          policy_loss: -0.10502943644920985\n",
      "          total_loss: -0.08625720971160465\n",
      "          vf_explained_var: 0.6515188217163086\n",
      "          vf_loss: 0.0420559110171679\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 22000\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.872580645161285\n",
      "    ram_util_percent: 31.608064516129033\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0376007731999407\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.251099118331563\n",
      "    mean_inference_ms: 1.686108941704573\n",
      "    mean_raw_obs_processing_ms: 0.8485372907180855\n",
      "  time_since_restore: 590.1917276382446\n",
      "  time_this_iter_s: 43.41431188583374\n",
      "  time_total_s: 590.1917276382446\n",
      "  timers:\n",
      "    learn_throughput: 1217.929\n",
      "    learn_time_ms: 821.066\n",
      "    load_throughput: 40393.038\n",
      "    load_time_ms: 24.757\n",
      "    sample_throughput: 39.263\n",
      "    sample_time_ms: 25469.08\n",
      "    update_time_ms: 2.011\n",
      "  timestamp: 1634843809\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 22\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         590.192</td><td style=\"text-align: right;\">22000</td><td style=\"text-align: right;\">-4.24774</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           354.532</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 23000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-17-13\n",
      "  done: false\n",
      "  episode_len_mean: 354.078125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -4.221249999999972\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 64\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5028707371817696\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009894294171934798\n",
      "          policy_loss: -0.07479286359416114\n",
      "          total_loss: -0.09234633867939314\n",
      "          vf_explained_var: 0.7045983076095581\n",
      "          vf_loss: 0.005496374322360174\n",
      "    num_agent_steps_sampled: 23000\n",
      "    num_agent_steps_trained: 23000\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 23000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.142424242424234\n",
      "    ram_util_percent: 31.760606060606058\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03759657339369499\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.159168892853785\n",
      "    mean_inference_ms: 1.6860538091372934\n",
      "    mean_raw_obs_processing_ms: 0.8764506468043824\n",
      "  time_since_restore: 613.3371751308441\n",
      "  time_this_iter_s: 23.145447492599487\n",
      "  time_total_s: 613.3371751308441\n",
      "  timers:\n",
      "    learn_throughput: 1214.798\n",
      "    learn_time_ms: 823.182\n",
      "    load_throughput: 40125.284\n",
      "    load_time_ms: 24.922\n",
      "    sample_throughput: 39.416\n",
      "    sample_time_ms: 25370.111\n",
      "    update_time_ms: 1.999\n",
      "  timestamp: 1634843833\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 23\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         613.337</td><td style=\"text-align: right;\">23000</td><td style=\"text-align: right;\">-4.22125</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           354.078</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-17-35\n",
      "  done: false\n",
      "  episode_len_mean: 355.13432835820896\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -4.289402985074599\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 67\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5435376061333552\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011915845363487589\n",
      "          policy_loss: 0.043648906383249496\n",
      "          total_loss: 0.23171312092906898\n",
      "          vf_explained_var: 0.2352427989244461\n",
      "          vf_loss: 0.21111642270245487\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.72727272727273\n",
      "    ram_util_percent: 31.71212121212121\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03759236668756985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.027212522898278\n",
      "    mean_inference_ms: 1.6859780799283826\n",
      "    mean_raw_obs_processing_ms: 0.9124053748736245\n",
      "  time_since_restore: 636.1672503948212\n",
      "  time_this_iter_s: 22.83007526397705\n",
      "  time_total_s: 636.1672503948212\n",
      "  timers:\n",
      "    learn_throughput: 1215.856\n",
      "    learn_time_ms: 822.466\n",
      "    load_throughput: 39929.971\n",
      "    load_time_ms: 25.044\n",
      "    sample_throughput: 39.386\n",
      "    sample_time_ms: 25390.042\n",
      "    update_time_ms: 2.0\n",
      "  timestamp: 1634843855\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         636.167</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\"> -4.2894</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           355.134</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 25000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-18-00\n",
      "  done: false\n",
      "  episode_len_mean: 354.8857142857143\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -4.255285714285686\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 70\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4670934677124023\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014185566255452765\n",
      "          policy_loss: -0.05962835806939337\n",
      "          total_loss: -0.071238055659665\n",
      "          vf_explained_var: 0.8122956156730652\n",
      "          vf_loss: 0.010224123920003573\n",
      "    num_agent_steps_sampled: 25000\n",
      "    num_agent_steps_trained: 25000\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 25000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.58285714285715\n",
      "    ram_util_percent: 31.65428571428571\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03758872760254441\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.905762959600906\n",
      "    mean_inference_ms: 1.68593393381374\n",
      "    mean_raw_obs_processing_ms: 0.9427993371980627\n",
      "  time_since_restore: 660.7751932144165\n",
      "  time_this_iter_s: 24.607942819595337\n",
      "  time_total_s: 660.7751932144165\n",
      "  timers:\n",
      "    learn_throughput: 1216.178\n",
      "    learn_time_ms: 822.248\n",
      "    load_throughput: 39751.01\n",
      "    load_time_ms: 25.157\n",
      "    sample_throughput: 39.383\n",
      "    sample_time_ms: 25391.901\n",
      "    update_time_ms: 1.986\n",
      "  timestamp: 1634843880\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 25\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         660.775</td><td style=\"text-align: right;\">25000</td><td style=\"text-align: right;\">-4.25529</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           354.886</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 26000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-18-24\n",
      "  done: false\n",
      "  episode_len_mean: 354.8767123287671\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -4.265890410958876\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 73\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.509858504931132\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010216447214029895\n",
      "          policy_loss: -0.06794710755348206\n",
      "          total_loss: -0.06402433953351444\n",
      "          vf_explained_var: 0.3501269817352295\n",
      "          vf_loss: 0.02697806451987061\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 26000\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.90294117647059\n",
      "    ram_util_percent: 31.650000000000006\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03758582524839279\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.79219416234812\n",
      "    mean_inference_ms: 1.6859016528957964\n",
      "    mean_raw_obs_processing_ms: 0.9684883902590474\n",
      "  time_since_restore: 684.486166715622\n",
      "  time_this_iter_s: 23.710973501205444\n",
      "  time_total_s: 684.486166715622\n",
      "  timers:\n",
      "    learn_throughput: 1213.287\n",
      "    learn_time_ms: 824.207\n",
      "    load_throughput: 39998.055\n",
      "    load_time_ms: 25.001\n",
      "    sample_throughput: 39.711\n",
      "    sample_time_ms: 25181.769\n",
      "    update_time_ms: 2.02\n",
      "  timestamp: 1634843904\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 26\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         684.486</td><td style=\"text-align: right;\">26000</td><td style=\"text-align: right;\">-4.26589</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           354.877</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-18-49\n",
      "  done: false\n",
      "  episode_len_mean: 354.9605263157895\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -4.2384210526315504\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 76\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4988795359929403\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011778774979066186\n",
      "          policy_loss: -0.03491289644605584\n",
      "          total_loss: -0.05223692013985581\n",
      "          vf_explained_var: 0.7818740606307983\n",
      "          vf_loss: 0.005309017434612744\n",
      "    num_agent_steps_sampled: 27000\n",
      "    num_agent_steps_trained: 27000\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 27000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.16\n",
      "    ram_util_percent: 31.62857142857143\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03758274759104119\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.687327290924983\n",
      "    mean_inference_ms: 1.685872352394993\n",
      "    mean_raw_obs_processing_ms: 0.9901893327501193\n",
      "  time_since_restore: 709.2871732711792\n",
      "  time_this_iter_s: 24.80100655555725\n",
      "  time_total_s: 709.2871732711792\n",
      "  timers:\n",
      "    learn_throughput: 1213.843\n",
      "    learn_time_ms: 823.83\n",
      "    load_throughput: 40067.558\n",
      "    load_time_ms: 24.958\n",
      "    sample_throughput: 39.545\n",
      "    sample_time_ms: 25287.586\n",
      "    update_time_ms: 2.021\n",
      "  timestamp: 1634843929\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 27\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         709.287</td><td style=\"text-align: right;\">27000</td><td style=\"text-align: right;\">-4.23842</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           354.961</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-19-12\n",
      "  done: false\n",
      "  episode_len_mean: 354.5128205128205\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -4.216282051282023\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 78\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4680516137017143\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018767075395102178\n",
      "          policy_loss: -0.13482291052738826\n",
      "          total_loss: -0.14986090146832995\n",
      "          vf_explained_var: 0.6926149725914001\n",
      "          vf_loss: 0.00588911201339215\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.80882352941177\n",
      "    ram_util_percent: 31.747058823529414\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03758092376432523\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.620580488845036\n",
      "    mean_inference_ms: 1.6858650098686674\n",
      "    mean_raw_obs_processing_ms: 1.0025274615529114\n",
      "  time_since_restore: 732.8130860328674\n",
      "  time_this_iter_s: 23.525912761688232\n",
      "  time_total_s: 732.8130860328674\n",
      "  timers:\n",
      "    learn_throughput: 1217.005\n",
      "    learn_time_ms: 821.689\n",
      "    load_throughput: 39942.443\n",
      "    load_time_ms: 25.036\n",
      "    sample_throughput: 39.85\n",
      "    sample_time_ms: 25094.21\n",
      "    update_time_ms: 2.007\n",
      "  timestamp: 1634843952\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 28\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         732.813</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">-4.21628</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           354.513</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 29000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-19-38\n",
      "  done: false\n",
      "  episode_len_mean: 353.6341463414634\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -4.174756097560947\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 82\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.359245038032532\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012098480223652943\n",
      "          policy_loss: -0.08322449955675337\n",
      "          total_loss: -0.09764446318149567\n",
      "          vf_explained_var: 0.7542027235031128\n",
      "          vf_loss: 0.006752788177173999\n",
      "    num_agent_steps_sampled: 29000\n",
      "    num_agent_steps_trained: 29000\n",
      "    num_steps_sampled: 29000\n",
      "    num_steps_trained: 29000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.18648648648649\n",
      "    ram_util_percent: 31.743243243243242\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037577763123518654\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.498920808100536\n",
      "    mean_inference_ms: 1.6858683254342712\n",
      "    mean_raw_obs_processing_ms: 1.0233477177232457\n",
      "  time_since_restore: 758.9991106987\n",
      "  time_this_iter_s: 26.18602466583252\n",
      "  time_total_s: 758.9991106987\n",
      "  timers:\n",
      "    learn_throughput: 1215.054\n",
      "    learn_time_ms: 823.009\n",
      "    load_throughput: 40047.779\n",
      "    load_time_ms: 24.97\n",
      "    sample_throughput: 39.651\n",
      "    sample_time_ms: 25220.198\n",
      "    update_time_ms: 2.017\n",
      "  timestamp: 1634843978\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 29\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         758.999</td><td style=\"text-align: right;\">29000</td><td style=\"text-align: right;\">-4.17476</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           353.634</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-20-03\n",
      "  done: false\n",
      "  episode_len_mean: 353.9047619047619\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -4.162261904761876\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 84\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.449101177851359\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016135060573323954\n",
      "          policy_loss: -0.0029557673467530145\n",
      "          total_loss: -0.021469786018133163\n",
      "          vf_explained_var: 0.9237102270126343\n",
      "          vf_loss: 0.002749980036686692\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.837142857142865\n",
      "    ram_util_percent: 31.797142857142852\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03757613446140614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.4420342617202\n",
      "    mean_inference_ms: 1.6858711805208846\n",
      "    mean_raw_obs_processing_ms: 1.0320456637866566\n",
      "  time_since_restore: 783.4571096897125\n",
      "  time_this_iter_s: 24.457998991012573\n",
      "  time_total_s: 783.4571096897125\n",
      "  timers:\n",
      "    learn_throughput: 1212.415\n",
      "    learn_time_ms: 824.8\n",
      "    load_throughput: 39924.991\n",
      "    load_time_ms: 25.047\n",
      "    sample_throughput: 39.432\n",
      "    sample_time_ms: 25360.197\n",
      "    update_time_ms: 2.008\n",
      "  timestamp: 1634844003\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 30\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         783.457</td><td style=\"text-align: right;\">30000</td><td style=\"text-align: right;\">-4.16226</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           353.905</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 31000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-20-27\n",
      "  done: false\n",
      "  episode_len_mean: 353.3333333333333\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -4.135057471264339\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 87\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.394706222746107\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012615684133375677\n",
      "          policy_loss: -0.08365947736634148\n",
      "          total_loss: -0.10075405554638969\n",
      "          vf_explained_var: 0.8578882217407227\n",
      "          vf_loss: 0.004329344477607972\n",
      "    num_agent_steps_sampled: 31000\n",
      "    num_agent_steps_trained: 31000\n",
      "    num_steps_sampled: 31000\n",
      "    num_steps_trained: 31000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.83142857142857\n",
      "    ram_util_percent: 31.837142857142855\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03757415364374312\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.361082520125557\n",
      "    mean_inference_ms: 1.6858793455314698\n",
      "    mean_raw_obs_processing_ms: 1.043057946089203\n",
      "  time_since_restore: 807.9506108760834\n",
      "  time_this_iter_s: 24.49350118637085\n",
      "  time_total_s: 807.9506108760834\n",
      "  timers:\n",
      "    learn_throughput: 1209.612\n",
      "    learn_time_ms: 826.711\n",
      "    load_throughput: 40121.293\n",
      "    load_time_ms: 24.924\n",
      "    sample_throughput: 39.587\n",
      "    sample_time_ms: 25260.547\n",
      "    update_time_ms: 2.008\n",
      "  timestamp: 1634844027\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 31\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         807.951</td><td style=\"text-align: right;\">31000</td><td style=\"text-align: right;\">-4.13506</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           353.333</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-21-08\n",
      "  done: false\n",
      "  episode_len_mean: 353.6666666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -4.118333333333305\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 90\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4541764656702676\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019330933148889944\n",
      "          policy_loss: 0.054230979540281825\n",
      "          total_loss: 0.037253228947520256\n",
      "          vf_explained_var: 0.8804128170013428\n",
      "          vf_loss: 0.003697824929582162\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.78947368421053\n",
      "    ram_util_percent: 31.801754385964912\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037572789204185414\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.28355562191128\n",
      "    mean_inference_ms: 1.685896270354924\n",
      "    mean_raw_obs_processing_ms: 1.0698973698202656\n",
      "  time_since_restore: 848.0259563922882\n",
      "  time_this_iter_s: 40.075345516204834\n",
      "  time_total_s: 848.0259563922882\n",
      "  timers:\n",
      "    learn_throughput: 1218.14\n",
      "    learn_time_ms: 820.924\n",
      "    load_throughput: 40514.812\n",
      "    load_time_ms: 24.682\n",
      "    sample_throughput: 40.108\n",
      "    sample_time_ms: 24932.675\n",
      "    update_time_ms: 2.003\n",
      "  timestamp: 1634844068\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 32\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         848.026</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">-4.11833</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           353.667</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 33000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-21-33\n",
      "  done: false\n",
      "  episode_len_mean: 354.2043010752688\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -4.10494623655911\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 93\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.429580497741699\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012962746333017637\n",
      "          policy_loss: 0.009590322151780128\n",
      "          total_loss: -0.007586473971605301\n",
      "          vf_explained_var: 0.7449100017547607\n",
      "          vf_loss: 0.0045264594664331526\n",
      "    num_agent_steps_sampled: 33000\n",
      "    num_agent_steps_trained: 33000\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 33000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.140540540540535\n",
      "    ram_util_percent: 31.57297297297297\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037571926753240774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.21184045069391\n",
      "    mean_inference_ms: 1.6859130372958746\n",
      "    mean_raw_obs_processing_ms: 1.093425474551136\n",
      "  time_since_restore: 873.7282431125641\n",
      "  time_this_iter_s: 25.70228672027588\n",
      "  time_total_s: 873.7282431125641\n",
      "  timers:\n",
      "    learn_throughput: 1219.75\n",
      "    learn_time_ms: 819.84\n",
      "    load_throughput: 40720.528\n",
      "    load_time_ms: 24.558\n",
      "    sample_throughput: 39.699\n",
      "    sample_time_ms: 25189.556\n",
      "    update_time_ms: 2.019\n",
      "  timestamp: 1634844093\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 33\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         873.728</td><td style=\"text-align: right;\">33000</td><td style=\"text-align: right;\">-4.10495</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           354.204</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 34000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-21-55\n",
      "  done: false\n",
      "  episode_len_mean: 354.8421052631579\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -4.099473684210497\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 95\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4045631700091894\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012189056063088089\n",
      "          policy_loss: -0.11888370878166622\n",
      "          total_loss: -0.1358645257850488\n",
      "          vf_explained_var: 0.8543549180030823\n",
      "          vf_loss: 0.004627000807603407\n",
      "    num_agent_steps_sampled: 34000\n",
      "    num_agent_steps_trained: 34000\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 34000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.8\n",
      "    ram_util_percent: 31.625\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03757122799346954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.164803205875696\n",
      "    mean_inference_ms: 1.6859224821885557\n",
      "    mean_raw_obs_processing_ms: 1.1072880215839436\n",
      "  time_since_restore: 895.7725691795349\n",
      "  time_this_iter_s: 22.044326066970825\n",
      "  time_total_s: 895.7725691795349\n",
      "  timers:\n",
      "    learn_throughput: 1218.618\n",
      "    learn_time_ms: 820.602\n",
      "    load_throughput: 40998.396\n",
      "    load_time_ms: 24.391\n",
      "    sample_throughput: 39.824\n",
      "    sample_time_ms: 25110.373\n",
      "    update_time_ms: 2.032\n",
      "  timestamp: 1634844115\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 34\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         895.773</td><td style=\"text-align: right;\">34000</td><td style=\"text-align: right;\">-4.09947</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           354.842</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 35000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-22-16\n",
      "  done: false\n",
      "  episode_len_mean: 356.53061224489795\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -4.099489795918338\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 98\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.409894824028015\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0145439934413892\n",
      "          policy_loss: -0.03977619724141227\n",
      "          total_loss: -0.05576403786738714\n",
      "          vf_explained_var: 0.7290173768997192\n",
      "          vf_loss: 0.0052023021633633305\n",
      "    num_agent_steps_sampled: 35000\n",
      "    num_agent_steps_trained: 35000\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 35000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.04137931034483\n",
      "    ram_util_percent: 31.613793103448277\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03757010532423613\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.09395832833261\n",
      "    mean_inference_ms: 1.6859320265872226\n",
      "    mean_raw_obs_processing_ms: 1.12567958161114\n",
      "  time_since_restore: 916.1062886714935\n",
      "  time_this_iter_s: 20.333719491958618\n",
      "  time_total_s: 916.1062886714935\n",
      "  timers:\n",
      "    learn_throughput: 1222.911\n",
      "    learn_time_ms: 817.721\n",
      "    load_throughput: 41003.526\n",
      "    load_time_ms: 24.388\n",
      "    sample_throughput: 40.509\n",
      "    sample_time_ms: 24685.841\n",
      "    update_time_ms: 2.029\n",
      "  timestamp: 1634844136\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 35\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         916.106</td><td style=\"text-align: right;\">35000</td><td style=\"text-align: right;\">-4.09949</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">           356.531</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-22-35\n",
      "  done: false\n",
      "  episode_len_mean: 358.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -4.10389999999997\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 100\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.393940464655558\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012290325742053227\n",
      "          policy_loss: 0.06612243155638377\n",
      "          total_loss: 0.04911440759897232\n",
      "          vf_explained_var: 0.38442447781562805\n",
      "          vf_loss: 0.0044733094588284275\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.47857142857142\n",
      "    ram_util_percent: 31.59642857142857\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03756941041564141\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.04640020277724\n",
      "    mean_inference_ms: 1.6859361387625058\n",
      "    mean_raw_obs_processing_ms: 1.1364833546771949\n",
      "  time_since_restore: 935.9034950733185\n",
      "  time_this_iter_s: 19.79720640182495\n",
      "  time_total_s: 935.9034950733185\n",
      "  timers:\n",
      "    learn_throughput: 1225.181\n",
      "    learn_time_ms: 816.206\n",
      "    load_throughput: 40673.814\n",
      "    load_time_ms: 24.586\n",
      "    sample_throughput: 41.159\n",
      "    sample_time_ms: 24295.837\n",
      "    update_time_ms: 1.992\n",
      "  timestamp: 1634844155\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 36\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         935.903</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\"> -4.1039</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">            358.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 37000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-22-55\n",
      "  done: false\n",
      "  episode_len_mean: 358.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -4.108299999999969\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 102\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3802956422170003\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016276576555315246\n",
      "          policy_loss: -0.15758133083581924\n",
      "          total_loss: -0.17333394951290554\n",
      "          vf_explained_var: 0.6834703683853149\n",
      "          vf_loss: 0.004795018858082282\n",
      "    num_agent_steps_sampled: 37000\n",
      "    num_agent_steps_trained: 37000\n",
      "    num_steps_sampled: 37000\n",
      "    num_steps_trained: 37000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.21071428571429\n",
      "    ram_util_percent: 31.585714285714285\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037547287531822765\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.373244418669827\n",
      "    mean_inference_ms: 1.6850824746630246\n",
      "    mean_raw_obs_processing_ms: 1.165142801106263\n",
      "  time_since_restore: 955.696380853653\n",
      "  time_this_iter_s: 19.792885780334473\n",
      "  time_total_s: 955.696380853653\n",
      "  timers:\n",
      "    learn_throughput: 1224.487\n",
      "    learn_time_ms: 816.669\n",
      "    load_throughput: 40683.479\n",
      "    load_time_ms: 24.58\n",
      "    sample_throughput: 42.026\n",
      "    sample_time_ms: 23794.518\n",
      "    update_time_ms: 2.008\n",
      "  timestamp: 1634844175\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 37\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         955.696</td><td style=\"text-align: right;\">37000</td><td style=\"text-align: right;\"> -4.1083</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">            358.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 38000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-23-15\n",
      "  done: false\n",
      "  episode_len_mean: 359.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -4.10289999999997\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 105\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3723293860753376\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01514372777350178\n",
      "          policy_loss: -0.01580399920543035\n",
      "          total_loss: -0.029669371743996937\n",
      "          vf_explained_var: 0.3043735921382904\n",
      "          vf_loss: 0.0068291767086419795\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 38000\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.96896551724138\n",
      "    ram_util_percent: 31.644827586206898\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03752574438485694\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.89725478432308\n",
      "    mean_inference_ms: 1.6843989717929473\n",
      "    mean_raw_obs_processing_ms: 1.2063934474043245\n",
      "  time_since_restore: 975.6247324943542\n",
      "  time_this_iter_s: 19.928351640701294\n",
      "  time_total_s: 975.6247324943542\n",
      "  timers:\n",
      "    learn_throughput: 1225.135\n",
      "    learn_time_ms: 816.237\n",
      "    load_throughput: 40871.711\n",
      "    load_time_ms: 24.467\n",
      "    sample_throughput: 42.671\n",
      "    sample_time_ms: 23435.308\n",
      "    update_time_ms: 2.003\n",
      "  timestamp: 1634844195\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 38\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         975.625</td><td style=\"text-align: right;\">38000</td><td style=\"text-align: right;\"> -4.1029</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">            359.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 39000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-23-35\n",
      "  done: false\n",
      "  episode_len_mean: 359.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -4.102699999999969\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 107\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.356879120402866\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012658020611351913\n",
      "          policy_loss: -0.05009599228700002\n",
      "          total_loss: -0.06352692693471909\n",
      "          vf_explained_var: 0.5957118272781372\n",
      "          vf_loss: 0.007606246171053499\n",
      "    num_agent_steps_sampled: 39000\n",
      "    num_agent_steps_trained: 39000\n",
      "    num_steps_sampled: 39000\n",
      "    num_steps_trained: 39000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.66785714285714\n",
      "    ram_util_percent: 31.696428571428573\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03751082457694072\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.709385858479227\n",
      "    mean_inference_ms: 1.6840906267582567\n",
      "    mean_raw_obs_processing_ms: 1.2333563723946026\n",
      "  time_since_restore: 995.116819858551\n",
      "  time_this_iter_s: 19.492087364196777\n",
      "  time_total_s: 995.116819858551\n",
      "  timers:\n",
      "    learn_throughput: 1227.614\n",
      "    learn_time_ms: 814.589\n",
      "    load_throughput: 40727.448\n",
      "    load_time_ms: 24.553\n",
      "    sample_throughput: 43.922\n",
      "    sample_time_ms: 22767.484\n",
      "    update_time_ms: 1.998\n",
      "  timestamp: 1634844215\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 39\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         995.117</td><td style=\"text-align: right;\">39000</td><td style=\"text-align: right;\"> -4.1027</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">            359.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-23-54\n",
      "  done: false\n",
      "  episode_len_mean: 359.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -4.10599999999997\n",
      "  episode_reward_min: -30.350000000000193\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 109\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.326289510726929\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012081803115909897\n",
      "          policy_loss: -0.1178959290186564\n",
      "          total_loss: -0.130240402619044\n",
      "          vf_explained_var: 0.3549363613128662\n",
      "          vf_loss: 0.00850205537216324\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.433333333333344\n",
      "    ram_util_percent: 31.755555555555556\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03750443029246592\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.581468539301294\n",
      "    mean_inference_ms: 1.6839780571492455\n",
      "    mean_raw_obs_processing_ms: 1.259731421133571\n",
      "  time_since_restore: 1014.3855929374695\n",
      "  time_this_iter_s: 19.268773078918457\n",
      "  time_total_s: 1014.3855929374695\n",
      "  timers:\n",
      "    learn_throughput: 1231.005\n",
      "    learn_time_ms: 812.344\n",
      "    load_throughput: 41155.771\n",
      "    load_time_ms: 24.298\n",
      "    sample_throughput: 44.942\n",
      "    sample_time_ms: 22251.047\n",
      "    update_time_ms: 2.019\n",
      "  timestamp: 1634844234\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 40\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         1014.39</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  -4.106</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -30.35</td><td style=\"text-align: right;\">            359.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 41000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-24-14\n",
      "  done: false\n",
      "  episode_len_mean: 360.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -3.8540999999999666\n",
      "  episode_reward_min: -11.289999999999973\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 112\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.307250973913405\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010911622580891962\n",
      "          policy_loss: 0.023840767476293774\n",
      "          total_loss: 0.011434699098269144\n",
      "          vf_explained_var: 0.5740939974784851\n",
      "          vf_loss: 0.008484115827983866\n",
      "    num_agent_steps_sampled: 41000\n",
      "    num_agent_steps_trained: 41000\n",
      "    num_steps_sampled: 41000\n",
      "    num_steps_trained: 41000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.15714285714285\n",
      "    ram_util_percent: 31.739285714285717\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03750005082708781\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.43366883862536\n",
      "    mean_inference_ms: 1.683953291782207\n",
      "    mean_raw_obs_processing_ms: 1.2981770058652498\n",
      "  time_since_restore: 1034.2477116584778\n",
      "  time_this_iter_s: 19.8621187210083\n",
      "  time_total_s: 1034.2477116584778\n",
      "  timers:\n",
      "    learn_throughput: 1232.575\n",
      "    learn_time_ms: 811.31\n",
      "    load_throughput: 41569.126\n",
      "    load_time_ms: 24.056\n",
      "    sample_throughput: 45.894\n",
      "    sample_time_ms: 21789.17\n",
      "    update_time_ms: 2.022\n",
      "  timestamp: 1634844254\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 41000\n",
      "  training_iteration: 41\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         1034.25</td><td style=\"text-align: right;\">41000</td><td style=\"text-align: right;\"> -3.8541</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -11.29</td><td style=\"text-align: right;\">            360.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 42000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-24-35\n",
      "  done: false\n",
      "  episode_len_mean: 361.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -3.862099999999967\n",
      "  episode_reward_min: -11.289999999999973\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 114\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.319406541188558\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012451853551461294\n",
      "          policy_loss: -0.11745893723434872\n",
      "          total_loss: -0.12949712740050423\n",
      "          vf_explained_var: 0.6026422381401062\n",
      "          vf_loss: 0.008665504421676613\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.26451612903226\n",
      "    ram_util_percent: 31.803225806451607\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03750022015798795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.35589176227988\n",
      "    mean_inference_ms: 1.6840132551084306\n",
      "    mean_raw_obs_processing_ms: 1.3231702237905967\n",
      "  time_since_restore: 1055.4148287773132\n",
      "  time_this_iter_s: 21.16711711883545\n",
      "  time_total_s: 1055.4148287773132\n",
      "  timers:\n",
      "    learn_throughput: 1233.418\n",
      "    learn_time_ms: 810.755\n",
      "    load_throughput: 41743.049\n",
      "    load_time_ms: 23.956\n",
      "    sample_throughput: 50.254\n",
      "    sample_time_ms: 19898.982\n",
      "    update_time_ms: 2.019\n",
      "  timestamp: 1634844275\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 42\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         1055.41</td><td style=\"text-align: right;\">42000</td><td style=\"text-align: right;\"> -3.8621</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -11.29</td><td style=\"text-align: right;\">            361.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 43000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-24-56\n",
      "  done: false\n",
      "  episode_len_mean: 362.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -3.8110999999999664\n",
      "  episode_reward_min: -11.289999999999973\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 117\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.271255368656582\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008666866915657954\n",
      "          policy_loss: 0.05737675055861473\n",
      "          total_loss: 0.04537973205248515\n",
      "          vf_explained_var: 0.4399288594722748\n",
      "          vf_loss: 0.008982159820799198\n",
      "    num_agent_steps_sampled: 43000\n",
      "    num_agent_steps_trained: 43000\n",
      "    num_steps_sampled: 43000\n",
      "    num_steps_trained: 43000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.262068965517244\n",
      "    ram_util_percent: 31.806896551724133\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03750000030412387\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.249134629072593\n",
      "    mean_inference_ms: 1.6840963252243488\n",
      "    mean_raw_obs_processing_ms: 1.3597204905549356\n",
      "  time_since_restore: 1075.8819584846497\n",
      "  time_this_iter_s: 20.467129707336426\n",
      "  time_total_s: 1075.8819584846497\n",
      "  timers:\n",
      "    learn_throughput: 1232.028\n",
      "    learn_time_ms: 811.67\n",
      "    load_throughput: 41752.315\n",
      "    load_time_ms: 23.951\n",
      "    sample_throughput: 51.614\n",
      "    sample_time_ms: 19374.542\n",
      "    update_time_ms: 2.018\n",
      "  timestamp: 1634844296\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 43000\n",
      "  training_iteration: 43\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         1075.88</td><td style=\"text-align: right;\">43000</td><td style=\"text-align: right;\"> -3.8111</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -11.29</td><td style=\"text-align: right;\">             362.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-25-16\n",
      "  done: false\n",
      "  episode_len_mean: 363.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -3.8217999999999663\n",
      "  episode_reward_min: -11.289999999999973\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 119\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.257058220439487\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01299609386735039\n",
      "          policy_loss: -0.11695978724294238\n",
      "          total_loss: -0.12725812097390493\n",
      "          vf_explained_var: 0.49232977628707886\n",
      "          vf_loss: 0.009673030470083985\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.179310344827584\n",
      "    ram_util_percent: 31.82068965517241\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03750037417146161\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.18513712504766\n",
      "    mean_inference_ms: 1.6841604904027594\n",
      "    mean_raw_obs_processing_ms: 1.3835339025841296\n",
      "  time_since_restore: 1096.2827141284943\n",
      "  time_this_iter_s: 20.400755643844604\n",
      "  time_total_s: 1096.2827141284943\n",
      "  timers:\n",
      "    learn_throughput: 1231.725\n",
      "    learn_time_ms: 811.87\n",
      "    load_throughput: 42152.877\n",
      "    load_time_ms: 23.723\n",
      "    sample_throughput: 52.056\n",
      "    sample_time_ms: 19210.197\n",
      "    update_time_ms: 2.01\n",
      "  timestamp: 1634844316\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 44\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         1096.28</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\"> -3.8218</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -11.29</td><td style=\"text-align: right;\">            363.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 45000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-25-56\n",
      "  done: false\n",
      "  episode_len_mean: 363.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -3.7722999999999667\n",
      "  episode_reward_min: -11.289999999999973\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 122\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1923533545600042\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01211721544751666\n",
      "          policy_loss: -0.07909980416297913\n",
      "          total_loss: -0.08413823520143827\n",
      "          vf_explained_var: 0.32320061326026917\n",
      "          vf_loss: 0.014461655666430791\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_agent_steps_trained: 45000\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.94912280701755\n",
      "    ram_util_percent: 31.785964912280704\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03750340677216809\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.102148729490665\n",
      "    mean_inference_ms: 1.6843025087319656\n",
      "    mean_raw_obs_processing_ms: 1.4297336245755594\n",
      "  time_since_restore: 1136.3495781421661\n",
      "  time_this_iter_s: 40.066864013671875\n",
      "  time_total_s: 1136.3495781421661\n",
      "  timers:\n",
      "    learn_throughput: 1226.505\n",
      "    learn_time_ms: 815.325\n",
      "    load_throughput: 42973.237\n",
      "    load_time_ms: 23.27\n",
      "    sample_throughput: 47.213\n",
      "    sample_time_ms: 21180.501\n",
      "    update_time_ms: 2.019\n",
      "  timestamp: 1634844356\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 45\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         1136.35</td><td style=\"text-align: right;\">45000</td><td style=\"text-align: right;\"> -3.7723</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -11.29</td><td style=\"text-align: right;\">            363.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 46000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-26-18\n",
      "  done: false\n",
      "  episode_len_mean: 364.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -3.6990999999999667\n",
      "  episode_reward_min: -9.639999999999967\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 125\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1948877493540446\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010199339058834678\n",
      "          policy_loss: 0.06583171412348747\n",
      "          total_loss: 0.05624578007393413\n",
      "          vf_explained_var: 0.5068028569221497\n",
      "          vf_loss: 0.010323072144658201\n",
      "    num_agent_steps_sampled: 46000\n",
      "    num_agent_steps_trained: 46000\n",
      "    num_steps_sampled: 46000\n",
      "    num_steps_trained: 46000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.60625\n",
      "    ram_util_percent: 31.831249999999997\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03750623778728756\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.025846249052833\n",
      "    mean_inference_ms: 1.6844458693843252\n",
      "    mean_raw_obs_processing_ms: 1.4748874042454827\n",
      "  time_since_restore: 1158.3780074119568\n",
      "  time_this_iter_s: 22.02842926979065\n",
      "  time_total_s: 1158.3780074119568\n",
      "  timers:\n",
      "    learn_throughput: 1223.018\n",
      "    learn_time_ms: 817.649\n",
      "    load_throughput: 43411.836\n",
      "    load_time_ms: 23.035\n",
      "    sample_throughput: 46.726\n",
      "    sample_time_ms: 21401.528\n",
      "    update_time_ms: 2.028\n",
      "  timestamp: 1634844378\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 46\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         1158.38</td><td style=\"text-align: right;\">46000</td><td style=\"text-align: right;\"> -3.6991</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">               -9.64</td><td style=\"text-align: right;\">            364.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 47000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-26-41\n",
      "  done: false\n",
      "  episode_len_mean: 364.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -3.6976999999999665\n",
      "  episode_reward_min: -9.639999999999967\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 128\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1475623899035985\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008415855648194966\n",
      "          policy_loss: 0.012890707453091938\n",
      "          total_loss: 0.004590056836605072\n",
      "          vf_explained_var: 0.44939982891082764\n",
      "          vf_loss: 0.01149180284830638\n",
      "    num_agent_steps_sampled: 47000\n",
      "    num_agent_steps_trained: 47000\n",
      "    num_steps_sampled: 47000\n",
      "    num_steps_trained: 47000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.78125\n",
      "    ram_util_percent: 31.65\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03750933879134661\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.957599751443837\n",
      "    mean_inference_ms: 1.6845979640628224\n",
      "    mean_raw_obs_processing_ms: 1.519061591444919\n",
      "  time_since_restore: 1180.9326977729797\n",
      "  time_this_iter_s: 22.55469036102295\n",
      "  time_total_s: 1180.9326977729797\n",
      "  timers:\n",
      "    learn_throughput: 1225.018\n",
      "    learn_time_ms: 816.315\n",
      "    load_throughput: 43373.722\n",
      "    load_time_ms: 23.055\n",
      "    sample_throughput: 46.127\n",
      "    sample_time_ms: 21679.071\n",
      "    update_time_ms: 2.013\n",
      "  timestamp: 1634844401\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47000\n",
      "  training_iteration: 47\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         1180.93</td><td style=\"text-align: right;\">47000</td><td style=\"text-align: right;\"> -3.6977</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">               -9.64</td><td style=\"text-align: right;\">            364.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-27-04\n",
      "  done: false\n",
      "  episode_len_mean: 364.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -3.7002999999999666\n",
      "  episode_reward_min: -9.639999999999967\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 130\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1225398778915405\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01001393439847755\n",
      "          policy_loss: -0.15209758977095286\n",
      "          total_loss: -0.1598799863623248\n",
      "          vf_explained_var: 0.42644399404525757\n",
      "          vf_loss: 0.011440210534621858\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.66969696969697\n",
      "    ram_util_percent: 31.575757575757574\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037511295621589\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.914884904449362\n",
      "    mean_inference_ms: 1.6846918291178667\n",
      "    mean_raw_obs_processing_ms: 1.532637227070789\n",
      "  time_since_restore: 1203.7540233135223\n",
      "  time_this_iter_s: 22.821325540542603\n",
      "  time_total_s: 1203.7540233135223\n",
      "  timers:\n",
      "    learn_throughput: 1223.184\n",
      "    learn_time_ms: 817.539\n",
      "    load_throughput: 42910.193\n",
      "    load_time_ms: 23.304\n",
      "    sample_throughput: 45.523\n",
      "    sample_time_ms: 21966.886\n",
      "    update_time_ms: 2.028\n",
      "  timestamp: 1634844424\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 48\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         1203.75</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> -3.7003</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">               -9.64</td><td style=\"text-align: right;\">            364.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 49000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-27-25\n",
      "  done: false\n",
      "  episode_len_mean: 364.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -3.7053999999999663\n",
      "  episode_reward_min: -9.639999999999967\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 133\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.992095586988661\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010923694023524928\n",
      "          policy_loss: -0.1028696283698082\n",
      "          total_loss: -0.10825804935561287\n",
      "          vf_explained_var: 0.41333287954330444\n",
      "          vf_loss: 0.012347789926247464\n",
      "    num_agent_steps_sampled: 49000\n",
      "    num_agent_steps_trained: 49000\n",
      "    num_steps_sampled: 49000\n",
      "    num_steps_trained: 49000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.43870967741936\n",
      "    ram_util_percent: 31.59677419354839\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03751363144841609\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.852340016521875\n",
      "    mean_inference_ms: 1.6848097799944606\n",
      "    mean_raw_obs_processing_ms: 1.5304314134310237\n",
      "  time_since_restore: 1225.5601971149445\n",
      "  time_this_iter_s: 21.80617380142212\n",
      "  time_total_s: 1225.5601971149445\n",
      "  timers:\n",
      "    learn_throughput: 1220.03\n",
      "    learn_time_ms: 819.652\n",
      "    load_throughput: 42647.231\n",
      "    load_time_ms: 23.448\n",
      "    sample_throughput: 45.053\n",
      "    sample_time_ms: 22196.03\n",
      "    update_time_ms: 2.028\n",
      "  timestamp: 1634844445\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49000\n",
      "  training_iteration: 49\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         1225.56</td><td style=\"text-align: right;\">49000</td><td style=\"text-align: right;\"> -3.7054</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">               -9.64</td><td style=\"text-align: right;\">            364.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 50000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-27-48\n",
      "  done: false\n",
      "  episode_len_mean: 365.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -3.7074999999999663\n",
      "  episode_reward_min: -9.639999999999967\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 136\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.108651857905918\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012297173409767579\n",
      "          policy_loss: 0.010052080949147542\n",
      "          total_loss: 0.0023709690405262843\n",
      "          vf_explained_var: 0.33697646856307983\n",
      "          vf_loss: 0.010945972502748999\n",
      "    num_agent_steps_sampled: 50000\n",
      "    num_agent_steps_trained: 50000\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.325806451612905\n",
      "    ram_util_percent: 31.600000000000005\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0375159200431589\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.79350082479245\n",
      "    mean_inference_ms: 1.6849094828540023\n",
      "    mean_raw_obs_processing_ms: 1.5307494136174449\n",
      "  time_since_restore: 1247.7194910049438\n",
      "  time_this_iter_s: 22.15929388999939\n",
      "  time_total_s: 1247.7194910049438\n",
      "  timers:\n",
      "    learn_throughput: 1221.68\n",
      "    learn_time_ms: 818.545\n",
      "    load_throughput: 42022.21\n",
      "    load_time_ms: 23.797\n",
      "    sample_throughput: 44.472\n",
      "    sample_time_ms: 22485.879\n",
      "    update_time_ms: 1.992\n",
      "  timestamp: 1634844468\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 50\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         1247.72</td><td style=\"text-align: right;\">50000</td><td style=\"text-align: right;\"> -3.7075</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">               -9.64</td><td style=\"text-align: right;\">               365</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 51000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-28-09\n",
      "  done: false\n",
      "  episode_len_mean: 365.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -3.7100999999999664\n",
      "  episode_reward_min: -9.639999999999967\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 139\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0598877986272175\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012585803507369498\n",
      "          policy_loss: 0.023948162297407784\n",
      "          total_loss: 0.01760787078075939\n",
      "          vf_explained_var: 0.27037858963012695\n",
      "          vf_loss: 0.011741425221165022\n",
      "    num_agent_steps_sampled: 51000\n",
      "    num_agent_steps_trained: 51000\n",
      "    num_steps_sampled: 51000\n",
      "    num_steps_trained: 51000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.19333333333332\n",
      "    ram_util_percent: 31.67666666666667\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03751661601730644\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.73846709252441\n",
      "    mean_inference_ms: 1.68496603075616\n",
      "    mean_raw_obs_processing_ms: 1.5340427817243671\n",
      "  time_since_restore: 1268.7304725646973\n",
      "  time_this_iter_s: 21.010981559753418\n",
      "  time_total_s: 1268.7304725646973\n",
      "  timers:\n",
      "    learn_throughput: 1222.257\n",
      "    learn_time_ms: 818.159\n",
      "    load_throughput: 41386.842\n",
      "    load_time_ms: 24.162\n",
      "    sample_throughput: 44.246\n",
      "    sample_time_ms: 22600.769\n",
      "    update_time_ms: 1.985\n",
      "  timestamp: 1634844489\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 51000\n",
      "  training_iteration: 51\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         1268.73</td><td style=\"text-align: right;\">51000</td><td style=\"text-align: right;\"> -3.7101</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">               -9.64</td><td style=\"text-align: right;\">            365.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-28-30\n",
      "  done: false\n",
      "  episode_len_mean: 366.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -3.719299999999966\n",
      "  episode_reward_min: -9.639999999999967\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 141\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0100540227360195\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01123721669620544\n",
      "          policy_loss: -0.11726658013131884\n",
      "          total_loss: -0.1242384456925922\n",
      "          vf_explained_var: 0.16120211780071259\n",
      "          vf_loss: 0.010881230581112\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.16333333333333\n",
      "    ram_util_percent: 31.710000000000004\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037516765544992496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.700937988610725\n",
      "    mean_inference_ms: 1.6849847687125914\n",
      "    mean_raw_obs_processing_ms: 1.5370417029271648\n",
      "  time_since_restore: 1289.8003458976746\n",
      "  time_this_iter_s: 21.069873332977295\n",
      "  time_total_s: 1289.8003458976746\n",
      "  timers:\n",
      "    learn_throughput: 1223.944\n",
      "    learn_time_ms: 817.031\n",
      "    load_throughput: 41299.796\n",
      "    load_time_ms: 24.213\n",
      "    sample_throughput: 44.263\n",
      "    sample_time_ms: 22592.146\n",
      "    update_time_ms: 1.978\n",
      "  timestamp: 1634844510\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 52\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">          1289.8</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\"> -3.7193</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">               -9.64</td><td style=\"text-align: right;\">            366.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 53000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-28-52\n",
      "  done: false\n",
      "  episode_len_mean: 367.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -3.733899999999965\n",
      "  episode_reward_min: -9.639999999999967\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 144\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9680156813727485\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009928170891450818\n",
      "          policy_loss: -0.00345260136657291\n",
      "          total_loss: -0.009542571587695016\n",
      "          vf_explained_var: 0.2288280874490738\n",
      "          vf_loss: 0.011604552591840426\n",
      "    num_agent_steps_sampled: 53000\n",
      "    num_agent_steps_trained: 53000\n",
      "    num_steps_sampled: 53000\n",
      "    num_steps_trained: 53000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.01875\n",
      "    ram_util_percent: 31.746875000000003\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0375173456796404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.64440220498904\n",
      "    mean_inference_ms: 1.684999522167143\n",
      "    mean_raw_obs_processing_ms: 1.5425902225565338\n",
      "  time_since_restore: 1311.8846271038055\n",
      "  time_this_iter_s: 22.08428120613098\n",
      "  time_total_s: 1311.8846271038055\n",
      "  timers:\n",
      "    learn_throughput: 1227.104\n",
      "    learn_time_ms: 814.927\n",
      "    load_throughput: 41017.12\n",
      "    load_time_ms: 24.38\n",
      "    sample_throughput: 43.945\n",
      "    sample_time_ms: 22755.806\n",
      "    update_time_ms: 1.969\n",
      "  timestamp: 1634844532\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 53000\n",
      "  training_iteration: 53\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         1311.88</td><td style=\"text-align: right;\">53000</td><td style=\"text-align: right;\"> -3.7339</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">               -9.64</td><td style=\"text-align: right;\">            367.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 54000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-29-15\n",
      "  done: false\n",
      "  episode_len_mean: 367.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -3.735899999999965\n",
      "  episode_reward_min: -9.639999999999967\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 147\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9882067388958402\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008255381422357072\n",
      "          policy_loss: 0.01999501221709781\n",
      "          total_loss: 0.013708692044019699\n",
      "          vf_explained_var: -0.10807877779006958\n",
      "          vf_loss: 0.011944672148416026\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 54000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.029411764705884\n",
      "    ram_util_percent: 31.791176470588237\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03751783367135692\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.589792815980374\n",
      "    mean_inference_ms: 1.6850101679133485\n",
      "    mean_raw_obs_processing_ms: 1.549325869604723\n",
      "  time_since_restore: 1335.4960525035858\n",
      "  time_this_iter_s: 23.611425399780273\n",
      "  time_total_s: 1335.4960525035858\n",
      "  timers:\n",
      "    learn_throughput: 1228.988\n",
      "    learn_time_ms: 813.677\n",
      "    load_throughput: 40646.38\n",
      "    load_time_ms: 24.602\n",
      "    sample_throughput: 43.331\n",
      "    sample_time_ms: 23077.925\n",
      "    update_time_ms: 1.96\n",
      "  timestamp: 1634844555\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 54\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">          1335.5</td><td style=\"text-align: right;\">54000</td><td style=\"text-align: right;\"> -3.7359</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">               -9.64</td><td style=\"text-align: right;\">            367.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 55000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-29-56\n",
      "  done: false\n",
      "  episode_len_mean: 367.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -3.734499999999966\n",
      "  episode_reward_min: -9.639999999999967\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 150\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9181897322336832\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01480288802160256\n",
      "          policy_loss: -0.10791927932037247\n",
      "          total_loss: -0.10893536797828145\n",
      "          vf_explained_var: 0.23004089295864105\n",
      "          vf_loss: 0.015205232002254989\n",
      "    num_agent_steps_sampled: 55000\n",
      "    num_agent_steps_trained: 55000\n",
      "    num_steps_sampled: 55000\n",
      "    num_steps_trained: 55000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.863793103448266\n",
      "    ram_util_percent: 31.822413793103454\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037518529089760756\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.53561808055719\n",
      "    mean_inference_ms: 1.6850206372158678\n",
      "    mean_raw_obs_processing_ms: 1.5654978417264802\n",
      "  time_since_restore: 1376.5440738201141\n",
      "  time_this_iter_s: 41.04802131652832\n",
      "  time_total_s: 1376.5440738201141\n",
      "  timers:\n",
      "    learn_throughput: 1230.16\n",
      "    learn_time_ms: 812.902\n",
      "    load_throughput: 39641.794\n",
      "    load_time_ms: 25.226\n",
      "    sample_throughput: 43.148\n",
      "    sample_time_ms: 23176.174\n",
      "    update_time_ms: 1.973\n",
      "  timestamp: 1634844596\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55000\n",
      "  training_iteration: 55\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         1376.54</td><td style=\"text-align: right;\">55000</td><td style=\"text-align: right;\"> -3.7345</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">               -9.64</td><td style=\"text-align: right;\">             367.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-30-23\n",
      "  done: false\n",
      "  episode_len_mean: 367.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -3.7330999999999652\n",
      "  episode_reward_min: -9.639999999999967\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 153\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9338653829362658\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012931630082598507\n",
      "          policy_loss: -0.14561938982870842\n",
      "          total_loss: -0.1486085266702705\n",
      "          vf_explained_var: 0.3794640302658081\n",
      "          vf_loss: 0.013763191054264705\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.53157894736842\n",
      "    ram_util_percent: 31.913157894736848\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03751906378310132\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.483703428349063\n",
      "    mean_inference_ms: 1.6850231293085502\n",
      "    mean_raw_obs_processing_ms: 1.5824405079460957\n",
      "  time_since_restore: 1403.1902792453766\n",
      "  time_this_iter_s: 26.64620542526245\n",
      "  time_total_s: 1403.1902792453766\n",
      "  timers:\n",
      "    learn_throughput: 1231.578\n",
      "    learn_time_ms: 811.966\n",
      "    load_throughput: 39449.292\n",
      "    load_time_ms: 25.349\n",
      "    sample_throughput: 42.303\n",
      "    sample_time_ms: 23638.763\n",
      "    update_time_ms: 1.972\n",
      "  timestamp: 1634844623\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 56\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         1403.19</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\"> -3.7331</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">               -9.64</td><td style=\"text-align: right;\">            367.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 57000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-30-47\n",
      "  done: false\n",
      "  episode_len_mean: 366.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -3.722499999999967\n",
      "  episode_reward_min: -9.639999999999967\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 156\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9845070507791307\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013980680032921317\n",
      "          policy_loss: -0.11591532097922431\n",
      "          total_loss: -0.12359949077169101\n",
      "          vf_explained_var: 0.715305745601654\n",
      "          vf_loss: 0.009364767015601199\n",
      "    num_agent_steps_sampled: 57000\n",
      "    num_agent_steps_trained: 57000\n",
      "    num_steps_sampled: 57000\n",
      "    num_steps_trained: 57000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.05714285714285\n",
      "    ram_util_percent: 31.911428571428573\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03751972174194071\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.43535159054402\n",
      "    mean_inference_ms: 1.685026124833176\n",
      "    mean_raw_obs_processing_ms: 1.60048113204475\n",
      "  time_since_restore: 1427.503700017929\n",
      "  time_this_iter_s: 24.31342077255249\n",
      "  time_total_s: 1427.503700017929\n",
      "  timers:\n",
      "    learn_throughput: 1231.095\n",
      "    learn_time_ms: 812.285\n",
      "    load_throughput: 39265.046\n",
      "    load_time_ms: 25.468\n",
      "    sample_throughput: 41.992\n",
      "    sample_time_ms: 23814.201\n",
      "    update_time_ms: 1.958\n",
      "  timestamp: 1634844647\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 57000\n",
      "  training_iteration: 57\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">          1427.5</td><td style=\"text-align: right;\">57000</td><td style=\"text-align: right;\"> -3.7225</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">               -9.64</td><td style=\"text-align: right;\">             366.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 58000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-31-11\n",
      "  done: false\n",
      "  episode_len_mean: 366.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3099999999999791\n",
      "  episode_reward_mean: -3.722899999999966\n",
      "  episode_reward_min: -9.639999999999967\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 160\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8766119162241617\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007463995670362719\n",
      "          policy_loss: -0.04342284256385432\n",
      "          total_loss: -0.052431420029865373\n",
      "          vf_explained_var: 0.6617512702941895\n",
      "          vf_loss: 0.008264742313056357\n",
      "    num_agent_steps_sampled: 58000\n",
      "    num_agent_steps_trained: 58000\n",
      "    num_steps_sampled: 58000\n",
      "    num_steps_trained: 58000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.52058823529412\n",
      "    ram_util_percent: 31.84705882352941\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037521515973049764\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.36955271599769\n",
      "    mean_inference_ms: 1.685030410852217\n",
      "    mean_raw_obs_processing_ms: 1.6101497102493498\n",
      "  time_since_restore: 1451.4954237937927\n",
      "  time_this_iter_s: 23.991723775863647\n",
      "  time_total_s: 1451.4954237937927\n",
      "  timers:\n",
      "    learn_throughput: 1230.149\n",
      "    learn_time_ms: 812.909\n",
      "    load_throughput: 39567.898\n",
      "    load_time_ms: 25.273\n",
      "    sample_throughput: 41.787\n",
      "    sample_time_ms: 23930.842\n",
      "    update_time_ms: 1.932\n",
      "  timestamp: 1634844671\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 58000\n",
      "  training_iteration: 58\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">          1451.5</td><td style=\"text-align: right;\">58000</td><td style=\"text-align: right;\"> -3.7229</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">               -9.64</td><td style=\"text-align: right;\">            366.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 59000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-31-35\n",
      "  done: false\n",
      "  episode_len_mean: 367.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.899999999999982\n",
      "  episode_reward_mean: -3.757999999999966\n",
      "  episode_reward_min: -9.639999999999967\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 162\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.990265056822035\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01140665509373865\n",
      "          policy_loss: -0.08861447340912289\n",
      "          total_loss: -0.09569992969433466\n",
      "          vf_explained_var: 0.48468950390815735\n",
      "          vf_loss: 0.01053585526274724\n",
      "    num_agent_steps_sampled: 59000\n",
      "    num_agent_steps_trained: 59000\n",
      "    num_steps_sampled: 59000\n",
      "    num_steps_trained: 59000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.9969696969697\n",
      "    ram_util_percent: 31.87272727272727\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03752244568122906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.335891420308116\n",
      "    mean_inference_ms: 1.685029695610142\n",
      "    mean_raw_obs_processing_ms: 1.6072097087932775\n",
      "  time_since_restore: 1474.55557179451\n",
      "  time_this_iter_s: 23.060148000717163\n",
      "  time_total_s: 1474.55557179451\n",
      "  timers:\n",
      "    learn_throughput: 1229.179\n",
      "    learn_time_ms: 813.551\n",
      "    load_throughput: 39655.287\n",
      "    load_time_ms: 25.217\n",
      "    sample_throughput: 41.57\n",
      "    sample_time_ms: 24055.65\n",
      "    update_time_ms: 1.936\n",
      "  timestamp: 1634844695\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 59000\n",
      "  training_iteration: 59\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         1474.56</td><td style=\"text-align: right;\">59000</td><td style=\"text-align: right;\">  -3.758</td><td style=\"text-align: right;\">                -2.9</td><td style=\"text-align: right;\">               -9.64</td><td style=\"text-align: right;\">               367</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-31-58\n",
      "  done: false\n",
      "  episode_len_mean: 366.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.899999999999982\n",
      "  episode_reward_mean: -3.753099999999966\n",
      "  episode_reward_min: -9.639999999999967\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 165\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.907264647218916\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011334383616652843\n",
      "          policy_loss: -0.15263043886257543\n",
      "          total_loss: -0.16113194889492458\n",
      "          vf_explained_var: 0.7306860089302063\n",
      "          vf_loss: 0.008304259688076045\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.17878787878788\n",
      "    ram_util_percent: 31.87272727272727\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0375230369874916\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.28886071390243\n",
      "    mean_inference_ms: 1.6850161742238194\n",
      "    mean_raw_obs_processing_ms: 1.6048120178938887\n",
      "  time_since_restore: 1497.675268650055\n",
      "  time_this_iter_s: 23.119696855545044\n",
      "  time_total_s: 1497.675268650055\n",
      "  timers:\n",
      "    learn_throughput: 1227.084\n",
      "    learn_time_ms: 814.94\n",
      "    load_throughput: 39684.29\n",
      "    load_time_ms: 25.199\n",
      "    sample_throughput: 41.407\n",
      "    sample_time_ms: 24150.253\n",
      "    update_time_ms: 2.015\n",
      "  timestamp: 1634844718\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 60\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         1497.68</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\"> -3.7531</td><td style=\"text-align: right;\">                -2.9</td><td style=\"text-align: right;\">               -9.64</td><td style=\"text-align: right;\">            366.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 61000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-32-21\n",
      "  done: false\n",
      "  episode_len_mean: 365.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.899999999999982\n",
      "  episode_reward_mean: -3.685499999999966\n",
      "  episode_reward_min: -6.599999999999968\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 168\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8818542732132806\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013687607824975117\n",
      "          policy_loss: -0.13400154879523649\n",
      "          total_loss: -0.14020450086229377\n",
      "          vf_explained_var: 0.7122263312339783\n",
      "          vf_loss: 0.00987806796717147\n",
      "    num_agent_steps_sampled: 61000\n",
      "    num_agent_steps_trained: 61000\n",
      "    num_steps_sampled: 61000\n",
      "    num_steps_trained: 61000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.00588235294118\n",
      "    ram_util_percent: 31.929411764705886\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03752270270124405\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.243886938553015\n",
      "    mean_inference_ms: 1.684997554262826\n",
      "    mean_raw_obs_processing_ms: 1.6035382892199426\n",
      "  time_since_restore: 1521.0808579921722\n",
      "  time_this_iter_s: 23.40558934211731\n",
      "  time_total_s: 1521.0808579921722\n",
      "  timers:\n",
      "    learn_throughput: 1225.072\n",
      "    learn_time_ms: 816.278\n",
      "    load_throughput: 40054.395\n",
      "    load_time_ms: 24.966\n",
      "    sample_throughput: 41.005\n",
      "    sample_time_ms: 24387.304\n",
      "    update_time_ms: 3.318\n",
      "  timestamp: 1634844741\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 61000\n",
      "  training_iteration: 61\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         1521.08</td><td style=\"text-align: right;\">61000</td><td style=\"text-align: right;\"> -3.6855</td><td style=\"text-align: right;\">                -2.9</td><td style=\"text-align: right;\">                -6.6</td><td style=\"text-align: right;\">            365.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 62000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-32-44\n",
      "  done: false\n",
      "  episode_len_mean: 365.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.899999999999982\n",
      "  episode_reward_mean: -3.6820999999999655\n",
      "  episode_reward_min: -6.599999999999968\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 171\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.852655169698927\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.027784833774426136\n",
      "          policy_loss: -0.12572577902012402\n",
      "          total_loss: -0.1281279705050919\n",
      "          vf_explained_var: 0.6279292106628418\n",
      "          vf_loss: 0.010567389852884744\n",
      "    num_agent_steps_sampled: 62000\n",
      "    num_agent_steps_trained: 62000\n",
      "    num_steps_sampled: 62000\n",
      "    num_steps_trained: 62000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.740625\n",
      "    ram_util_percent: 31.987499999999997\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03752222062897695\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.19952123796791\n",
      "    mean_inference_ms: 1.6849668958455237\n",
      "    mean_raw_obs_processing_ms: 1.6032913345513733\n",
      "  time_since_restore: 1544.051555633545\n",
      "  time_this_iter_s: 22.97069764137268\n",
      "  time_total_s: 1544.051555633545\n",
      "  timers:\n",
      "    learn_throughput: 1225.477\n",
      "    learn_time_ms: 816.009\n",
      "    load_throughput: 39519.432\n",
      "    load_time_ms: 25.304\n",
      "    sample_throughput: 40.688\n",
      "    sample_time_ms: 24577.311\n",
      "    update_time_ms: 3.332\n",
      "  timestamp: 1634844764\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 62000\n",
      "  training_iteration: 62\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         1544.05</td><td style=\"text-align: right;\">62000</td><td style=\"text-align: right;\"> -3.6821</td><td style=\"text-align: right;\">                -2.9</td><td style=\"text-align: right;\">                -6.6</td><td style=\"text-align: right;\">            365.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 63000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-33-07\n",
      "  done: false\n",
      "  episode_len_mean: 364.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.899999999999982\n",
      "  episode_reward_mean: -3.6456999999999664\n",
      "  episode_reward_min: -4.549999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 174\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.789022327793969\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017258653596784374\n",
      "          policy_loss: -0.12080993056297303\n",
      "          total_loss: -0.12498603413502375\n",
      "          vf_explained_var: 0.6210938096046448\n",
      "          vf_loss: 0.008536522711316745\n",
      "    num_agent_steps_sampled: 63000\n",
      "    num_agent_steps_trained: 63000\n",
      "    num_steps_sampled: 63000\n",
      "    num_steps_trained: 63000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.28484848484848\n",
      "    ram_util_percent: 31.981818181818184\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03752156307750456\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.155874482855783\n",
      "    mean_inference_ms: 1.6849335113432027\n",
      "    mean_raw_obs_processing_ms: 1.6039612565146766\n",
      "  time_since_restore: 1566.8078582286835\n",
      "  time_this_iter_s: 22.75630259513855\n",
      "  time_total_s: 1566.8078582286835\n",
      "  timers:\n",
      "    learn_throughput: 1222.591\n",
      "    learn_time_ms: 817.935\n",
      "    load_throughput: 39716.306\n",
      "    load_time_ms: 25.179\n",
      "    sample_throughput: 40.58\n",
      "    sample_time_ms: 24642.718\n",
      "    update_time_ms: 3.34\n",
      "  timestamp: 1634844787\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63000\n",
      "  training_iteration: 63\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         1566.81</td><td style=\"text-align: right;\">63000</td><td style=\"text-align: right;\"> -3.6457</td><td style=\"text-align: right;\">                -2.9</td><td style=\"text-align: right;\">               -4.55</td><td style=\"text-align: right;\">            364.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-33-30\n",
      "  done: false\n",
      "  episode_len_mean: 364.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.899999999999982\n",
      "  episode_reward_mean: -3.649599999999966\n",
      "  episode_reward_min: -4.549999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 177\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9359246889750164\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023425225745818245\n",
      "          policy_loss: -0.019322933753331502\n",
      "          total_loss: -0.024855818019972907\n",
      "          vf_explained_var: 0.6785748600959778\n",
      "          vf_loss: 0.006798795379129135\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.1375\n",
      "    ram_util_percent: 32.003125\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037521041535918315\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.11253079337735\n",
      "    mean_inference_ms: 1.6848938193070069\n",
      "    mean_raw_obs_processing_ms: 1.6054564972353251\n",
      "  time_since_restore: 1589.5730776786804\n",
      "  time_this_iter_s: 22.76521944999695\n",
      "  time_total_s: 1589.5730776786804\n",
      "  timers:\n",
      "    learn_throughput: 1224.481\n",
      "    learn_time_ms: 816.673\n",
      "    load_throughput: 39784.568\n",
      "    load_time_ms: 25.135\n",
      "    sample_throughput: 40.718\n",
      "    sample_time_ms: 24559.399\n",
      "    update_time_ms: 3.339\n",
      "  timestamp: 1634844810\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 64\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         1589.57</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\"> -3.6496</td><td style=\"text-align: right;\">                -2.9</td><td style=\"text-align: right;\">               -4.55</td><td style=\"text-align: right;\">            364.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 65000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-34-10\n",
      "  done: false\n",
      "  episode_len_mean: 364.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.899999999999982\n",
      "  episode_reward_mean: -3.6401999999999664\n",
      "  episode_reward_min: -4.549999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 180\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9951653798421225\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01381501595963047\n",
      "          policy_loss: -0.13559356646405327\n",
      "          total_loss: -0.1399110685620043\n",
      "          vf_explained_var: 0.6045981049537659\n",
      "          vf_loss: 0.009417394890139501\n",
      "    num_agent_steps_sampled: 65000\n",
      "    num_agent_steps_trained: 65000\n",
      "    num_steps_sampled: 65000\n",
      "    num_steps_trained: 65000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.62241379310345\n",
      "    ram_util_percent: 31.989655172413794\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037520389235312986\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.069495896752883\n",
      "    mean_inference_ms: 1.684837262326627\n",
      "    mean_raw_obs_processing_ms: 1.6157617122504342\n",
      "  time_since_restore: 1629.928862810135\n",
      "  time_this_iter_s: 40.35578513145447\n",
      "  time_total_s: 1629.928862810135\n",
      "  timers:\n",
      "    learn_throughput: 1224.338\n",
      "    learn_time_ms: 816.768\n",
      "    load_throughput: 39856.625\n",
      "    load_time_ms: 25.09\n",
      "    sample_throughput: 40.833\n",
      "    sample_time_ms: 24490.146\n",
      "    update_time_ms: 3.31\n",
      "  timestamp: 1634844850\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 65000\n",
      "  training_iteration: 65\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         1629.93</td><td style=\"text-align: right;\">65000</td><td style=\"text-align: right;\"> -3.6402</td><td style=\"text-align: right;\">                -2.9</td><td style=\"text-align: right;\">               -4.55</td><td style=\"text-align: right;\">            364.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 66000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-34-36\n",
      "  done: false\n",
      "  episode_len_mean: 363.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.899999999999982\n",
      "  episode_reward_mean: -3.6327999999999663\n",
      "  episode_reward_min: -4.549999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 183\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8176592879825169\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013169312095146463\n",
      "          policy_loss: -0.15393986817863253\n",
      "          total_loss: -0.16064100844992532\n",
      "          vf_explained_var: 0.798921525478363\n",
      "          vf_loss: 0.005549262324348092\n",
      "    num_agent_steps_sampled: 66000\n",
      "    num_agent_steps_trained: 66000\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 66000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.497368421052634\n",
      "    ram_util_percent: 31.997368421052634\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037519709181574205\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.02752430710494\n",
      "    mean_inference_ms: 1.6847772976712476\n",
      "    mean_raw_obs_processing_ms: 1.6261806992059422\n",
      "  time_since_restore: 1656.3188047409058\n",
      "  time_this_iter_s: 26.389941930770874\n",
      "  time_total_s: 1656.3188047409058\n",
      "  timers:\n",
      "    learn_throughput: 1227.312\n",
      "    learn_time_ms: 814.788\n",
      "    load_throughput: 39963.489\n",
      "    load_time_ms: 25.023\n",
      "    sample_throughput: 40.872\n",
      "    sample_time_ms: 24466.553\n",
      "    update_time_ms: 3.317\n",
      "  timestamp: 1634844876\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 66\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         1656.32</td><td style=\"text-align: right;\">66000</td><td style=\"text-align: right;\"> -3.6328</td><td style=\"text-align: right;\">                -2.9</td><td style=\"text-align: right;\">               -4.55</td><td style=\"text-align: right;\">            363.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 67000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-35-01\n",
      "  done: false\n",
      "  episode_len_mean: 363.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.899999999999982\n",
      "  episode_reward_mean: -3.634099999999967\n",
      "  episode_reward_min: -4.549999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 186\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.754891324043274\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01146111394165585\n",
      "          policy_loss: -0.12186181338296996\n",
      "          total_loss: -0.12930425215098593\n",
      "          vf_explained_var: 0.8468206524848938\n",
      "          vf_loss: 0.004948971922405892\n",
      "    num_agent_steps_sampled: 67000\n",
      "    num_agent_steps_trained: 67000\n",
      "    num_steps_sampled: 67000\n",
      "    num_steps_trained: 67000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.620588235294115\n",
      "    ram_util_percent: 31.96764705882353\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03751895908229026\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.98641027103196\n",
      "    mean_inference_ms: 1.684711395463983\n",
      "    mean_raw_obs_processing_ms: 1.637467358660733\n",
      "  time_since_restore: 1680.332388162613\n",
      "  time_this_iter_s: 24.013583421707153\n",
      "  time_total_s: 1680.332388162613\n",
      "  timers:\n",
      "    learn_throughput: 1228.246\n",
      "    learn_time_ms: 814.169\n",
      "    load_throughput: 40288.86\n",
      "    load_time_ms: 24.821\n",
      "    sample_throughput: 40.921\n",
      "    sample_time_ms: 24437.383\n",
      "    update_time_ms: 3.317\n",
      "  timestamp: 1634844901\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 67000\n",
      "  training_iteration: 67\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         1680.33</td><td style=\"text-align: right;\">67000</td><td style=\"text-align: right;\"> -3.6341</td><td style=\"text-align: right;\">                -2.9</td><td style=\"text-align: right;\">               -4.55</td><td style=\"text-align: right;\">            363.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-35-26\n",
      "  done: false\n",
      "  episode_len_mean: 361.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.899999999999982\n",
      "  episode_reward_mean: -3.611599999999967\n",
      "  episode_reward_min: -4.549999999999947\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 190\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7996381998062134\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012178030630403154\n",
      "          policy_loss: 0.051040599329604044\n",
      "          total_loss: 0.04307532285650571\n",
      "          vf_explained_var: 0.8885879516601562\n",
      "          vf_loss: 0.004550993377860222\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.218918918918924\n",
      "    ram_util_percent: 31.835135135135133\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03751730581517954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.934837350139013\n",
      "    mean_inference_ms: 1.684616127815335\n",
      "    mean_raw_obs_processing_ms: 1.6372422213823266\n",
      "  time_since_restore: 1706.111675977707\n",
      "  time_this_iter_s: 25.779287815093994\n",
      "  time_total_s: 1706.111675977707\n",
      "  timers:\n",
      "    learn_throughput: 1227.681\n",
      "    learn_time_ms: 814.544\n",
      "    load_throughput: 40301.325\n",
      "    load_time_ms: 24.813\n",
      "    sample_throughput: 40.624\n",
      "    sample_time_ms: 24615.75\n",
      "    update_time_ms: 3.33\n",
      "  timestamp: 1634844926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 68\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         1706.11</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\"> -3.6116</td><td style=\"text-align: right;\">                -2.9</td><td style=\"text-align: right;\">               -4.55</td><td style=\"text-align: right;\">            361.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 69000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-35-51\n",
      "  done: false\n",
      "  episode_len_mean: 359.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.899999999999982\n",
      "  episode_reward_mean: -3.594999999999968\n",
      "  episode_reward_min: -4.549999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 193\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.776658648914761\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00860495438640519\n",
      "          policy_loss: -0.015670580913623174\n",
      "          total_loss: -0.020734242101510366\n",
      "          vf_explained_var: 0.524091899394989\n",
      "          vf_loss: 0.008830698002647195\n",
      "    num_agent_steps_sampled: 69000\n",
      "    num_agent_steps_trained: 69000\n",
      "    num_steps_sampled: 69000\n",
      "    num_steps_trained: 69000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.28285714285714\n",
      "    ram_util_percent: 31.871428571428567\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03751550783301147\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.89636858311712\n",
      "    mean_inference_ms: 1.684541304923679\n",
      "    mean_raw_obs_processing_ms: 1.6341644748370134\n",
      "  time_since_restore: 1730.8830263614655\n",
      "  time_this_iter_s: 24.771350383758545\n",
      "  time_total_s: 1730.8830263614655\n",
      "  timers:\n",
      "    learn_throughput: 1231.736\n",
      "    learn_time_ms: 811.862\n",
      "    load_throughput: 40399.263\n",
      "    load_time_ms: 24.753\n",
      "    sample_throughput: 40.339\n",
      "    sample_time_ms: 24789.624\n",
      "    update_time_ms: 3.314\n",
      "  timestamp: 1634844951\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 69000\n",
      "  training_iteration: 69\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         1730.88</td><td style=\"text-align: right;\">69000</td><td style=\"text-align: right;\">  -3.595</td><td style=\"text-align: right;\">                -2.9</td><td style=\"text-align: right;\">               -4.55</td><td style=\"text-align: right;\">             359.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 70000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-36-17\n",
      "  done: false\n",
      "  episode_len_mean: 356.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.899999999999982\n",
      "  episode_reward_mean: -3.569399999999968\n",
      "  episode_reward_min: -4.549999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 196\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7151250229941475\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010617842042853374\n",
      "          policy_loss: 0.06458487924602296\n",
      "          total_loss: 0.060127165582444936\n",
      "          vf_explained_var: 0.556567370891571\n",
      "          vf_loss: 0.007915510507559197\n",
      "    num_agent_steps_sampled: 70000\n",
      "    num_agent_steps_trained: 70000\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.13243243243243\n",
      "    ram_util_percent: 31.9054054054054\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03751382812497293\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.86245240990693\n",
      "    mean_inference_ms: 1.6844712178047176\n",
      "    mean_raw_obs_processing_ms: 1.6323162449206876\n",
      "  time_since_restore: 1756.3470120429993\n",
      "  time_this_iter_s: 25.463985681533813\n",
      "  time_total_s: 1756.3470120429993\n",
      "  timers:\n",
      "    learn_throughput: 1233.519\n",
      "    learn_time_ms: 810.689\n",
      "    load_throughput: 40751.032\n",
      "    load_time_ms: 24.539\n",
      "    sample_throughput: 39.959\n",
      "    sample_time_ms: 25025.446\n",
      "    update_time_ms: 3.227\n",
      "  timestamp: 1634844977\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 70\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         1756.35</td><td style=\"text-align: right;\">70000</td><td style=\"text-align: right;\"> -3.5694</td><td style=\"text-align: right;\">                -2.9</td><td style=\"text-align: right;\">               -4.55</td><td style=\"text-align: right;\">            356.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 71000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-36-42\n",
      "  done: false\n",
      "  episode_len_mean: 353.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.899999999999982\n",
      "  episode_reward_mean: -3.539299999999969\n",
      "  episode_reward_min: -4.549999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 199\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7531708505418566\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011724906352685239\n",
      "          policy_loss: -0.0494936595360438\n",
      "          total_loss: -0.05890826541516516\n",
      "          vf_explained_var: 0.9341296553611755\n",
      "          vf_loss: 0.0028408923192829306\n",
      "    num_agent_steps_sampled: 71000\n",
      "    num_agent_steps_trained: 71000\n",
      "    num_steps_sampled: 71000\n",
      "    num_steps_trained: 71000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.53333333333333\n",
      "    ram_util_percent: 31.944444444444443\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03751229972714107\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.83323315652561\n",
      "    mean_inference_ms: 1.6844075203062565\n",
      "    mean_raw_obs_processing_ms: 1.6311543733487854\n",
      "  time_since_restore: 1781.9214856624603\n",
      "  time_this_iter_s: 25.57447361946106\n",
      "  time_total_s: 1781.9214856624603\n",
      "  timers:\n",
      "    learn_throughput: 1234.082\n",
      "    learn_time_ms: 810.319\n",
      "    load_throughput: 40081.342\n",
      "    load_time_ms: 24.949\n",
      "    sample_throughput: 39.614\n",
      "    sample_time_ms: 25243.62\n",
      "    update_time_ms: 1.928\n",
      "  timestamp: 1634845002\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 71000\n",
      "  training_iteration: 71\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         1781.92</td><td style=\"text-align: right;\">71000</td><td style=\"text-align: right;\"> -3.5393</td><td style=\"text-align: right;\">                -2.9</td><td style=\"text-align: right;\">               -4.55</td><td style=\"text-align: right;\">            353.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-37-06\n",
      "  done: false\n",
      "  episode_len_mean: 351.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.899999999999982\n",
      "  episode_reward_mean: -3.5133999999999683\n",
      "  episode_reward_min: -4.549999999999947\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 202\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8543269832928975\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012484833468773123\n",
      "          policy_loss: -0.026266560206810632\n",
      "          total_loss: -0.03595552560355928\n",
      "          vf_explained_var: 0.9218326210975647\n",
      "          vf_loss: 0.003236128459684551\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.10285714285714\n",
      "    ram_util_percent: 31.98\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03751035643992131\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.809422044108825\n",
      "    mean_inference_ms: 1.6843479444173786\n",
      "    mean_raw_obs_processing_ms: 1.6310375367988599\n",
      "  time_since_restore: 1805.9800283908844\n",
      "  time_this_iter_s: 24.058542728424072\n",
      "  time_total_s: 1805.9800283908844\n",
      "  timers:\n",
      "    learn_throughput: 1231.237\n",
      "    learn_time_ms: 812.192\n",
      "    load_throughput: 40531.451\n",
      "    load_time_ms: 24.672\n",
      "    sample_throughput: 39.446\n",
      "    sample_time_ms: 25350.823\n",
      "    update_time_ms: 1.921\n",
      "  timestamp: 1634845026\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 72\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         1805.98</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\"> -3.5134</td><td style=\"text-align: right;\">                -2.9</td><td style=\"text-align: right;\">               -4.55</td><td style=\"text-align: right;\">            351.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 73000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-37-30\n",
      "  done: false\n",
      "  episode_len_mean: 348.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.899999999999982\n",
      "  episode_reward_mean: -3.48379999999997\n",
      "  episode_reward_min: -4.4999999999999485\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 205\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9195989807446798\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01746496835023758\n",
      "          policy_loss: -0.03529327466256089\n",
      "          total_loss: -0.0425175199078189\n",
      "          vf_explained_var: 0.9294339418411255\n",
      "          vf_loss: 0.004112508235913184\n",
      "    num_agent_steps_sampled: 73000\n",
      "    num_agent_steps_trained: 73000\n",
      "    num_steps_sampled: 73000\n",
      "    num_steps_trained: 73000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.415151515151514\n",
      "    ram_util_percent: 32.02121212121212\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037508367016719675\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.790450111403505\n",
      "    mean_inference_ms: 1.684296238973961\n",
      "    mean_raw_obs_processing_ms: 1.6318415330186917\n",
      "  time_since_restore: 1829.697538137436\n",
      "  time_this_iter_s: 23.717509746551514\n",
      "  time_total_s: 1829.697538137436\n",
      "  timers:\n",
      "    learn_throughput: 1233.318\n",
      "    learn_time_ms: 810.821\n",
      "    load_throughput: 40622.681\n",
      "    load_time_ms: 24.617\n",
      "    sample_throughput: 39.295\n",
      "    sample_time_ms: 25448.353\n",
      "    update_time_ms: 1.909\n",
      "  timestamp: 1634845050\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 73000\n",
      "  training_iteration: 73\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">          1829.7</td><td style=\"text-align: right;\">73000</td><td style=\"text-align: right;\"> -3.4838</td><td style=\"text-align: right;\">                -2.9</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">            348.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 74000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-37-55\n",
      "  done: false\n",
      "  episode_len_mean: 345.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.899999999999982\n",
      "  episode_reward_mean: -3.45229999999997\n",
      "  episode_reward_min: -4.4999999999999485\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 208\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7534406450059679\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.022059097867222575\n",
      "          policy_loss: -0.03403558929761251\n",
      "          total_loss: -0.03883902596102821\n",
      "          vf_explained_var: 0.9645445942878723\n",
      "          vf_loss: 0.0028043742783160673\n",
      "    num_agent_steps_sampled: 74000\n",
      "    num_agent_steps_trained: 74000\n",
      "    num_steps_sampled: 74000\n",
      "    num_steps_trained: 74000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.06388888888889\n",
      "    ram_util_percent: 32.047222222222224\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03750630384600394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.777001574902286\n",
      "    mean_inference_ms: 1.6842506434530475\n",
      "    mean_raw_obs_processing_ms: 1.633483868005642\n",
      "  time_since_restore: 1854.9225373268127\n",
      "  time_this_iter_s: 25.22499918937683\n",
      "  time_total_s: 1854.9225373268127\n",
      "  timers:\n",
      "    learn_throughput: 1231.044\n",
      "    learn_time_ms: 812.319\n",
      "    load_throughput: 40741.927\n",
      "    load_time_ms: 24.545\n",
      "    sample_throughput: 38.921\n",
      "    sample_time_ms: 25692.926\n",
      "    update_time_ms: 1.904\n",
      "  timestamp: 1634845075\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 74000\n",
      "  training_iteration: 74\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         1854.92</td><td style=\"text-align: right;\">74000</td><td style=\"text-align: right;\"> -3.4523</td><td style=\"text-align: right;\">                -2.9</td><td style=\"text-align: right;\">                -4.5</td><td style=\"text-align: right;\">            345.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 75000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-38-37\n",
      "  done: false\n",
      "  episode_len_mean: 342.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.899999999999982\n",
      "  episode_reward_mean: -3.4218999999999706\n",
      "  episode_reward_min: -4.229999999999954\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 211\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7611312773492602\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009828018001889186\n",
      "          policy_loss: -0.173153598441018\n",
      "          total_loss: -0.1767379942867491\n",
      "          vf_explained_var: 0.7978947758674622\n",
      "          vf_loss: 0.0073930030753318636\n",
      "    num_agent_steps_sampled: 75000\n",
      "    num_agent_steps_trained: 75000\n",
      "    num_steps_sampled: 75000\n",
      "    num_steps_trained: 75000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.48833333333333\n",
      "    ram_util_percent: 32.01833333333333\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03750441642128877\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.768495342906796\n",
      "    mean_inference_ms: 1.684211212333021\n",
      "    mean_raw_obs_processing_ms: 1.642682206371501\n",
      "  time_since_restore: 1896.7650260925293\n",
      "  time_this_iter_s: 41.84248876571655\n",
      "  time_total_s: 1896.7650260925293\n",
      "  timers:\n",
      "    learn_throughput: 1230.884\n",
      "    learn_time_ms: 812.424\n",
      "    load_throughput: 41152.581\n",
      "    load_time_ms: 24.3\n",
      "    sample_throughput: 38.697\n",
      "    sample_time_ms: 25841.723\n",
      "    update_time_ms: 1.916\n",
      "  timestamp: 1634845117\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75000\n",
      "  training_iteration: 75\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         1896.77</td><td style=\"text-align: right;\">75000</td><td style=\"text-align: right;\"> -3.4219</td><td style=\"text-align: right;\">                -2.9</td><td style=\"text-align: right;\">               -4.23</td><td style=\"text-align: right;\">            342.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-39-03\n",
      "  done: false\n",
      "  episode_len_mean: 339.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.899999999999982\n",
      "  episode_reward_mean: -3.3940999999999715\n",
      "  episode_reward_min: -4.229999999999954\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 215\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.775638915432824\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01394694802611285\n",
      "          policy_loss: -0.12994551145368152\n",
      "          total_loss: -0.13079359067810906\n",
      "          vf_explained_var: 0.8677961826324463\n",
      "          vf_loss: 0.007494121255715274\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.381081081081085\n",
      "    ram_util_percent: 31.994594594594595\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03750200939416073\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.762266467782815\n",
      "    mean_inference_ms: 1.6841699705476012\n",
      "    mean_raw_obs_processing_ms: 1.6557604698212285\n",
      "  time_since_restore: 1922.380844593048\n",
      "  time_this_iter_s: 25.6158185005188\n",
      "  time_total_s: 1922.380844593048\n",
      "  timers:\n",
      "    learn_throughput: 1229.183\n",
      "    learn_time_ms: 813.548\n",
      "    load_throughput: 41156.458\n",
      "    load_time_ms: 24.298\n",
      "    sample_throughput: 38.815\n",
      "    sample_time_ms: 25763.203\n",
      "    update_time_ms: 1.909\n",
      "  timestamp: 1634845143\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 76\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         1922.38</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\"> -3.3941</td><td style=\"text-align: right;\">                -2.9</td><td style=\"text-align: right;\">               -4.23</td><td style=\"text-align: right;\">            339.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 77000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-39-27\n",
      "  done: false\n",
      "  episode_len_mean: 337.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.899999999999982\n",
      "  episode_reward_mean: -3.3711999999999716\n",
      "  episode_reward_min: -4.229999999999954\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 218\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8111559947331746\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013686745008987403\n",
      "          policy_loss: -0.1030316105319394\n",
      "          total_loss: -0.10884830864767234\n",
      "          vf_explained_var: 0.941127598285675\n",
      "          vf_loss: 0.003056308264300848\n",
      "    num_agent_steps_sampled: 77000\n",
      "    num_agent_steps_trained: 77000\n",
      "    num_steps_sampled: 77000\n",
      "    num_steps_trained: 77000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.27714285714286\n",
      "    ram_util_percent: 31.882857142857137\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03750015205410756\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.760970650857146\n",
      "    mean_inference_ms: 1.6841456200319345\n",
      "    mean_raw_obs_processing_ms: 1.666117750435387\n",
      "  time_since_restore: 1946.9434030056\n",
      "  time_this_iter_s: 24.56255841255188\n",
      "  time_total_s: 1946.9434030056\n",
      "  timers:\n",
      "    learn_throughput: 1228.953\n",
      "    learn_time_ms: 813.701\n",
      "    load_throughput: 40743.946\n",
      "    load_time_ms: 24.544\n",
      "    sample_throughput: 38.733\n",
      "    sample_time_ms: 25817.72\n",
      "    update_time_ms: 1.902\n",
      "  timestamp: 1634845167\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 77000\n",
      "  training_iteration: 77\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         1946.94</td><td style=\"text-align: right;\">77000</td><td style=\"text-align: right;\"> -3.3712</td><td style=\"text-align: right;\">                -2.9</td><td style=\"text-align: right;\">               -4.23</td><td style=\"text-align: right;\">            337.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 78000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-39-52\n",
      "  done: false\n",
      "  episode_len_mean: 334.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.3479999999999723\n",
      "  episode_reward_min: -3.8799999999999613\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 221\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6723890980084737\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012353271935467921\n",
      "          policy_loss: -0.03594511664576001\n",
      "          total_loss: -0.041949816275801924\n",
      "          vf_explained_var: 0.9560829997062683\n",
      "          vf_loss: 0.002380731110719757\n",
      "    num_agent_steps_sampled: 78000\n",
      "    num_agent_steps_trained: 78000\n",
      "    num_steps_sampled: 78000\n",
      "    num_steps_trained: 78000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.26666666666667\n",
      "    ram_util_percent: 31.811111111111114\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03749809406198638\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.762018653787973\n",
      "    mean_inference_ms: 1.6841220823960676\n",
      "    mean_raw_obs_processing_ms: 1.6694383469805385\n",
      "  time_since_restore: 1972.050246477127\n",
      "  time_this_iter_s: 25.1068434715271\n",
      "  time_total_s: 1972.050246477127\n",
      "  timers:\n",
      "    learn_throughput: 1233.686\n",
      "    learn_time_ms: 810.579\n",
      "    load_throughput: 40621.461\n",
      "    load_time_ms: 24.618\n",
      "    sample_throughput: 38.83\n",
      "    sample_time_ms: 25753.528\n",
      "    update_time_ms: 1.899\n",
      "  timestamp: 1634845192\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 78000\n",
      "  training_iteration: 78\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         1972.05</td><td style=\"text-align: right;\">78000</td><td style=\"text-align: right;\">  -3.348</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -3.88</td><td style=\"text-align: right;\">             334.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 79000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-40-17\n",
      "  done: false\n",
      "  episode_len_mean: 333.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.334699999999972\n",
      "  episode_reward_min: -3.8799999999999613\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 224\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.697612342569563\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012743004773629909\n",
      "          policy_loss: 0.038573953592114976\n",
      "          total_loss: 0.033573534753587514\n",
      "          vf_explained_var: 0.9159453511238098\n",
      "          vf_loss: 0.0033741776689162686\n",
      "    num_agent_steps_sampled: 79000\n",
      "    num_agent_steps_trained: 79000\n",
      "    num_steps_sampled: 79000\n",
      "    num_steps_trained: 79000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.358823529411765\n",
      "    ram_util_percent: 31.808823529411764\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03749598358102791\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.764119629120202\n",
      "    mean_inference_ms: 1.6841012537713005\n",
      "    mean_raw_obs_processing_ms: 1.6693445101179036\n",
      "  time_since_restore: 1996.2720880508423\n",
      "  time_this_iter_s: 24.22184157371521\n",
      "  time_total_s: 1996.2720880508423\n",
      "  timers:\n",
      "    learn_throughput: 1232.808\n",
      "    learn_time_ms: 811.156\n",
      "    load_throughput: 40665.138\n",
      "    load_time_ms: 24.591\n",
      "    sample_throughput: 38.914\n",
      "    sample_time_ms: 25698.019\n",
      "    update_time_ms: 1.911\n",
      "  timestamp: 1634845217\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79000\n",
      "  training_iteration: 79\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         1996.27</td><td style=\"text-align: right;\">79000</td><td style=\"text-align: right;\"> -3.3347</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -3.88</td><td style=\"text-align: right;\">            333.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-40-44\n",
      "  done: false\n",
      "  episode_len_mean: 331.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.318099999999973\n",
      "  episode_reward_min: -3.8799999999999613\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 227\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5618544989162022\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013928507556560197\n",
      "          policy_loss: -0.006660119278563394\n",
      "          total_loss: -0.009587628187404738\n",
      "          vf_explained_var: 0.9274080991744995\n",
      "          vf_loss: 0.003289292561304238\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.31794871794872\n",
      "    ram_util_percent: 31.85641025641027\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03749393217018817\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.768353126552938\n",
      "    mean_inference_ms: 1.6840859738795886\n",
      "    mean_raw_obs_processing_ms: 1.6696607614964865\n",
      "  time_since_restore: 2023.4897832870483\n",
      "  time_this_iter_s: 27.217695236206055\n",
      "  time_total_s: 2023.4897832870483\n",
      "  timers:\n",
      "    learn_throughput: 1230.024\n",
      "    learn_time_ms: 812.992\n",
      "    load_throughput: 40441.449\n",
      "    load_time_ms: 24.727\n",
      "    sample_throughput: 38.653\n",
      "    sample_time_ms: 25871.496\n",
      "    update_time_ms: 1.915\n",
      "  timestamp: 1634845244\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 80\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         2023.49</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> -3.3181</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -3.88</td><td style=\"text-align: right;\">            331.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 81000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-41-08\n",
      "  done: false\n",
      "  episode_len_mean: 331.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.311199999999973\n",
      "  episode_reward_min: -3.8799999999999613\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 230\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7299380011028713\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011960163002864179\n",
      "          policy_loss: -0.07082239497039053\n",
      "          total_loss: -0.07311199800007873\n",
      "          vf_explained_var: 0.7338865399360657\n",
      "          vf_loss: 0.006936666009844177\n",
      "    num_agent_steps_sampled: 81000\n",
      "    num_agent_steps_trained: 81000\n",
      "    num_steps_sampled: 81000\n",
      "    num_steps_trained: 81000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.26470588235294\n",
      "    ram_util_percent: 31.90588235294118\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0374918388720568\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.77325453444419\n",
      "    mean_inference_ms: 1.6840731930130346\n",
      "    mean_raw_obs_processing_ms: 1.6703667365267918\n",
      "  time_since_restore: 2047.5462355613708\n",
      "  time_this_iter_s: 24.05645227432251\n",
      "  time_total_s: 2047.5462355613708\n",
      "  timers:\n",
      "    learn_throughput: 1230.047\n",
      "    learn_time_ms: 812.977\n",
      "    load_throughput: 41053.131\n",
      "    load_time_ms: 24.359\n",
      "    sample_throughput: 38.88\n",
      "    sample_time_ms: 25720.064\n",
      "    update_time_ms: 1.907\n",
      "  timestamp: 1634845268\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 81000\n",
      "  training_iteration: 81\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         2047.55</td><td style=\"text-align: right;\">81000</td><td style=\"text-align: right;\"> -3.3112</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -3.88</td><td style=\"text-align: right;\">            331.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 82000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-41-32\n",
      "  done: false\n",
      "  episode_len_mean: 330.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.3020999999999727\n",
      "  episode_reward_min: -3.8799999999999613\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 233\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7002437909444172\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012735000165441197\n",
      "          policy_loss: -0.036350368460019426\n",
      "          total_loss: -0.04180424354142613\n",
      "          vf_explained_var: 0.9371273517608643\n",
      "          vf_loss: 0.0029524371286647187\n",
      "    num_agent_steps_sampled: 82000\n",
      "    num_agent_steps_trained: 82000\n",
      "    num_steps_sampled: 82000\n",
      "    num_steps_trained: 82000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.682857142857145\n",
      "    ram_util_percent: 31.937142857142856\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03748983344284642\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.7796718697172\n",
      "    mean_inference_ms: 1.6840646247136613\n",
      "    mean_raw_obs_processing_ms: 1.6717242442351372\n",
      "  time_since_restore: 2071.9465415477753\n",
      "  time_this_iter_s: 24.40030598640442\n",
      "  time_total_s: 2071.9465415477753\n",
      "  timers:\n",
      "    learn_throughput: 1232.942\n",
      "    learn_time_ms: 811.068\n",
      "    load_throughput: 41331.703\n",
      "    load_time_ms: 24.195\n",
      "    sample_throughput: 38.826\n",
      "    sample_time_ms: 25756.263\n",
      "    update_time_ms: 1.938\n",
      "  timestamp: 1634845292\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 82000\n",
      "  training_iteration: 82\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         2071.95</td><td style=\"text-align: right;\">82000</td><td style=\"text-align: right;\"> -3.3021</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -3.88</td><td style=\"text-align: right;\">            330.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 83000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-41-56\n",
      "  done: false\n",
      "  episode_len_mean: 329.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.2980999999999727\n",
      "  episode_reward_min: -3.8799999999999613\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 236\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5875127845340304\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016325715254218092\n",
      "          policy_loss: -0.022360521720515357\n",
      "          total_loss: -0.02019483811325497\n",
      "          vf_explained_var: 0.596435010433197\n",
      "          vf_loss: 0.0070209501078352336\n",
      "    num_agent_steps_sampled: 83000\n",
      "    num_agent_steps_trained: 83000\n",
      "    num_steps_sampled: 83000\n",
      "    num_steps_trained: 83000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.505714285714284\n",
      "    ram_util_percent: 32.097142857142856\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03748811761429952\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.786986423679636\n",
      "    mean_inference_ms: 1.6840669774354575\n",
      "    mean_raw_obs_processing_ms: 1.6734005754414658\n",
      "  time_since_restore: 2095.919486284256\n",
      "  time_this_iter_s: 23.972944736480713\n",
      "  time_total_s: 2095.919486284256\n",
      "  timers:\n",
      "    learn_throughput: 1230.101\n",
      "    learn_time_ms: 812.942\n",
      "    load_throughput: 41650.859\n",
      "    load_time_ms: 24.009\n",
      "    sample_throughput: 38.79\n",
      "    sample_time_ms: 25780.136\n",
      "    update_time_ms: 1.948\n",
      "  timestamp: 1634845316\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 83000\n",
      "  training_iteration: 83\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         2095.92</td><td style=\"text-align: right;\">83000</td><td style=\"text-align: right;\"> -3.2981</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -3.88</td><td style=\"text-align: right;\">            329.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-42-21\n",
      "  done: false\n",
      "  episode_len_mean: 328.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.2845999999999735\n",
      "  episode_reward_min: -3.8799999999999613\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 239\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.622867586877611\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010489769691238255\n",
      "          policy_loss: -0.0337921222878827\n",
      "          total_loss: -0.0383623245689604\n",
      "          vf_explained_var: 0.8712539672851562\n",
      "          vf_loss: 0.004577881943744918\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.20285714285714\n",
      "    ram_util_percent: 32.20000000000001\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037486878828558605\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.79615957364705\n",
      "    mean_inference_ms: 1.6840810384295966\n",
      "    mean_raw_obs_processing_ms: 1.6753789295530868\n",
      "  time_since_restore: 2120.810494184494\n",
      "  time_this_iter_s: 24.891007900238037\n",
      "  time_total_s: 2120.810494184494\n",
      "  timers:\n",
      "    learn_throughput: 1222.911\n",
      "    learn_time_ms: 817.721\n",
      "    load_throughput: 41898.0\n",
      "    load_time_ms: 23.867\n",
      "    sample_throughput: 38.847\n",
      "    sample_time_ms: 25742.078\n",
      "    update_time_ms: 1.966\n",
      "  timestamp: 1634845341\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 84\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         2120.81</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\"> -3.2846</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -3.88</td><td style=\"text-align: right;\">            328.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 85000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-43-05\n",
      "  done: false\n",
      "  episode_len_mean: 326.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.263999999999975\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 242\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6333614309628806\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013611164999919505\n",
      "          policy_loss: -0.11137625699241956\n",
      "          total_loss: -0.11528905034065247\n",
      "          vf_explained_var: 0.9261769652366638\n",
      "          vf_loss: 0.0032332860411972636\n",
      "    num_agent_steps_sampled: 85000\n",
      "    num_agent_steps_trained: 85000\n",
      "    num_steps_sampled: 85000\n",
      "    num_steps_trained: 85000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.128571428571426\n",
      "    ram_util_percent: 32.20793650793651\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03748578129647588\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.808205296564893\n",
      "    mean_inference_ms: 1.6841022508232697\n",
      "    mean_raw_obs_processing_ms: 1.6838359565377816\n",
      "  time_since_restore: 2164.9607639312744\n",
      "  time_this_iter_s: 44.150269746780396\n",
      "  time_total_s: 2164.9607639312744\n",
      "  timers:\n",
      "    learn_throughput: 1220.443\n",
      "    learn_time_ms: 819.375\n",
      "    load_throughput: 41401.222\n",
      "    load_time_ms: 24.154\n",
      "    sample_throughput: 38.505\n",
      "    sample_time_ms: 25970.854\n",
      "    update_time_ms: 1.986\n",
      "  timestamp: 1634845385\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 85000\n",
      "  training_iteration: 85\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         2164.96</td><td style=\"text-align: right;\">85000</td><td style=\"text-align: right;\">  -3.264</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">             326.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 86000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-43-30\n",
      "  done: false\n",
      "  episode_len_mean: 325.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.258999999999974\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 245\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6507338020536635\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010034941886881851\n",
      "          policy_loss: -0.08049346150623428\n",
      "          total_loss: -0.08373910047941738\n",
      "          vf_explained_var: 0.793397843837738\n",
      "          vf_loss: 0.006488113211364382\n",
      "    num_agent_steps_sampled: 86000\n",
      "    num_agent_steps_trained: 86000\n",
      "    num_steps_sampled: 86000\n",
      "    num_steps_trained: 86000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.74571428571429\n",
      "    ram_util_percent: 32.43428571428572\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03748495939020183\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.8209795768755\n",
      "    mean_inference_ms: 1.684136820520645\n",
      "    mean_raw_obs_processing_ms: 1.6924877819512685\n",
      "  time_since_restore: 2189.6049683094025\n",
      "  time_this_iter_s: 24.64420437812805\n",
      "  time_total_s: 2189.6049683094025\n",
      "  timers:\n",
      "    learn_throughput: 1217.961\n",
      "    learn_time_ms: 821.044\n",
      "    load_throughput: 41705.734\n",
      "    load_time_ms: 23.978\n",
      "    sample_throughput: 38.652\n",
      "    sample_time_ms: 25872.182\n",
      "    update_time_ms: 1.992\n",
      "  timestamp: 1634845410\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 86000\n",
      "  training_iteration: 86\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">          2189.6</td><td style=\"text-align: right;\">86000</td><td style=\"text-align: right;\">  -3.259</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">             325.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 87000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-43-55\n",
      "  done: false\n",
      "  episode_len_mean: 325.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.255799999999975\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 248\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6482422682974074\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012391607387465559\n",
      "          policy_loss: -0.17039236393239762\n",
      "          total_loss: -0.1763384610000584\n",
      "          vf_explained_var: 0.9458114504814148\n",
      "          vf_loss: 0.0021719912830222812\n",
      "    num_agent_steps_sampled: 87000\n",
      "    num_agent_steps_trained: 87000\n",
      "    num_steps_sampled: 87000\n",
      "    num_steps_trained: 87000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.511111111111106\n",
      "    ram_util_percent: 32.35833333333333\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0374845777816122\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.83381972966509\n",
      "    mean_inference_ms: 1.6841790212931955\n",
      "    mean_raw_obs_processing_ms: 1.698325531387126\n",
      "  time_since_restore: 2214.5306146144867\n",
      "  time_this_iter_s: 24.92564630508423\n",
      "  time_total_s: 2214.5306146144867\n",
      "  timers:\n",
      "    learn_throughput: 1213.042\n",
      "    learn_time_ms: 824.374\n",
      "    load_throughput: 41920.277\n",
      "    load_time_ms: 23.855\n",
      "    sample_throughput: 38.602\n",
      "    sample_time_ms: 25905.278\n",
      "    update_time_ms: 2.007\n",
      "  timestamp: 1634845435\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 87000\n",
      "  training_iteration: 87\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         2214.53</td><td style=\"text-align: right;\">87000</td><td style=\"text-align: right;\"> -3.2558</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            325.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-44-22\n",
      "  done: false\n",
      "  episode_len_mean: 325.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.256699999999974\n",
      "  episode_reward_min: -3.849999999999962\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 252\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5690282649464078\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009499824938822638\n",
      "          policy_loss: -0.022941428195271226\n",
      "          total_loss: -0.028975855973031785\n",
      "          vf_explained_var: 0.9021323919296265\n",
      "          vf_loss: 0.0032434725558333514\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.9128205128205\n",
      "    ram_util_percent: 32.407692307692315\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03748439634030787\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.850646399340445\n",
      "    mean_inference_ms: 1.684245933785146\n",
      "    mean_raw_obs_processing_ms: 1.6986219504952538\n",
      "  time_since_restore: 2241.5708537101746\n",
      "  time_this_iter_s: 27.040239095687866\n",
      "  time_total_s: 2241.5708537101746\n",
      "  timers:\n",
      "    learn_throughput: 1208.064\n",
      "    learn_time_ms: 827.771\n",
      "    load_throughput: 42335.191\n",
      "    load_time_ms: 23.621\n",
      "    sample_throughput: 38.321\n",
      "    sample_time_ms: 26095.474\n",
      "    update_time_ms: 1.997\n",
      "  timestamp: 1634845462\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 88\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         2241.57</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\"> -3.2567</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -3.85</td><td style=\"text-align: right;\">            325.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 89000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-44-46\n",
      "  done: false\n",
      "  episode_len_mean: 326.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.2678999999999743\n",
      "  episode_reward_min: -3.9499999999999598\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 254\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6750827021068997\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011745323348224682\n",
      "          policy_loss: -0.06618222329351638\n",
      "          total_loss: -0.06363073347343338\n",
      "          vf_explained_var: 0.489666610956192\n",
      "          vf_loss: 0.011374220828939643\n",
      "    num_agent_steps_sampled: 89000\n",
      "    num_agent_steps_trained: 89000\n",
      "    num_steps_sampled: 89000\n",
      "    num_steps_trained: 89000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.78787878787879\n",
      "    ram_util_percent: 32.64848484848485\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03748464396830224\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.8582358217406\n",
      "    mean_inference_ms: 1.6842875744508887\n",
      "    mean_raw_obs_processing_ms: 1.6989617452695538\n",
      "  time_since_restore: 2264.9985485076904\n",
      "  time_this_iter_s: 23.42769479751587\n",
      "  time_total_s: 2264.9985485076904\n",
      "  timers:\n",
      "    learn_throughput: 1202.74\n",
      "    learn_time_ms: 831.435\n",
      "    load_throughput: 43027.77\n",
      "    load_time_ms: 23.241\n",
      "    sample_throughput: 38.443\n",
      "    sample_time_ms: 26012.773\n",
      "    update_time_ms: 2.001\n",
      "  timestamp: 1634845486\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 89000\n",
      "  training_iteration: 89\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">            2265</td><td style=\"text-align: right;\">89000</td><td style=\"text-align: right;\"> -3.2679</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -3.95</td><td style=\"text-align: right;\">            326.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 90000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-45-10\n",
      "  done: false\n",
      "  episode_len_mean: 327.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.273799999999974\n",
      "  episode_reward_min: -3.9499999999999598\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 257\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7520219087600708\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009664379380642114\n",
      "          policy_loss: -0.11438539202014605\n",
      "          total_loss: -0.11999995393885507\n",
      "          vf_explained_var: 0.8133434057235718\n",
      "          vf_loss: 0.005382199456087417\n",
      "    num_agent_steps_sampled: 90000\n",
      "    num_agent_steps_trained: 90000\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.22571428571429\n",
      "    ram_util_percent: 33.034285714285716\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037485233897847574\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.86952702597512\n",
      "    mean_inference_ms: 1.6843582186172095\n",
      "    mean_raw_obs_processing_ms: 1.6996186284554526\n",
      "  time_since_restore: 2289.5748047828674\n",
      "  time_this_iter_s: 24.576256275177002\n",
      "  time_total_s: 2289.5748047828674\n",
      "  timers:\n",
      "    learn_throughput: 1195.994\n",
      "    learn_time_ms: 836.124\n",
      "    load_throughput: 43945.229\n",
      "    load_time_ms: 22.756\n",
      "    sample_throughput: 38.846\n",
      "    sample_time_ms: 25742.497\n",
      "    update_time_ms: 3.111\n",
      "  timestamp: 1634845510\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 90\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 16.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         2289.57</td><td style=\"text-align: right;\">90000</td><td style=\"text-align: right;\"> -3.2738</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -3.95</td><td style=\"text-align: right;\">            327.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 91000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-45-36\n",
      "  done: false\n",
      "  episode_len_mean: 328.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.281199999999974\n",
      "  episode_reward_min: -3.9499999999999598\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 260\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5878500964906481\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007571491578506734\n",
      "          policy_loss: 0.10232799442278015\n",
      "          total_loss: 0.09705682906011741\n",
      "          vf_explained_var: 0.78887540102005\n",
      "          vf_loss: 0.0054965785626942916\n",
      "    num_agent_steps_sampled: 91000\n",
      "    num_agent_steps_trained: 91000\n",
      "    num_steps_sampled: 91000\n",
      "    num_steps_trained: 91000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.52972972972974\n",
      "    ram_util_percent: 38.4972972972973\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03748661908188442\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.881348326902813\n",
      "    mean_inference_ms: 1.6844518527882664\n",
      "    mean_raw_obs_processing_ms: 1.7002914142647032\n",
      "  time_since_restore: 2315.7604711055756\n",
      "  time_this_iter_s: 26.18566632270813\n",
      "  time_total_s: 2315.7604711055756\n",
      "  timers:\n",
      "    learn_throughput: 1182.752\n",
      "    learn_time_ms: 845.486\n",
      "    load_throughput: 44920.121\n",
      "    load_time_ms: 22.262\n",
      "    sample_throughput: 38.544\n",
      "    sample_time_ms: 25944.504\n",
      "    update_time_ms: 4.202\n",
      "  timestamp: 1634845536\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 91000\n",
      "  training_iteration: 91\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 19.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         2315.76</td><td style=\"text-align: right;\">91000</td><td style=\"text-align: right;\"> -3.2812</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -3.95</td><td style=\"text-align: right;\">            328.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-46-18\n",
      "  done: false\n",
      "  episode_len_mean: 327.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.278399999999974\n",
      "  episode_reward_min: -3.9499999999999598\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 263\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5622868339220684\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013354463512514849\n",
      "          policy_loss: -0.01308398296435674\n",
      "          total_loss: -0.01715466868546274\n",
      "          vf_explained_var: 0.9348082542419434\n",
      "          vf_loss: 0.0025379218355131646\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.59666666666666\n",
      "    ram_util_percent: 43.958333333333336\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037489786170908125\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.89912643745731\n",
      "    mean_inference_ms: 1.6846054670227941\n",
      "    mean_raw_obs_processing_ms: 1.7015082123141665\n",
      "  time_since_restore: 2357.404149532318\n",
      "  time_this_iter_s: 41.643678426742554\n",
      "  time_total_s: 2357.404149532318\n",
      "  timers:\n",
      "    learn_throughput: 1171.504\n",
      "    learn_time_ms: 853.603\n",
      "    load_throughput: 43690.803\n",
      "    load_time_ms: 22.888\n",
      "    sample_throughput: 36.154\n",
      "    sample_time_ms: 27659.693\n",
      "    update_time_ms: 4.359\n",
      "  timestamp: 1634845578\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 92\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">          2357.4</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\"> -3.2784</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -3.95</td><td style=\"text-align: right;\">            327.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 93000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-46-47\n",
      "  done: false\n",
      "  episode_len_mean: 328.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.2842999999999747\n",
      "  episode_reward_min: -4.149999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 266\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6611911217371622\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013984002303852\n",
      "          policy_loss: -0.027311735124223763\n",
      "          total_loss: -0.027745522869129974\n",
      "          vf_explained_var: 0.6957138180732727\n",
      "          vf_loss: 0.006738920500760691\n",
      "    num_agent_steps_sampled: 93000\n",
      "    num_agent_steps_trained: 93000\n",
      "    num_steps_sampled: 93000\n",
      "    num_steps_trained: 93000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.49249999999999\n",
      "    ram_util_percent: 45.15749999999999\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03749397986766168\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.918405897697518\n",
      "    mean_inference_ms: 1.6847917400426773\n",
      "    mean_raw_obs_processing_ms: 1.702979469855486\n",
      "  time_since_restore: 2385.835131883621\n",
      "  time_this_iter_s: 28.4309823513031\n",
      "  time_total_s: 2385.835131883621\n",
      "  timers:\n",
      "    learn_throughput: 1174.42\n",
      "    learn_time_ms: 851.484\n",
      "    load_throughput: 45034.042\n",
      "    load_time_ms: 22.205\n",
      "    sample_throughput: 35.577\n",
      "    sample_time_ms: 28107.996\n",
      "    update_time_ms: 4.459\n",
      "  timestamp: 1634845607\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 93000\n",
      "  training_iteration: 93\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         2385.84</td><td style=\"text-align: right;\">93000</td><td style=\"text-align: right;\"> -3.2843</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -4.15</td><td style=\"text-align: right;\">            328.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 94000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-47-12\n",
      "  done: false\n",
      "  episode_len_mean: 329.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.296499999999974\n",
      "  episode_reward_min: -4.179999999999955\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 268\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7191457682185702\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013505825688684601\n",
      "          policy_loss: -0.15599573006232578\n",
      "          total_loss: -0.15333501332336003\n",
      "          vf_explained_var: 0.4276440441608429\n",
      "          vf_loss: 0.010735739793421493\n",
      "    num_agent_steps_sampled: 94000\n",
      "    num_agent_steps_trained: 94000\n",
      "    num_steps_sampled: 94000\n",
      "    num_steps_trained: 94000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.93243243243244\n",
      "    ram_util_percent: 45.172972972972964\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037497366901011346\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.931622737080946\n",
      "    mean_inference_ms: 1.6849309480525454\n",
      "    mean_raw_obs_processing_ms: 1.7039601601717118\n",
      "  time_since_restore: 2411.71501660347\n",
      "  time_this_iter_s: 25.879884719848633\n",
      "  time_total_s: 2411.71501660347\n",
      "  timers:\n",
      "    learn_throughput: 1173.663\n",
      "    learn_time_ms: 852.033\n",
      "    load_throughput: 46481.235\n",
      "    load_time_ms: 21.514\n",
      "    sample_throughput: 35.452\n",
      "    sample_time_ms: 28206.911\n",
      "    update_time_ms: 4.525\n",
      "  timestamp: 1634845632\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 94000\n",
      "  training_iteration: 94\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         2411.72</td><td style=\"text-align: right;\">94000</td><td style=\"text-align: right;\"> -3.2965</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -4.18</td><td style=\"text-align: right;\">            329.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 95000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-47-55\n",
      "  done: false\n",
      "  episode_len_mean: 331.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.314299999999973\n",
      "  episode_reward_min: -4.2199999999999545\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 271\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5886069218317667\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009487690291975387\n",
      "          policy_loss: 0.0327508135802216\n",
      "          total_loss: 0.03131039316455523\n",
      "          vf_explained_var: 0.7071877121925354\n",
      "          vf_loss: 0.008041456753077606\n",
      "    num_agent_steps_sampled: 95000\n",
      "    num_agent_steps_trained: 95000\n",
      "    num_steps_sampled: 95000\n",
      "    num_steps_trained: 95000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.4737704918033\n",
      "    ram_util_percent: 45.1016393442623\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03750317906958075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.952170890516136\n",
      "    mean_inference_ms: 1.6851637797573071\n",
      "    mean_raw_obs_processing_ms: 1.7108790074066946\n",
      "  time_since_restore: 2454.0719516277313\n",
      "  time_this_iter_s: 42.356935024261475\n",
      "  time_total_s: 2454.0719516277313\n",
      "  timers:\n",
      "    learn_throughput: 1171.004\n",
      "    learn_time_ms: 853.968\n",
      "    load_throughput: 48792.593\n",
      "    load_time_ms: 20.495\n",
      "    sample_throughput: 35.681\n",
      "    sample_time_ms: 28026.204\n",
      "    update_time_ms: 4.91\n",
      "  timestamp: 1634845675\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 95000\n",
      "  training_iteration: 95\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         2454.07</td><td style=\"text-align: right;\">95000</td><td style=\"text-align: right;\"> -3.3143</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -4.22</td><td style=\"text-align: right;\">            331.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-48-18\n",
      "  done: false\n",
      "  episode_len_mean: 333.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.332699999999973\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 273\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8597992777824401\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011574251027867799\n",
      "          policy_loss: -0.0947430744767189\n",
      "          total_loss: -0.09692153417401844\n",
      "          vf_explained_var: 0.43876615166664124\n",
      "          vf_loss: 0.008606914723188513\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.82727272727273\n",
      "    ram_util_percent: 45.087878787878786\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03750780183424876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.965936543258668\n",
      "    mean_inference_ms: 1.6853349043387624\n",
      "    mean_raw_obs_processing_ms: 1.7155946423142723\n",
      "  time_since_restore: 2477.57364153862\n",
      "  time_this_iter_s: 23.501689910888672\n",
      "  time_total_s: 2477.57364153862\n",
      "  timers:\n",
      "    learn_throughput: 1170.477\n",
      "    learn_time_ms: 854.352\n",
      "    load_throughput: 50387.598\n",
      "    load_time_ms: 19.846\n",
      "    sample_throughput: 35.828\n",
      "    sample_time_ms: 27911.26\n",
      "    update_time_ms: 5.871\n",
      "  timestamp: 1634845698\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 96\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         2477.57</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\"> -3.3327</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            333.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 97000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-48-45\n",
      "  done: false\n",
      "  episode_len_mean: 334.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.347299999999973\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 276\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7841231730249194\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013010415136792138\n",
      "          policy_loss: -0.007614829142888387\n",
      "          total_loss: -0.010155757309661971\n",
      "          vf_explained_var: 0.6389760375022888\n",
      "          vf_loss: 0.006518274792728739\n",
      "    num_agent_steps_sampled: 97000\n",
      "    num_agent_steps_trained: 97000\n",
      "    num_steps_sampled: 97000\n",
      "    num_steps_trained: 97000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.32368421052632\n",
      "    ram_util_percent: 45.05789473684211\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03751621886453268\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.987487328078828\n",
      "    mean_inference_ms: 1.6856282820036252\n",
      "    mean_raw_obs_processing_ms: 1.7226105330989958\n",
      "  time_since_restore: 2504.188191175461\n",
      "  time_this_iter_s: 26.61454963684082\n",
      "  time_total_s: 2504.188191175461\n",
      "  timers:\n",
      "    learn_throughput: 1167.735\n",
      "    learn_time_ms: 856.358\n",
      "    load_throughput: 51933.291\n",
      "    load_time_ms: 19.255\n",
      "    sample_throughput: 35.614\n",
      "    sample_time_ms: 28078.475\n",
      "    update_time_ms: 5.872\n",
      "  timestamp: 1634845725\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 97000\n",
      "  training_iteration: 97\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         2504.19</td><td style=\"text-align: right;\">97000</td><td style=\"text-align: right;\"> -3.3473</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            334.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 98000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-49-10\n",
      "  done: false\n",
      "  episode_len_mean: 336.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.3659999999999717\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 278\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8553438199890984\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011049566544593069\n",
      "          policy_loss: 0.0017921638157632617\n",
      "          total_loss: -0.003626095006863276\n",
      "          vf_explained_var: 0.38847824931144714\n",
      "          vf_loss: 0.005676723069821795\n",
      "    num_agent_steps_sampled: 98000\n",
      "    num_agent_steps_trained: 98000\n",
      "    num_steps_sampled: 98000\n",
      "    num_steps_trained: 98000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.12432432432433\n",
      "    ram_util_percent: 45.0054054054054\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03752263267441223\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.002056713359863\n",
      "    mean_inference_ms: 1.6858496493241504\n",
      "    mean_raw_obs_processing_ms: 1.7247671960714421\n",
      "  time_since_restore: 2529.5306198596954\n",
      "  time_this_iter_s: 25.34242868423462\n",
      "  time_total_s: 2529.5306198596954\n",
      "  timers:\n",
      "    learn_throughput: 1154.622\n",
      "    learn_time_ms: 866.084\n",
      "    load_throughput: 51923.584\n",
      "    load_time_ms: 19.259\n",
      "    sample_throughput: 35.844\n",
      "    sample_time_ms: 27898.435\n",
      "    update_time_ms: 6.296\n",
      "  timestamp: 1634845750\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 98000\n",
      "  training_iteration: 98\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         2529.53</td><td style=\"text-align: right;\">98000</td><td style=\"text-align: right;\">  -3.366</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">             336.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 99000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-49-37\n",
      "  done: false\n",
      "  episode_len_mean: 338.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.3881999999999715\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 280\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.823193493154314\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011343001202129974\n",
      "          policy_loss: -0.08008379820320341\n",
      "          total_loss: -0.08284990754392411\n",
      "          vf_explained_var: 0.202444925904274\n",
      "          vf_loss: 0.007809297807721628\n",
      "    num_agent_steps_sampled: 99000\n",
      "    num_agent_steps_trained: 99000\n",
      "    num_steps_sampled: 99000\n",
      "    num_steps_trained: 99000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.90263157894738\n",
      "    ram_util_percent: 45.16052631578947\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037532267158738385\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.017041385074982\n",
      "    mean_inference_ms: 1.6860974043934192\n",
      "    mean_raw_obs_processing_ms: 1.7242738502245027\n",
      "  time_since_restore: 2556.4007964134216\n",
      "  time_this_iter_s: 26.870176553726196\n",
      "  time_total_s: 2556.4007964134216\n",
      "  timers:\n",
      "    learn_throughput: 1141.146\n",
      "    learn_time_ms: 876.312\n",
      "    load_throughput: 48795.374\n",
      "    load_time_ms: 20.494\n",
      "    sample_throughput: 35.423\n",
      "    sample_time_ms: 28230.442\n",
      "    update_time_ms: 7.044\n",
      "  timestamp: 1634845777\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 99000\n",
      "  training_iteration: 99\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">          2556.4</td><td style=\"text-align: right;\">99000</td><td style=\"text-align: right;\"> -3.3882</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            338.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-50-03\n",
      "  done: false\n",
      "  episode_len_mean: 341.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.4170999999999707\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 283\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6765416198306613\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010287904677237636\n",
      "          policy_loss: 0.05228787221842342\n",
      "          total_loss: 0.0531657214793894\n",
      "          vf_explained_var: 0.48273319005966187\n",
      "          vf_loss: 0.010698930915289869\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.53783783783786\n",
      "    ram_util_percent: 45.229729729729726\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03754801096836024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.038523205088936\n",
      "    mean_inference_ms: 1.6865053317593401\n",
      "    mean_raw_obs_processing_ms: 1.7237943862797231\n",
      "  time_since_restore: 2582.0119121074677\n",
      "  time_this_iter_s: 25.61111569404602\n",
      "  time_total_s: 2582.0119121074677\n",
      "  timers:\n",
      "    learn_throughput: 1138.535\n",
      "    learn_time_ms: 878.322\n",
      "    load_throughput: 49013.709\n",
      "    load_time_ms: 20.402\n",
      "    sample_throughput: 35.294\n",
      "    sample_time_ms: 28333.542\n",
      "    update_time_ms: 6.315\n",
      "  timestamp: 1634845803\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 100\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         2582.01</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\"> -3.4171</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            341.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 101000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-50-33\n",
      "  done: false\n",
      "  episode_len_mean: 342.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.421999999999971\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 285\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.524422726366255\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007118011638650914\n",
      "          policy_loss: -0.08772384002804756\n",
      "          total_loss: -0.08994325829876794\n",
      "          vf_explained_var: 0.5794902443885803\n",
      "          vf_loss: 0.008220147834314654\n",
      "    num_agent_steps_sampled: 101000\n",
      "    num_agent_steps_trained: 101000\n",
      "    num_steps_sampled: 101000\n",
      "    num_steps_trained: 101000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.56904761904761\n",
      "    ram_util_percent: 45.25\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037559132692393964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.053774873908573\n",
      "    mean_inference_ms: 1.686797270211101\n",
      "    mean_raw_obs_processing_ms: 1.7236307770215924\n",
      "  time_since_restore: 2611.855197906494\n",
      "  time_this_iter_s: 29.84328579902649\n",
      "  time_total_s: 2611.855197906494\n",
      "  timers:\n",
      "    learn_throughput: 1146.942\n",
      "    learn_time_ms: 871.884\n",
      "    load_throughput: 48817.41\n",
      "    load_time_ms: 20.484\n",
      "    sample_throughput: 34.834\n",
      "    sample_time_ms: 28707.501\n",
      "    update_time_ms: 5.273\n",
      "  timestamp: 1634845833\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 101000\n",
      "  training_iteration: 101\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         2611.86</td><td style=\"text-align: right;\">101000</td><td style=\"text-align: right;\">  -3.422</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">             342.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 102000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-51-00\n",
      "  done: false\n",
      "  episode_len_mean: 344.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.445199999999971\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 288\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.770877312289344\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010950622387989817\n",
      "          policy_loss: 0.037512409438689547\n",
      "          total_loss: 0.0318484420577685\n",
      "          vf_explained_var: 0.6265181303024292\n",
      "          vf_loss: 0.004653137544583943\n",
      "    num_agent_steps_sampled: 102000\n",
      "    num_agent_steps_trained: 102000\n",
      "    num_steps_sampled: 102000\n",
      "    num_steps_trained: 102000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.56153846153846\n",
      "    ram_util_percent: 44.94358974358974\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03757656850562701\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.076732636917715\n",
      "    mean_inference_ms: 1.6872679571879894\n",
      "    mean_raw_obs_processing_ms: 1.7233835815404022\n",
      "  time_since_restore: 2638.824585199356\n",
      "  time_this_iter_s: 26.96938729286194\n",
      "  time_total_s: 2638.824585199356\n",
      "  timers:\n",
      "    learn_throughput: 1149.171\n",
      "    learn_time_ms: 870.192\n",
      "    load_throughput: 51818.764\n",
      "    load_time_ms: 19.298\n",
      "    sample_throughput: 36.706\n",
      "    sample_time_ms: 27243.151\n",
      "    update_time_ms: 5.197\n",
      "  timestamp: 1634845860\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 102000\n",
      "  training_iteration: 102\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         2638.82</td><td style=\"text-align: right;\">102000</td><td style=\"text-align: right;\"> -3.4452</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            344.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 103000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-51-27\n",
      "  done: false\n",
      "  episode_len_mean: 346.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.4661999999999704\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 291\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7108456505669487\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009451068811840062\n",
      "          policy_loss: 0.06405104804370139\n",
      "          total_loss: 0.05766386356618669\n",
      "          vf_explained_var: 0.6868724226951599\n",
      "          vf_loss: 0.004341797127077977\n",
      "    num_agent_steps_sampled: 103000\n",
      "    num_agent_steps_trained: 103000\n",
      "    num_steps_sampled: 103000\n",
      "    num_steps_trained: 103000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.4102564102564\n",
      "    ram_util_percent: 45.64102564102564\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03759513737482644\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.100045537360558\n",
      "    mean_inference_ms: 1.6877662728245448\n",
      "    mean_raw_obs_processing_ms: 1.7231476053097978\n",
      "  time_since_restore: 2666.3069756031036\n",
      "  time_this_iter_s: 27.48239040374756\n",
      "  time_total_s: 2666.3069756031036\n",
      "  timers:\n",
      "    learn_throughput: 1139.537\n",
      "    learn_time_ms: 877.55\n",
      "    load_throughput: 51735.99\n",
      "    load_time_ms: 19.329\n",
      "    sample_throughput: 36.844\n",
      "    sample_time_ms: 27141.1\n",
      "    update_time_ms: 5.114\n",
      "  timestamp: 1634845887\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 103000\n",
      "  training_iteration: 103\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         2666.31</td><td style=\"text-align: right;\">103000</td><td style=\"text-align: right;\"> -3.4662</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            346.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-51-55\n",
      "  done: false\n",
      "  episode_len_mean: 347.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.47729999999997\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 293\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.761409346262614\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010500487429519501\n",
      "          policy_loss: -0.08507671604553858\n",
      "          total_loss: -0.08928972002532747\n",
      "          vf_explained_var: 0.18073861300945282\n",
      "          vf_loss: 0.006313257302261061\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.85\n",
      "    ram_util_percent: 45.7275\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03760795456660987\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.115997660143197\n",
      "    mean_inference_ms: 1.6881126691383699\n",
      "    mean_raw_obs_processing_ms: 1.7229931460800898\n",
      "  time_since_restore: 2694.159978866577\n",
      "  time_this_iter_s: 27.85300326347351\n",
      "  time_total_s: 2694.159978866577\n",
      "  timers:\n",
      "    learn_throughput: 1140.184\n",
      "    learn_time_ms: 877.051\n",
      "    load_throughput: 51535.18\n",
      "    load_time_ms: 19.404\n",
      "    sample_throughput: 36.578\n",
      "    sample_time_ms: 27338.878\n",
      "    update_time_ms: 5.066\n",
      "  timestamp: 1634845915\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 104\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         2694.16</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\"> -3.4773</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            347.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 105000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-52-21\n",
      "  done: false\n",
      "  episode_len_mean: 349.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.4957999999999703\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 296\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7641837000846863\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009667610971264177\n",
      "          policy_loss: -0.0009599348737133874\n",
      "          total_loss: -0.007811731431219313\n",
      "          vf_explained_var: 0.8548159003257751\n",
      "          vf_loss: 0.004264403839543875\n",
      "    num_agent_steps_sampled: 105000\n",
      "    num_agent_steps_trained: 105000\n",
      "    num_steps_sampled: 105000\n",
      "    num_steps_trained: 105000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.65675675675676\n",
      "    ram_util_percent: 45.77297297297297\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03762805078651982\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.139466028297196\n",
      "    mean_inference_ms: 1.6886423688733885\n",
      "    mean_raw_obs_processing_ms: 1.7229822155548613\n",
      "  time_since_restore: 2720.4050447940826\n",
      "  time_this_iter_s: 26.245065927505493\n",
      "  time_total_s: 2720.4050447940826\n",
      "  timers:\n",
      "    learn_throughput: 1079.386\n",
      "    learn_time_ms: 926.453\n",
      "    load_throughput: 50732.741\n",
      "    load_time_ms: 19.711\n",
      "    sample_throughput: 38.944\n",
      "    sample_time_ms: 25678.137\n",
      "    update_time_ms: 5.012\n",
      "  timestamp: 1634845941\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 105000\n",
      "  training_iteration: 105\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         2720.41</td><td style=\"text-align: right;\">105000</td><td style=\"text-align: right;\"> -3.4958</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            349.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 106000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-52-51\n",
      "  done: false\n",
      "  episode_len_mean: 351.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.5102999999999698\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 299\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7562740431891548\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007175060868073283\n",
      "          policy_loss: 0.06277348846197128\n",
      "          total_loss: 0.060313392761680816\n",
      "          vf_explained_var: 0.655014157295227\n",
      "          vf_loss: 0.01025947879275514\n",
      "    num_agent_steps_sampled: 106000\n",
      "    num_agent_steps_trained: 106000\n",
      "    num_steps_sampled: 106000\n",
      "    num_steps_trained: 106000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.81627906976745\n",
      "    ram_util_percent: 46.07209302325582\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03765179995413273\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.163543474356484\n",
      "    mean_inference_ms: 1.689207966755877\n",
      "    mean_raw_obs_processing_ms: 1.7231877760542003\n",
      "  time_since_restore: 2750.2076139450073\n",
      "  time_this_iter_s: 29.802569150924683\n",
      "  time_total_s: 2750.2076139450073\n",
      "  timers:\n",
      "    learn_throughput: 1069.246\n",
      "    learn_time_ms: 935.238\n",
      "    load_throughput: 50599.99\n",
      "    load_time_ms: 19.763\n",
      "    sample_throughput: 38.023\n",
      "    sample_time_ms: 26299.849\n",
      "    update_time_ms: 4.286\n",
      "  timestamp: 1634845971\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 106000\n",
      "  training_iteration: 106\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         2750.21</td><td style=\"text-align: right;\">106000</td><td style=\"text-align: right;\"> -3.5103</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            351.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 107000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-53-36\n",
      "  done: false\n",
      "  episode_len_mean: 352.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.5234999999999688\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 302\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.723007196850247\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008972413121857568\n",
      "          policy_loss: -0.0461468104687002\n",
      "          total_loss: -0.045427858498361375\n",
      "          vf_explained_var: 0.3567286729812622\n",
      "          vf_loss: 0.011892641141700248\n",
      "    num_agent_steps_sampled: 107000\n",
      "    num_agent_steps_trained: 107000\n",
      "    num_steps_sampled: 107000\n",
      "    num_steps_trained: 107000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.46190476190473\n",
      "    ram_util_percent: 46.07619047619046\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03767649360830842\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.188202262302877\n",
      "    mean_inference_ms: 1.6898069014528618\n",
      "    mean_raw_obs_processing_ms: 1.7284402608640483\n",
      "  time_since_restore: 2794.8220613002777\n",
      "  time_this_iter_s: 44.614447355270386\n",
      "  time_total_s: 2794.8220613002777\n",
      "  timers:\n",
      "    learn_throughput: 1071.689\n",
      "    learn_time_ms: 933.107\n",
      "    load_throughput: 51319.466\n",
      "    load_time_ms: 19.486\n",
      "    sample_throughput: 35.584\n",
      "    sample_time_ms: 28102.434\n",
      "    update_time_ms: 4.29\n",
      "  timestamp: 1634846016\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 107000\n",
      "  training_iteration: 107\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         2794.82</td><td style=\"text-align: right;\">107000</td><td style=\"text-align: right;\"> -3.5235</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            352.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-54-01\n",
      "  done: false\n",
      "  episode_len_mean: 353.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.5340999999999685\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 304\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8178303546375698\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011001014188769039\n",
      "          policy_loss: 0.007645521892441644\n",
      "          total_loss: 0.004443001829915577\n",
      "          vf_explained_var: 0.5125846266746521\n",
      "          vf_loss: 0.007550096118615734\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.57567567567568\n",
      "    ram_util_percent: 46.00000000000001\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03769324617059796\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.204786909698587\n",
      "    mean_inference_ms: 1.6902140019046608\n",
      "    mean_raw_obs_processing_ms: 1.7320324442412498\n",
      "  time_since_restore: 2820.274957895279\n",
      "  time_this_iter_s: 25.45289659500122\n",
      "  time_total_s: 2820.274957895279\n",
      "  timers:\n",
      "    learn_throughput: 1082.415\n",
      "    learn_time_ms: 923.86\n",
      "    load_throughput: 52306.99\n",
      "    load_time_ms: 19.118\n",
      "    sample_throughput: 35.558\n",
      "    sample_time_ms: 28123.371\n",
      "    update_time_ms: 4.084\n",
      "  timestamp: 1634846041\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 108\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         2820.27</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\"> -3.5341</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            353.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 109000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-54-25\n",
      "  done: false\n",
      "  episode_len_mean: 356.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.563199999999968\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 307\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.829928465684255\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012614137315900997\n",
      "          policy_loss: -0.03336004780398475\n",
      "          total_loss: -0.03362179969747861\n",
      "          vf_explained_var: 0.35329627990722656\n",
      "          vf_loss: 0.00952299019942681\n",
      "    num_agent_steps_sampled: 109000\n",
      "    num_agent_steps_trained: 109000\n",
      "    num_steps_sampled: 109000\n",
      "    num_steps_trained: 109000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.73529411764706\n",
      "    ram_util_percent: 46.029411764705884\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037718836017165185\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.22905029672696\n",
      "    mean_inference_ms: 1.690836520580857\n",
      "    mean_raw_obs_processing_ms: 1.7373706941390816\n",
      "  time_since_restore: 2844.2120656967163\n",
      "  time_this_iter_s: 23.937107801437378\n",
      "  time_total_s: 2844.2120656967163\n",
      "  timers:\n",
      "    learn_throughput: 1093.544\n",
      "    learn_time_ms: 914.458\n",
      "    load_throughput: 57268.254\n",
      "    load_time_ms: 17.462\n",
      "    sample_throughput: 35.917\n",
      "    sample_time_ms: 27841.701\n",
      "    update_time_ms: 3.438\n",
      "  timestamp: 1634846065\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 109000\n",
      "  training_iteration: 109\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         2844.21</td><td style=\"text-align: right;\">109000</td><td style=\"text-align: right;\"> -3.5632</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            356.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 110000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-54-49\n",
      "  done: false\n",
      "  episode_len_mean: 357.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.5744999999999676\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 309\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.75730472140842\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006185479392753616\n",
      "          policy_loss: 0.044134987890720366\n",
      "          total_loss: 0.03814969989988539\n",
      "          vf_explained_var: 0.293763130903244\n",
      "          vf_loss: 0.007412558805663138\n",
      "    num_agent_steps_sampled: 110000\n",
      "    num_agent_steps_trained: 110000\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.50588235294117\n",
      "    ram_util_percent: 45.97941176470589\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03773620211363751\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.24476611946164\n",
      "    mean_inference_ms: 1.6912607276519207\n",
      "    mean_raw_obs_processing_ms: 1.7386917622844846\n",
      "  time_since_restore: 2867.9434707164764\n",
      "  time_this_iter_s: 23.731405019760132\n",
      "  time_total_s: 2867.9434707164764\n",
      "  timers:\n",
      "    learn_throughput: 1099.171\n",
      "    learn_time_ms: 909.777\n",
      "    load_throughput: 52744.874\n",
      "    load_time_ms: 18.959\n",
      "    sample_throughput: 36.157\n",
      "    sample_time_ms: 27657.258\n",
      "    update_time_ms: 3.08\n",
      "  timestamp: 1634846089\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 110\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         2867.94</td><td style=\"text-align: right;\">110000</td><td style=\"text-align: right;\"> -3.5745</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            357.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 111000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-55-15\n",
      "  done: false\n",
      "  episode_len_mean: 359.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.5936999999999664\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 312\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8498931659592524\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007388139986647207\n",
      "          policy_loss: 0.04091938626435068\n",
      "          total_loss: 0.035551922768354415\n",
      "          vf_explained_var: -0.0609564371407032\n",
      "          vf_loss: 0.008144473657011986\n",
      "    num_agent_steps_sampled: 111000\n",
      "    num_agent_steps_trained: 111000\n",
      "    num_steps_sampled: 111000\n",
      "    num_steps_trained: 111000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.42432432432435\n",
      "    ram_util_percent: 45.991891891891896\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037762646943226376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.268184749457077\n",
      "    mean_inference_ms: 1.6919038129360808\n",
      "    mean_raw_obs_processing_ms: 1.7373494805233676\n",
      "  time_since_restore: 2893.7386581897736\n",
      "  time_this_iter_s: 25.79518747329712\n",
      "  time_total_s: 2893.7386581897736\n",
      "  timers:\n",
      "    learn_throughput: 1103.156\n",
      "    learn_time_ms: 906.49\n",
      "    load_throughput: 53948.044\n",
      "    load_time_ms: 18.536\n",
      "    sample_throughput: 36.689\n",
      "    sample_time_ms: 27256.109\n",
      "    update_time_ms: 3.144\n",
      "  timestamp: 1634846115\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 111000\n",
      "  training_iteration: 111\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         2893.74</td><td style=\"text-align: right;\">111000</td><td style=\"text-align: right;\"> -3.5937</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            359.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-55-41\n",
      "  done: false\n",
      "  episode_len_mean: 360.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.603799999999967\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 314\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6566617117987739\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011054015652901702\n",
      "          policy_loss: -0.11250258071555032\n",
      "          total_loss: -0.11106710367732578\n",
      "          vf_explained_var: 0.14755181968212128\n",
      "          vf_loss: 0.010540633876290586\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.47837837837838\n",
      "    ram_util_percent: 46.008108108108104\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03778101614156298\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.28373375500829\n",
      "    mean_inference_ms: 1.6923388121318725\n",
      "    mean_raw_obs_processing_ms: 1.7364540915174325\n",
      "  time_since_restore: 2919.9165263175964\n",
      "  time_this_iter_s: 26.177868127822876\n",
      "  time_total_s: 2919.9165263175964\n",
      "  timers:\n",
      "    learn_throughput: 1094.732\n",
      "    learn_time_ms: 913.466\n",
      "    load_throughput: 54795.557\n",
      "    load_time_ms: 18.25\n",
      "    sample_throughput: 36.805\n",
      "    sample_time_ms: 27170.506\n",
      "    update_time_ms: 3.029\n",
      "  timestamp: 1634846141\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 112\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         2919.92</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\"> -3.6038</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            360.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 113000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-56-03\n",
      "  done: false\n",
      "  episode_len_mean: 362.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8799999999999826\n",
      "  episode_reward_mean: -3.627399999999966\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 317\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6822173648410372\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009253425540809504\n",
      "          policy_loss: 0.004041025539239248\n",
      "          total_loss: 0.0051990896463394165\n",
      "          vf_explained_var: 0.2789665162563324\n",
      "          vf_loss: 0.011734175979573694\n",
      "    num_agent_steps_sampled: 113000\n",
      "    num_agent_steps_trained: 113000\n",
      "    num_steps_sampled: 113000\n",
      "    num_steps_trained: 113000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.678125\n",
      "    ram_util_percent: 45.971875000000004\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03780873286658809\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.306202732454945\n",
      "    mean_inference_ms: 1.692990777010439\n",
      "    mean_raw_obs_processing_ms: 1.7351242019752997\n",
      "  time_since_restore: 2942.1355090141296\n",
      "  time_this_iter_s: 22.218982696533203\n",
      "  time_total_s: 2942.1355090141296\n",
      "  timers:\n",
      "    learn_throughput: 1097.534\n",
      "    learn_time_ms: 911.133\n",
      "    load_throughput: 54832.234\n",
      "    load_time_ms: 18.237\n",
      "    sample_throughput: 37.529\n",
      "    sample_time_ms: 26646.2\n",
      "    update_time_ms: 3.337\n",
      "  timestamp: 1634846163\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 113000\n",
      "  training_iteration: 113\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         2942.14</td><td style=\"text-align: right;\">113000</td><td style=\"text-align: right;\"> -3.6274</td><td style=\"text-align: right;\">               -2.88</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            362.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 114000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-56-28\n",
      "  done: false\n",
      "  episode_len_mean: 364.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.9399999999999813\n",
      "  episode_reward_mean: -3.648699999999966\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 319\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9094383001327515\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009452763569628337\n",
      "          policy_loss: -0.11542415105634266\n",
      "          total_loss: -0.12067046115795771\n",
      "          vf_explained_var: 0.5128559470176697\n",
      "          vf_loss: 0.007467454652457187\n",
      "    num_agent_steps_sampled: 114000\n",
      "    num_agent_steps_trained: 114000\n",
      "    num_steps_sampled: 114000\n",
      "    num_steps_trained: 114000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.80000000000001\n",
      "    ram_util_percent: 46.099999999999994\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0378275038708157\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.321005434239364\n",
      "    mean_inference_ms: 1.6934317339469032\n",
      "    mean_raw_obs_processing_ms: 1.7343053620130702\n",
      "  time_since_restore: 2967.162703514099\n",
      "  time_this_iter_s: 25.027194499969482\n",
      "  time_total_s: 2967.162703514099\n",
      "  timers:\n",
      "    learn_throughput: 1098.283\n",
      "    learn_time_ms: 910.512\n",
      "    load_throughput: 52903.476\n",
      "    load_time_ms: 18.902\n",
      "    sample_throughput: 37.931\n",
      "    sample_time_ms: 26363.533\n",
      "    update_time_ms: 3.315\n",
      "  timestamp: 1634846188\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 114000\n",
      "  training_iteration: 114\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         2967.16</td><td style=\"text-align: right;\">114000</td><td style=\"text-align: right;\"> -3.6487</td><td style=\"text-align: right;\">               -2.94</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            364.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 115000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-56-54\n",
      "  done: false\n",
      "  episode_len_mean: 366.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.959999999999981\n",
      "  episode_reward_mean: -3.666799999999966\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 322\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7485107011265224\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008787533672437724\n",
      "          policy_loss: -0.007612153970532947\n",
      "          total_loss: -0.009824752228127585\n",
      "          vf_explained_var: 0.4154188334941864\n",
      "          vf_loss: 0.009340921599262704\n",
      "    num_agent_steps_sampled: 115000\n",
      "    num_agent_steps_trained: 115000\n",
      "    num_steps_sampled: 115000\n",
      "    num_steps_trained: 115000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.71666666666667\n",
      "    ram_util_percent: 46.08888888888889\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037856202056700476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.343066909308728\n",
      "    mean_inference_ms: 1.6941030030346593\n",
      "    mean_raw_obs_processing_ms: 1.7331868417953828\n",
      "  time_since_restore: 2992.5311994552612\n",
      "  time_this_iter_s: 25.36849594116211\n",
      "  time_total_s: 2992.5311994552612\n",
      "  timers:\n",
      "    learn_throughput: 1162.391\n",
      "    learn_time_ms: 860.296\n",
      "    load_throughput: 52757.214\n",
      "    load_time_ms: 18.955\n",
      "    sample_throughput: 37.985\n",
      "    sample_time_ms: 26326.229\n",
      "    update_time_ms: 3.038\n",
      "  timestamp: 1634846214\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 115000\n",
      "  training_iteration: 115\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         2992.53</td><td style=\"text-align: right;\">115000</td><td style=\"text-align: right;\"> -3.6668</td><td style=\"text-align: right;\">               -2.96</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            366.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-57-22\n",
      "  done: false\n",
      "  episode_len_mean: 368.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.959999999999981\n",
      "  episode_reward_mean: -3.680799999999966\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 324\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.774581205844879\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00954129687696644\n",
      "          policy_loss: -0.11455721706151963\n",
      "          total_loss: -0.11615499961707328\n",
      "          vf_explained_var: 0.330475389957428\n",
      "          vf_loss: 0.00970765218242175\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.9075\n",
      "    ram_util_percent: 45.942499999999995\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03787572396136709\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.358275032633973\n",
      "    mean_inference_ms: 1.6945622283155999\n",
      "    mean_raw_obs_processing_ms: 1.7324452688784913\n",
      "  time_since_restore: 3020.562376976013\n",
      "  time_this_iter_s: 28.031177520751953\n",
      "  time_total_s: 3020.562376976013\n",
      "  timers:\n",
      "    learn_throughput: 1171.134\n",
      "    learn_time_ms: 853.873\n",
      "    load_throughput: 52843.356\n",
      "    load_time_ms: 18.924\n",
      "    sample_throughput: 38.234\n",
      "    sample_time_ms: 26155.043\n",
      "    update_time_ms: 3.382\n",
      "  timestamp: 1634846242\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 116\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         3020.56</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\"> -3.6808</td><td style=\"text-align: right;\">               -2.96</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            368.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 117000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-57-50\n",
      "  done: false\n",
      "  episode_len_mean: 370.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.959999999999981\n",
      "  episode_reward_mean: -3.7045999999999646\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 327\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.750360295507643\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010229883145609787\n",
      "          policy_loss: -0.04675368327233526\n",
      "          total_loss: -0.04984231276644601\n",
      "          vf_explained_var: 0.18529391288757324\n",
      "          vf_loss: 0.007509802734582788\n",
      "    num_agent_steps_sampled: 117000\n",
      "    num_agent_steps_trained: 117000\n",
      "    num_steps_sampled: 117000\n",
      "    num_steps_trained: 117000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.45897435897436\n",
      "    ram_util_percent: 45.87179487179487\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037906039602512244\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.38043887870404\n",
      "    mean_inference_ms: 1.6952769403861145\n",
      "    mean_raw_obs_processing_ms: 1.7315369275695065\n",
      "  time_since_restore: 3048.3482933044434\n",
      "  time_this_iter_s: 27.785916328430176\n",
      "  time_total_s: 3048.3482933044434\n",
      "  timers:\n",
      "    learn_throughput: 1106.984\n",
      "    learn_time_ms: 903.355\n",
      "    load_throughput: 52691.666\n",
      "    load_time_ms: 18.978\n",
      "    sample_throughput: 40.946\n",
      "    sample_time_ms: 24422.648\n",
      "    update_time_ms: 3.37\n",
      "  timestamp: 1634846270\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 117000\n",
      "  training_iteration: 117\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         3048.35</td><td style=\"text-align: right;\">117000</td><td style=\"text-align: right;\"> -3.7046</td><td style=\"text-align: right;\">               -2.96</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            370.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 118000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-58-32\n",
      "  done: false\n",
      "  episode_len_mean: 371.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.959999999999981\n",
      "  episode_reward_mean: -3.7180999999999647\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 330\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.847240924835205\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008775943006681964\n",
      "          policy_loss: 0.020769144346316656\n",
      "          total_loss: 0.01619096663263109\n",
      "          vf_explained_var: 0.23511812090873718\n",
      "          vf_loss: 0.007970466610923823\n",
      "    num_agent_steps_sampled: 118000\n",
      "    num_agent_steps_trained: 118000\n",
      "    num_steps_sampled: 118000\n",
      "    num_steps_trained: 118000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.4950819672131\n",
      "    ram_util_percent: 45.959016393442624\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03793715456941475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.402886315284814\n",
      "    mean_inference_ms: 1.6960129017687793\n",
      "    mean_raw_obs_processing_ms: 1.7349661767288305\n",
      "  time_since_restore: 3090.960499048233\n",
      "  time_this_iter_s: 42.61220574378967\n",
      "  time_total_s: 3090.960499048233\n",
      "  timers:\n",
      "    learn_throughput: 1096.69\n",
      "    learn_time_ms: 911.835\n",
      "    load_throughput: 53492.294\n",
      "    load_time_ms: 18.694\n",
      "    sample_throughput: 38.27\n",
      "    sample_time_ms: 26130.044\n",
      "    update_time_ms: 3.608\n",
      "  timestamp: 1634846312\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 118000\n",
      "  training_iteration: 118\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         3090.96</td><td style=\"text-align: right;\">118000</td><td style=\"text-align: right;\"> -3.7181</td><td style=\"text-align: right;\">               -2.96</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            371.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 119000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-59-05\n",
      "  done: false\n",
      "  episode_len_mean: 372.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.959999999999981\n",
      "  episode_reward_mean: -3.727799999999965\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 332\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7840480897161695\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007543980298829488\n",
      "          policy_loss: -0.11097852033045556\n",
      "          total_loss: -0.11272697606020504\n",
      "          vf_explained_var: 0.4655277132987976\n",
      "          vf_loss: 0.010999835555493418\n",
      "    num_agent_steps_sampled: 119000\n",
      "    num_agent_steps_trained: 119000\n",
      "    num_steps_sampled: 119000\n",
      "    num_steps_trained: 119000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.67021276595743\n",
      "    ram_util_percent: 46.480851063829775\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037958778746290237\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.419025086475884\n",
      "    mean_inference_ms: 1.696524302052735\n",
      "    mean_raw_obs_processing_ms: 1.7373446077901251\n",
      "  time_since_restore: 3123.674141407013\n",
      "  time_this_iter_s: 32.71364235877991\n",
      "  time_total_s: 3123.674141407013\n",
      "  timers:\n",
      "    learn_throughput: 1089.344\n",
      "    learn_time_ms: 917.983\n",
      "    load_throughput: 53665.522\n",
      "    load_time_ms: 18.634\n",
      "    sample_throughput: 37.036\n",
      "    sample_time_ms: 27000.706\n",
      "    update_time_ms: 4.392\n",
      "  timestamp: 1634846345\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 119000\n",
      "  training_iteration: 119\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         3123.67</td><td style=\"text-align: right;\">119000</td><td style=\"text-align: right;\"> -3.7278</td><td style=\"text-align: right;\">               -2.96</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            372.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_19-59-35\n",
      "  done: false\n",
      "  episode_len_mean: 373.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.959999999999981\n",
      "  episode_reward_mean: -3.7380999999999642\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 335\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7792821884155274\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009985876547491302\n",
      "          policy_loss: -0.019977965205907822\n",
      "          total_loss: -0.02063113475839297\n",
      "          vf_explained_var: 0.5690925121307373\n",
      "          vf_loss: 0.010399183073443258\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.67209302325581\n",
      "    ram_util_percent: 46.70930232558139\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03799188897124907\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.44447900015976\n",
      "    mean_inference_ms: 1.6973139740709053\n",
      "    mean_raw_obs_processing_ms: 1.740880931140495\n",
      "  time_since_restore: 3153.6395316123962\n",
      "  time_this_iter_s: 29.9653902053833\n",
      "  time_total_s: 3153.6395316123962\n",
      "  timers:\n",
      "    learn_throughput: 1090.313\n",
      "    learn_time_ms: 917.168\n",
      "    load_throughput: 58347.01\n",
      "    load_time_ms: 17.139\n",
      "    sample_throughput: 36.198\n",
      "    sample_time_ms: 27625.711\n",
      "    update_time_ms: 4.675\n",
      "  timestamp: 1634846375\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 120\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">         3153.64</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\"> -3.7381</td><td style=\"text-align: right;\">               -2.96</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            373.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 121000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-00-02\n",
      "  done: false\n",
      "  episode_len_mean: 374.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.959999999999981\n",
      "  episode_reward_mean: -3.7433999999999634\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 338\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7643639167149863\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010289313119310231\n",
      "          policy_loss: -0.02617086172103882\n",
      "          total_loss: -0.025438368486033546\n",
      "          vf_explained_var: -0.0695653185248375\n",
      "          vf_loss: 0.011430847062729298\n",
      "    num_agent_steps_sampled: 121000\n",
      "    num_agent_steps_trained: 121000\n",
      "    num_steps_sampled: 121000\n",
      "    num_steps_trained: 121000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.67631578947366\n",
      "    ram_util_percent: 46.455263157894734\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0380251604547085\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.47020759942644\n",
      "    mean_inference_ms: 1.698107932326471\n",
      "    mean_raw_obs_processing_ms: 1.7445492398132\n",
      "  time_since_restore: 3180.5994594097137\n",
      "  time_this_iter_s: 26.959927797317505\n",
      "  time_total_s: 3180.5994594097137\n",
      "  timers:\n",
      "    learn_throughput: 1085.213\n",
      "    learn_time_ms: 921.478\n",
      "    load_throughput: 58110.318\n",
      "    load_time_ms: 17.209\n",
      "    sample_throughput: 36.052\n",
      "    sample_time_ms: 27737.806\n",
      "    update_time_ms: 4.778\n",
      "  timestamp: 1634846402\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 121000\n",
      "  training_iteration: 121\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">          3180.6</td><td style=\"text-align: right;\">121000</td><td style=\"text-align: right;\"> -3.7434</td><td style=\"text-align: right;\">               -2.96</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            374.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 122000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-00-30\n",
      "  done: false\n",
      "  episode_len_mean: 375.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.959999999999981\n",
      "  episode_reward_mean: -3.7592999999999632\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 341\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8270638492372302\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008870821051451866\n",
      "          policy_loss: -0.011909172518385781\n",
      "          total_loss: -0.016151626573668587\n",
      "          vf_explained_var: 0.632205069065094\n",
      "          vf_loss: 0.008040380427458634\n",
      "    num_agent_steps_sampled: 122000\n",
      "    num_agent_steps_trained: 122000\n",
      "    num_steps_sampled: 122000\n",
      "    num_steps_trained: 122000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.74634146341464\n",
      "    ram_util_percent: 46.609756097560975\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038058972803025955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.495867443274555\n",
      "    mean_inference_ms: 1.6989256407877886\n",
      "    mean_raw_obs_processing_ms: 1.74439688087324\n",
      "  time_since_restore: 3209.0973823070526\n",
      "  time_this_iter_s: 28.497922897338867\n",
      "  time_total_s: 3209.0973823070526\n",
      "  timers:\n",
      "    learn_throughput: 1093.446\n",
      "    learn_time_ms: 914.54\n",
      "    load_throughput: 57499.777\n",
      "    load_time_ms: 17.391\n",
      "    sample_throughput: 35.745\n",
      "    sample_time_ms: 27975.701\n",
      "    update_time_ms: 5.591\n",
      "  timestamp: 1634846430\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 122000\n",
      "  training_iteration: 122\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">          3209.1</td><td style=\"text-align: right;\">122000</td><td style=\"text-align: right;\"> -3.7593</td><td style=\"text-align: right;\">               -2.96</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            375.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 123000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-00-57\n",
      "  done: false\n",
      "  episode_len_mean: 376.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.959999999999981\n",
      "  episode_reward_mean: -3.7675999999999634\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 343\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8158988793691\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010288687287801837\n",
      "          policy_loss: -0.13609144025378758\n",
      "          total_loss: -0.1449117393957244\n",
      "          vf_explained_var: 0.7783871293067932\n",
      "          vf_loss: 0.0023938243529604128\n",
      "    num_agent_steps_sampled: 123000\n",
      "    num_agent_steps_trained: 123000\n",
      "    num_steps_sampled: 123000\n",
      "    num_steps_trained: 123000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.03421052631579\n",
      "    ram_util_percent: 46.681578947368415\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03808166327989936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.512865286779416\n",
      "    mean_inference_ms: 1.6994810092193893\n",
      "    mean_raw_obs_processing_ms: 1.7430209757430168\n",
      "  time_since_restore: 3235.4409806728363\n",
      "  time_this_iter_s: 26.34359836578369\n",
      "  time_total_s: 3235.4409806728363\n",
      "  timers:\n",
      "    learn_throughput: 1098.846\n",
      "    learn_time_ms: 910.045\n",
      "    load_throughput: 54018.914\n",
      "    load_time_ms: 18.512\n",
      "    sample_throughput: 35.221\n",
      "    sample_time_ms: 28391.953\n",
      "    update_time_ms: 5.259\n",
      "  timestamp: 1634846457\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 123000\n",
      "  training_iteration: 123\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         3235.44</td><td style=\"text-align: right;\">123000</td><td style=\"text-align: right;\"> -3.7676</td><td style=\"text-align: right;\">               -2.96</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            376.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-01-24\n",
      "  done: false\n",
      "  episode_len_mean: 377.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.959999999999981\n",
      "  episode_reward_mean: -3.7713999999999634\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 346\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7542603386773004\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011257231260314802\n",
      "          policy_loss: -0.09330875972906748\n",
      "          total_loss: -0.08993862494826317\n",
      "          vf_explained_var: 0.40646666288375854\n",
      "          vf_loss: 0.013314104005176988\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.1076923076923\n",
      "    ram_util_percent: 46.5025641025641\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038115779992987736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.538776112493387\n",
      "    mean_inference_ms: 1.7003156538810977\n",
      "    mean_raw_obs_processing_ms: 1.7410459156182716\n",
      "  time_since_restore: 3262.991712331772\n",
      "  time_this_iter_s: 27.550731658935547\n",
      "  time_total_s: 3262.991712331772\n",
      "  timers:\n",
      "    learn_throughput: 1100.162\n",
      "    learn_time_ms: 908.957\n",
      "    load_throughput: 56161.279\n",
      "    load_time_ms: 17.806\n",
      "    sample_throughput: 34.91\n",
      "    sample_time_ms: 28645.295\n",
      "    update_time_ms: 5.975\n",
      "  timestamp: 1634846484\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 124\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         3262.99</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\"> -3.7714</td><td style=\"text-align: right;\">               -2.96</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            377.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 125000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-01-51\n",
      "  done: false\n",
      "  episode_len_mean: 378.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.959999999999981\n",
      "  episode_reward_mean: -3.783999999999963\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 349\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7611743463410272\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010601608458577709\n",
      "          policy_loss: 0.00363114294078615\n",
      "          total_loss: 0.0025654060973061455\n",
      "          vf_explained_var: 0.3454798460006714\n",
      "          vf_loss: 0.009389924589130613\n",
      "    num_agent_steps_sampled: 125000\n",
      "    num_agent_steps_trained: 125000\n",
      "    num_steps_sampled: 125000\n",
      "    num_steps_trained: 125000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.74054054054054\n",
      "    ram_util_percent: 46.62702702702702\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038150383681957084\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.564442852464087\n",
      "    mean_inference_ms: 1.7011578595793098\n",
      "    mean_raw_obs_processing_ms: 1.7392461203512934\n",
      "  time_since_restore: 3289.2857115268707\n",
      "  time_this_iter_s: 26.293999195098877\n",
      "  time_total_s: 3289.2857115268707\n",
      "  timers:\n",
      "    learn_throughput: 1101.027\n",
      "    learn_time_ms: 908.243\n",
      "    load_throughput: 57176.68\n",
      "    load_time_ms: 17.49\n",
      "    sample_throughput: 34.796\n",
      "    sample_time_ms: 28738.769\n",
      "    update_time_ms: 5.9\n",
      "  timestamp: 1634846511\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 125000\n",
      "  training_iteration: 125\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         3289.29</td><td style=\"text-align: right;\">125000</td><td style=\"text-align: right;\">  -3.784</td><td style=\"text-align: right;\">               -2.96</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">             378.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 126000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-02-19\n",
      "  done: false\n",
      "  episode_len_mean: 379.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0199999999999796\n",
      "  episode_reward_mean: -3.7972999999999626\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 352\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7612710899776882\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009736842260675754\n",
      "          policy_loss: 0.03256858959794044\n",
      "          total_loss: 0.025670911040571003\n",
      "          vf_explained_var: 0.6357409358024597\n",
      "          vf_loss: 0.004142666213576578\n",
      "    num_agent_steps_sampled: 126000\n",
      "    num_agent_steps_trained: 126000\n",
      "    num_steps_sampled: 126000\n",
      "    num_steps_trained: 126000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.93170731707319\n",
      "    ram_util_percent: 46.57317073170732\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03818527550282369\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.590303836311694\n",
      "    mean_inference_ms: 1.7020099772896617\n",
      "    mean_raw_obs_processing_ms: 1.7374413128564752\n",
      "  time_since_restore: 3317.9527802467346\n",
      "  time_this_iter_s: 28.66706871986389\n",
      "  time_total_s: 3317.9527802467346\n",
      "  timers:\n",
      "    learn_throughput: 1102.865\n",
      "    learn_time_ms: 906.73\n",
      "    load_throughput: 57021.528\n",
      "    load_time_ms: 17.537\n",
      "    sample_throughput: 34.717\n",
      "    sample_time_ms: 28804.654\n",
      "    update_time_ms: 5.31\n",
      "  timestamp: 1634846539\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 126000\n",
      "  training_iteration: 126\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         3317.95</td><td style=\"text-align: right;\">126000</td><td style=\"text-align: right;\"> -3.7973</td><td style=\"text-align: right;\">               -3.02</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            379.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 127000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-02-46\n",
      "  done: false\n",
      "  episode_len_mean: 380.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0199999999999796\n",
      "  episode_reward_mean: -3.8002999999999627\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 354\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8932478944460551\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010950378584156754\n",
      "          policy_loss: -0.052752756244606444\n",
      "          total_loss: -0.05912778104345004\n",
      "          vf_explained_var: 0.8939918279647827\n",
      "          vf_loss: 0.005165949960549672\n",
      "    num_agent_steps_sampled: 127000\n",
      "    num_agent_steps_trained: 127000\n",
      "    num_steps_sampled: 127000\n",
      "    num_steps_trained: 127000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.77894736842104\n",
      "    ram_util_percent: 46.46578947368421\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03820850673521907\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.60786707553088\n",
      "    mean_inference_ms: 1.7025792300956768\n",
      "    mean_raw_obs_processing_ms: 1.736360519316594\n",
      "  time_since_restore: 3344.186286211014\n",
      "  time_this_iter_s: 26.233505964279175\n",
      "  time_total_s: 3344.186286211014\n",
      "  timers:\n",
      "    learn_throughput: 1164.783\n",
      "    learn_time_ms: 858.529\n",
      "    load_throughput: 56445.307\n",
      "    load_time_ms: 17.716\n",
      "    sample_throughput: 34.847\n",
      "    sample_time_ms: 28697.101\n",
      "    update_time_ms: 5.626\n",
      "  timestamp: 1634846566\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 127000\n",
      "  training_iteration: 127\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         3344.19</td><td style=\"text-align: right;\">127000</td><td style=\"text-align: right;\"> -3.8003</td><td style=\"text-align: right;\">               -3.02</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            380.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-03-13\n",
      "  done: false\n",
      "  episode_len_mean: 380.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -3.8080999999999623\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 357\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.876827727423774\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010663931627965464\n",
      "          policy_loss: -0.034548917495542106\n",
      "          total_loss: -0.04354866908656226\n",
      "          vf_explained_var: 0.9442715048789978\n",
      "          vf_loss: 0.002570370570497794\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.82368421052631\n",
      "    ram_util_percent: 46.526315789473685\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03824360027797789\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.634494992291888\n",
      "    mean_inference_ms: 1.7034366283962425\n",
      "    mean_raw_obs_processing_ms: 1.7349079744218685\n",
      "  time_since_restore: 3371.1715035438538\n",
      "  time_this_iter_s: 26.985217332839966\n",
      "  time_total_s: 3371.1715035438538\n",
      "  timers:\n",
      "    learn_throughput: 1170.902\n",
      "    learn_time_ms: 854.043\n",
      "    load_throughput: 56628.283\n",
      "    load_time_ms: 17.659\n",
      "    sample_throughput: 36.848\n",
      "    sample_time_ms: 27138.553\n",
      "    update_time_ms: 6.019\n",
      "  timestamp: 1634846593\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 128\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">         3371.17</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\"> -3.8081</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            380.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 129000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-03-56\n",
      "  done: false\n",
      "  episode_len_mean: 380.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -3.8084999999999627\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 360\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8296072867181565\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011134066314359044\n",
      "          policy_loss: -0.0919903292424149\n",
      "          total_loss: -0.09916773910323778\n",
      "          vf_explained_var: 0.8877294063568115\n",
      "          vf_loss: 0.003603170209357308\n",
      "    num_agent_steps_sampled: 129000\n",
      "    num_agent_steps_trained: 129000\n",
      "    num_steps_sampled: 129000\n",
      "    num_steps_trained: 129000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.54444444444445\n",
      "    ram_util_percent: 46.66825396825395\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03827813232893542\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.66088446774563\n",
      "    mean_inference_ms: 1.704282152359723\n",
      "    mean_raw_obs_processing_ms: 1.737527154444895\n",
      "  time_since_restore: 3414.817774772644\n",
      "  time_this_iter_s: 43.64627122879028\n",
      "  time_total_s: 3414.817774772644\n",
      "  timers:\n",
      "    learn_throughput: 1183.755\n",
      "    learn_time_ms: 844.769\n",
      "    load_throughput: 55184.217\n",
      "    load_time_ms: 18.121\n",
      "    sample_throughput: 35.409\n",
      "    sample_time_ms: 28241.587\n",
      "    update_time_ms: 5.189\n",
      "  timestamp: 1634846636\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 129000\n",
      "  training_iteration: 129\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         3414.82</td><td style=\"text-align: right;\">129000</td><td style=\"text-align: right;\"> -3.8085</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            380.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 130000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-04-25\n",
      "  done: false\n",
      "  episode_len_mean: 381.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -3.8187999999999622\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 362\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8981634656588235\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016341147453607485\n",
      "          policy_loss: -0.1036822173330519\n",
      "          total_loss: -0.1005121524963114\n",
      "          vf_explained_var: 0.6805427670478821\n",
      "          vf_loss: 0.0111214237442861\n",
      "    num_agent_steps_sampled: 130000\n",
      "    num_agent_steps_trained: 130000\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.66\n",
      "    ram_util_percent: 46.605000000000004\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03830045765367819\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.67519640320805\n",
      "    mean_inference_ms: 1.7048187101743668\n",
      "    mean_raw_obs_processing_ms: 1.7393397609158217\n",
      "  time_since_restore: 3442.9729154109955\n",
      "  time_this_iter_s: 28.15514063835144\n",
      "  time_total_s: 3442.9729154109955\n",
      "  timers:\n",
      "    learn_throughput: 1181.062\n",
      "    learn_time_ms: 846.695\n",
      "    load_throughput: 53137.656\n",
      "    load_time_ms: 18.819\n",
      "    sample_throughput: 35.64\n",
      "    sample_time_ms: 28058.528\n",
      "    update_time_ms: 4.927\n",
      "  timestamp: 1634846665\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 130\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">         3442.97</td><td style=\"text-align: right;\">130000</td><td style=\"text-align: right;\"> -3.8188</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            381.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 131000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-04-49\n",
      "  done: false\n",
      "  episode_len_mean: 383.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -3.838999999999962\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 365\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.984800069861942\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011136595673429442\n",
      "          policy_loss: -0.01628938148626023\n",
      "          total_loss: -0.02497022259566519\n",
      "          vf_explained_var: 0.7886059284210205\n",
      "          vf_loss: 0.0036499570861148337\n",
      "    num_agent_steps_sampled: 131000\n",
      "    num_agent_steps_trained: 131000\n",
      "    num_steps_sampled: 131000\n",
      "    num_steps_trained: 131000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.7457142857143\n",
      "    ram_util_percent: 46.645714285714284\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03833358061066664\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.69560181002752\n",
      "    mean_inference_ms: 1.705610649822293\n",
      "    mean_raw_obs_processing_ms: 1.7420156374886526\n",
      "  time_since_restore: 3467.257951259613\n",
      "  time_this_iter_s: 24.285035848617554\n",
      "  time_total_s: 3467.257951259613\n",
      "  timers:\n",
      "    learn_throughput: 1179.111\n",
      "    learn_time_ms: 848.096\n",
      "    load_throughput: 53009.05\n",
      "    load_time_ms: 18.865\n",
      "    sample_throughput: 35.985\n",
      "    sample_time_ms: 27789.721\n",
      "    update_time_ms: 4.734\n",
      "  timestamp: 1634846689\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 131000\n",
      "  training_iteration: 131\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         3467.26</td><td style=\"text-align: right;\">131000</td><td style=\"text-align: right;\">  -3.839</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">             383.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-05-12\n",
      "  done: false\n",
      "  episode_len_mean: 385.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -3.851699999999962\n",
      "  episode_reward_min: -4.619999999999946\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 367\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.018728154235416\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015363258024063638\n",
      "          policy_loss: 0.03286639071173138\n",
      "          total_loss: 0.02694117178519567\n",
      "          vf_explained_var: 0.8617231845855713\n",
      "          vf_loss: 0.0038918645654727395\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.8\n",
      "    ram_util_percent: 46.57272727272728\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038355459397799746\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.708534713220097\n",
      "    mean_inference_ms: 1.7061299173929307\n",
      "    mean_raw_obs_processing_ms: 1.7438220196700656\n",
      "  time_since_restore: 3490.852354288101\n",
      "  time_this_iter_s: 23.59440302848816\n",
      "  time_total_s: 3490.852354288101\n",
      "  timers:\n",
      "    learn_throughput: 1182.283\n",
      "    learn_time_ms: 845.821\n",
      "    load_throughput: 52861.538\n",
      "    load_time_ms: 18.917\n",
      "    sample_throughput: 36.627\n",
      "    sample_time_ms: 27302.291\n",
      "    update_time_ms: 3.959\n",
      "  timestamp: 1634846712\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 132\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         3490.85</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\"> -3.8517</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -4.62</td><td style=\"text-align: right;\">            385.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 133000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-05-33\n",
      "  done: false\n",
      "  episode_len_mean: 385.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -3.8551999999999613\n",
      "  episode_reward_min: -4.629999999999946\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 369\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.721878465016683\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014549991159373476\n",
      "          policy_loss: -0.09151605135864682\n",
      "          total_loss: -0.08889519315626886\n",
      "          vf_explained_var: 0.6295916438102722\n",
      "          vf_loss: 0.010018395212116754\n",
      "    num_agent_steps_sampled: 133000\n",
      "    num_agent_steps_trained: 133000\n",
      "    num_steps_sampled: 133000\n",
      "    num_steps_trained: 133000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.95517241379311\n",
      "    ram_util_percent: 46.589655172413785\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03837680070442275\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.720443560972637\n",
      "    mean_inference_ms: 1.7066296657090958\n",
      "    mean_raw_obs_processing_ms: 1.7439548875384003\n",
      "  time_since_restore: 3511.029709815979\n",
      "  time_this_iter_s: 20.177355527877808\n",
      "  time_total_s: 3511.029709815979\n",
      "  timers:\n",
      "    learn_throughput: 1174.933\n",
      "    learn_time_ms: 851.112\n",
      "    load_throughput: 53420.416\n",
      "    load_time_ms: 18.719\n",
      "    sample_throughput: 37.482\n",
      "    sample_time_ms: 26679.648\n",
      "    update_time_ms: 4.772\n",
      "  timestamp: 1634846733\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 133000\n",
      "  training_iteration: 133\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">         3511.03</td><td style=\"text-align: right;\">133000</td><td style=\"text-align: right;\"> -3.8552</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -4.63</td><td style=\"text-align: right;\">            385.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 134000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-05-56\n",
      "  done: false\n",
      "  episode_len_mean: 386.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -3.8665999999999623\n",
      "  episode_reward_min: -4.90999999999994\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 372\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9262057291136847\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011239578269929319\n",
      "          policy_loss: -0.04242124781012535\n",
      "          total_loss: -0.04702823220027818\n",
      "          vf_explained_var: 0.6636430025100708\n",
      "          vf_loss: 0.007068354874435398\n",
      "    num_agent_steps_sampled: 134000\n",
      "    num_agent_steps_trained: 134000\n",
      "    num_steps_sampled: 134000\n",
      "    num_steps_trained: 134000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.44411764705883\n",
      "    ram_util_percent: 46.582352941176474\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03840838853652308\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.73784856763478\n",
      "    mean_inference_ms: 1.707367606954611\n",
      "    mean_raw_obs_processing_ms: 1.7415948754723525\n",
      "  time_since_restore: 3534.5454881191254\n",
      "  time_this_iter_s: 23.515778303146362\n",
      "  time_total_s: 3534.5454881191254\n",
      "  timers:\n",
      "    learn_throughput: 1174.744\n",
      "    learn_time_ms: 851.249\n",
      "    load_throughput: 49731.253\n",
      "    load_time_ms: 20.108\n",
      "    sample_throughput: 38.059\n",
      "    sample_time_ms: 26275.05\n",
      "    update_time_ms: 4.387\n",
      "  timestamp: 1634846756\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 134000\n",
      "  training_iteration: 134\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         3534.55</td><td style=\"text-align: right;\">134000</td><td style=\"text-align: right;\"> -3.8666</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -4.91</td><td style=\"text-align: right;\">            386.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 135000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-06-19\n",
      "  done: false\n",
      "  episode_len_mean: 387.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -3.8699999999999615\n",
      "  episode_reward_min: -4.90999999999994\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 374\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9462492730882432\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013427210704804413\n",
      "          policy_loss: 0.08385181377331416\n",
      "          total_loss: 0.07679061740636825\n",
      "          vf_explained_var: 0.34897324442863464\n",
      "          vf_loss: 0.003337930397052939\n",
      "    num_agent_steps_sampled: 135000\n",
      "    num_agent_steps_trained: 135000\n",
      "    num_steps_sampled: 135000\n",
      "    num_steps_trained: 135000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.81875\n",
      "    ram_util_percent: 46.50625\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0384286481923043\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.749050469281965\n",
      "    mean_inference_ms: 1.7078412637861886\n",
      "    mean_raw_obs_processing_ms: 1.7401776916614191\n",
      "  time_since_restore: 3557.48103928566\n",
      "  time_this_iter_s: 22.935551166534424\n",
      "  time_total_s: 3557.48103928566\n",
      "  timers:\n",
      "    learn_throughput: 1169.643\n",
      "    learn_time_ms: 854.961\n",
      "    load_throughput: 47604.23\n",
      "    load_time_ms: 21.007\n",
      "    sample_throughput: 38.559\n",
      "    sample_time_ms: 25934.583\n",
      "    update_time_ms: 4.601\n",
      "  timestamp: 1634846779\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 135000\n",
      "  training_iteration: 135\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         3557.48</td><td style=\"text-align: right;\">135000</td><td style=\"text-align: right;\">   -3.87</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -4.91</td><td style=\"text-align: right;\">               387</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-06-44\n",
      "  done: false\n",
      "  episode_len_mean: 387.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -3.8728999999999614\n",
      "  episode_reward_min: -4.90999999999994\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 376\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9100001321898565\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010058607832678066\n",
      "          policy_loss: -0.15408888889683617\n",
      "          total_loss: -0.16305575453572804\n",
      "          vf_explained_var: 0.8783061504364014\n",
      "          vf_loss: 0.0033435709942649635\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.12777777777778\n",
      "    ram_util_percent: 46.63888888888889\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03844889914862366\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.75998177622349\n",
      "    mean_inference_ms: 1.7083101646199246\n",
      "    mean_raw_obs_processing_ms: 1.7386984225583615\n",
      "  time_since_restore: 3582.0611996650696\n",
      "  time_this_iter_s: 24.58016037940979\n",
      "  time_total_s: 3582.0611996650696\n",
      "  timers:\n",
      "    learn_throughput: 1169.225\n",
      "    learn_time_ms: 855.268\n",
      "    load_throughput: 47693.167\n",
      "    load_time_ms: 20.967\n",
      "    sample_throughput: 39.176\n",
      "    sample_time_ms: 25525.611\n",
      "    update_time_ms: 4.717\n",
      "  timestamp: 1634846804\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 136\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         3582.06</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\"> -3.8729</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -4.91</td><td style=\"text-align: right;\">            387.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 137000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-07-10\n",
      "  done: false\n",
      "  episode_len_mean: 386.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -3.8667999999999614\n",
      "  episode_reward_min: -4.90999999999994\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 379\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8385029872258505\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01336019670274307\n",
      "          policy_loss: -0.017708688125842146\n",
      "          total_loss: -0.02409726025329696\n",
      "          vf_explained_var: 0.7211112976074219\n",
      "          vf_loss: 0.0029783223387009152\n",
      "    num_agent_steps_sampled: 137000\n",
      "    num_agent_steps_trained: 137000\n",
      "    num_steps_sampled: 137000\n",
      "    num_steps_trained: 137000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.0027777777778\n",
      "    ram_util_percent: 46.75000000000001\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038476865700433474\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.77610083718688\n",
      "    mean_inference_ms: 1.708971578770465\n",
      "    mean_raw_obs_processing_ms: 1.7367882134768293\n",
      "  time_since_restore: 3607.9240293502808\n",
      "  time_this_iter_s: 25.86282968521118\n",
      "  time_total_s: 3607.9240293502808\n",
      "  timers:\n",
      "    learn_throughput: 1170.084\n",
      "    learn_time_ms: 854.64\n",
      "    load_throughput: 48085.203\n",
      "    load_time_ms: 20.796\n",
      "    sample_throughput: 39.232\n",
      "    sample_time_ms: 25489.396\n",
      "    update_time_ms: 4.673\n",
      "  timestamp: 1634846830\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 137000\n",
      "  training_iteration: 137\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         3607.92</td><td style=\"text-align: right;\">137000</td><td style=\"text-align: right;\"> -3.8668</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -4.91</td><td style=\"text-align: right;\">            386.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 138000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-07-33\n",
      "  done: false\n",
      "  episode_len_mean: 386.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -3.865799999999961\n",
      "  episode_reward_min: -4.90999999999994\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 381\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8728614542219373\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0163929443795458\n",
      "          policy_loss: -0.1599961398376359\n",
      "          total_loss: -0.16452438789937232\n",
      "          vf_explained_var: 0.8799504637718201\n",
      "          vf_loss: 0.0031351308092578416\n",
      "    num_agent_steps_sampled: 138000\n",
      "    num_agent_steps_trained: 138000\n",
      "    num_steps_sampled: 138000\n",
      "    num_steps_trained: 138000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.66764705882352\n",
      "    ram_util_percent: 46.732352941176465\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03849327213545553\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.78624903778265\n",
      "    mean_inference_ms: 1.7093876881718577\n",
      "    mean_raw_obs_processing_ms: 1.7356524030059313\n",
      "  time_since_restore: 3631.5550212860107\n",
      "  time_this_iter_s: 23.63099193572998\n",
      "  time_total_s: 3631.5550212860107\n",
      "  timers:\n",
      "    learn_throughput: 1174.112\n",
      "    learn_time_ms: 851.707\n",
      "    load_throughput: 46140.841\n",
      "    load_time_ms: 21.673\n",
      "    sample_throughput: 39.751\n",
      "    sample_time_ms: 25156.671\n",
      "    update_time_ms: 3.913\n",
      "  timestamp: 1634846853\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 138000\n",
      "  training_iteration: 138\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         3631.56</td><td style=\"text-align: right;\">138000</td><td style=\"text-align: right;\"> -3.8658</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -4.91</td><td style=\"text-align: right;\">            386.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 139000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-07-55\n",
      "  done: false\n",
      "  episode_len_mean: 387.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -3.8795999999999604\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 383\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0045690735181174\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014901557006504224\n",
      "          policy_loss: -0.09246994157632192\n",
      "          total_loss: -0.09529456595579783\n",
      "          vf_explained_var: 0.4001396894454956\n",
      "          vf_loss: 0.007162513940905531\n",
      "    num_agent_steps_sampled: 139000\n",
      "    num_agent_steps_trained: 139000\n",
      "    num_steps_sampled: 139000\n",
      "    num_steps_trained: 139000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.64838709677417\n",
      "    ram_util_percent: 46.764516129032245\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03850952905880284\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.795808822997834\n",
      "    mean_inference_ms: 1.7098009227441167\n",
      "    mean_raw_obs_processing_ms: 1.73445754874635\n",
      "  time_since_restore: 3653.1791367530823\n",
      "  time_this_iter_s: 21.624115467071533\n",
      "  time_total_s: 3653.1791367530823\n",
      "  timers:\n",
      "    learn_throughput: 1167.794\n",
      "    learn_time_ms: 856.316\n",
      "    load_throughput: 45956.061\n",
      "    load_time_ms: 21.76\n",
      "    sample_throughput: 43.573\n",
      "    sample_time_ms: 22949.764\n",
      "    update_time_ms: 3.905\n",
      "  timestamp: 1634846875\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 139000\n",
      "  training_iteration: 139\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         3653.18</td><td style=\"text-align: right;\">139000</td><td style=\"text-align: right;\"> -3.8796</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">            387.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-08-16\n",
      "  done: false\n",
      "  episode_len_mean: 391.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -3.91119999999996\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 385\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0528016090393066\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013388048046699592\n",
      "          policy_loss: -0.03351361552874247\n",
      "          total_loss: -0.042745577295621236\n",
      "          vf_explained_var: 0.6644542217254639\n",
      "          vf_loss: 0.00225912053267368\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.73\n",
      "    ram_util_percent: 46.753333333333345\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0385253690132444\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.80376891344139\n",
      "    mean_inference_ms: 1.7101989977612333\n",
      "    mean_raw_obs_processing_ms: 1.733358138226389\n",
      "  time_since_restore: 3674.342870950699\n",
      "  time_this_iter_s: 21.163734197616577\n",
      "  time_total_s: 3674.342870950699\n",
      "  timers:\n",
      "    learn_throughput: 1170.57\n",
      "    learn_time_ms: 854.284\n",
      "    load_throughput: 47269.457\n",
      "    load_time_ms: 21.155\n",
      "    sample_throughput: 44.938\n",
      "    sample_time_ms: 22252.924\n",
      "    update_time_ms: 4.301\n",
      "  timestamp: 1634846896\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 140\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         3674.34</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\"> -3.9112</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">            391.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 141000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-08-38\n",
      "  done: false\n",
      "  episode_len_mean: 392.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -3.9259999999999606\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 387\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9254741562737359\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018044946933184154\n",
      "          policy_loss: -0.13871926963329315\n",
      "          total_loss: -0.14268935786353218\n",
      "          vf_explained_var: 0.8915455341339111\n",
      "          vf_loss: 0.0031043127987585547\n",
      "    num_agent_steps_sampled: 141000\n",
      "    num_agent_steps_trained: 141000\n",
      "    num_steps_sampled: 141000\n",
      "    num_steps_trained: 141000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.59375\n",
      "    ram_util_percent: 46.765625\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0385408354866069\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.810883136236114\n",
      "    mean_inference_ms: 1.7105802288516003\n",
      "    mean_raw_obs_processing_ms: 1.7323416555525284\n",
      "  time_since_restore: 3696.5695118904114\n",
      "  time_this_iter_s: 22.226640939712524\n",
      "  time_total_s: 3696.5695118904114\n",
      "  timers:\n",
      "    learn_throughput: 1177.22\n",
      "    learn_time_ms: 849.459\n",
      "    load_throughput: 47069.946\n",
      "    load_time_ms: 21.245\n",
      "    sample_throughput: 45.348\n",
      "    sample_time_ms: 22051.859\n",
      "    update_time_ms: 4.371\n",
      "  timestamp: 1634846918\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 141000\n",
      "  training_iteration: 141\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         3696.57</td><td style=\"text-align: right;\">141000</td><td style=\"text-align: right;\">  -3.926</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">             392.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 142000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-09-00\n",
      "  done: false\n",
      "  episode_len_mean: 395.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -3.95089999999996\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 389\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.993272340297699\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013600774073767042\n",
      "          policy_loss: -0.08201627681652705\n",
      "          total_loss: -0.09067837016450034\n",
      "          vf_explained_var: 0.6092401146888733\n",
      "          vf_loss: 0.002090107177435938\n",
      "    num_agent_steps_sampled: 142000\n",
      "    num_agent_steps_trained: 142000\n",
      "    num_steps_sampled: 142000\n",
      "    num_steps_trained: 142000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.72\n",
      "    ram_util_percent: 46.76999999999999\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03855615858574273\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.817214846162063\n",
      "    mean_inference_ms: 1.7109585951044033\n",
      "    mean_raw_obs_processing_ms: 1.7312628619844872\n",
      "  time_since_restore: 3717.84130525589\n",
      "  time_this_iter_s: 21.271793365478516\n",
      "  time_total_s: 3717.84130525589\n",
      "  timers:\n",
      "    learn_throughput: 1177.198\n",
      "    learn_time_ms: 849.475\n",
      "    load_throughput: 47312.7\n",
      "    load_time_ms: 21.136\n",
      "    sample_throughput: 45.83\n",
      "    sample_time_ms: 21819.727\n",
      "    update_time_ms: 4.362\n",
      "  timestamp: 1634846940\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 142000\n",
      "  training_iteration: 142\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         3717.84</td><td style=\"text-align: right;\">142000</td><td style=\"text-align: right;\"> -3.9509</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">            395.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 143000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-09-40\n",
      "  done: false\n",
      "  episode_len_mean: 396.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -3.9644999999999597\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 391\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9399574716885886\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010940909920922406\n",
      "          policy_loss: -0.11911661633186871\n",
      "          total_loss: -0.12878670675887002\n",
      "          vf_explained_var: 0.9235522150993347\n",
      "          vf_loss: 0.0023443699174094945\n",
      "    num_agent_steps_sampled: 143000\n",
      "    num_agent_steps_trained: 143000\n",
      "    num_steps_sampled: 143000\n",
      "    num_steps_trained: 143000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.1948275862069\n",
      "    ram_util_percent: 46.72241379310344\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03857133567484149\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.8228746122655\n",
      "    mean_inference_ms: 1.711332091684971\n",
      "    mean_raw_obs_processing_ms: 1.7326694673198988\n",
      "  time_since_restore: 3758.0519485473633\n",
      "  time_this_iter_s: 40.21064329147339\n",
      "  time_total_s: 3758.0519485473633\n",
      "  timers:\n",
      "    learn_throughput: 1178.826\n",
      "    learn_time_ms: 848.302\n",
      "    load_throughput: 48252.709\n",
      "    load_time_ms: 20.724\n",
      "    sample_throughput: 41.973\n",
      "    sample_time_ms: 23824.833\n",
      "    update_time_ms: 3.983\n",
      "  timestamp: 1634846980\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 143000\n",
      "  training_iteration: 143\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         3758.05</td><td style=\"text-align: right;\">143000</td><td style=\"text-align: right;\"> -3.9645</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">            396.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-10-00\n",
      "  done: false\n",
      "  episode_len_mean: 399.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -3.993399999999959\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 393\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0663515779707167\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01108272074653498\n",
      "          policy_loss: -0.12524331741862826\n",
      "          total_loss: -0.1313438536392318\n",
      "          vf_explained_var: 0.07557974755764008\n",
      "          vf_loss: 0.007082143075563686\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.56206896551724\n",
      "    ram_util_percent: 46.58275862068965\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038586621881801085\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.82725172751943\n",
      "    mean_inference_ms: 1.7116948800691478\n",
      "    mean_raw_obs_processing_ms: 1.7341471580902543\n",
      "  time_since_restore: 3778.1829402446747\n",
      "  time_this_iter_s: 20.1309916973114\n",
      "  time_total_s: 3778.1829402446747\n",
      "  timers:\n",
      "    learn_throughput: 1182.221\n",
      "    learn_time_ms: 845.866\n",
      "    load_throughput: 51859.512\n",
      "    load_time_ms: 19.283\n",
      "    sample_throughput: 42.57\n",
      "    sample_time_ms: 23490.728\n",
      "    update_time_ms: 3.66\n",
      "  timestamp: 1634847000\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 144\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         3778.18</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\"> -3.9934</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">            399.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 145000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-10-19\n",
      "  done: false\n",
      "  episode_len_mean: 402.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.020799999999959\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 395\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.874463763501909\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015135289777197386\n",
      "          policy_loss: -0.062046399464209874\n",
      "          total_loss: -0.06706442683935165\n",
      "          vf_explained_var: 0.9077306389808655\n",
      "          vf_loss: 0.0035102854566907305\n",
      "    num_agent_steps_sampled: 145000\n",
      "    num_agent_steps_trained: 145000\n",
      "    num_steps_sampled: 145000\n",
      "    num_steps_trained: 145000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.61481481481482\n",
      "    ram_util_percent: 46.59259259259258\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038601335445303434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.83061235089187\n",
      "    mean_inference_ms: 1.7120461311571105\n",
      "    mean_raw_obs_processing_ms: 1.735681904583376\n",
      "  time_since_restore: 3797.112991809845\n",
      "  time_this_iter_s: 18.930051565170288\n",
      "  time_total_s: 3797.112991809845\n",
      "  timers:\n",
      "    learn_throughput: 1186.121\n",
      "    learn_time_ms: 843.084\n",
      "    load_throughput: 51848.55\n",
      "    load_time_ms: 19.287\n",
      "    sample_throughput: 43.303\n",
      "    sample_time_ms: 23093.084\n",
      "    update_time_ms: 3.487\n",
      "  timestamp: 1634847019\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 145000\n",
      "  training_iteration: 145\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   145</td><td style=\"text-align: right;\">         3797.11</td><td style=\"text-align: right;\">145000</td><td style=\"text-align: right;\"> -4.0208</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">            402.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 146000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-10-40\n",
      "  done: false\n",
      "  episode_len_mean: 404.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.0448999999999575\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 397\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0361708588070337\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011258871053425304\n",
      "          policy_loss: -0.07218389080630408\n",
      "          total_loss: -0.08216878328886297\n",
      "          vf_explained_var: 0.8215012550354004\n",
      "          vf_loss: 0.0027770730912581912\n",
      "    num_agent_steps_sampled: 146000\n",
      "    num_agent_steps_trained: 146000\n",
      "    num_steps_sampled: 146000\n",
      "    num_steps_trained: 146000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.31333333333332\n",
      "    ram_util_percent: 46.563333333333325\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03861490391391574\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.832946660329867\n",
      "    mean_inference_ms: 1.7123885972378725\n",
      "    mean_raw_obs_processing_ms: 1.7371385346181238\n",
      "  time_since_restore: 3818.2177996635437\n",
      "  time_this_iter_s: 21.10480785369873\n",
      "  time_total_s: 3818.2177996635437\n",
      "  timers:\n",
      "    learn_throughput: 1126.127\n",
      "    learn_time_ms: 887.999\n",
      "    load_throughput: 51995.16\n",
      "    load_time_ms: 19.233\n",
      "    sample_throughput: 44.051\n",
      "    sample_time_ms: 22700.884\n",
      "    update_time_ms: 3.373\n",
      "  timestamp: 1634847040\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 146000\n",
      "  training_iteration: 146\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         3818.22</td><td style=\"text-align: right;\">146000</td><td style=\"text-align: right;\"> -4.0449</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">            404.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 147000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-11-01\n",
      "  done: false\n",
      "  episode_len_mean: 407.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.075199999999956\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 399\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9217669076389736\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01609710611093692\n",
      "          policy_loss: -0.08349425991376241\n",
      "          total_loss: -0.08188786821232902\n",
      "          vf_explained_var: 0.615752637386322\n",
      "          vf_loss: 0.009958513075899747\n",
      "    num_agent_steps_sampled: 147000\n",
      "    num_agent_steps_trained: 147000\n",
      "    num_steps_sampled: 147000\n",
      "    num_steps_trained: 147000\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.66551724137932\n",
      "    ram_util_percent: 46.52068965517241\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862735661064262\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.834243191741262\n",
      "    mean_inference_ms: 1.7127215832623204\n",
      "    mean_raw_obs_processing_ms: 1.7385184324522016\n",
      "  time_since_restore: 3838.724676847458\n",
      "  time_this_iter_s: 20.506877183914185\n",
      "  time_total_s: 3838.724676847458\n",
      "  timers:\n",
      "    learn_throughput: 1126.418\n",
      "    learn_time_ms: 887.77\n",
      "    load_throughput: 52199.968\n",
      "    load_time_ms: 19.157\n",
      "    sample_throughput: 45.115\n",
      "    sample_time_ms: 22165.622\n",
      "    update_time_ms: 3.296\n",
      "  timestamp: 1634847061\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 147000\n",
      "  training_iteration: 147\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         3838.72</td><td style=\"text-align: right;\">147000</td><td style=\"text-align: right;\"> -4.0752</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">            407.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-11-24\n",
      "  done: false\n",
      "  episode_len_mean: 409.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.099599999999956\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 402\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8522197710143196\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013721412278313987\n",
      "          policy_loss: -0.14225716474983427\n",
      "          total_loss: -0.14377478294902377\n",
      "          vf_explained_var: 0.5676441192626953\n",
      "          vf_loss: 0.007742624683305621\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.55588235294118\n",
      "    ram_util_percent: 46.43529411764706\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386457083850903\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.835228726117126\n",
      "    mean_inference_ms: 1.7131955905846865\n",
      "    mean_raw_obs_processing_ms: 1.735842058248845\n",
      "  time_since_restore: 3862.3918776512146\n",
      "  time_this_iter_s: 23.667200803756714\n",
      "  time_total_s: 3862.3918776512146\n",
      "  timers:\n",
      "    learn_throughput: 1127.135\n",
      "    learn_time_ms: 887.205\n",
      "    load_throughput: 54706.363\n",
      "    load_time_ms: 18.279\n",
      "    sample_throughput: 45.105\n",
      "    sample_time_ms: 22170.532\n",
      "    update_time_ms: 3.485\n",
      "  timestamp: 1634847084\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 148\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">         3862.39</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\"> -4.0996</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">            409.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 149000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-11-48\n",
      "  done: false\n",
      "  episode_len_mean: 410.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.109099999999956\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 404\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8261810037824842\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012974738846280736\n",
      "          policy_loss: 0.101591331180599\n",
      "          total_loss: 0.09404165657858053\n",
      "          vf_explained_var: 0.4373360872268677\n",
      "          vf_loss: 0.0019541842387601114\n",
      "    num_agent_steps_sampled: 149000\n",
      "    num_agent_steps_trained: 149000\n",
      "    num_steps_sampled: 149000\n",
      "    num_steps_trained: 149000\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.36666666666666\n",
      "    ram_util_percent: 46.45151515151515\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038657882897844796\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.835535999487938\n",
      "    mean_inference_ms: 1.7135066661985023\n",
      "    mean_raw_obs_processing_ms: 1.7341498908847115\n",
      "  time_since_restore: 3885.7479968070984\n",
      "  time_this_iter_s: 23.35611915588379\n",
      "  time_total_s: 3885.7479968070984\n",
      "  timers:\n",
      "    learn_throughput: 1116.701\n",
      "    learn_time_ms: 895.495\n",
      "    load_throughput: 55315.874\n",
      "    load_time_ms: 18.078\n",
      "    sample_throughput: 44.771\n",
      "    sample_time_ms: 22335.782\n",
      "    update_time_ms: 3.435\n",
      "  timestamp: 1634847108\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 149000\n",
      "  training_iteration: 149\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         3885.75</td><td style=\"text-align: right;\">149000</td><td style=\"text-align: right;\"> -4.1091</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">            410.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 150000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-12-11\n",
      "  done: false\n",
      "  episode_len_mean: 411.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.118799999999956\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 406\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9595688965585496\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013856522775528804\n",
      "          policy_loss: -0.03763953381114536\n",
      "          total_loss: -0.0434476919265257\n",
      "          vf_explained_var: 0.5211743116378784\n",
      "          vf_loss: 0.004434377746656537\n",
      "    num_agent_steps_sampled: 150000\n",
      "    num_agent_steps_trained: 150000\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.2764705882353\n",
      "    ram_util_percent: 46.56764705882353\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038670037857327394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.835814963946248\n",
      "    mean_inference_ms: 1.7138143721856351\n",
      "    mean_raw_obs_processing_ms: 1.7325430372543835\n",
      "  time_since_restore: 3909.2130846977234\n",
      "  time_this_iter_s: 23.465087890625\n",
      "  time_total_s: 3909.2130846977234\n",
      "  timers:\n",
      "    learn_throughput: 1115.181\n",
      "    learn_time_ms: 896.716\n",
      "    load_throughput: 55363.626\n",
      "    load_time_ms: 18.062\n",
      "    sample_throughput: 44.316\n",
      "    sample_time_ms: 22564.966\n",
      "    update_time_ms: 3.0\n",
      "  timestamp: 1634847131\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 150\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         3909.21</td><td style=\"text-align: right;\">150000</td><td style=\"text-align: right;\"> -4.1188</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">            411.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 151000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-12-34\n",
      "  done: false\n",
      "  episode_len_mean: 412.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.1222999999999566\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 408\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8109898726145426\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012841080270814981\n",
      "          policy_loss: -0.24094852151142226\n",
      "          total_loss: -0.24534452847308583\n",
      "          vf_explained_var: 0.8325231075286865\n",
      "          vf_loss: 0.005046161018415458\n",
      "    num_agent_steps_sampled: 151000\n",
      "    num_agent_steps_trained: 151000\n",
      "    num_steps_sampled: 151000\n",
      "    num_steps_trained: 151000\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.62121212121212\n",
      "    ram_util_percent: 46.778787878787874\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03868219827661482\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.835992219889658\n",
      "    mean_inference_ms: 1.7141224396549504\n",
      "    mean_raw_obs_processing_ms: 1.730880270402694\n",
      "  time_since_restore: 3932.44215631485\n",
      "  time_this_iter_s: 23.229071617126465\n",
      "  time_total_s: 3932.44215631485\n",
      "  timers:\n",
      "    learn_throughput: 1113.328\n",
      "    learn_time_ms: 898.208\n",
      "    load_throughput: 55789.489\n",
      "    load_time_ms: 17.925\n",
      "    sample_throughput: 44.124\n",
      "    sample_time_ms: 22663.359\n",
      "    update_time_ms: 3.467\n",
      "  timestamp: 1634847154\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 151000\n",
      "  training_iteration: 151\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         3932.44</td><td style=\"text-align: right;\">151000</td><td style=\"text-align: right;\"> -4.1223</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">            412.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-12-59\n",
      "  done: false\n",
      "  episode_len_mean: 412.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.126999999999957\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 411\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7942807899581061\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0120594023373756\n",
      "          policy_loss: -0.011284977694352468\n",
      "          total_loss: -0.01605472829606798\n",
      "          vf_explained_var: 0.7155802249908447\n",
      "          vf_loss: 0.005032961628037609\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.58235294117645\n",
      "    ram_util_percent: 46.82941176470588\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03870012783720651\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.8360320147089\n",
      "    mean_inference_ms: 1.7145777609990005\n",
      "    mean_raw_obs_processing_ms: 1.7285858448139393\n",
      "  time_since_restore: 3956.627587080002\n",
      "  time_this_iter_s: 24.185430765151978\n",
      "  time_total_s: 3956.627587080002\n",
      "  timers:\n",
      "    learn_throughput: 1086.463\n",
      "    learn_time_ms: 920.418\n",
      "    load_throughput: 55475.148\n",
      "    load_time_ms: 18.026\n",
      "    sample_throughput: 43.606\n",
      "    sample_time_ms: 22932.542\n",
      "    update_time_ms: 3.434\n",
      "  timestamp: 1634847179\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 152\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         3956.63</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">  -4.127</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">             412.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 153000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-13-23\n",
      "  done: false\n",
      "  episode_len_mean: 413.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.136399999999957\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 413\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7803523580233256\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01097894056370282\n",
      "          policy_loss: -0.030321122871504888\n",
      "          total_loss: -0.035524192286862266\n",
      "          vf_explained_var: 0.7925684452056885\n",
      "          vf_loss: 0.005189671900330318\n",
      "    num_agent_steps_sampled: 153000\n",
      "    num_agent_steps_trained: 153000\n",
      "    num_steps_sampled: 153000\n",
      "    num_steps_trained: 153000\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.41470588235293\n",
      "    ram_util_percent: 46.9\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03871177925668411\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.835748882214986\n",
      "    mean_inference_ms: 1.7148808255062158\n",
      "    mean_raw_obs_processing_ms: 1.727091343603138\n",
      "  time_since_restore: 3980.4242482185364\n",
      "  time_this_iter_s: 23.796661138534546\n",
      "  time_total_s: 3980.4242482185364\n",
      "  timers:\n",
      "    learn_throughput: 1087.831\n",
      "    learn_time_ms: 919.26\n",
      "    load_throughput: 55758.191\n",
      "    load_time_ms: 17.935\n",
      "    sample_throughput: 46.964\n",
      "    sample_time_ms: 21292.984\n",
      "    update_time_ms: 3.075\n",
      "  timestamp: 1634847203\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 153000\n",
      "  training_iteration: 153\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">         3980.42</td><td style=\"text-align: right;\">153000</td><td style=\"text-align: right;\"> -4.1364</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">            413.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 154000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-13-44\n",
      "  done: false\n",
      "  episode_len_mean: 415.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.152599999999956\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 415\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8549485405286152\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01259375153100242\n",
      "          policy_loss: -0.13110278424703412\n",
      "          total_loss: -0.13628613899151484\n",
      "          vf_explained_var: 0.793481171131134\n",
      "          vf_loss: 0.00486535418442347\n",
      "    num_agent_steps_sampled: 154000\n",
      "    num_agent_steps_trained: 154000\n",
      "    num_steps_sampled: 154000\n",
      "    num_steps_trained: 154000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.48064516129031\n",
      "    ram_util_percent: 46.87096774193548\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03872317448324132\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.83512690518303\n",
      "    mean_inference_ms: 1.7151864457858266\n",
      "    mean_raw_obs_processing_ms: 1.7256734506805105\n",
      "  time_since_restore: 4001.817380666733\n",
      "  time_this_iter_s: 21.39313244819641\n",
      "  time_total_s: 4001.817380666733\n",
      "  timers:\n",
      "    learn_throughput: 1084.541\n",
      "    learn_time_ms: 922.049\n",
      "    load_throughput: 55290.789\n",
      "    load_time_ms: 18.086\n",
      "    sample_throughput: 46.694\n",
      "    sample_time_ms: 21416.053\n",
      "    update_time_ms: 3.125\n",
      "  timestamp: 1634847224\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 154000\n",
      "  training_iteration: 154\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         4001.82</td><td style=\"text-align: right;\">154000</td><td style=\"text-align: right;\"> -4.1526</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">            415.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 155000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-14-07\n",
      "  done: false\n",
      "  episode_len_mean: 417.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.171699999999956\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 417\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7347839381959704\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013621129302934893\n",
      "          policy_loss: -0.21641272753477098\n",
      "          total_loss: -0.21928636282682418\n",
      "          vf_explained_var: 0.8403806090354919\n",
      "          vf_loss: 0.005279939912078488\n",
      "    num_agent_steps_sampled: 155000\n",
      "    num_agent_steps_trained: 155000\n",
      "    num_steps_sampled: 155000\n",
      "    num_steps_trained: 155000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.64545454545456\n",
      "    ram_util_percent: 46.91818181818181\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0387349126414793\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.83450595901346\n",
      "    mean_inference_ms: 1.7154980471817751\n",
      "    mean_raw_obs_processing_ms: 1.7241989776463527\n",
      "  time_since_restore: 4024.760785341263\n",
      "  time_this_iter_s: 22.94340467453003\n",
      "  time_total_s: 4024.760785341263\n",
      "  timers:\n",
      "    learn_throughput: 1086.669\n",
      "    learn_time_ms: 920.244\n",
      "    load_throughput: 57015.249\n",
      "    load_time_ms: 17.539\n",
      "    sample_throughput: 45.83\n",
      "    sample_time_ms: 21819.733\n",
      "    update_time_ms: 3.142\n",
      "  timestamp: 1634847247\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 155000\n",
      "  training_iteration: 155\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         4024.76</td><td style=\"text-align: right;\">155000</td><td style=\"text-align: right;\"> -4.1717</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">            417.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-14-46\n",
      "  done: false\n",
      "  episode_len_mean: 418.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.181799999999955\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 420\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5919799433814155\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00987156614400821\n",
      "          policy_loss: -0.09025455233123568\n",
      "          total_loss: -0.0938754700952106\n",
      "          vf_explained_var: 0.7233960628509521\n",
      "          vf_loss: 0.0056355791813176535\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.21090909090908\n",
      "    ram_util_percent: 46.88545454545456\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03875305145713535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.832878799885847\n",
      "    mean_inference_ms: 1.715961053614338\n",
      "    mean_raw_obs_processing_ms: 1.7254873793502208\n",
      "  time_since_restore: 4063.5270144939423\n",
      "  time_this_iter_s: 38.76622915267944\n",
      "  time_total_s: 4063.5270144939423\n",
      "  timers:\n",
      "    learn_throughput: 1145.591\n",
      "    learn_time_ms: 872.912\n",
      "    load_throughput: 53192.849\n",
      "    load_time_ms: 18.8\n",
      "    sample_throughput: 42.316\n",
      "    sample_time_ms: 23631.901\n",
      "    update_time_ms: 3.132\n",
      "  timestamp: 1634847286\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 156\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">         4063.53</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\"> -4.1818</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">            418.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 157000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-15-10\n",
      "  done: false\n",
      "  episode_len_mean: 419.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.194199999999955\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 422\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6759228295750088\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012661924502539692\n",
      "          policy_loss: 0.1022762102385362\n",
      "          total_loss: 0.09724218845367431\n",
      "          vf_explained_var: 0.8806154131889343\n",
      "          vf_loss: 0.003178410235300867\n",
      "    num_agent_steps_sampled: 157000\n",
      "    num_agent_steps_trained: 157000\n",
      "    num_steps_sampled: 157000\n",
      "    num_steps_trained: 157000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.57714285714285\n",
      "    ram_util_percent: 46.657142857142865\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038764971210976024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.831665270826576\n",
      "    mean_inference_ms: 1.716270381505142\n",
      "    mean_raw_obs_processing_ms: 1.7263186832958324\n",
      "  time_since_restore: 4087.7238912582397\n",
      "  time_this_iter_s: 24.196876764297485\n",
      "  time_total_s: 4087.7238912582397\n",
      "  timers:\n",
      "    learn_throughput: 1146.766\n",
      "    learn_time_ms: 872.018\n",
      "    load_throughput: 52370.276\n",
      "    load_time_ms: 19.095\n",
      "    sample_throughput: 41.665\n",
      "    sample_time_ms: 24001.08\n",
      "    update_time_ms: 3.538\n",
      "  timestamp: 1634847310\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 157000\n",
      "  training_iteration: 157\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">         4087.72</td><td style=\"text-align: right;\">157000</td><td style=\"text-align: right;\"> -4.1942</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">            419.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 158000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-15-33\n",
      "  done: false\n",
      "  episode_len_mean: 421.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.210499999999954\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 424\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.785823608769311\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019256833420319343\n",
      "          policy_loss: 0.017444659935103524\n",
      "          total_loss: 0.016055497692690955\n",
      "          vf_explained_var: 0.5012524127960205\n",
      "          vf_loss: 0.003470710743891282\n",
      "    num_agent_steps_sampled: 158000\n",
      "    num_agent_steps_trained: 158000\n",
      "    num_steps_sampled: 158000\n",
      "    num_steps_trained: 158000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.865625\n",
      "    ram_util_percent: 46.715624999999996\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038776745383856864\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.829655539707904\n",
      "    mean_inference_ms: 1.716570596006336\n",
      "    mean_raw_obs_processing_ms: 1.7272080314327685\n",
      "  time_since_restore: 4110.532169818878\n",
      "  time_this_iter_s: 22.808278560638428\n",
      "  time_total_s: 4110.532169818878\n",
      "  timers:\n",
      "    learn_throughput: 1146.677\n",
      "    learn_time_ms: 872.085\n",
      "    load_throughput: 52274.525\n",
      "    load_time_ms: 19.13\n",
      "    sample_throughput: 41.814\n",
      "    sample_time_ms: 23915.533\n",
      "    update_time_ms: 3.258\n",
      "  timestamp: 1634847333\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 158000\n",
      "  training_iteration: 158\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">         4110.53</td><td style=\"text-align: right;\">158000</td><td style=\"text-align: right;\"> -4.2105</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">            421.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 159000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-15-56\n",
      "  done: false\n",
      "  episode_len_mean: 421.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.217099999999954\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 426\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.605957990222507\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014123516409076508\n",
      "          policy_loss: -0.16739065241482523\n",
      "          total_loss: -0.1710142806586292\n",
      "          vf_explained_var: 0.9170829057693481\n",
      "          vf_loss: 0.002902578761697643\n",
      "    num_agent_steps_sampled: 159000\n",
      "    num_agent_steps_trained: 159000\n",
      "    num_steps_sampled: 159000\n",
      "    num_steps_trained: 159000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.47575757575758\n",
      "    ram_util_percent: 46.687878787878795\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03878811691052803\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.827001451381367\n",
      "    mean_inference_ms: 1.7168562338372155\n",
      "    mean_raw_obs_processing_ms: 1.7281435621183159\n",
      "  time_since_restore: 4133.356969118118\n",
      "  time_this_iter_s: 22.824799299240112\n",
      "  time_total_s: 4133.356969118118\n",
      "  timers:\n",
      "    learn_throughput: 1159.247\n",
      "    learn_time_ms: 862.629\n",
      "    load_throughput: 51502.97\n",
      "    load_time_ms: 19.416\n",
      "    sample_throughput: 41.891\n",
      "    sample_time_ms: 23871.435\n",
      "    update_time_ms: 3.324\n",
      "  timestamp: 1634847356\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 159000\n",
      "  training_iteration: 159\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">         4133.36</td><td style=\"text-align: right;\">159000</td><td style=\"text-align: right;\"> -4.2171</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">            421.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-16-19\n",
      "  done: false\n",
      "  episode_len_mean: 423.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.233599999999954\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 429\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6096563551161025\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01352076904949883\n",
      "          policy_loss: 0.020315307958258522\n",
      "          total_loss: 0.017900158961613972\n",
      "          vf_explained_var: 0.808492124080658\n",
      "          vf_loss: 0.004554896756437504\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.71818181818182\n",
      "    ram_util_percent: 46.627272727272725\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03880497037853273\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.822541146176018\n",
      "    mean_inference_ms: 1.717283621105924\n",
      "    mean_raw_obs_processing_ms: 1.726744673300666\n",
      "  time_since_restore: 4156.713132858276\n",
      "  time_this_iter_s: 23.35616374015808\n",
      "  time_total_s: 4156.713132858276\n",
      "  timers:\n",
      "    learn_throughput: 1159.264\n",
      "    learn_time_ms: 862.616\n",
      "    load_throughput: 51358.929\n",
      "    load_time_ms: 19.471\n",
      "    sample_throughput: 41.91\n",
      "    sample_time_ms: 23860.677\n",
      "    update_time_ms: 3.318\n",
      "  timestamp: 1634847379\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 160\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         4156.71</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\"> -4.2336</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">            423.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 161000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-16-42\n",
      "  done: false\n",
      "  episode_len_mean: 424.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.2447999999999535\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 431\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8229707585440742\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013922012960405026\n",
      "          policy_loss: 0.03842991408374574\n",
      "          total_loss: 0.03342998375495275\n",
      "          vf_explained_var: 0.36459460854530334\n",
      "          vf_loss: 0.0038324159059104405\n",
      "    num_agent_steps_sampled: 161000\n",
      "    num_agent_steps_trained: 161000\n",
      "    num_steps_sampled: 161000\n",
      "    num_steps_trained: 161000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.63529411764706\n",
      "    ram_util_percent: 46.6764705882353\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038815707936914905\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.818681198948216\n",
      "    mean_inference_ms: 1.7175572407988005\n",
      "    mean_raw_obs_processing_ms: 1.7249152174363866\n",
      "  time_since_restore: 4180.082977771759\n",
      "  time_this_iter_s: 23.369844913482666\n",
      "  time_total_s: 4180.082977771759\n",
      "  timers:\n",
      "    learn_throughput: 1158.547\n",
      "    learn_time_ms: 863.15\n",
      "    load_throughput: 50928.572\n",
      "    load_time_ms: 19.635\n",
      "    sample_throughput: 41.886\n",
      "    sample_time_ms: 23874.466\n",
      "    update_time_ms: 2.89\n",
      "  timestamp: 1634847402\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 161000\n",
      "  training_iteration: 161\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         4180.08</td><td style=\"text-align: right;\">161000</td><td style=\"text-align: right;\"> -4.2448</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">            424.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 162000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-17-05\n",
      "  done: false\n",
      "  episode_len_mean: 426.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.2649999999999535\n",
      "  episode_reward_min: -5.539999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 433\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7054019252459207\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010849721157929398\n",
      "          policy_loss: -0.06885756187968783\n",
      "          total_loss: -0.07341295364830229\n",
      "          vf_explained_var: 0.7198887467384338\n",
      "          vf_loss: 0.005175065858768196\n",
      "    num_agent_steps_sampled: 162000\n",
      "    num_agent_steps_trained: 162000\n",
      "    num_steps_sampled: 162000\n",
      "    num_steps_trained: 162000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.709375\n",
      "    ram_util_percent: 46.734375\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03882583979395509\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.81348403568227\n",
      "    mean_inference_ms: 1.717814557894094\n",
      "    mean_raw_obs_processing_ms: 1.7231588619410922\n",
      "  time_since_restore: 4202.54011464119\n",
      "  time_this_iter_s: 22.457136869430542\n",
      "  time_total_s: 4202.54011464119\n",
      "  timers:\n",
      "    learn_throughput: 1188.267\n",
      "    learn_time_ms: 841.562\n",
      "    load_throughput: 50872.792\n",
      "    load_time_ms: 19.657\n",
      "    sample_throughput: 42.154\n",
      "    sample_time_ms: 23722.304\n",
      "    update_time_ms: 3.638\n",
      "  timestamp: 1634847425\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 162000\n",
      "  training_iteration: 162\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         4202.54</td><td style=\"text-align: right;\">162000</td><td style=\"text-align: right;\">  -4.265</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.54</td><td style=\"text-align: right;\">             426.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 163000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-17-25\n",
      "  done: false\n",
      "  episode_len_mean: 429.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.294499999999953\n",
      "  episode_reward_min: -5.549999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 435\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4822293334537082\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009767853269656178\n",
      "          policy_loss: -0.04659725551803907\n",
      "          total_loss: -0.04894050069981151\n",
      "          vf_explained_var: 0.16413085162639618\n",
      "          vf_loss: 0.005885752649757907\n",
      "    num_agent_steps_sampled: 163000\n",
      "    num_agent_steps_trained: 163000\n",
      "    num_steps_sampled: 163000\n",
      "    num_steps_trained: 163000\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.425\n",
      "    ram_util_percent: 46.80357142857142\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03883585524329398\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.807304957249542\n",
      "    mean_inference_ms: 1.7180671659781945\n",
      "    mean_raw_obs_processing_ms: 1.7213477866246263\n",
      "  time_since_restore: 4222.485567808151\n",
      "  time_this_iter_s: 19.94545316696167\n",
      "  time_total_s: 4222.485567808151\n",
      "  timers:\n",
      "    learn_throughput: 1188.617\n",
      "    learn_time_ms: 841.314\n",
      "    load_throughput: 52084.78\n",
      "    load_time_ms: 19.199\n",
      "    sample_throughput: 42.849\n",
      "    sample_time_ms: 23337.83\n",
      "    update_time_ms: 3.624\n",
      "  timestamp: 1634847445\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 163000\n",
      "  training_iteration: 163\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   163</td><td style=\"text-align: right;\">         4222.49</td><td style=\"text-align: right;\">163000</td><td style=\"text-align: right;\"> -4.2945</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.55</td><td style=\"text-align: right;\">            429.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-17-50\n",
      "  done: false\n",
      "  episode_len_mean: 431.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.311399999999952\n",
      "  episode_reward_min: -5.549999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 438\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.49455273548762\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008588304105649907\n",
      "          policy_loss: -0.03698330140776104\n",
      "          total_loss: -0.03950633224513796\n",
      "          vf_explained_var: 0.7330474257469177\n",
      "          vf_loss: 0.006625390147221171\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.61944444444447\n",
      "    ram_util_percent: 46.83888888888889\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038850719392809444\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.797592248142905\n",
      "    mean_inference_ms: 1.718441921519025\n",
      "    mean_raw_obs_processing_ms: 1.718748655713688\n",
      "  time_since_restore: 4247.470060825348\n",
      "  time_this_iter_s: 24.984493017196655\n",
      "  time_total_s: 4247.470060825348\n",
      "  timers:\n",
      "    learn_throughput: 1187.307\n",
      "    learn_time_ms: 842.242\n",
      "    load_throughput: 51679.894\n",
      "    load_time_ms: 19.35\n",
      "    sample_throughput: 42.201\n",
      "    sample_time_ms: 23695.848\n",
      "    update_time_ms: 3.657\n",
      "  timestamp: 1634847470\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 164\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         4247.47</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\"> -4.3114</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.55</td><td style=\"text-align: right;\">            431.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 165000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-18-11\n",
      "  done: false\n",
      "  episode_len_mean: 433.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.332999999999952\n",
      "  episode_reward_min: -5.549999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 440\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6168416765001086\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010695358012236393\n",
      "          policy_loss: 0.012686432815260357\n",
      "          total_loss: 0.008011821202105945\n",
      "          vf_explained_var: 0.4470149278640747\n",
      "          vf_loss: 0.0042744367163524855\n",
      "    num_agent_steps_sampled: 165000\n",
      "    num_agent_steps_trained: 165000\n",
      "    num_steps_sampled: 165000\n",
      "    num_steps_trained: 165000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.66000000000001\n",
      "    ram_util_percent: 46.91333333333333\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03886026717262696\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.790164724242285\n",
      "    mean_inference_ms: 1.7186757452963033\n",
      "    mean_raw_obs_processing_ms: 1.7170826640919292\n",
      "  time_since_restore: 4268.808587551117\n",
      "  time_this_iter_s: 21.338526725769043\n",
      "  time_total_s: 4268.808587551117\n",
      "  timers:\n",
      "    learn_throughput: 1192.178\n",
      "    learn_time_ms: 838.801\n",
      "    load_throughput: 52882.532\n",
      "    load_time_ms: 18.91\n",
      "    sample_throughput: 42.482\n",
      "    sample_time_ms: 23539.265\n",
      "    update_time_ms: 3.701\n",
      "  timestamp: 1634847491\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 165000\n",
      "  training_iteration: 165\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">         4268.81</td><td style=\"text-align: right;\">165000</td><td style=\"text-align: right;\">  -4.333</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.55</td><td style=\"text-align: right;\">             433.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 166000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-18-33\n",
      "  done: false\n",
      "  episode_len_mean: 434.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.347299999999951\n",
      "  episode_reward_min: -5.549999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 442\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6283488790194194\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012143154696756753\n",
      "          policy_loss: 0.034840885466999476\n",
      "          total_loss: 0.03381567531161838\n",
      "          vf_explained_var: 0.28987523913383484\n",
      "          vf_loss: 0.007061651699607157\n",
      "    num_agent_steps_sampled: 166000\n",
      "    num_agent_steps_trained: 166000\n",
      "    num_steps_sampled: 166000\n",
      "    num_steps_trained: 166000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.72258064516127\n",
      "    ram_util_percent: 46.8967741935484\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038869828006905464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.782253827079394\n",
      "    mean_inference_ms: 1.718906975152435\n",
      "    mean_raw_obs_processing_ms: 1.7153648955626386\n",
      "  time_since_restore: 4290.251407623291\n",
      "  time_this_iter_s: 21.442820072174072\n",
      "  time_total_s: 4290.251407623291\n",
      "  timers:\n",
      "    learn_throughput: 1191.337\n",
      "    learn_time_ms: 839.393\n",
      "    load_throughput: 53855.292\n",
      "    load_time_ms: 18.568\n",
      "    sample_throughput: 45.858\n",
      "    sample_time_ms: 21806.289\n",
      "    update_time_ms: 4.11\n",
      "  timestamp: 1634847513\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 166000\n",
      "  training_iteration: 166\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         4290.25</td><td style=\"text-align: right;\">166000</td><td style=\"text-align: right;\"> -4.3473</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.55</td><td style=\"text-align: right;\">            434.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 167000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-18-55\n",
      "  done: false\n",
      "  episode_len_mean: 436.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.0699999999999785\n",
      "  episode_reward_mean: -4.366199999999951\n",
      "  episode_reward_min: -5.549999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 444\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6655850410461426\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013115652058517835\n",
      "          policy_loss: -0.11472271821565098\n",
      "          total_loss: -0.11571007370948791\n",
      "          vf_explained_var: 0.6472861170768738\n",
      "          vf_loss: 0.006815427002341797\n",
      "    num_agent_steps_sampled: 167000\n",
      "    num_agent_steps_trained: 167000\n",
      "    num_steps_sampled: 167000\n",
      "    num_steps_trained: 167000\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.73870967741934\n",
      "    ram_util_percent: 46.9225806451613\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03887927055567434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.773708288893914\n",
      "    mean_inference_ms: 1.7191334400576956\n",
      "    mean_raw_obs_processing_ms: 1.7137191742574902\n",
      "  time_since_restore: 4312.22585606575\n",
      "  time_this_iter_s: 21.974448442459106\n",
      "  time_total_s: 4312.22585606575\n",
      "  timers:\n",
      "    learn_throughput: 1190.56\n",
      "    learn_time_ms: 839.941\n",
      "    load_throughput: 51627.286\n",
      "    load_time_ms: 19.37\n",
      "    sample_throughput: 46.332\n",
      "    sample_time_ms: 21583.296\n",
      "    update_time_ms: 3.505\n",
      "  timestamp: 1634847535\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 167000\n",
      "  training_iteration: 167\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         4312.23</td><td style=\"text-align: right;\">167000</td><td style=\"text-align: right;\"> -4.3662</td><td style=\"text-align: right;\">               -3.07</td><td style=\"text-align: right;\">               -5.55</td><td style=\"text-align: right;\">            436.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-19-14\n",
      "  done: false\n",
      "  episode_len_mean: 440.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.2999999999999736\n",
      "  episode_reward_mean: -4.40159999999995\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 446\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.659861813651191\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009988556405910703\n",
      "          policy_loss: -0.07825696178608471\n",
      "          total_loss: -0.08320243178556362\n",
      "          vf_explained_var: 0.24101798236370087\n",
      "          vf_loss: 0.004910873855826342\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.41785714285713\n",
      "    ram_util_percent: 46.960714285714296\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03888868855829575\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.764371529998616\n",
      "    mean_inference_ms: 1.719354433846732\n",
      "    mean_raw_obs_processing_ms: 1.71202130368777\n",
      "  time_since_restore: 4331.695075750351\n",
      "  time_this_iter_s: 19.46921968460083\n",
      "  time_total_s: 4331.695075750351\n",
      "  timers:\n",
      "    learn_throughput: 1194.342\n",
      "    learn_time_ms: 837.281\n",
      "    load_throughput: 51571.552\n",
      "    load_time_ms: 19.391\n",
      "    sample_throughput: 47.054\n",
      "    sample_time_ms: 21252.092\n",
      "    update_time_ms: 3.504\n",
      "  timestamp: 1634847554\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 168\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">          4331.7</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\"> -4.4016</td><td style=\"text-align: right;\">                -3.3</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">            440.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 169000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-19-35\n",
      "  done: false\n",
      "  episode_len_mean: 442.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.2999999999999736\n",
      "  episode_reward_mean: -4.42239999999995\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 448\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.65183893971973\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010006095136982899\n",
      "          policy_loss: -0.0960389532148838\n",
      "          total_loss: -0.102244876159562\n",
      "          vf_explained_var: 0.3743036091327667\n",
      "          vf_loss: 0.003558351539767399\n",
      "    num_agent_steps_sampled: 169000\n",
      "    num_agent_steps_trained: 169000\n",
      "    num_steps_sampled: 169000\n",
      "    num_steps_trained: 169000\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.75333333333333\n",
      "    ram_util_percent: 46.90333333333334\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0388977613020873\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.754371093361268\n",
      "    mean_inference_ms: 1.7195703036846144\n",
      "    mean_raw_obs_processing_ms: 1.7103894061908738\n",
      "  time_since_restore: 4352.348770856857\n",
      "  time_this_iter_s: 20.653695106506348\n",
      "  time_total_s: 4352.348770856857\n",
      "  timers:\n",
      "    learn_throughput: 1194.064\n",
      "    learn_time_ms: 837.476\n",
      "    load_throughput: 50476.617\n",
      "    load_time_ms: 19.811\n",
      "    sample_throughput: 47.542\n",
      "    sample_time_ms: 21034.202\n",
      "    update_time_ms: 3.51\n",
      "  timestamp: 1634847575\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 169000\n",
      "  training_iteration: 169\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         4352.35</td><td style=\"text-align: right;\">169000</td><td style=\"text-align: right;\"> -4.4224</td><td style=\"text-align: right;\">                -3.3</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">            442.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 170000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-20-15\n",
      "  done: false\n",
      "  episode_len_mean: 444.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.2999999999999736\n",
      "  episode_reward_mean: -4.44229999999995\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 450\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6882088396284316\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018773510444455285\n",
      "          policy_loss: -0.10693675668703186\n",
      "          total_loss: -0.10546580391625564\n",
      "          vf_explained_var: 0.8593601584434509\n",
      "          vf_loss: 0.005680919924957885\n",
      "    num_agent_steps_sampled: 170000\n",
      "    num_agent_steps_trained: 170000\n",
      "    num_steps_sampled: 170000\n",
      "    num_steps_trained: 170000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.7877192982456\n",
      "    ram_util_percent: 46.77543859649123\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03890694286874718\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.74381765448324\n",
      "    mean_inference_ms: 1.719783414785273\n",
      "    mean_raw_obs_processing_ms: 1.71081309078242\n",
      "  time_since_restore: 4392.457335472107\n",
      "  time_this_iter_s: 40.108564615249634\n",
      "  time_total_s: 4392.457335472107\n",
      "  timers:\n",
      "    learn_throughput: 1190.941\n",
      "    learn_time_ms: 839.672\n",
      "    load_throughput: 51696.901\n",
      "    load_time_ms: 19.344\n",
      "    sample_throughput: 44.038\n",
      "    sample_time_ms: 22707.565\n",
      "    update_time_ms: 3.659\n",
      "  timestamp: 1634847615\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 170000\n",
      "  training_iteration: 170\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">         4392.46</td><td style=\"text-align: right;\">170000</td><td style=\"text-align: right;\"> -4.4423</td><td style=\"text-align: right;\">                -3.3</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">            444.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 171000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-20-39\n",
      "  done: false\n",
      "  episode_len_mean: 446.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.2999999999999736\n",
      "  episode_reward_mean: -4.463099999999949\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 453\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4940922061602275\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012640010198728093\n",
      "          policy_loss: -0.006280012594328986\n",
      "          total_loss: -0.00905369259417057\n",
      "          vf_explained_var: 0.9292841553688049\n",
      "          vf_loss: 0.0036352345732868544\n",
      "    num_agent_steps_sampled: 171000\n",
      "    num_agent_steps_trained: 171000\n",
      "    num_steps_sampled: 171000\n",
      "    num_steps_trained: 171000\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.73529411764706\n",
      "    ram_util_percent: 46.632352941176464\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038920626189066415\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.72739098877543\n",
      "    mean_inference_ms: 1.720100598625378\n",
      "    mean_raw_obs_processing_ms: 1.7114778017771275\n",
      "  time_since_restore: 4416.742245197296\n",
      "  time_this_iter_s: 24.28490972518921\n",
      "  time_total_s: 4416.742245197296\n",
      "  timers:\n",
      "    learn_throughput: 1173.395\n",
      "    learn_time_ms: 852.228\n",
      "    load_throughput: 48484.804\n",
      "    load_time_ms: 20.625\n",
      "    sample_throughput: 43.889\n",
      "    sample_time_ms: 22784.96\n",
      "    update_time_ms: 3.856\n",
      "  timestamp: 1634847639\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 171000\n",
      "  training_iteration: 171\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         4416.74</td><td style=\"text-align: right;\">171000</td><td style=\"text-align: right;\"> -4.4631</td><td style=\"text-align: right;\">                -3.3</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">            446.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-21-04\n",
      "  done: false\n",
      "  episode_len_mean: 446.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.2999999999999736\n",
      "  episode_reward_mean: -4.465999999999949\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 455\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.589193332195282\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02345539363427657\n",
      "          policy_loss: -0.06836523032850689\n",
      "          total_loss: -0.06308170424567329\n",
      "          vf_explained_var: 0.7835512161254883\n",
      "          vf_loss: 0.0053430657096517585\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.72777777777777\n",
      "    ram_util_percent: 46.597222222222214\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038929711442540114\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.716244331159277\n",
      "    mean_inference_ms: 1.7203048404240449\n",
      "    mean_raw_obs_processing_ms: 1.7120115161969767\n",
      "  time_since_restore: 4441.902324676514\n",
      "  time_this_iter_s: 25.16007947921753\n",
      "  time_total_s: 4441.902324676514\n",
      "  timers:\n",
      "    learn_throughput: 1175.599\n",
      "    learn_time_ms: 850.63\n",
      "    load_throughput: 48820.592\n",
      "    load_time_ms: 20.483\n",
      "    sample_throughput: 43.37\n",
      "    sample_time_ms: 23057.469\n",
      "    update_time_ms: 3.293\n",
      "  timestamp: 1634847664\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 172\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">          4441.9</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">  -4.466</td><td style=\"text-align: right;\">                -3.3</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">             446.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 173000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-21-28\n",
      "  done: false\n",
      "  episode_len_mean: 448.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.46999999999997\n",
      "  episode_reward_mean: -4.481499999999949\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 458\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5664546449979146\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00877159976402015\n",
      "          policy_loss: -0.004102869000699785\n",
      "          total_loss: -0.008804197278287676\n",
      "          vf_explained_var: 0.943860650062561\n",
      "          vf_loss: 0.002081969730918192\n",
      "    num_agent_steps_sampled: 173000\n",
      "    num_agent_steps_trained: 173000\n",
      "    num_steps_sampled: 173000\n",
      "    num_steps_trained: 173000\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.72285714285714\n",
      "    ram_util_percent: 46.680000000000014\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038943163735531985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.69908213804796\n",
      "    mean_inference_ms: 1.7206053813953386\n",
      "    mean_raw_obs_processing_ms: 1.711531184455825\n",
      "  time_since_restore: 4465.921485900879\n",
      "  time_this_iter_s: 24.019161224365234\n",
      "  time_total_s: 4465.921485900879\n",
      "  timers:\n",
      "    learn_throughput: 1174.403\n",
      "    learn_time_ms: 851.496\n",
      "    load_throughput: 47655.126\n",
      "    load_time_ms: 20.984\n",
      "    sample_throughput: 42.62\n",
      "    sample_time_ms: 23463.278\n",
      "    update_time_ms: 3.469\n",
      "  timestamp: 1634847688\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 173000\n",
      "  training_iteration: 173\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         4465.92</td><td style=\"text-align: right;\">173000</td><td style=\"text-align: right;\"> -4.4815</td><td style=\"text-align: right;\">               -3.47</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">            448.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 174000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-21-53\n",
      "  done: false\n",
      "  episode_len_mean: 448.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.46999999999997\n",
      "  episode_reward_mean: -4.486999999999949\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 460\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6100574519899156\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012267176940595408\n",
      "          policy_loss: -0.006534375995397568\n",
      "          total_loss: -0.008076133413447274\n",
      "          vf_explained_var: 0.9378746747970581\n",
      "          vf_loss: 0.0021382943144999445\n",
      "    num_agent_steps_sampled: 174000\n",
      "    num_agent_steps_trained: 174000\n",
      "    num_steps_sampled: 174000\n",
      "    num_steps_trained: 174000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.67142857142858\n",
      "    ram_util_percent: 46.748571428571424\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03895214459250433\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.68748851849591\n",
      "    mean_inference_ms: 1.7208031920692886\n",
      "    mean_raw_obs_processing_ms: 1.7094505587134854\n",
      "  time_since_restore: 4490.6272802352905\n",
      "  time_this_iter_s: 24.70579433441162\n",
      "  time_total_s: 4490.6272802352905\n",
      "  timers:\n",
      "    learn_throughput: 1178.068\n",
      "    learn_time_ms: 848.847\n",
      "    load_throughput: 48252.931\n",
      "    load_time_ms: 20.724\n",
      "    sample_throughput: 42.665\n",
      "    sample_time_ms: 23438.333\n",
      "    update_time_ms: 3.593\n",
      "  timestamp: 1634847713\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 174000\n",
      "  training_iteration: 174\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         4490.63</td><td style=\"text-align: right;\">174000</td><td style=\"text-align: right;\">  -4.487</td><td style=\"text-align: right;\">               -3.47</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">             448.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 175000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-22-18\n",
      "  done: false\n",
      "  episode_len_mean: 448.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.46999999999997\n",
      "  episode_reward_mean: -4.488499999999949\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 463\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5228741394148932\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007848629648733861\n",
      "          policy_loss: -0.01568569971455468\n",
      "          total_loss: -0.01971391588449478\n",
      "          vf_explained_var: 0.9184638857841492\n",
      "          vf_loss: 0.0032537859528222017\n",
      "    num_agent_steps_sampled: 175000\n",
      "    num_agent_steps_trained: 175000\n",
      "    num_steps_sampled: 175000\n",
      "    num_steps_trained: 175000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.79166666666667\n",
      "    ram_util_percent: 46.8138888888889\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03896532215260726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.66954633911431\n",
      "    mean_inference_ms: 1.7210870029031171\n",
      "    mean_raw_obs_processing_ms: 1.7065580100028694\n",
      "  time_since_restore: 4515.6923089027405\n",
      "  time_this_iter_s: 25.06502866744995\n",
      "  time_total_s: 4515.6923089027405\n",
      "  timers:\n",
      "    learn_throughput: 1173.777\n",
      "    learn_time_ms: 851.951\n",
      "    load_throughput: 48192.333\n",
      "    load_time_ms: 20.75\n",
      "    sample_throughput: 42.003\n",
      "    sample_time_ms: 23807.82\n",
      "    update_time_ms: 3.587\n",
      "  timestamp: 1634847738\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 175000\n",
      "  training_iteration: 175\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         4515.69</td><td style=\"text-align: right;\">175000</td><td style=\"text-align: right;\"> -4.4885</td><td style=\"text-align: right;\">               -3.47</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">            448.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-22-43\n",
      "  done: false\n",
      "  episode_len_mean: 448.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.46999999999997\n",
      "  episode_reward_mean: -4.484299999999949\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 465\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6308146715164185\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008693048088515098\n",
      "          policy_loss: -0.039886906639569335\n",
      "          total_loss: -0.044144174001283114\n",
      "          vf_explained_var: 0.923541247844696\n",
      "          vf_loss: 0.0032491685424853737\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.94285714285712\n",
      "    ram_util_percent: 46.84571428571428\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038974678964817294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.657693381820938\n",
      "    mean_inference_ms: 1.7212765569400421\n",
      "    mean_raw_obs_processing_ms: 1.7046164025488972\n",
      "  time_since_restore: 4540.340596437454\n",
      "  time_this_iter_s: 24.648287534713745\n",
      "  time_total_s: 4540.340596437454\n",
      "  timers:\n",
      "    learn_throughput: 1172.675\n",
      "    learn_time_ms: 852.751\n",
      "    load_throughput: 50272.126\n",
      "    load_time_ms: 19.892\n",
      "    sample_throughput: 41.444\n",
      "    sample_time_ms: 24128.719\n",
      "    update_time_ms: 3.224\n",
      "  timestamp: 1634847763\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 176\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">         4540.34</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\"> -4.4843</td><td style=\"text-align: right;\">               -3.47</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">            448.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 177000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-23-08\n",
      "  done: false\n",
      "  episode_len_mean: 447.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.46999999999997\n",
      "  episode_reward_mean: -4.478099999999949\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 468\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5157599382930331\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008211699660964753\n",
      "          policy_loss: -0.09206119328737258\n",
      "          total_loss: -0.0945090886619356\n",
      "          vf_explained_var: 0.8897733092308044\n",
      "          vf_loss: 0.004395356353941477\n",
      "    num_agent_steps_sampled: 177000\n",
      "    num_agent_steps_trained: 177000\n",
      "    num_steps_sampled: 177000\n",
      "    num_steps_trained: 177000\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.67142857142858\n",
      "    ram_util_percent: 46.94857142857143\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03898941104805778\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.640666441244594\n",
      "    mean_inference_ms: 1.721557530020108\n",
      "    mean_raw_obs_processing_ms: 1.7019351677267378\n",
      "  time_since_restore: 4565.213492870331\n",
      "  time_this_iter_s: 24.872896432876587\n",
      "  time_total_s: 4565.213492870331\n",
      "  timers:\n",
      "    learn_throughput: 1176.864\n",
      "    learn_time_ms: 849.716\n",
      "    load_throughput: 52959.521\n",
      "    load_time_ms: 18.882\n",
      "    sample_throughput: 40.946\n",
      "    sample_time_ms: 24422.37\n",
      "    update_time_ms: 3.386\n",
      "  timestamp: 1634847788\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 177000\n",
      "  training_iteration: 177\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         4565.21</td><td style=\"text-align: right;\">177000</td><td style=\"text-align: right;\"> -4.4781</td><td style=\"text-align: right;\">               -3.47</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">            447.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 178000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-23-31\n",
      "  done: false\n",
      "  episode_len_mean: 446.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.46999999999997\n",
      "  episode_reward_mean: -4.467399999999949\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 470\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5590523838996888\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00807408830140209\n",
      "          policy_loss: 0.05646333032184177\n",
      "          total_loss: 0.05084997351384825\n",
      "          vf_explained_var: 0.9577431678771973\n",
      "          vf_loss: 0.0018021528204877136\n",
      "    num_agent_steps_sampled: 178000\n",
      "    num_agent_steps_trained: 178000\n",
      "    num_steps_sampled: 178000\n",
      "    num_steps_trained: 178000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.36363636363636\n",
      "    ram_util_percent: 46.92424242424242\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038999191025878774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.629731547333275\n",
      "    mean_inference_ms: 1.7217454513016444\n",
      "    mean_raw_obs_processing_ms: 1.7002509583715224\n",
      "  time_since_restore: 4588.0340411663055\n",
      "  time_this_iter_s: 22.82054829597473\n",
      "  time_total_s: 4588.0340411663055\n",
      "  timers:\n",
      "    learn_throughput: 1176.856\n",
      "    learn_time_ms: 849.721\n",
      "    load_throughput: 49867.897\n",
      "    load_time_ms: 20.053\n",
      "    sample_throughput: 40.394\n",
      "    sample_time_ms: 24756.296\n",
      "    update_time_ms: 3.399\n",
      "  timestamp: 1634847811\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 178000\n",
      "  training_iteration: 178\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   178</td><td style=\"text-align: right;\">         4588.03</td><td style=\"text-align: right;\">178000</td><td style=\"text-align: right;\"> -4.4674</td><td style=\"text-align: right;\">               -3.47</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">            446.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 179000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-23-56\n",
      "  done: false\n",
      "  episode_len_mean: 445.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.46999999999997\n",
      "  episode_reward_mean: -4.45299999999995\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 473\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5048082921240065\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0072321794435290425\n",
      "          policy_loss: -0.07310117267900043\n",
      "          total_loss: -0.07749543074104522\n",
      "          vf_explained_var: 0.890488862991333\n",
      "          vf_loss: 0.0033312419248330925\n",
      "    num_agent_steps_sampled: 179000\n",
      "    num_agent_steps_trained: 179000\n",
      "    num_steps_sampled: 179000\n",
      "    num_steps_trained: 179000\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.70277777777778\n",
      "    ram_util_percent: 46.95277777777778\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03901365153151982\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.613795456425166\n",
      "    mean_inference_ms: 1.722020564409579\n",
      "    mean_raw_obs_processing_ms: 1.69776627521526\n",
      "  time_since_restore: 4613.315026283264\n",
      "  time_this_iter_s: 25.280985116958618\n",
      "  time_total_s: 4613.315026283264\n",
      "  timers:\n",
      "    learn_throughput: 1178.165\n",
      "    learn_time_ms: 848.778\n",
      "    load_throughput: 52546.373\n",
      "    load_time_ms: 19.031\n",
      "    sample_throughput: 39.65\n",
      "    sample_time_ms: 25220.986\n",
      "    update_time_ms: 3.482\n",
      "  timestamp: 1634847836\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 179000\n",
      "  training_iteration: 179\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         4613.32</td><td style=\"text-align: right;\">179000</td><td style=\"text-align: right;\">  -4.453</td><td style=\"text-align: right;\">               -3.47</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">             445.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-24-20\n",
      "  done: false\n",
      "  episode_len_mean: 444.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.46999999999997\n",
      "  episode_reward_mean: -4.445899999999949\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 475\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5026196546024746\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009028913805103992\n",
      "          policy_loss: -0.015203424874279234\n",
      "          total_loss: -0.018918184749782085\n",
      "          vf_explained_var: 0.9083272218704224\n",
      "          vf_loss: 0.002169663195187847\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.77714285714286\n",
      "    ram_util_percent: 46.91142857142858\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039023222274586544\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.60338504506624\n",
      "    mean_inference_ms: 1.7222025568468826\n",
      "    mean_raw_obs_processing_ms: 1.6962134586360114\n",
      "  time_since_restore: 4637.636611938477\n",
      "  time_this_iter_s: 24.321585655212402\n",
      "  time_total_s: 4637.636611938477\n",
      "  timers:\n",
      "    learn_throughput: 1181.35\n",
      "    learn_time_ms: 846.489\n",
      "    load_throughput: 52511.374\n",
      "    load_time_ms: 19.043\n",
      "    sample_throughput: 42.293\n",
      "    sample_time_ms: 23644.679\n",
      "    update_time_ms: 3.327\n",
      "  timestamp: 1634847860\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 180\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         4637.64</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\"> -4.4459</td><td style=\"text-align: right;\">               -3.47</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">            444.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 181000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-24-46\n",
      "  done: false\n",
      "  episode_len_mean: 444.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.46999999999997\n",
      "  episode_reward_mean: -4.44719999999995\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 478\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4442136181725396\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0063403055068335665\n",
      "          policy_loss: -0.1745238231288062\n",
      "          total_loss: -0.18093461038337813\n",
      "          vf_explained_var: 0.9716921448707581\n",
      "          vf_loss: 0.0016117898934882961\n",
      "    num_agent_steps_sampled: 181000\n",
      "    num_agent_steps_trained: 181000\n",
      "    num_steps_sampled: 181000\n",
      "    num_steps_trained: 181000\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.02702702702705\n",
      "    ram_util_percent: 46.87567567567568\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039037165111786176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.587894021693458\n",
      "    mean_inference_ms: 1.7224725492072832\n",
      "    mean_raw_obs_processing_ms: 1.694036251400793\n",
      "  time_since_restore: 4663.442130088806\n",
      "  time_this_iter_s: 25.80551815032959\n",
      "  time_total_s: 4663.442130088806\n",
      "  timers:\n",
      "    learn_throughput: 1199.351\n",
      "    learn_time_ms: 833.784\n",
      "    load_throughput: 57356.748\n",
      "    load_time_ms: 17.435\n",
      "    sample_throughput: 41.999\n",
      "    sample_time_ms: 23810.299\n",
      "    update_time_ms: 3.623\n",
      "  timestamp: 1634847886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 181000\n",
      "  training_iteration: 181\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">         4663.44</td><td style=\"text-align: right;\">181000</td><td style=\"text-align: right;\"> -4.4472</td><td style=\"text-align: right;\">               -3.47</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">            444.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 182000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-25-26\n",
      "  done: false\n",
      "  episode_len_mean: 445.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.46999999999997\n",
      "  episode_reward_mean: -4.45129999999995\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 480\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.514094066619873\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012548123880705435\n",
      "          policy_loss: 0.09954904218514761\n",
      "          total_loss: 0.10371940926545196\n",
      "          vf_explained_var: 0.636283278465271\n",
      "          vf_loss: 0.006606334711088695\n",
      "    num_agent_steps_sampled: 182000\n",
      "    num_agent_steps_trained: 182000\n",
      "    num_steps_sampled: 182000\n",
      "    num_steps_trained: 182000\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.85964912280701\n",
      "    ram_util_percent: 46.82982456140351\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039046511902901385\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.57735493571408\n",
      "    mean_inference_ms: 1.7226547174017037\n",
      "    mean_raw_obs_processing_ms: 1.6945679664305675\n",
      "  time_since_restore: 4703.353160381317\n",
      "  time_this_iter_s: 39.911030292510986\n",
      "  time_total_s: 4703.353160381317\n",
      "  timers:\n",
      "    learn_throughput: 1194.918\n",
      "    learn_time_ms: 836.877\n",
      "    load_throughput: 57162.42\n",
      "    load_time_ms: 17.494\n",
      "    sample_throughput: 39.554\n",
      "    sample_time_ms: 25281.824\n",
      "    update_time_ms: 3.841\n",
      "  timestamp: 1634847926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 182000\n",
      "  training_iteration: 182\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         4703.35</td><td style=\"text-align: right;\">182000</td><td style=\"text-align: right;\"> -4.4513</td><td style=\"text-align: right;\">               -3.47</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">            445.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 183000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-25-49\n",
      "  done: false\n",
      "  episode_len_mean: 444.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.46999999999997\n",
      "  episode_reward_mean: -4.446899999999949\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 482\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.489677005343967\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00883880816783981\n",
      "          policy_loss: -0.012363350060251023\n",
      "          total_loss: -0.012443852755758497\n",
      "          vf_explained_var: 0.7718408107757568\n",
      "          vf_loss: 0.005866974482261058\n",
      "    num_agent_steps_sampled: 183000\n",
      "    num_agent_steps_trained: 183000\n",
      "    num_steps_sampled: 183000\n",
      "    num_steps_trained: 183000\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.72424242424243\n",
      "    ram_util_percent: 46.87575757575757\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03905596504416459\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.56700752654324\n",
      "    mean_inference_ms: 1.7228353249480222\n",
      "    mean_raw_obs_processing_ms: 1.6951478360714822\n",
      "  time_since_restore: 4726.454880475998\n",
      "  time_this_iter_s: 23.101720094680786\n",
      "  time_total_s: 4726.454880475998\n",
      "  timers:\n",
      "    learn_throughput: 1194.135\n",
      "    learn_time_ms: 837.426\n",
      "    load_throughput: 57095.112\n",
      "    load_time_ms: 17.515\n",
      "    sample_throughput: 39.699\n",
      "    sample_time_ms: 25189.596\n",
      "    update_time_ms: 3.618\n",
      "  timestamp: 1634847949\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 183000\n",
      "  training_iteration: 183\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">         4726.45</td><td style=\"text-align: right;\">183000</td><td style=\"text-align: right;\"> -4.4469</td><td style=\"text-align: right;\">               -3.47</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">            444.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-26-10\n",
      "  done: false\n",
      "  episode_len_mean: 445.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.46999999999997\n",
      "  episode_reward_mean: -4.45189999999995\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 484\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.440977864795261\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007714093540193037\n",
      "          policy_loss: 0.025975156616833476\n",
      "          total_loss: 0.027080045971605512\n",
      "          vf_explained_var: 0.7298194766044617\n",
      "          vf_loss: 0.007704151306663536\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.80689655172412\n",
      "    ram_util_percent: 46.896551724137936\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039065282732312315\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.556772833131834\n",
      "    mean_inference_ms: 1.7230151608679356\n",
      "    mean_raw_obs_processing_ms: 1.6957737581925392\n",
      "  time_since_restore: 4747.26126909256\n",
      "  time_this_iter_s: 20.80638861656189\n",
      "  time_total_s: 4747.26126909256\n",
      "  timers:\n",
      "    learn_throughput: 1190.301\n",
      "    learn_time_ms: 840.124\n",
      "    load_throughput: 57023.853\n",
      "    load_time_ms: 17.537\n",
      "    sample_throughput: 40.329\n",
      "    sample_time_ms: 24795.969\n",
      "    update_time_ms: 4.343\n",
      "  timestamp: 1634847970\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 184\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">         4747.26</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\"> -4.4519</td><td style=\"text-align: right;\">               -3.47</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">            445.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 185000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-26-33\n",
      "  done: false\n",
      "  episode_len_mean: 444.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.46999999999997\n",
      "  episode_reward_mean: -4.44509999999995\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 486\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5207295603222317\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00832126378563253\n",
      "          policy_loss: -0.03359204911523395\n",
      "          total_loss: -0.03319776819811927\n",
      "          vf_explained_var: 0.32322970032691956\n",
      "          vf_loss: 0.007176296676819524\n",
      "    num_agent_steps_sampled: 185000\n",
      "    num_agent_steps_trained: 185000\n",
      "    num_steps_sampled: 185000\n",
      "    num_steps_trained: 185000\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.93636363636364\n",
      "    ram_util_percent: 46.85151515151515\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03907462609710163\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.546810435410897\n",
      "    mean_inference_ms: 1.7231940696467745\n",
      "    mean_raw_obs_processing_ms: 1.6964446472299146\n",
      "  time_since_restore: 4769.974945068359\n",
      "  time_this_iter_s: 22.71367597579956\n",
      "  time_total_s: 4769.974945068359\n",
      "  timers:\n",
      "    learn_throughput: 1189.944\n",
      "    learn_time_ms: 840.376\n",
      "    load_throughput: 51914.65\n",
      "    load_time_ms: 19.262\n",
      "    sample_throughput: 40.72\n",
      "    sample_time_ms: 24558.045\n",
      "    update_time_ms: 5.087\n",
      "  timestamp: 1634847993\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 185000\n",
      "  training_iteration: 185\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">         4769.97</td><td style=\"text-align: right;\">185000</td><td style=\"text-align: right;\"> -4.4451</td><td style=\"text-align: right;\">               -3.47</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">            444.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 186000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-26-55\n",
      "  done: false\n",
      "  episode_len_mean: 443.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.46999999999997\n",
      "  episode_reward_mean: -4.43859999999995\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 488\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6209528247515361\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015883775101779622\n",
      "          policy_loss: -0.12991484685076607\n",
      "          total_loss: -0.1228571461306678\n",
      "          vf_explained_var: 0.6948795914649963\n",
      "          vf_loss: 0.00718490937207308\n",
      "    num_agent_steps_sampled: 186000\n",
      "    num_agent_steps_trained: 186000\n",
      "    num_steps_sampled: 186000\n",
      "    num_steps_trained: 186000\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.75625\n",
      "    ram_util_percent: 46.846875\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039084038625953624\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.53704767303191\n",
      "    mean_inference_ms: 1.723371748652991\n",
      "    mean_raw_obs_processing_ms: 1.6971622196121299\n",
      "  time_since_restore: 4792.171061992645\n",
      "  time_this_iter_s: 22.19611692428589\n",
      "  time_total_s: 4792.171061992645\n",
      "  timers:\n",
      "    learn_throughput: 1187.644\n",
      "    learn_time_ms: 842.003\n",
      "    load_throughput: 49674.475\n",
      "    load_time_ms: 20.131\n",
      "    sample_throughput: 41.135\n",
      "    sample_time_ms: 24310.075\n",
      "    update_time_ms: 5.304\n",
      "  timestamp: 1634848015\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 186000\n",
      "  training_iteration: 186\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">         4792.17</td><td style=\"text-align: right;\">186000</td><td style=\"text-align: right;\"> -4.4386</td><td style=\"text-align: right;\">               -3.47</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">            443.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 187000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-27-17\n",
      "  done: false\n",
      "  episode_len_mean: 444.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.46999999999997\n",
      "  episode_reward_mean: -4.43999999999995\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 491\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.564681315422058\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0077049924951512515\n",
      "          policy_loss: 0.09342755493190553\n",
      "          total_loss: 0.08896308756536908\n",
      "          vf_explained_var: 0.5204886794090271\n",
      "          vf_loss: 0.0033810407220799888\n",
      "    num_agent_steps_sampled: 187000\n",
      "    num_agent_steps_trained: 187000\n",
      "    num_steps_sampled: 187000\n",
      "    num_steps_trained: 187000\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.62903225806451\n",
      "    ram_util_percent: 46.82903225806452\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03909780011844982\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.52278375754336\n",
      "    mean_inference_ms: 1.7236374556947225\n",
      "    mean_raw_obs_processing_ms: 1.6958134898067425\n",
      "  time_since_restore: 4814.162093639374\n",
      "  time_this_iter_s: 21.991031646728516\n",
      "  time_total_s: 4814.162093639374\n",
      "  timers:\n",
      "    learn_throughput: 1180.794\n",
      "    learn_time_ms: 846.888\n",
      "    load_throughput: 49123.811\n",
      "    load_time_ms: 20.357\n",
      "    sample_throughput: 41.637\n",
      "    sample_time_ms: 24016.986\n",
      "    update_time_ms: 5.248\n",
      "  timestamp: 1634848037\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 187000\n",
      "  training_iteration: 187\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">         4814.16</td><td style=\"text-align: right;\">187000</td><td style=\"text-align: right;\">   -4.44</td><td style=\"text-align: right;\">               -3.47</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">               444</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-27-41\n",
      "  done: false\n",
      "  episode_len_mean: 440.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.406499999999951\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 493\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5151772763994005\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005752167148486759\n",
      "          policy_loss: -0.1610805683251884\n",
      "          total_loss: -0.15953966875871023\n",
      "          vf_explained_var: 0.07961095124483109\n",
      "          vf_loss: 0.010868598648812622\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.78235294117647\n",
      "    ram_util_percent: 46.82058823529411\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03910647294432793\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.514028847310772\n",
      "    mean_inference_ms: 1.7238130485066814\n",
      "    mean_raw_obs_processing_ms: 1.6941954334070062\n",
      "  time_since_restore: 4838.304941177368\n",
      "  time_this_iter_s: 24.142847537994385\n",
      "  time_total_s: 4838.304941177368\n",
      "  timers:\n",
      "    learn_throughput: 1181.028\n",
      "    learn_time_ms: 846.72\n",
      "    load_throughput: 50548.948\n",
      "    load_time_ms: 19.783\n",
      "    sample_throughput: 41.408\n",
      "    sample_time_ms: 24149.973\n",
      "    update_time_ms: 5.24\n",
      "  timestamp: 1634848061\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 188\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   188</td><td style=\"text-align: right;\">          4838.3</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\"> -4.4065</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">            440.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 189000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-28-00\n",
      "  done: false\n",
      "  episode_len_mean: 441.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.41279999999995\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 495\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.73355014456643\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0058130841962131955\n",
      "          policy_loss: -0.10620926519234976\n",
      "          total_loss: -0.11067495495080948\n",
      "          vf_explained_var: -0.032323796302080154\n",
      "          vf_loss: 0.006984064930778308\n",
      "    num_agent_steps_sampled: 189000\n",
      "    num_agent_steps_trained: 189000\n",
      "    num_steps_sampled: 189000\n",
      "    num_steps_trained: 189000\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.18888888888888\n",
      "    ram_util_percent: 46.87777777777778\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03911516476132202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.505476687059783\n",
      "    mean_inference_ms: 1.7239899200986093\n",
      "    mean_raw_obs_processing_ms: 1.6926368311807545\n",
      "  time_since_restore: 4856.962490797043\n",
      "  time_this_iter_s: 18.657549619674683\n",
      "  time_total_s: 4856.962490797043\n",
      "  timers:\n",
      "    learn_throughput: 1184.388\n",
      "    learn_time_ms: 844.318\n",
      "    load_throughput: 47377.58\n",
      "    load_time_ms: 21.107\n",
      "    sample_throughput: 42.573\n",
      "    sample_time_ms: 23488.987\n",
      "    update_time_ms: 5.075\n",
      "  timestamp: 1634848080\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 189000\n",
      "  training_iteration: 189\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">         4856.96</td><td style=\"text-align: right;\">189000</td><td style=\"text-align: right;\"> -4.4128</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">            441.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 190000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-28-22\n",
      "  done: false\n",
      "  episode_len_mean: 439.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.399899999999951\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 497\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.641976045237647\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009079749126236594\n",
      "          policy_loss: -0.07657364213632213\n",
      "          total_loss: -0.07935937055283122\n",
      "          vf_explained_var: 0.8013232946395874\n",
      "          vf_loss: 0.004440782657669237\n",
      "    num_agent_steps_sampled: 190000\n",
      "    num_agent_steps_trained: 190000\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.10322580645159\n",
      "    ram_util_percent: 46.903225806451616\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03912380331705869\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.497199600482354\n",
      "    mean_inference_ms: 1.7241664953666083\n",
      "    mean_raw_obs_processing_ms: 1.6911345253274868\n",
      "  time_since_restore: 4878.609557390213\n",
      "  time_this_iter_s: 21.647066593170166\n",
      "  time_total_s: 4878.609557390213\n",
      "  timers:\n",
      "    learn_throughput: 1184.868\n",
      "    learn_time_ms: 843.976\n",
      "    load_throughput: 45236.726\n",
      "    load_time_ms: 22.106\n",
      "    sample_throughput: 43.065\n",
      "    sample_time_ms: 23220.793\n",
      "    update_time_ms: 5.222\n",
      "  timestamp: 1634848102\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 190\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   190</td><td style=\"text-align: right;\">         4878.61</td><td style=\"text-align: right;\">190000</td><td style=\"text-align: right;\"> -4.3999</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">            439.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 191000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-28-41\n",
      "  done: false\n",
      "  episode_len_mean: 441.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.41119999999995\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 499\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7439887616369458\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009460900767451522\n",
      "          policy_loss: -0.059126893017027116\n",
      "          total_loss: -0.058803168104754555\n",
      "          vf_explained_var: -0.029999587684869766\n",
      "          vf_loss: 0.008184445658116601\n",
      "    num_agent_steps_sampled: 191000\n",
      "    num_agent_steps_trained: 191000\n",
      "    num_steps_sampled: 191000\n",
      "    num_steps_trained: 191000\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.65172413793104\n",
      "    ram_util_percent: 46.93793103448275\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03913246871881945\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.48903054029569\n",
      "    mean_inference_ms: 1.7243423672494425\n",
      "    mean_raw_obs_processing_ms: 1.6896888103711212\n",
      "  time_since_restore: 4898.54544711113\n",
      "  time_this_iter_s: 19.935889720916748\n",
      "  time_total_s: 4898.54544711113\n",
      "  timers:\n",
      "    learn_throughput: 1182.83\n",
      "    learn_time_ms: 845.43\n",
      "    load_throughput: 43117.341\n",
      "    load_time_ms: 23.193\n",
      "    sample_throughput: 44.185\n",
      "    sample_time_ms: 22632.231\n",
      "    update_time_ms: 4.771\n",
      "  timestamp: 1634848121\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 191000\n",
      "  training_iteration: 191\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">         4898.55</td><td style=\"text-align: right;\">191000</td><td style=\"text-align: right;\"> -4.4112</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">            441.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-29-04\n",
      "  done: false\n",
      "  episode_len_mean: 440.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.40319999999995\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 501\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.507932256327735\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008585579270483616\n",
      "          policy_loss: -0.028942386474874286\n",
      "          total_loss: -0.030860565271642472\n",
      "          vf_explained_var: 0.8260666728019714\n",
      "          vf_loss: 0.0044682478713285595\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.80967741935484\n",
      "    ram_util_percent: 46.91290322580647\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03914092365298182\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.48078623452423\n",
      "    mean_inference_ms: 1.724516821233481\n",
      "    mean_raw_obs_processing_ms: 1.6882905211536197\n",
      "  time_since_restore: 4920.704416513443\n",
      "  time_this_iter_s: 22.158969402313232\n",
      "  time_total_s: 4920.704416513443\n",
      "  timers:\n",
      "    learn_throughput: 1184.956\n",
      "    learn_time_ms: 843.913\n",
      "    load_throughput: 42349.895\n",
      "    load_time_ms: 23.613\n",
      "    sample_throughput: 47.943\n",
      "    sample_time_ms: 20858.293\n",
      "    update_time_ms: 4.778\n",
      "  timestamp: 1634848144\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 192\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">          4920.7</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\"> -4.4032</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">            440.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 193000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-29-23\n",
      "  done: false\n",
      "  episode_len_mean: 442.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.42659999999995\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 503\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.629946490128835\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010196206794061248\n",
      "          policy_loss: -0.09289681216080983\n",
      "          total_loss: -0.08772679070631663\n",
      "          vf_explained_var: 0.19400416314601898\n",
      "          vf_loss: 0.011145830475854584\n",
      "    num_agent_steps_sampled: 193000\n",
      "    num_agent_steps_trained: 193000\n",
      "    num_steps_sampled: 193000\n",
      "    num_steps_trained: 193000\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.69642857142857\n",
      "    ram_util_percent: 46.896428571428565\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03914937065948527\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.47217406920223\n",
      "    mean_inference_ms: 1.7246932819436154\n",
      "    mean_raw_obs_processing_ms: 1.6868465475189784\n",
      "  time_since_restore: 4940.510856151581\n",
      "  time_this_iter_s: 19.806439638137817\n",
      "  time_total_s: 4940.510856151581\n",
      "  timers:\n",
      "    learn_throughput: 1184.153\n",
      "    learn_time_ms: 844.486\n",
      "    load_throughput: 41554.258\n",
      "    load_time_ms: 24.065\n",
      "    sample_throughput: 48.715\n",
      "    sample_time_ms: 20527.69\n",
      "    update_time_ms: 4.861\n",
      "  timestamp: 1634848163\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 193000\n",
      "  training_iteration: 193\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   193</td><td style=\"text-align: right;\">         4940.51</td><td style=\"text-align: right;\">193000</td><td style=\"text-align: right;\"> -4.4266</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">            442.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 194000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-29-42\n",
      "  done: false\n",
      "  episode_len_mean: 444.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.445199999999949\n",
      "  episode_reward_min: -6.399999999999908\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 505\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7393096605936686\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006196450016658951\n",
      "          policy_loss: -0.027445056041081748\n",
      "          total_loss: -0.03180226153797573\n",
      "          vf_explained_var: 0.6765769720077515\n",
      "          vf_loss: 0.006761982020301123\n",
      "    num_agent_steps_sampled: 194000\n",
      "    num_agent_steps_trained: 194000\n",
      "    num_steps_sampled: 194000\n",
      "    num_steps_trained: 194000\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.55925925925928\n",
      "    ram_util_percent: 46.91851851851851\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039157716678902915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.463160664890147\n",
      "    mean_inference_ms: 1.724869143965156\n",
      "    mean_raw_obs_processing_ms: 1.6854569038566416\n",
      "  time_since_restore: 4959.184136867523\n",
      "  time_this_iter_s: 18.673280715942383\n",
      "  time_total_s: 4959.184136867523\n",
      "  timers:\n",
      "    learn_throughput: 1185.557\n",
      "    learn_time_ms: 843.486\n",
      "    load_throughput: 39707.207\n",
      "    load_time_ms: 25.184\n",
      "    sample_throughput: 49.224\n",
      "    sample_time_ms: 20315.374\n",
      "    update_time_ms: 3.91\n",
      "  timestamp: 1634848182\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 194000\n",
      "  training_iteration: 194\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   194</td><td style=\"text-align: right;\">         4959.18</td><td style=\"text-align: right;\">194000</td><td style=\"text-align: right;\"> -4.4452</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">                -6.4</td><td style=\"text-align: right;\">            444.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 195000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-30-03\n",
      "  done: false\n",
      "  episode_len_mean: 446.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.46559999999995\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 507\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6499567243787978\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010043550065101749\n",
      "          policy_loss: 0.004947047432263692\n",
      "          total_loss: 0.007168122132619222\n",
      "          vf_explained_var: -0.09100385010242462\n",
      "          vf_loss: 0.008551543789669975\n",
      "    num_agent_steps_sampled: 195000\n",
      "    num_agent_steps_trained: 195000\n",
      "    num_steps_sampled: 195000\n",
      "    num_steps_trained: 195000\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.60666666666665\n",
      "    ram_util_percent: 46.96333333333334\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03916599248264217\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.453951672871362\n",
      "    mean_inference_ms: 1.7250431459994717\n",
      "    mean_raw_obs_processing_ms: 1.6841199731695446\n",
      "  time_since_restore: 4979.827257633209\n",
      "  time_this_iter_s: 20.643120765686035\n",
      "  time_total_s: 4979.827257633209\n",
      "  timers:\n",
      "    learn_throughput: 1182.46\n",
      "    learn_time_ms: 845.695\n",
      "    load_throughput: 42616.422\n",
      "    load_time_ms: 23.465\n",
      "    sample_throughput: 49.731\n",
      "    sample_time_ms: 20107.981\n",
      "    update_time_ms: 3.89\n",
      "  timestamp: 1634848203\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 195000\n",
      "  training_iteration: 195\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   195</td><td style=\"text-align: right;\">         4979.83</td><td style=\"text-align: right;\">195000</td><td style=\"text-align: right;\"> -4.4656</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            446.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-30-45\n",
      "  done: false\n",
      "  episode_len_mean: 445.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.45449999999995\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 510\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4843126349978977\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003845163703738441\n",
      "          policy_loss: 0.024107761846648323\n",
      "          total_loss: 0.02307248819205496\n",
      "          vf_explained_var: 0.0038977814838290215\n",
      "          vf_loss: 0.009914622149113306\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.51999999999998\n",
      "    ram_util_percent: 46.97\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03917836978244207\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.440464284558253\n",
      "    mean_inference_ms: 1.7253011318350797\n",
      "    mean_raw_obs_processing_ms: 1.6848545368325696\n",
      "  time_since_restore: 5021.7953424453735\n",
      "  time_this_iter_s: 41.96808481216431\n",
      "  time_total_s: 5021.7953424453735\n",
      "  timers:\n",
      "    learn_throughput: 1181.632\n",
      "    learn_time_ms: 846.287\n",
      "    load_throughput: 42041.248\n",
      "    load_time_ms: 23.786\n",
      "    sample_throughput: 45.283\n",
      "    sample_time_ms: 22083.537\n",
      "    update_time_ms: 4.582\n",
      "  timestamp: 1634848245\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 196\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   196</td><td style=\"text-align: right;\">          5021.8</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\"> -4.4545</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            445.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 197000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-31-11\n",
      "  done: false\n",
      "  episode_len_mean: 444.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.44959999999995\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 512\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5505821466445924\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007218995445658896\n",
      "          policy_loss: 0.09378169336252742\n",
      "          total_loss: 0.08617317668265767\n",
      "          vf_explained_var: 0.49834001064300537\n",
      "          vf_loss: 0.004242687644889682\n",
      "    num_agent_steps_sampled: 197000\n",
      "    num_agent_steps_trained: 197000\n",
      "    num_steps_sampled: 197000\n",
      "    num_steps_trained: 197000\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.04054054054055\n",
      "    ram_util_percent: 46.7945945945946\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039186573806032445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.431754447418292\n",
      "    mean_inference_ms: 1.7254734722592278\n",
      "    mean_raw_obs_processing_ms: 1.685351445514459\n",
      "  time_since_restore: 5047.951704740524\n",
      "  time_this_iter_s: 26.156362295150757\n",
      "  time_total_s: 5047.951704740524\n",
      "  timers:\n",
      "    learn_throughput: 1182.534\n",
      "    learn_time_ms: 845.642\n",
      "    load_throughput: 40915.888\n",
      "    load_time_ms: 24.44\n",
      "    sample_throughput: 44.444\n",
      "    sample_time_ms: 22500.151\n",
      "    update_time_ms: 4.481\n",
      "  timestamp: 1634848271\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 197000\n",
      "  training_iteration: 197\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   197</td><td style=\"text-align: right;\">         5047.95</td><td style=\"text-align: right;\">197000</td><td style=\"text-align: right;\"> -4.4496</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            444.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 198000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-31-33\n",
      "  done: false\n",
      "  episode_len_mean: 444.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.44479999999995\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 514\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.621308802233802\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0127223796662461\n",
      "          policy_loss: -0.10432641518612702\n",
      "          total_loss: -0.10810279966228538\n",
      "          vf_explained_var: 0.765541672706604\n",
      "          vf_loss: 0.005995996253720174\n",
      "    num_agent_steps_sampled: 198000\n",
      "    num_agent_steps_trained: 198000\n",
      "    num_steps_sampled: 198000\n",
      "    num_steps_trained: 198000\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.71612903225807\n",
      "    ram_util_percent: 46.803225806451614\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03919473852753609\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.423079394300174\n",
      "    mean_inference_ms: 1.7256442582877827\n",
      "    mean_raw_obs_processing_ms: 1.6858915868026967\n",
      "  time_since_restore: 5069.909002780914\n",
      "  time_this_iter_s: 21.957298040390015\n",
      "  time_total_s: 5069.909002780914\n",
      "  timers:\n",
      "    learn_throughput: 1177.77\n",
      "    learn_time_ms: 849.062\n",
      "    load_throughput: 41899.465\n",
      "    load_time_ms: 23.867\n",
      "    sample_throughput: 44.886\n",
      "    sample_time_ms: 22278.698\n",
      "    update_time_ms: 4.521\n",
      "  timestamp: 1634848293\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 198000\n",
      "  training_iteration: 198\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   198</td><td style=\"text-align: right;\">         5069.91</td><td style=\"text-align: right;\">198000</td><td style=\"text-align: right;\"> -4.4448</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            444.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 199000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-31-52\n",
      "  done: false\n",
      "  episode_len_mean: 446.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.46489999999995\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 516\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.62907911406623\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01399192421927285\n",
      "          policy_loss: 0.07538899199830161\n",
      "          total_loss: 0.07359715956780645\n",
      "          vf_explained_var: -0.20689184963703156\n",
      "          vf_loss: 0.007415543959036263\n",
      "    num_agent_steps_sampled: 199000\n",
      "    num_agent_steps_trained: 199000\n",
      "    num_steps_sampled: 199000\n",
      "    num_steps_trained: 199000\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.37037037037037\n",
      "    ram_util_percent: 46.82592592592592\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039202931719774035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.414144685109832\n",
      "    mean_inference_ms: 1.72581426341459\n",
      "    mean_raw_obs_processing_ms: 1.686471629666329\n",
      "  time_since_restore: 5088.377918958664\n",
      "  time_this_iter_s: 18.468916177749634\n",
      "  time_total_s: 5088.377918958664\n",
      "  timers:\n",
      "    learn_throughput: 1176.393\n",
      "    learn_time_ms: 850.056\n",
      "    load_throughput: 42083.853\n",
      "    load_time_ms: 23.762\n",
      "    sample_throughput: 44.926\n",
      "    sample_time_ms: 22258.699\n",
      "    update_time_ms: 4.736\n",
      "  timestamp: 1634848312\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 199000\n",
      "  training_iteration: 199\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         5088.38</td><td style=\"text-align: right;\">199000</td><td style=\"text-align: right;\"> -4.4649</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            446.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-32-12\n",
      "  done: false\n",
      "  episode_len_mean: 447.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.4736999999999485\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 518\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8021632075309753\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012871983033957809\n",
      "          policy_loss: -0.05880159835020701\n",
      "          total_loss: -0.06459464124507375\n",
      "          vf_explained_var: 0.6725721955299377\n",
      "          vf_loss: 0.005712148006488052\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.40714285714284\n",
      "    ram_util_percent: 46.84999999999999\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039210543793084876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.40509195103246\n",
      "    mean_inference_ms: 1.725976174948311\n",
      "    mean_raw_obs_processing_ms: 1.6860038965825341\n",
      "  time_since_restore: 5108.434329748154\n",
      "  time_this_iter_s: 20.056410789489746\n",
      "  time_total_s: 5108.434329748154\n",
      "  timers:\n",
      "    learn_throughput: 1174.688\n",
      "    learn_time_ms: 851.29\n",
      "    load_throughput: 42193.373\n",
      "    load_time_ms: 23.7\n",
      "    sample_throughput: 45.252\n",
      "    sample_time_ms: 22098.671\n",
      "    update_time_ms: 4.58\n",
      "  timestamp: 1634848332\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 200\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         5108.43</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\"> -4.4737</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            447.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 201000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-32-34\n",
      "  done: false\n",
      "  episode_len_mean: 447.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.473899999999949\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 520\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7119348539246453\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009183729215483697\n",
      "          policy_loss: -0.09937776252627373\n",
      "          total_loss: -0.1050605457690027\n",
      "          vf_explained_var: 0.6461626291275024\n",
      "          vf_loss: 0.006787301732563518\n",
      "    num_agent_steps_sampled: 201000\n",
      "    num_agent_steps_trained: 201000\n",
      "    num_steps_sampled: 201000\n",
      "    num_steps_trained: 201000\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.22121212121213\n",
      "    ram_util_percent: 46.775757575757574\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03921785169949448\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.39606199249305\n",
      "    mean_inference_ms: 1.7261402481836563\n",
      "    mean_raw_obs_processing_ms: 1.684398766636016\n",
      "  time_since_restore: 5131.056764364243\n",
      "  time_this_iter_s: 22.622434616088867\n",
      "  time_total_s: 5131.056764364243\n",
      "  timers:\n",
      "    learn_throughput: 1174.192\n",
      "    learn_time_ms: 851.649\n",
      "    load_throughput: 44011.769\n",
      "    load_time_ms: 22.721\n",
      "    sample_throughput: 44.707\n",
      "    sample_time_ms: 22368.015\n",
      "    update_time_ms: 4.562\n",
      "  timestamp: 1634848354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 201000\n",
      "  training_iteration: 201\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   201</td><td style=\"text-align: right;\">         5131.06</td><td style=\"text-align: right;\">201000</td><td style=\"text-align: right;\"> -4.4739</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            447.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 202000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-32-57\n",
      "  done: false\n",
      "  episode_len_mean: 447.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.47209999999995\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 522\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5745886034435697\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008917796523873239\n",
      "          policy_loss: -0.09043490721119775\n",
      "          total_loss: -0.09374486340416802\n",
      "          vf_explained_var: 0.7086960077285767\n",
      "          vf_loss: 0.0079212926612753\n",
      "    num_agent_steps_sampled: 202000\n",
      "    num_agent_steps_trained: 202000\n",
      "    num_steps_sampled: 202000\n",
      "    num_steps_trained: 202000\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.775\n",
      "    ram_util_percent: 46.796875\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03922517513231758\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.38691923269353\n",
      "    mean_inference_ms: 1.7263016351683438\n",
      "    mean_raw_obs_processing_ms: 1.6828455015752473\n",
      "  time_since_restore: 5153.53848528862\n",
      "  time_this_iter_s: 22.48172092437744\n",
      "  time_total_s: 5153.53848528862\n",
      "  timers:\n",
      "    learn_throughput: 1176.052\n",
      "    learn_time_ms: 850.303\n",
      "    load_throughput: 44021.978\n",
      "    load_time_ms: 22.716\n",
      "    sample_throughput: 44.639\n",
      "    sample_time_ms: 22401.807\n",
      "    update_time_ms: 4.252\n",
      "  timestamp: 1634848377\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 202000\n",
      "  training_iteration: 202\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   202</td><td style=\"text-align: right;\">         5153.54</td><td style=\"text-align: right;\">202000</td><td style=\"text-align: right;\"> -4.4721</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            447.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 203000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-33-19\n",
      "  done: false\n",
      "  episode_len_mean: 446.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.469699999999949\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 525\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5775938802295262\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007976523396710355\n",
      "          policy_loss: 0.034346793757544625\n",
      "          total_loss: 0.027749809126059215\n",
      "          vf_explained_var: 0.7830249071121216\n",
      "          vf_loss: 0.0051408378180996\n",
      "    num_agent_steps_sampled: 203000\n",
      "    num_agent_steps_trained: 203000\n",
      "    num_steps_sampled: 203000\n",
      "    num_steps_trained: 203000\n",
      "  iterations_since_restore: 203\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.53437500000001\n",
      "    ram_util_percent: 46.784375000000004\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039235826443901674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.37336898903758\n",
      "    mean_inference_ms: 1.726543203869226\n",
      "    mean_raw_obs_processing_ms: 1.6806934790079677\n",
      "  time_since_restore: 5176.033686161041\n",
      "  time_this_iter_s: 22.495200872421265\n",
      "  time_total_s: 5176.033686161041\n",
      "  timers:\n",
      "    learn_throughput: 1152.268\n",
      "    learn_time_ms: 867.853\n",
      "    load_throughput: 43986.476\n",
      "    load_time_ms: 22.734\n",
      "    sample_throughput: 44.144\n",
      "    sample_time_ms: 22653.285\n",
      "    update_time_ms: 4.249\n",
      "  timestamp: 1634848399\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 203000\n",
      "  training_iteration: 203\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   203</td><td style=\"text-align: right;\">         5176.03</td><td style=\"text-align: right;\">203000</td><td style=\"text-align: right;\"> -4.4697</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            446.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-33-40\n",
      "  done: false\n",
      "  episode_len_mean: 448.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.4835999999999485\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 527\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6617190029886033\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00824456843331076\n",
      "          policy_loss: 0.04626018504301707\n",
      "          total_loss: 0.04138828499449624\n",
      "          vf_explained_var: 0.035145826637744904\n",
      "          vf_loss: 0.007571477005452228\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.7241379310345\n",
      "    ram_util_percent: 46.834482758620695\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039243077456137775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.36422952727388\n",
      "    mean_inference_ms: 1.7267037211540184\n",
      "    mean_raw_obs_processing_ms: 1.6793373704075325\n",
      "  time_since_restore: 5196.837745428085\n",
      "  time_this_iter_s: 20.804059267044067\n",
      "  time_total_s: 5196.837745428085\n",
      "  timers:\n",
      "    learn_throughput: 1151.062\n",
      "    learn_time_ms: 868.763\n",
      "    load_throughput: 44377.548\n",
      "    load_time_ms: 22.534\n",
      "    sample_throughput: 43.735\n",
      "    sample_time_ms: 22865.235\n",
      "    update_time_ms: 4.667\n",
      "  timestamp: 1634848420\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 204\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   204</td><td style=\"text-align: right;\">         5196.84</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\"> -4.4836</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            448.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 205000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-34-02\n",
      "  done: false\n",
      "  episode_len_mean: 449.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.493699999999949\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 529\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7395236757066515\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013017999031461022\n",
      "          policy_loss: 0.10871850666072634\n",
      "          total_loss: 0.10003906480140157\n",
      "          vf_explained_var: 0.7911511659622192\n",
      "          vf_loss: 0.002125430443104253\n",
      "    num_agent_steps_sampled: 205000\n",
      "    num_agent_steps_trained: 205000\n",
      "    num_steps_sampled: 205000\n",
      "    num_steps_trained: 205000\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.046875\n",
      "    ram_util_percent: 46.784375000000004\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0392503808031321\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.354992693576104\n",
      "    mean_inference_ms: 1.7268661482134724\n",
      "    mean_raw_obs_processing_ms: 1.6779348121570479\n",
      "  time_since_restore: 5219.17195391655\n",
      "  time_this_iter_s: 22.334208488464355\n",
      "  time_total_s: 5219.17195391655\n",
      "  timers:\n",
      "    learn_throughput: 1153.435\n",
      "    learn_time_ms: 866.976\n",
      "    load_throughput: 42808.019\n",
      "    load_time_ms: 23.36\n",
      "    sample_throughput: 43.41\n",
      "    sample_time_ms: 23036.055\n",
      "    update_time_ms: 3.85\n",
      "  timestamp: 1634848442\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 205000\n",
      "  training_iteration: 205\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   205</td><td style=\"text-align: right;\">         5219.17</td><td style=\"text-align: right;\">205000</td><td style=\"text-align: right;\"> -4.4937</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            449.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 206000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-34-27\n",
      "  done: false\n",
      "  episode_len_mean: 448.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.483699999999949\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 531\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.515536708301968\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013288623776615636\n",
      "          policy_loss: -0.1283436647719807\n",
      "          total_loss: -0.12661019828584458\n",
      "          vf_explained_var: -0.028382182121276855\n",
      "          vf_loss: 0.010161468741070065\n",
      "    num_agent_steps_sampled: 206000\n",
      "    num_agent_steps_trained: 206000\n",
      "    num_steps_sampled: 206000\n",
      "    num_steps_trained: 206000\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.84166666666667\n",
      "    ram_util_percent: 46.786111111111104\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0392576922197028\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.345956743844404\n",
      "    mean_inference_ms: 1.727026559172243\n",
      "    mean_raw_obs_processing_ms: 1.6765800385767164\n",
      "  time_since_restore: 5243.821097135544\n",
      "  time_this_iter_s: 24.64914321899414\n",
      "  time_total_s: 5243.821097135544\n",
      "  timers:\n",
      "    learn_throughput: 1156.307\n",
      "    learn_time_ms: 864.822\n",
      "    load_throughput: 44886.999\n",
      "    load_time_ms: 22.278\n",
      "    sample_throughput: 46.93\n",
      "    sample_time_ms: 21308.354\n",
      "    update_time_ms: 3.062\n",
      "  timestamp: 1634848467\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 206000\n",
      "  training_iteration: 206\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   206</td><td style=\"text-align: right;\">         5243.82</td><td style=\"text-align: right;\">206000</td><td style=\"text-align: right;\"> -4.4837</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            448.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 207000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-34-50\n",
      "  done: false\n",
      "  episode_len_mean: 447.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.479499999999949\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 534\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6964053617583381\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008628872431728456\n",
      "          policy_loss: -0.023756872945361666\n",
      "          total_loss: -0.026416250897778406\n",
      "          vf_explained_var: 0.36219409108161926\n",
      "          vf_loss: 0.009936311467835265\n",
      "    num_agent_steps_sampled: 207000\n",
      "    num_agent_steps_trained: 207000\n",
      "    num_steps_sampled: 207000\n",
      "    num_steps_trained: 207000\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.59687500000001\n",
      "    ram_util_percent: 46.8375\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039268697376267446\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.332822146367022\n",
      "    mean_inference_ms: 1.7272629130304498\n",
      "    mean_raw_obs_processing_ms: 1.674714131856\n",
      "  time_since_restore: 5266.262549877167\n",
      "  time_this_iter_s: 22.441452741622925\n",
      "  time_total_s: 5266.262549877167\n",
      "  timers:\n",
      "    learn_throughput: 1158.419\n",
      "    learn_time_ms: 863.245\n",
      "    load_throughput: 46670.324\n",
      "    load_time_ms: 21.427\n",
      "    sample_throughput: 47.757\n",
      "    sample_time_ms: 20939.159\n",
      "    update_time_ms: 3.201\n",
      "  timestamp: 1634848490\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 207000\n",
      "  training_iteration: 207\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   207</td><td style=\"text-align: right;\">         5266.26</td><td style=\"text-align: right;\">207000</td><td style=\"text-align: right;\"> -4.4795</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            447.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-35-12\n",
      "  done: false\n",
      "  episode_len_mean: 446.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.465899999999951\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 536\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6918159272935656\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008583499380748292\n",
      "          policy_loss: 0.11221294452746709\n",
      "          total_loss: 0.10343473181128501\n",
      "          vf_explained_var: 0.16903886198997498\n",
      "          vf_loss: 0.003794550358563558\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.65937500000001\n",
      "    ram_util_percent: 46.84375\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039275898863221324\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.32426252478094\n",
      "    mean_inference_ms: 1.7274203651039548\n",
      "    mean_raw_obs_processing_ms: 1.6735423052075316\n",
      "  time_since_restore: 5288.808299541473\n",
      "  time_this_iter_s: 22.54574966430664\n",
      "  time_total_s: 5288.808299541473\n",
      "  timers:\n",
      "    learn_throughput: 1159.381\n",
      "    learn_time_ms: 862.529\n",
      "    load_throughput: 46846.302\n",
      "    load_time_ms: 21.346\n",
      "    sample_throughput: 47.622\n",
      "    sample_time_ms: 20998.63\n",
      "    update_time_ms: 3.294\n",
      "  timestamp: 1634848512\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 208\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   208</td><td style=\"text-align: right;\">         5288.81</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\"> -4.4659</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            446.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 209000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-35-36\n",
      "  done: false\n",
      "  episode_len_mean: 447.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.474399999999949\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 538\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7530821363131206\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01081943989793367\n",
      "          policy_loss: -0.0807313397526741\n",
      "          total_loss: -0.08440955012208885\n",
      "          vf_explained_var: 0.7590985894203186\n",
      "          vf_loss: 0.00837527122736598\n",
      "    num_agent_steps_sampled: 209000\n",
      "    num_agent_steps_trained: 209000\n",
      "    num_steps_sampled: 209000\n",
      "    num_steps_trained: 209000\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.71212121212122\n",
      "    ram_util_percent: 46.857575757575766\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03928326772666746\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.315622652104313\n",
      "    mean_inference_ms: 1.7275769406400523\n",
      "    mean_raw_obs_processing_ms: 1.6723224597598523\n",
      "  time_since_restore: 5312.285959720612\n",
      "  time_this_iter_s: 23.477660179138184\n",
      "  time_total_s: 5312.285959720612\n",
      "  timers:\n",
      "    learn_throughput: 1158.806\n",
      "    learn_time_ms: 862.957\n",
      "    load_throughput: 48580.327\n",
      "    load_time_ms: 20.584\n",
      "    sample_throughput: 46.512\n",
      "    sample_time_ms: 21499.741\n",
      "    update_time_ms: 3.302\n",
      "  timestamp: 1634848536\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 209000\n",
      "  training_iteration: 209\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   209</td><td style=\"text-align: right;\">         5312.29</td><td style=\"text-align: right;\">209000</td><td style=\"text-align: right;\"> -4.4744</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            447.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 210000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-36-16\n",
      "  done: false\n",
      "  episode_len_mean: 446.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.464299999999949\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 541\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8137421528498332\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01289750578097109\n",
      "          policy_loss: -0.05971008183227645\n",
      "          total_loss: -0.06142771566907565\n",
      "          vf_explained_var: 0.5965110659599304\n",
      "          vf_loss: 0.009890424002272387\n",
      "    num_agent_steps_sampled: 210000\n",
      "    num_agent_steps_trained: 210000\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.33448275862068\n",
      "    ram_util_percent: 46.872413793103455\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03929450191981955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.30329312995031\n",
      "    mean_inference_ms: 1.7278102619473001\n",
      "    mean_raw_obs_processing_ms: 1.673063676892449\n",
      "  time_since_restore: 5352.50209403038\n",
      "  time_this_iter_s: 40.21613430976868\n",
      "  time_total_s: 5352.50209403038\n",
      "  timers:\n",
      "    learn_throughput: 1156.339\n",
      "    learn_time_ms: 864.798\n",
      "    load_throughput: 48122.719\n",
      "    load_time_ms: 20.78\n",
      "    sample_throughput: 42.529\n",
      "    sample_time_ms: 23513.395\n",
      "    update_time_ms: 3.43\n",
      "  timestamp: 1634848576\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 210\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   210</td><td style=\"text-align: right;\">          5352.5</td><td style=\"text-align: right;\">210000</td><td style=\"text-align: right;\"> -4.4643</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            446.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 211000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-36-38\n",
      "  done: false\n",
      "  episode_len_mean: 445.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.45399999999995\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 543\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7891202012697855\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012640837815248437\n",
      "          policy_loss: 0.07560515387190712\n",
      "          total_loss: 0.06803659018543032\n",
      "          vf_explained_var: 0.837950587272644\n",
      "          vf_loss: 0.003923214333028429\n",
      "    num_agent_steps_sampled: 211000\n",
      "    num_agent_steps_trained: 211000\n",
      "    num_steps_sampled: 211000\n",
      "    num_steps_trained: 211000\n",
      "  iterations_since_restore: 211\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.56451612903226\n",
      "    ram_util_percent: 46.803225806451614\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03930170301660504\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.295263688067358\n",
      "    mean_inference_ms: 1.7279574517047993\n",
      "    mean_raw_obs_processing_ms: 1.6736195656544302\n",
      "  time_since_restore: 5374.398665428162\n",
      "  time_this_iter_s: 21.896571397781372\n",
      "  time_total_s: 5374.398665428162\n",
      "  timers:\n",
      "    learn_throughput: 1155.846\n",
      "    learn_time_ms: 865.167\n",
      "    load_throughput: 45691.826\n",
      "    load_time_ms: 21.886\n",
      "    sample_throughput: 42.666\n",
      "    sample_time_ms: 23438.083\n",
      "    update_time_ms: 3.932\n",
      "  timestamp: 1634848598\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 211000\n",
      "  training_iteration: 211\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   211</td><td style=\"text-align: right;\">          5374.4</td><td style=\"text-align: right;\">211000</td><td style=\"text-align: right;\">  -4.454</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">             445.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-37-00\n",
      "  done: false\n",
      "  episode_len_mean: 444.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.4419999999999495\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 545\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9396327720748054\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015055967504181408\n",
      "          policy_loss: -0.03298876070313984\n",
      "          total_loss: -0.03632350564002991\n",
      "          vf_explained_var: 0.08660761266946793\n",
      "          vf_loss: 0.008439500441050364\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.19375\n",
      "    ram_util_percent: 46.809375\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03930886023702568\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.28751285606325\n",
      "    mean_inference_ms: 1.728108079614775\n",
      "    mean_raw_obs_processing_ms: 1.6742115883480182\n",
      "  time_since_restore: 5396.810712337494\n",
      "  time_this_iter_s: 22.412046909332275\n",
      "  time_total_s: 5396.810712337494\n",
      "  timers:\n",
      "    learn_throughput: 1153.585\n",
      "    learn_time_ms: 866.863\n",
      "    load_throughput: 45950.271\n",
      "    load_time_ms: 21.763\n",
      "    sample_throughput: 42.681\n",
      "    sample_time_ms: 23429.382\n",
      "    update_time_ms: 4.048\n",
      "  timestamp: 1634848620\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 212\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   212</td><td style=\"text-align: right;\">         5396.81</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">  -4.442</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">             444.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 213000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-37-25\n",
      "  done: false\n",
      "  episode_len_mean: 442.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.425299999999949\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 548\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7275126443968878\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012571929596397899\n",
      "          policy_loss: -0.04150966927409172\n",
      "          total_loss: -0.0413457410203086\n",
      "          vf_explained_var: 0.25696444511413574\n",
      "          vf_loss: 0.011074514004091422\n",
      "    num_agent_steps_sampled: 213000\n",
      "    num_agent_steps_trained: 213000\n",
      "    num_steps_sampled: 213000\n",
      "    num_steps_trained: 213000\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.30833333333334\n",
      "    ram_util_percent: 46.83333333333334\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03931952324521966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.276915804216845\n",
      "    mean_inference_ms: 1.728340312346827\n",
      "    mean_raw_obs_processing_ms: 1.6751966832934277\n",
      "  time_since_restore: 5421.95753288269\n",
      "  time_this_iter_s: 25.146820545196533\n",
      "  time_total_s: 5421.95753288269\n",
      "  timers:\n",
      "    learn_throughput: 1181.406\n",
      "    learn_time_ms: 846.449\n",
      "    load_throughput: 47094.31\n",
      "    load_time_ms: 21.234\n",
      "    sample_throughput: 42.167\n",
      "    sample_time_ms: 23715.226\n",
      "    update_time_ms: 4.186\n",
      "  timestamp: 1634848645\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 213000\n",
      "  training_iteration: 213\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   213</td><td style=\"text-align: right;\">         5421.96</td><td style=\"text-align: right;\">213000</td><td style=\"text-align: right;\"> -4.4253</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            442.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 214000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-37-50\n",
      "  done: false\n",
      "  episode_len_mean: 441.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.41439999999995\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 550\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8544203559557597\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014452280684854449\n",
      "          policy_loss: -0.01774571670426263\n",
      "          total_loss: -0.024496745566527048\n",
      "          vf_explained_var: 0.5839664936065674\n",
      "          vf_loss: 0.004476704104389582\n",
      "    num_agent_steps_sampled: 214000\n",
      "    num_agent_steps_trained: 214000\n",
      "    num_steps_sampled: 214000\n",
      "    num_steps_trained: 214000\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.80857142857144\n",
      "    ram_util_percent: 46.699999999999996\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03932640502247862\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.2703178737408\n",
      "    mean_inference_ms: 1.7284926084141552\n",
      "    mean_raw_obs_processing_ms: 1.6738338613602037\n",
      "  time_since_restore: 5446.781268119812\n",
      "  time_this_iter_s: 24.823735237121582\n",
      "  time_total_s: 5446.781268119812\n",
      "  timers:\n",
      "    learn_throughput: 1163.042\n",
      "    learn_time_ms: 859.814\n",
      "    load_throughput: 48844.3\n",
      "    load_time_ms: 20.473\n",
      "    sample_throughput: 41.485\n",
      "    sample_time_ms: 24104.846\n",
      "    update_time_ms: 3.935\n",
      "  timestamp: 1634848670\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 214000\n",
      "  training_iteration: 214\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   214</td><td style=\"text-align: right;\">         5446.78</td><td style=\"text-align: right;\">214000</td><td style=\"text-align: right;\"> -4.4144</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            441.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 215000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-38-15\n",
      "  done: false\n",
      "  episode_len_mean: 441.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.41439999999995\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 552\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9022307091289097\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011867710223689423\n",
      "          policy_loss: -0.10461855679750443\n",
      "          total_loss: -0.10889219972822402\n",
      "          vf_explained_var: 0.2560470402240753\n",
      "          vf_loss: 0.008740637703643491\n",
      "    num_agent_steps_sampled: 215000\n",
      "    num_agent_steps_trained: 215000\n",
      "    num_steps_sampled: 215000\n",
      "    num_steps_trained: 215000\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.25999999999999\n",
      "    ram_util_percent: 46.74\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039333410484835594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.263756100285576\n",
      "    mean_inference_ms: 1.7286433866975797\n",
      "    mean_raw_obs_processing_ms: 1.6725092419343008\n",
      "  time_since_restore: 5471.165281534195\n",
      "  time_this_iter_s: 24.384013414382935\n",
      "  time_total_s: 5471.165281534195\n",
      "  timers:\n",
      "    learn_throughput: 1122.9\n",
      "    learn_time_ms: 890.551\n",
      "    load_throughput: 50335.534\n",
      "    load_time_ms: 19.867\n",
      "    sample_throughput: 41.187\n",
      "    sample_time_ms: 24279.274\n",
      "    update_time_ms: 4.322\n",
      "  timestamp: 1634848695\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 215000\n",
      "  training_iteration: 215\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   215</td><td style=\"text-align: right;\">         5471.17</td><td style=\"text-align: right;\">215000</td><td style=\"text-align: right;\"> -4.4144</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            441.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-38-40\n",
      "  done: false\n",
      "  episode_len_mean: 440.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.404699999999949\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 555\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7327238506740994\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0124359631828482\n",
      "          policy_loss: 0.002392347902059555\n",
      "          total_loss: 0.0017851812971962822\n",
      "          vf_explained_var: 0.08986862003803253\n",
      "          vf_loss: 0.010424362589967333\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.89166666666667\n",
      "    ram_util_percent: 46.74722222222223\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0393437274447218\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.253997432331754\n",
      "    mean_inference_ms: 1.7288714336633544\n",
      "    mean_raw_obs_processing_ms: 1.670504470163341\n",
      "  time_since_restore: 5496.5082376003265\n",
      "  time_this_iter_s: 25.342956066131592\n",
      "  time_total_s: 5496.5082376003265\n",
      "  timers:\n",
      "    learn_throughput: 1126.22\n",
      "    learn_time_ms: 887.926\n",
      "    load_throughput: 50792.273\n",
      "    load_time_ms: 19.688\n",
      "    sample_throughput: 41.066\n",
      "    sample_time_ms: 24350.964\n",
      "    update_time_ms: 4.718\n",
      "  timestamp: 1634848720\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 216\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   216</td><td style=\"text-align: right;\">         5496.51</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\"> -4.4047</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            440.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 217000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-39-05\n",
      "  done: false\n",
      "  episode_len_mean: 440.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.40599999999995\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 557\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7156095306078594\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014045047788315973\n",
      "          policy_loss: -0.17180969400538337\n",
      "          total_loss: -0.17397526419825024\n",
      "          vf_explained_var: 0.4564470648765564\n",
      "          vf_loss: 0.00788021524300954\n",
      "    num_agent_steps_sampled: 217000\n",
      "    num_agent_steps_trained: 217000\n",
      "    num_steps_sampled: 217000\n",
      "    num_steps_trained: 217000\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.68611111111112\n",
      "    ram_util_percent: 46.875\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03935046026153123\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.24761819512316\n",
      "    mean_inference_ms: 1.7290212794423783\n",
      "    mean_raw_obs_processing_ms: 1.6692652360243485\n",
      "  time_since_restore: 5521.755169868469\n",
      "  time_this_iter_s: 25.2469322681427\n",
      "  time_total_s: 5521.755169868469\n",
      "  timers:\n",
      "    learn_throughput: 1130.308\n",
      "    learn_time_ms: 884.714\n",
      "    load_throughput: 49704.438\n",
      "    load_time_ms: 20.119\n",
      "    sample_throughput: 40.594\n",
      "    sample_time_ms: 24634.187\n",
      "    update_time_ms: 4.831\n",
      "  timestamp: 1634848745\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 217000\n",
      "  training_iteration: 217\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   217</td><td style=\"text-align: right;\">         5521.76</td><td style=\"text-align: right;\">217000</td><td style=\"text-align: right;\">  -4.406</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">             440.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 218000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-39-27\n",
      "  done: false\n",
      "  episode_len_mean: 442.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.329999999999973\n",
      "  episode_reward_mean: -4.422999999999949\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 560\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7645941880014209\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018536625849793806\n",
      "          policy_loss: -0.03134800692399343\n",
      "          total_loss: -0.03480520769953728\n",
      "          vf_explained_var: 0.6920276880264282\n",
      "          vf_loss: 0.004804572332068347\n",
      "    num_agent_steps_sampled: 218000\n",
      "    num_agent_steps_trained: 218000\n",
      "    num_steps_sampled: 218000\n",
      "    num_steps_trained: 218000\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.75625\n",
      "    ram_util_percent: 46.946875\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03936042929554328\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.237709843352672\n",
      "    mean_inference_ms: 1.729247329881894\n",
      "    mean_raw_obs_processing_ms: 1.6673831231735483\n",
      "  time_since_restore: 5543.675218820572\n",
      "  time_this_iter_s: 21.92004895210266\n",
      "  time_total_s: 5543.675218820572\n",
      "  timers:\n",
      "    learn_throughput: 1130.233\n",
      "    learn_time_ms: 884.774\n",
      "    load_throughput: 47003.693\n",
      "    load_time_ms: 21.275\n",
      "    sample_throughput: 40.7\n",
      "    sample_time_ms: 24570.098\n",
      "    update_time_ms: 5.115\n",
      "  timestamp: 1634848767\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 218000\n",
      "  training_iteration: 218\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   218</td><td style=\"text-align: right;\">         5543.68</td><td style=\"text-align: right;\">218000</td><td style=\"text-align: right;\">  -4.423</td><td style=\"text-align: right;\">               -3.33</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">             442.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 219000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-39-52\n",
      "  done: false\n",
      "  episode_len_mean: 442.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.139999999999977\n",
      "  episode_reward_mean: -4.42019999999995\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 562\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7857899003558688\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01866701933094785\n",
      "          policy_loss: 0.03805352921287219\n",
      "          total_loss: 0.03766315579414368\n",
      "          vf_explained_var: -0.22747687995433807\n",
      "          vf_loss: 0.008017338866678377\n",
      "    num_agent_steps_sampled: 219000\n",
      "    num_agent_steps_trained: 219000\n",
      "    num_steps_sampled: 219000\n",
      "    num_steps_trained: 219000\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.67777777777779\n",
      "    ram_util_percent: 46.930555555555564\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039366817023059726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.231112551798287\n",
      "    mean_inference_ms: 1.7293961100540094\n",
      "    mean_raw_obs_processing_ms: 1.6662211932589814\n",
      "  time_since_restore: 5568.825458526611\n",
      "  time_this_iter_s: 25.15023970603943\n",
      "  time_total_s: 5568.825458526611\n",
      "  timers:\n",
      "    learn_throughput: 1128.655\n",
      "    learn_time_ms: 886.01\n",
      "    load_throughput: 46096.369\n",
      "    load_time_ms: 21.694\n",
      "    sample_throughput: 40.427\n",
      "    sample_time_ms: 24735.929\n",
      "    update_time_ms: 4.935\n",
      "  timestamp: 1634848792\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 219000\n",
      "  training_iteration: 219\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   219</td><td style=\"text-align: right;\">         5568.83</td><td style=\"text-align: right;\">219000</td><td style=\"text-align: right;\"> -4.4202</td><td style=\"text-align: right;\">               -3.14</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            442.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-40-15\n",
      "  done: false\n",
      "  episode_len_mean: 442.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.139999999999977\n",
      "  episode_reward_mean: -4.4244999999999495\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 564\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8742796540260316\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012701317605798578\n",
      "          policy_loss: -0.15546003381411236\n",
      "          total_loss: -0.15850243187612958\n",
      "          vf_explained_var: -0.09149697422981262\n",
      "          vf_loss: 0.009270360900296105\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.69375\n",
      "    ram_util_percent: 46.915625000000006\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03937296842123028\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.22435943489254\n",
      "    mean_inference_ms: 1.7295443221310864\n",
      "    mean_raw_obs_processing_ms: 1.665014391579315\n",
      "  time_since_restore: 5591.329194545746\n",
      "  time_this_iter_s: 22.50373601913452\n",
      "  time_total_s: 5591.329194545746\n",
      "  timers:\n",
      "    learn_throughput: 1130.234\n",
      "    learn_time_ms: 884.772\n",
      "    load_throughput: 46450.401\n",
      "    load_time_ms: 21.528\n",
      "    sample_throughput: 43.543\n",
      "    sample_time_ms: 22965.987\n",
      "    update_time_ms: 4.826\n",
      "  timestamp: 1634848815\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 220\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   220</td><td style=\"text-align: right;\">         5591.33</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\"> -4.4245</td><td style=\"text-align: right;\">               -3.14</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            442.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 221000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-40-36\n",
      "  done: false\n",
      "  episode_len_mean: 444.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.139999999999977\n",
      "  episode_reward_mean: -4.4419999999999495\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 566\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8712700181537205\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012441249195872937\n",
      "          policy_loss: -0.15509439143869613\n",
      "          total_loss: -0.15928689506318833\n",
      "          vf_explained_var: 0.496136337518692\n",
      "          vf_loss: 0.008221810744402723\n",
      "    num_agent_steps_sampled: 221000\n",
      "    num_agent_steps_trained: 221000\n",
      "    num_steps_sampled: 221000\n",
      "    num_steps_trained: 221000\n",
      "  iterations_since_restore: 221\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.80000000000001\n",
      "    ram_util_percent: 46.92758620689656\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03937866820306312\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.217255470189166\n",
      "    mean_inference_ms: 1.729691422224892\n",
      "    mean_raw_obs_processing_ms: 1.6638434111504605\n",
      "  time_since_restore: 5611.958406925201\n",
      "  time_this_iter_s: 20.629212379455566\n",
      "  time_total_s: 5611.958406925201\n",
      "  timers:\n",
      "    learn_throughput: 1131.816\n",
      "    learn_time_ms: 883.536\n",
      "    load_throughput: 48773.812\n",
      "    load_time_ms: 20.503\n",
      "    sample_throughput: 43.777\n",
      "    sample_time_ms: 22842.905\n",
      "    update_time_ms: 4.172\n",
      "  timestamp: 1634848836\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 221000\n",
      "  training_iteration: 221\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   221</td><td style=\"text-align: right;\">         5611.96</td><td style=\"text-align: right;\">221000</td><td style=\"text-align: right;\">  -4.442</td><td style=\"text-align: right;\">               -3.14</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">             444.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 222000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-40-55\n",
      "  done: false\n",
      "  episode_len_mean: 446.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.139999999999977\n",
      "  episode_reward_mean: -4.467899999999949\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 568\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8135874695248073\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0100496148822056\n",
      "          policy_loss: -0.11347367862860362\n",
      "          total_loss: -0.11848569909731548\n",
      "          vf_explained_var: -0.06047467142343521\n",
      "          vf_loss: 0.008036231051664799\n",
      "    num_agent_steps_sampled: 222000\n",
      "    num_agent_steps_trained: 222000\n",
      "    num_steps_sampled: 222000\n",
      "    num_steps_trained: 222000\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.29285714285713\n",
      "    ram_util_percent: 46.971428571428575\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03938399236571284\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.209746840537655\n",
      "    mean_inference_ms: 1.729837289569835\n",
      "    mean_raw_obs_processing_ms: 1.6626248766226694\n",
      "  time_since_restore: 5631.76035118103\n",
      "  time_this_iter_s: 19.801944255828857\n",
      "  time_total_s: 5631.76035118103\n",
      "  timers:\n",
      "    learn_throughput: 1133.722\n",
      "    learn_time_ms: 882.05\n",
      "    load_throughput: 47460.568\n",
      "    load_time_ms: 21.07\n",
      "    sample_throughput: 44.28\n",
      "    sample_time_ms: 22583.436\n",
      "    update_time_ms: 3.979\n",
      "  timestamp: 1634848855\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 222000\n",
      "  training_iteration: 222\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   222</td><td style=\"text-align: right;\">         5631.76</td><td style=\"text-align: right;\">222000</td><td style=\"text-align: right;\"> -4.4679</td><td style=\"text-align: right;\">               -3.14</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            446.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 223000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-41-36\n",
      "  done: false\n",
      "  episode_len_mean: 448.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.139999999999977\n",
      "  episode_reward_mean: -4.480799999999949\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 571\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7144882016711764\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013068494735144418\n",
      "          policy_loss: -0.03673382484250599\n",
      "          total_loss: -0.03982201135820813\n",
      "          vf_explained_var: 0.05175161361694336\n",
      "          vf_loss: 0.007440769511999355\n",
      "    num_agent_steps_sampled: 223000\n",
      "    num_agent_steps_trained: 223000\n",
      "    num_steps_sampled: 223000\n",
      "    num_steps_trained: 223000\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.44999999999999\n",
      "    ram_util_percent: 46.88965517241379\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03939182445487441\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.198653649650414\n",
      "    mean_inference_ms: 1.7300538587553553\n",
      "    mean_raw_obs_processing_ms: 1.6631615548647514\n",
      "  time_since_restore: 5672.099791049957\n",
      "  time_this_iter_s: 40.339439868927\n",
      "  time_total_s: 5672.099791049957\n",
      "  timers:\n",
      "    learn_throughput: 1126.814\n",
      "    learn_time_ms: 887.458\n",
      "    load_throughput: 46226.223\n",
      "    load_time_ms: 21.633\n",
      "    sample_throughput: 41.5\n",
      "    sample_time_ms: 24096.58\n",
      "    update_time_ms: 4.273\n",
      "  timestamp: 1634848896\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 223000\n",
      "  training_iteration: 223\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   223</td><td style=\"text-align: right;\">          5672.1</td><td style=\"text-align: right;\">223000</td><td style=\"text-align: right;\"> -4.4808</td><td style=\"text-align: right;\">               -3.14</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            448.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-41-59\n",
      "  done: false\n",
      "  episode_len_mean: 450.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.139999999999977\n",
      "  episode_reward_mean: -4.500799999999948\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 573\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9781366189320881\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013030120808040804\n",
      "          policy_loss: 0.01788151040673256\n",
      "          total_loss: 0.007872876359356774\n",
      "          vf_explained_var: 0.6841318011283875\n",
      "          vf_loss: 0.00317623153484116\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.8\n",
      "    ram_util_percent: 46.54545454545455\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03939720654652408\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.191111627809235\n",
      "    mean_inference_ms: 1.7302004581209531\n",
      "    mean_raw_obs_processing_ms: 1.6634953305225537\n",
      "  time_since_restore: 5695.244404792786\n",
      "  time_this_iter_s: 23.14461374282837\n",
      "  time_total_s: 5695.244404792786\n",
      "  timers:\n",
      "    learn_throughput: 1146.333\n",
      "    learn_time_ms: 872.347\n",
      "    load_throughput: 44259.353\n",
      "    load_time_ms: 22.594\n",
      "    sample_throughput: 41.766\n",
      "    sample_time_ms: 23942.712\n",
      "    update_time_ms: 4.333\n",
      "  timestamp: 1634848919\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 224\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   224</td><td style=\"text-align: right;\">         5695.24</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\"> -4.5008</td><td style=\"text-align: right;\">               -3.14</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            450.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 225000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-42-22\n",
      "  done: false\n",
      "  episode_len_mean: 451.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.139999999999977\n",
      "  episode_reward_mean: -4.515399999999948\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 575\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7631278620825874\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013490544302854415\n",
      "          policy_loss: 0.07130702129668659\n",
      "          total_loss: 0.06551669811209043\n",
      "          vf_explained_var: 0.05849691852927208\n",
      "          vf_loss: 0.005011364981894278\n",
      "    num_agent_steps_sampled: 225000\n",
      "    num_agent_steps_trained: 225000\n",
      "    num_steps_sampled: 225000\n",
      "    num_steps_trained: 225000\n",
      "  iterations_since_restore: 225\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.71176470588235\n",
      "    ram_util_percent: 46.567647058823525\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03940238262601881\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.183527505150334\n",
      "    mean_inference_ms: 1.7303470448040945\n",
      "    mean_raw_obs_processing_ms: 1.6638591655041015\n",
      "  time_since_restore: 5718.716311216354\n",
      "  time_this_iter_s: 23.471906423568726\n",
      "  time_total_s: 5718.716311216354\n",
      "  timers:\n",
      "    learn_throughput: 1188.098\n",
      "    learn_time_ms: 841.682\n",
      "    load_throughput: 44538.191\n",
      "    load_time_ms: 22.453\n",
      "    sample_throughput: 41.872\n",
      "    sample_time_ms: 23882.56\n",
      "    update_time_ms: 4.047\n",
      "  timestamp: 1634848942\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 225000\n",
      "  training_iteration: 225\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   225</td><td style=\"text-align: right;\">         5718.72</td><td style=\"text-align: right;\">225000</td><td style=\"text-align: right;\"> -4.5154</td><td style=\"text-align: right;\">               -3.14</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            451.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 226000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-42-45\n",
      "  done: false\n",
      "  episode_len_mean: 453.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -3.139999999999977\n",
      "  episode_reward_mean: -4.537299999999947\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 577\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0164630744192333\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013261216022098152\n",
      "          policy_loss: -0.0037578899413347245\n",
      "          total_loss: -0.010368082630965446\n",
      "          vf_explained_var: -0.0790354534983635\n",
      "          vf_loss: 0.006840948797374343\n",
      "    num_agent_steps_sampled: 226000\n",
      "    num_agent_steps_trained: 226000\n",
      "    num_steps_sampled: 226000\n",
      "    num_steps_trained: 226000\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.515625\n",
      "    ram_util_percent: 46.6125\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03940748373639044\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.175646250284252\n",
      "    mean_inference_ms: 1.7304915071548068\n",
      "    mean_raw_obs_processing_ms: 1.6642471522035094\n",
      "  time_since_restore: 5741.07977437973\n",
      "  time_this_iter_s: 22.363463163375854\n",
      "  time_total_s: 5741.07977437973\n",
      "  timers:\n",
      "    learn_throughput: 1181.464\n",
      "    learn_time_ms: 846.407\n",
      "    load_throughput: 44564.029\n",
      "    load_time_ms: 22.44\n",
      "    sample_throughput: 42.409\n",
      "    sample_time_ms: 23580.035\n",
      "    update_time_ms: 3.723\n",
      "  timestamp: 1634848965\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 226000\n",
      "  training_iteration: 226\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   226</td><td style=\"text-align: right;\">         5741.08</td><td style=\"text-align: right;\">226000</td><td style=\"text-align: right;\"> -4.5373</td><td style=\"text-align: right;\">               -3.14</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            453.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 227000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-43-13\n",
      "  done: false\n",
      "  episode_len_mean: 451.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8399999999999834\n",
      "  episode_reward_mean: -4.513599999999947\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 580\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7437911417749192\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012902288421272601\n",
      "          policy_loss: -0.02638280259238349\n",
      "          total_loss: -0.0314079847600725\n",
      "          vf_explained_var: 0.5805396437644958\n",
      "          vf_loss: 0.005880951885289202\n",
      "    num_agent_steps_sampled: 227000\n",
      "    num_agent_steps_trained: 227000\n",
      "    num_steps_sampled: 227000\n",
      "    num_steps_trained: 227000\n",
      "  iterations_since_restore: 227\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.08\n",
      "    ram_util_percent: 46.652499999999996\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03941498895980473\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.16466374668414\n",
      "    mean_inference_ms: 1.7307090918192274\n",
      "    mean_raw_obs_processing_ms: 1.6628405259777184\n",
      "  time_since_restore: 5769.555039644241\n",
      "  time_this_iter_s: 28.47526526451111\n",
      "  time_total_s: 5769.555039644241\n",
      "  timers:\n",
      "    learn_throughput: 1174.165\n",
      "    learn_time_ms: 851.669\n",
      "    load_throughput: 45657.805\n",
      "    load_time_ms: 21.902\n",
      "    sample_throughput: 41.844\n",
      "    sample_time_ms: 23898.063\n",
      "    update_time_ms: 3.758\n",
      "  timestamp: 1634848993\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 227000\n",
      "  training_iteration: 227\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   227</td><td style=\"text-align: right;\">         5769.56</td><td style=\"text-align: right;\">227000</td><td style=\"text-align: right;\"> -4.5136</td><td style=\"text-align: right;\">               -2.84</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            451.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-43-39\n",
      "  done: false\n",
      "  episode_len_mean: 450.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8399999999999834\n",
      "  episode_reward_mean: -4.508699999999949\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 582\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6932110124164157\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014210678163036958\n",
      "          policy_loss: 0.05074218478467729\n",
      "          total_loss: 0.046736684110429555\n",
      "          vf_explained_var: 0.6091158986091614\n",
      "          vf_loss: 0.005732454337541841\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.75945945945946\n",
      "    ram_util_percent: 46.745945945945955\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03941964080995004\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.157694107075837\n",
      "    mean_inference_ms: 1.730848450773541\n",
      "    mean_raw_obs_processing_ms: 1.6613440668309005\n",
      "  time_since_restore: 5795.030539035797\n",
      "  time_this_iter_s: 25.475499391555786\n",
      "  time_total_s: 5795.030539035797\n",
      "  timers:\n",
      "    learn_throughput: 1173.385\n",
      "    learn_time_ms: 852.235\n",
      "    load_throughput: 46141.4\n",
      "    load_time_ms: 21.673\n",
      "    sample_throughput: 41.231\n",
      "    sample_time_ms: 24253.767\n",
      "    update_time_ms: 3.341\n",
      "  timestamp: 1634849019\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 228\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   228</td><td style=\"text-align: right;\">         5795.03</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\"> -4.5087</td><td style=\"text-align: right;\">               -2.84</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            450.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 229000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-44-11\n",
      "  done: false\n",
      "  episode_len_mean: 444.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8299999999999836\n",
      "  episode_reward_mean: -4.4446999999999495\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 585\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2858545899391174\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004439651183765056\n",
      "          policy_loss: -0.12429724824097421\n",
      "          total_loss: -0.12203420003255208\n",
      "          vf_explained_var: 0.2402952015399933\n",
      "          vf_loss: 0.012874023047172362\n",
      "    num_agent_steps_sampled: 229000\n",
      "    num_agent_steps_trained: 229000\n",
      "    num_steps_sampled: 229000\n",
      "    num_steps_trained: 229000\n",
      "  iterations_since_restore: 229\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.94347826086955\n",
      "    ram_util_percent: 46.86521739130435\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0394265763608845\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.1489826241994\n",
      "    mean_inference_ms: 1.731051114565088\n",
      "    mean_raw_obs_processing_ms: 1.6592438723753369\n",
      "  time_since_restore: 5827.380596876144\n",
      "  time_this_iter_s: 32.35005784034729\n",
      "  time_total_s: 5827.380596876144\n",
      "  timers:\n",
      "    learn_throughput: 1159.003\n",
      "    learn_time_ms: 862.811\n",
      "    load_throughput: 45709.603\n",
      "    load_time_ms: 21.877\n",
      "    sample_throughput: 40.059\n",
      "    sample_time_ms: 24963.097\n",
      "    update_time_ms: 3.308\n",
      "  timestamp: 1634849051\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 229000\n",
      "  training_iteration: 229\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   229</td><td style=\"text-align: right;\">         5827.38</td><td style=\"text-align: right;\">229000</td><td style=\"text-align: right;\"> -4.4447</td><td style=\"text-align: right;\">               -2.83</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            444.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 230000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-44-36\n",
      "  done: false\n",
      "  episode_len_mean: 443.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8299999999999836\n",
      "  episode_reward_mean: -4.43339999999995\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 588\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0701202591260275\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017167470764504443\n",
      "          policy_loss: 0.020663808700111176\n",
      "          total_loss: 0.008709804796510272\n",
      "          vf_explained_var: 0.6705447435379028\n",
      "          vf_loss: 0.004401680565853086\n",
      "    num_agent_steps_sampled: 230000\n",
      "    num_agent_steps_trained: 230000\n",
      "    num_steps_sampled: 230000\n",
      "    num_steps_trained: 230000\n",
      "  iterations_since_restore: 230\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.7942857142857\n",
      "    ram_util_percent: 46.93428571428571\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03943317323457045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.140775678227968\n",
      "    mean_inference_ms: 1.7312477799740975\n",
      "    mean_raw_obs_processing_ms: 1.6572846313498564\n",
      "  time_since_restore: 5852.193825244904\n",
      "  time_this_iter_s: 24.813228368759155\n",
      "  time_total_s: 5852.193825244904\n",
      "  timers:\n",
      "    learn_throughput: 1159.91\n",
      "    learn_time_ms: 862.136\n",
      "    load_throughput: 45775.752\n",
      "    load_time_ms: 21.846\n",
      "    sample_throughput: 39.691\n",
      "    sample_time_ms: 25194.606\n",
      "    update_time_ms: 3.799\n",
      "  timestamp: 1634849076\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 230000\n",
      "  training_iteration: 230\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   230</td><td style=\"text-align: right;\">         5852.19</td><td style=\"text-align: right;\">230000</td><td style=\"text-align: right;\"> -4.4334</td><td style=\"text-align: right;\">               -2.83</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            443.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 231000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-45-08\n",
      "  done: false\n",
      "  episode_len_mean: 437.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7499999999999853\n",
      "  episode_reward_mean: -4.378299999999951\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 591\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.288594122727712\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.028840573065503663\n",
      "          policy_loss: 0.0820071113606294\n",
      "          total_loss: 0.08145001447863048\n",
      "          vf_explained_var: 0.8516587018966675\n",
      "          vf_loss: 0.005028572585433722\n",
      "    num_agent_steps_sampled: 231000\n",
      "    num_agent_steps_trained: 231000\n",
      "    num_steps_sampled: 231000\n",
      "    num_steps_trained: 231000\n",
      "  iterations_since_restore: 231\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.53695652173911\n",
      "    ram_util_percent: 47.01086956521739\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039439781516353564\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.134132552570573\n",
      "    mean_inference_ms: 1.731436755796114\n",
      "    mean_raw_obs_processing_ms: 1.6554585431487217\n",
      "  time_since_restore: 5884.42901134491\n",
      "  time_this_iter_s: 32.2351861000061\n",
      "  time_total_s: 5884.42901134491\n",
      "  timers:\n",
      "    learn_throughput: 1163.656\n",
      "    learn_time_ms: 859.361\n",
      "    load_throughput: 44159.54\n",
      "    load_time_ms: 22.645\n",
      "    sample_throughput: 37.94\n",
      "    sample_time_ms: 26357.223\n",
      "    update_time_ms: 3.81\n",
      "  timestamp: 1634849108\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 231000\n",
      "  training_iteration: 231\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   231</td><td style=\"text-align: right;\">         5884.43</td><td style=\"text-align: right;\">231000</td><td style=\"text-align: right;\"> -4.3783</td><td style=\"text-align: right;\">               -2.75</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            437.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-45-34\n",
      "  done: false\n",
      "  episode_len_mean: 436.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7499999999999853\n",
      "  episode_reward_mean: -4.366999999999951\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 594\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4047291994094848\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014704786877606029\n",
      "          policy_loss: 0.0035385769688420824\n",
      "          total_loss: 0.0014910714907778633\n",
      "          vf_explained_var: 0.6843506693840027\n",
      "          vf_loss: 0.006416563588815431\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.8054054054054\n",
      "    ram_util_percent: 47.043243243243246\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03944616139322848\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.12803397780179\n",
      "    mean_inference_ms: 1.7316204444504086\n",
      "    mean_raw_obs_processing_ms: 1.6537618557653584\n",
      "  time_since_restore: 5910.207580089569\n",
      "  time_this_iter_s: 25.778568744659424\n",
      "  time_total_s: 5910.207580089569\n",
      "  timers:\n",
      "    learn_throughput: 1167.659\n",
      "    learn_time_ms: 856.414\n",
      "    load_throughput: 45634.705\n",
      "    load_time_ms: 21.913\n",
      "    sample_throughput: 37.094\n",
      "    sample_time_ms: 26958.372\n",
      "    update_time_ms: 3.838\n",
      "  timestamp: 1634849134\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 232\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   232</td><td style=\"text-align: right;\">         5910.21</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">  -4.367</td><td style=\"text-align: right;\">               -2.75</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">             436.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 233000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-46-06\n",
      "  done: false\n",
      "  episode_len_mean: 431.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7499999999999853\n",
      "  episode_reward_mean: -4.314099999999952\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 597\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2656516194343568\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005085503761990578\n",
      "          policy_loss: -0.08094472115238507\n",
      "          total_loss: -0.084324761480093\n",
      "          vf_explained_var: 0.4986784756183624\n",
      "          vf_loss: 0.007345572186426984\n",
      "    num_agent_steps_sampled: 233000\n",
      "    num_agent_steps_trained: 233000\n",
      "    num_steps_sampled: 233000\n",
      "    num_steps_trained: 233000\n",
      "  iterations_since_restore: 233\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.0\n",
      "    ram_util_percent: 47.032608695652186\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039452822508196886\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.123797310914696\n",
      "    mean_inference_ms: 1.7318063146600133\n",
      "    mean_raw_obs_processing_ms: 1.6521982974600393\n",
      "  time_since_restore: 5942.053649902344\n",
      "  time_this_iter_s: 31.846069812774658\n",
      "  time_total_s: 5942.053649902344\n",
      "  timers:\n",
      "    learn_throughput: 1174.481\n",
      "    learn_time_ms: 851.44\n",
      "    load_throughput: 47117.005\n",
      "    load_time_ms: 21.224\n",
      "    sample_throughput: 38.292\n",
      "    sample_time_ms: 26115.126\n",
      "    update_time_ms: 3.39\n",
      "  timestamp: 1634849166\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 233000\n",
      "  training_iteration: 233\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   233</td><td style=\"text-align: right;\">         5942.05</td><td style=\"text-align: right;\">233000</td><td style=\"text-align: right;\"> -4.3141</td><td style=\"text-align: right;\">               -2.75</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            431.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 234000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-46-53\n",
      "  done: false\n",
      "  episode_len_mean: 423.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7499999999999853\n",
      "  episode_reward_mean: -4.232799999999954\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 601\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2800638927353754\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015634307738721927\n",
      "          policy_loss: -0.021263501793146133\n",
      "          total_loss: -0.020325018879440097\n",
      "          vf_explained_var: 0.4807233214378357\n",
      "          vf_loss: 0.00780297103855345\n",
      "    num_agent_steps_sampled: 234000\n",
      "    num_agent_steps_trained: 234000\n",
      "    num_steps_sampled: 234000\n",
      "    num_steps_trained: 234000\n",
      "  iterations_since_restore: 234\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.27164179104479\n",
      "    ram_util_percent: 46.985074626865675\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039461592800905804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.120519746006707\n",
      "    mean_inference_ms: 1.7320518711469248\n",
      "    mean_raw_obs_processing_ms: 1.6533484472244686\n",
      "  time_since_restore: 5989.503402709961\n",
      "  time_this_iter_s: 47.44975280761719\n",
      "  time_total_s: 5989.503402709961\n",
      "  timers:\n",
      "    learn_throughput: 1171.525\n",
      "    learn_time_ms: 853.588\n",
      "    load_throughput: 47821.061\n",
      "    load_time_ms: 20.911\n",
      "    sample_throughput: 35.035\n",
      "    sample_time_ms: 28542.893\n",
      "    update_time_ms: 3.659\n",
      "  timestamp: 1634849213\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 234000\n",
      "  training_iteration: 234\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   234</td><td style=\"text-align: right;\">          5989.5</td><td style=\"text-align: right;\">234000</td><td style=\"text-align: right;\"> -4.2328</td><td style=\"text-align: right;\">               -2.75</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">            423.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 235000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-47-26\n",
      "  done: false\n",
      "  episode_len_mean: 416.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7499999999999853\n",
      "  episode_reward_mean: -4.164999999999955\n",
      "  episode_reward_min: -6.649999999999903\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 604\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.469964443312751\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018092306352477768\n",
      "          policy_loss: 0.023159708537989192\n",
      "          total_loss: 0.02371932069460551\n",
      "          vf_explained_var: 0.2799548804759979\n",
      "          vf_loss: 0.008389836715327368\n",
      "    num_agent_steps_sampled: 235000\n",
      "    num_agent_steps_trained: 235000\n",
      "    num_steps_sampled: 235000\n",
      "    num_steps_trained: 235000\n",
      "  iterations_since_restore: 235\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.06739130434784\n",
      "    ram_util_percent: 46.91304347826088\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03946796098157195\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.120220427550525\n",
      "    mean_inference_ms: 1.7322296172360678\n",
      "    mean_raw_obs_processing_ms: 1.6544350979323321\n",
      "  time_since_restore: 6021.643523931503\n",
      "  time_this_iter_s: 32.14012122154236\n",
      "  time_total_s: 6021.643523931503\n",
      "  timers:\n",
      "    learn_throughput: 1169.99\n",
      "    learn_time_ms: 854.708\n",
      "    load_throughput: 45993.756\n",
      "    load_time_ms: 21.742\n",
      "    sample_throughput: 34.004\n",
      "    sample_time_ms: 29407.882\n",
      "    update_time_ms: 3.584\n",
      "  timestamp: 1634849246\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 235000\n",
      "  training_iteration: 235\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   235</td><td style=\"text-align: right;\">         6021.64</td><td style=\"text-align: right;\">235000</td><td style=\"text-align: right;\">  -4.165</td><td style=\"text-align: right;\">               -2.75</td><td style=\"text-align: right;\">               -6.65</td><td style=\"text-align: right;\">             416.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-47-59\n",
      "  done: false\n",
      "  episode_len_mean: 406.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.719999999999986\n",
      "  episode_reward_mean: -4.0669999999999575\n",
      "  episode_reward_min: -6.009999999999916\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 608\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3043718099594117\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007256337161149428\n",
      "          policy_loss: 0.015106704375810093\n",
      "          total_loss: 0.016830618017249638\n",
      "          vf_explained_var: 0.09907380491495132\n",
      "          vf_loss: 0.01201248982300361\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "  iterations_since_restore: 236\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.29583333333333\n",
      "    ram_util_percent: 46.98125000000001\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03947690057717144\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.122652517056295\n",
      "    mean_inference_ms: 1.732463792215031\n",
      "    mean_raw_obs_processing_ms: 1.6552646956053503\n",
      "  time_since_restore: 6055.189582824707\n",
      "  time_this_iter_s: 33.546058893203735\n",
      "  time_total_s: 6055.189582824707\n",
      "  timers:\n",
      "    learn_throughput: 1160.795\n",
      "    learn_time_ms: 861.479\n",
      "    load_throughput: 44264.304\n",
      "    load_time_ms: 22.592\n",
      "    sample_throughput: 32.767\n",
      "    sample_time_ms: 30518.784\n",
      "    update_time_ms: 3.527\n",
      "  timestamp: 1634849279\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 236\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   236</td><td style=\"text-align: right;\">         6055.19</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">  -4.067</td><td style=\"text-align: right;\">               -2.72</td><td style=\"text-align: right;\">               -6.01</td><td style=\"text-align: right;\">             406.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 237000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-48-35\n",
      "  done: false\n",
      "  episode_len_mean: 403.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.619999999999988\n",
      "  episode_reward_mean: -4.032999999999958\n",
      "  episode_reward_min: -6.009999999999916\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 611\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3406651496887207\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004945689046754885\n",
      "          policy_loss: -0.09768299733599027\n",
      "          total_loss: -0.09749595026175181\n",
      "          vf_explained_var: 0.17928609251976013\n",
      "          vf_loss: 0.011715884517050452\n",
      "    num_agent_steps_sampled: 237000\n",
      "    num_agent_steps_trained: 237000\n",
      "    num_steps_sampled: 237000\n",
      "    num_steps_trained: 237000\n",
      "  iterations_since_restore: 237\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.94117647058823\n",
      "    ram_util_percent: 47.09019607843137\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03948357504428919\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.12589221974595\n",
      "    mean_inference_ms: 1.7326430956118748\n",
      "    mean_raw_obs_processing_ms: 1.6540381387094534\n",
      "  time_since_restore: 6091.038702249527\n",
      "  time_this_iter_s: 35.849119424819946\n",
      "  time_total_s: 6091.038702249527\n",
      "  timers:\n",
      "    learn_throughput: 1159.448\n",
      "    learn_time_ms: 862.479\n",
      "    load_throughput: 42285.937\n",
      "    load_time_ms: 23.649\n",
      "    sample_throughput: 31.995\n",
      "    sample_time_ms: 31254.496\n",
      "    update_time_ms: 3.257\n",
      "  timestamp: 1634849315\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 237000\n",
      "  training_iteration: 237\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   237</td><td style=\"text-align: right;\">         6091.04</td><td style=\"text-align: right;\">237000</td><td style=\"text-align: right;\">  -4.033</td><td style=\"text-align: right;\">               -2.62</td><td style=\"text-align: right;\">               -6.01</td><td style=\"text-align: right;\">             403.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 238000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-49-10\n",
      "  done: false\n",
      "  episode_len_mean: 395.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5899999999999888\n",
      "  episode_reward_mean: -3.95579999999996\n",
      "  episode_reward_min: -5.869999999999919\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 615\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2351368519994947\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008429021271791548\n",
      "          policy_loss: -0.020013201236724853\n",
      "          total_loss: -0.01784528460767534\n",
      "          vf_explained_var: 0.14275266230106354\n",
      "          vf_loss: 0.012919086124747991\n",
      "    num_agent_steps_sampled: 238000\n",
      "    num_agent_steps_trained: 238000\n",
      "    num_steps_sampled: 238000\n",
      "    num_steps_trained: 238000\n",
      "  iterations_since_restore: 238\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.98800000000001\n",
      "    ram_util_percent: 47.071999999999996\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03949216884576395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.132378097275527\n",
      "    mean_inference_ms: 1.7328741051423517\n",
      "    mean_raw_obs_processing_ms: 1.652669468029972\n",
      "  time_since_restore: 6126.089174032211\n",
      "  time_this_iter_s: 35.050471782684326\n",
      "  time_total_s: 6126.089174032211\n",
      "  timers:\n",
      "    learn_throughput: 1165.714\n",
      "    learn_time_ms: 857.843\n",
      "    load_throughput: 43996.534\n",
      "    load_time_ms: 22.729\n",
      "    sample_throughput: 31.039\n",
      "    sample_time_ms: 32217.311\n",
      "    update_time_ms: 3.342\n",
      "  timestamp: 1634849350\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 238000\n",
      "  training_iteration: 238\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   238</td><td style=\"text-align: right;\">         6126.09</td><td style=\"text-align: right;\">238000</td><td style=\"text-align: right;\"> -3.9558</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">               -5.87</td><td style=\"text-align: right;\">            395.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 239000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-49-42\n",
      "  done: false\n",
      "  episode_len_mean: 386.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.579999999999989\n",
      "  episode_reward_mean: -3.8656999999999613\n",
      "  episode_reward_min: -5.749999999999922\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 619\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2639268543985156\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00808959299627428\n",
      "          policy_loss: 0.02422562970055474\n",
      "          total_loss: 0.025489246514108447\n",
      "          vf_explained_var: 0.1667165607213974\n",
      "          vf_loss: 0.012367124845170312\n",
      "    num_agent_steps_sampled: 239000\n",
      "    num_agent_steps_trained: 239000\n",
      "    num_steps_sampled: 239000\n",
      "    num_steps_trained: 239000\n",
      "  iterations_since_restore: 239\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.22391304347828\n",
      "    ram_util_percent: 47.071739130434786\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03950030453478239\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.141819868691986\n",
      "    mean_inference_ms: 1.7330955262206569\n",
      "    mean_raw_obs_processing_ms: 1.6516622050748577\n",
      "  time_since_restore: 6158.244802713394\n",
      "  time_this_iter_s: 32.15562868118286\n",
      "  time_total_s: 6158.244802713394\n",
      "  timers:\n",
      "    learn_throughput: 1181.556\n",
      "    learn_time_ms: 846.342\n",
      "    load_throughput: 44173.911\n",
      "    load_time_ms: 22.638\n",
      "    sample_throughput: 31.047\n",
      "    sample_time_ms: 32209.428\n",
      "    update_time_ms: 3.345\n",
      "  timestamp: 1634849382\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 239000\n",
      "  training_iteration: 239\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   239</td><td style=\"text-align: right;\">         6158.24</td><td style=\"text-align: right;\">239000</td><td style=\"text-align: right;\"> -3.8657</td><td style=\"text-align: right;\">               -2.58</td><td style=\"text-align: right;\">               -5.75</td><td style=\"text-align: right;\">            386.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-50-18\n",
      "  done: false\n",
      "  episode_len_mean: 378.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.579999999999989\n",
      "  episode_reward_mean: -3.7897999999999628\n",
      "  episode_reward_min: -5.749999999999922\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 623\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2484918607605828\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006619578937074758\n",
      "          policy_loss: 0.017127209777633347\n",
      "          total_loss: 0.019131553421417872\n",
      "          vf_explained_var: 0.19227854907512665\n",
      "          vf_loss: 0.013232575967493984\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.91960784313727\n",
      "    ram_util_percent: 47.07450980392157\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03950842881685497\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.15383041262625\n",
      "    mean_inference_ms: 1.7333117457048375\n",
      "    mean_raw_obs_processing_ms: 1.651005784028045\n",
      "  time_since_restore: 6193.572284460068\n",
      "  time_this_iter_s: 35.327481746673584\n",
      "  time_total_s: 6193.572284460068\n",
      "  timers:\n",
      "    learn_throughput: 1182.72\n",
      "    learn_time_ms: 845.509\n",
      "    load_throughput: 43966.695\n",
      "    load_time_ms: 22.744\n",
      "    sample_throughput: 30.064\n",
      "    sample_time_ms: 33261.946\n",
      "    update_time_ms: 2.961\n",
      "  timestamp: 1634849418\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 240\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   240</td><td style=\"text-align: right;\">         6193.57</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\"> -3.7898</td><td style=\"text-align: right;\">               -2.58</td><td style=\"text-align: right;\">               -5.75</td><td style=\"text-align: right;\">            378.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 241000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-50-53\n",
      "  done: false\n",
      "  episode_len_mean: 370.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5499999999999896\n",
      "  episode_reward_mean: -3.7022999999999655\n",
      "  episode_reward_min: -5.609999999999925\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 627\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2480980224079556\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01305883787756754\n",
      "          policy_loss: 0.0025432618955771127\n",
      "          total_loss: 0.0056574713852670456\n",
      "          vf_explained_var: 0.23598796129226685\n",
      "          vf_loss: 0.013116052374243736\n",
      "    num_agent_steps_sampled: 241000\n",
      "    num_agent_steps_trained: 241000\n",
      "    num_steps_sampled: 241000\n",
      "    num_steps_trained: 241000\n",
      "  iterations_since_restore: 241\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.022\n",
      "    ram_util_percent: 47.01\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03951636615506501\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.168331142955005\n",
      "    mean_inference_ms: 1.7335203560176646\n",
      "    mean_raw_obs_processing_ms: 1.650547286733335\n",
      "  time_since_restore: 6229.00230884552\n",
      "  time_this_iter_s: 35.43002438545227\n",
      "  time_total_s: 6229.00230884552\n",
      "  timers:\n",
      "    learn_throughput: 1178.732\n",
      "    learn_time_ms: 848.37\n",
      "    load_throughput: 43754.384\n",
      "    load_time_ms: 22.855\n",
      "    sample_throughput: 29.781\n",
      "    sample_time_ms: 33578.284\n",
      "    update_time_ms: 3.039\n",
      "  timestamp: 1634849453\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 241000\n",
      "  training_iteration: 241\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   241</td><td style=\"text-align: right;\">            6229</td><td style=\"text-align: right;\">241000</td><td style=\"text-align: right;\"> -3.7023</td><td style=\"text-align: right;\">               -2.55</td><td style=\"text-align: right;\">               -5.61</td><td style=\"text-align: right;\">            370.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 242000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-51-47\n",
      "  done: false\n",
      "  episode_len_mean: 364.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.648599999999966\n",
      "  episode_reward_min: -5.609999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 630\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.208608631292979\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009419861377853422\n",
      "          policy_loss: -0.1467158599032296\n",
      "          total_loss: -0.1500170444448789\n",
      "          vf_explained_var: 0.5896352529525757\n",
      "          vf_loss: 0.006996600350572003\n",
      "    num_agent_steps_sampled: 242000\n",
      "    num_agent_steps_trained: 242000\n",
      "    num_steps_sampled: 242000\n",
      "    num_steps_trained: 242000\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.63506493506493\n",
      "    ram_util_percent: 47.01818181818182\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03952182252981241\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.18121366964614\n",
      "    mean_inference_ms: 1.7336654834194263\n",
      "    mean_raw_obs_processing_ms: 1.652530786839477\n",
      "  time_since_restore: 6282.926093816757\n",
      "  time_this_iter_s: 53.92378497123718\n",
      "  time_total_s: 6282.926093816757\n",
      "  timers:\n",
      "    learn_throughput: 1170.517\n",
      "    learn_time_ms: 854.323\n",
      "    load_throughput: 43447.001\n",
      "    load_time_ms: 23.017\n",
      "    sample_throughput: 27.483\n",
      "    sample_time_ms: 36386.718\n",
      "    update_time_ms: 3.069\n",
      "  timestamp: 1634849507\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 242000\n",
      "  training_iteration: 242\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   242</td><td style=\"text-align: right;\">         6282.93</td><td style=\"text-align: right;\">242000</td><td style=\"text-align: right;\"> -3.6486</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -5.61</td><td style=\"text-align: right;\">            364.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 243000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-52-21\n",
      "  done: false\n",
      "  episode_len_mean: 358.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.5870999999999675\n",
      "  episode_reward_min: -5.609999999999925\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 634\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2067522989379036\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016716233924910028\n",
      "          policy_loss: -0.04417988417877091\n",
      "          total_loss: -0.041997621705134706\n",
      "          vf_explained_var: 0.38775724172592163\n",
      "          vf_loss: 0.011076311173383147\n",
      "    num_agent_steps_sampled: 243000\n",
      "    num_agent_steps_trained: 243000\n",
      "    num_steps_sampled: 243000\n",
      "    num_steps_trained: 243000\n",
      "  iterations_since_restore: 243\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.15833333333335\n",
      "    ram_util_percent: 47.08124999999999\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039528716684106985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.20001765646357\n",
      "    mean_inference_ms: 1.7338560579267903\n",
      "    mean_raw_obs_processing_ms: 1.6553292830762478\n",
      "  time_since_restore: 6316.406631708145\n",
      "  time_this_iter_s: 33.48053789138794\n",
      "  time_total_s: 6316.406631708145\n",
      "  timers:\n",
      "    learn_throughput: 1166.469\n",
      "    learn_time_ms: 857.288\n",
      "    load_throughput: 42539.443\n",
      "    load_time_ms: 23.508\n",
      "    sample_throughput: 27.362\n",
      "    sample_time_ms: 36546.613\n",
      "    update_time_ms: 3.138\n",
      "  timestamp: 1634849541\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 243000\n",
      "  training_iteration: 243\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   243</td><td style=\"text-align: right;\">         6316.41</td><td style=\"text-align: right;\">243000</td><td style=\"text-align: right;\"> -3.5871</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -5.61</td><td style=\"text-align: right;\">            358.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-52-56\n",
      "  done: false\n",
      "  episode_len_mean: 352.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.520899999999969\n",
      "  episode_reward_min: -5.609999999999925\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 638\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2359293129709032\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0062650829070183215\n",
      "          policy_loss: -0.015353414333528943\n",
      "          total_loss: -0.01422071937057707\n",
      "          vf_explained_var: 0.2528209984302521\n",
      "          vf_loss: 0.012302601606481604\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_agent_steps_trained: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.32800000000002\n",
      "    ram_util_percent: 47.102\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039535408965399386\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.220888691824193\n",
      "    mean_inference_ms: 1.7340456048143762\n",
      "    mean_raw_obs_processing_ms: 1.6583735679117575\n",
      "  time_since_restore: 6351.469693899155\n",
      "  time_this_iter_s: 35.06306219100952\n",
      "  time_total_s: 6351.469693899155\n",
      "  timers:\n",
      "    learn_throughput: 1119.893\n",
      "    learn_time_ms: 892.943\n",
      "    load_throughput: 42316.227\n",
      "    load_time_ms: 23.632\n",
      "    sample_throughput: 28.35\n",
      "    sample_time_ms: 35273.246\n",
      "    update_time_ms: 2.777\n",
      "  timestamp: 1634849576\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 244\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   244</td><td style=\"text-align: right;\">         6351.47</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\"> -3.5209</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -5.61</td><td style=\"text-align: right;\">            352.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 245000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-53-30\n",
      "  done: false\n",
      "  episode_len_mean: 347.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.4769999999999697\n",
      "  episode_reward_min: -5.609999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 641\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.096436991956499\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017568515275588857\n",
      "          policy_loss: -0.07955612316727638\n",
      "          total_loss: -0.07986336110366715\n",
      "          vf_explained_var: 0.68871009349823\n",
      "          vf_loss: 0.007321860594674945\n",
      "    num_agent_steps_sampled: 245000\n",
      "    num_agent_steps_trained: 245000\n",
      "    num_steps_sampled: 245000\n",
      "    num_steps_trained: 245000\n",
      "  iterations_since_restore: 245\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.91836734693878\n",
      "    ram_util_percent: 47.008163265306116\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03954004202037428\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.237905066668496\n",
      "    mean_inference_ms: 1.7341856612908364\n",
      "    mean_raw_obs_processing_ms: 1.6583667615955215\n",
      "  time_since_restore: 6386.014297962189\n",
      "  time_this_iter_s: 34.54460406303406\n",
      "  time_total_s: 6386.014297962189\n",
      "  timers:\n",
      "    learn_throughput: 1119.479\n",
      "    learn_time_ms: 893.273\n",
      "    load_throughput: 43838.164\n",
      "    load_time_ms: 22.811\n",
      "    sample_throughput: 28.158\n",
      "    sample_time_ms: 35514.272\n",
      "    update_time_ms: 2.747\n",
      "  timestamp: 1634849610\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 245000\n",
      "  training_iteration: 245\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   245</td><td style=\"text-align: right;\">         6386.01</td><td style=\"text-align: right;\">245000</td><td style=\"text-align: right;\">  -3.477</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -5.61</td><td style=\"text-align: right;\">             347.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 246000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-54-04\n",
      "  done: false\n",
      "  episode_len_mean: 340.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.4042999999999717\n",
      "  episode_reward_min: -5.609999999999925\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 645\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1898437500000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.220313963625166\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.04503033069769842\n",
      "          policy_loss: 0.031043880350059932\n",
      "          total_loss: 0.0357806823319859\n",
      "          vf_explained_var: 0.7050850987434387\n",
      "          vf_loss: 0.008391215238306257\n",
      "    num_agent_steps_sampled: 246000\n",
      "    num_agent_steps_trained: 246000\n",
      "    num_steps_sampled: 246000\n",
      "    num_steps_trained: 246000\n",
      "  iterations_since_restore: 246\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.32708333333333\n",
      "    ram_util_percent: 47.07708333333334\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03954629687119829\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.2625717933106\n",
      "    mean_inference_ms: 1.7343714124799814\n",
      "    mean_raw_obs_processing_ms: 1.6585487189318717\n",
      "  time_since_restore: 6419.399386644363\n",
      "  time_this_iter_s: 33.38508868217468\n",
      "  time_total_s: 6419.399386644363\n",
      "  timers:\n",
      "    learn_throughput: 1131.122\n",
      "    learn_time_ms: 884.078\n",
      "    load_throughput: 43433.099\n",
      "    load_time_ms: 23.024\n",
      "    sample_throughput: 28.163\n",
      "    sample_time_ms: 35507.264\n",
      "    update_time_ms: 2.668\n",
      "  timestamp: 1634849644\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 246000\n",
      "  training_iteration: 246\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   246</td><td style=\"text-align: right;\">          6419.4</td><td style=\"text-align: right;\">246000</td><td style=\"text-align: right;\"> -3.4043</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -5.61</td><td style=\"text-align: right;\">            340.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 247000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-54-35\n",
      "  done: false\n",
      "  episode_len_mean: 337.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.3714999999999717\n",
      "  episode_reward_min: -5.609999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 648\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2171455899874368\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017294287984130623\n",
      "          policy_loss: -0.08998554237186909\n",
      "          total_loss: -0.0865792820437087\n",
      "          vf_explained_var: 0.3892359733581543\n",
      "          vf_loss: 0.010652895168297821\n",
      "    num_agent_steps_sampled: 247000\n",
      "    num_agent_steps_trained: 247000\n",
      "    num_steps_sampled: 247000\n",
      "    num_steps_trained: 247000\n",
      "  iterations_since_restore: 247\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.06222222222223\n",
      "    ram_util_percent: 47.062222222222225\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039550946241990854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.281882915750398\n",
      "    mean_inference_ms: 1.7345022582702108\n",
      "    mean_raw_obs_processing_ms: 1.6588155550622539\n",
      "  time_since_restore: 6451.051648855209\n",
      "  time_this_iter_s: 31.652262210845947\n",
      "  time_total_s: 6451.051648855209\n",
      "  timers:\n",
      "    learn_throughput: 1133.314\n",
      "    learn_time_ms: 882.368\n",
      "    load_throughput: 43501.074\n",
      "    load_time_ms: 22.988\n",
      "    sample_throughput: 28.499\n",
      "    sample_time_ms: 35089.29\n",
      "    update_time_ms: 2.642\n",
      "  timestamp: 1634849675\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 247000\n",
      "  training_iteration: 247\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   247</td><td style=\"text-align: right;\">         6451.05</td><td style=\"text-align: right;\">247000</td><td style=\"text-align: right;\"> -3.3715</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -5.61</td><td style=\"text-align: right;\">            337.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-55-03\n",
      "  done: false\n",
      "  episode_len_mean: 336.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.363799999999972\n",
      "  episode_reward_min: -5.609999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 651\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5000933435228136\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03437974316321504\n",
      "          policy_loss: -0.05864810666276349\n",
      "          total_loss: -0.057822983298036784\n",
      "          vf_explained_var: 0.5941292643547058\n",
      "          vf_loss: 0.006035889315211938\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "  iterations_since_restore: 248\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.17692307692306\n",
      "    ram_util_percent: 47.03333333333333\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039555451114626064\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.30140917557424\n",
      "    mean_inference_ms: 1.7346340663167095\n",
      "    mean_raw_obs_processing_ms: 1.6591827355775879\n",
      "  time_since_restore: 6478.37199139595\n",
      "  time_this_iter_s: 27.320342540740967\n",
      "  time_total_s: 6478.37199139595\n",
      "  timers:\n",
      "    learn_throughput: 1125.976\n",
      "    learn_time_ms: 888.119\n",
      "    load_throughput: 41805.208\n",
      "    load_time_ms: 23.92\n",
      "    sample_throughput: 29.147\n",
      "    sample_time_ms: 34309.074\n",
      "    update_time_ms: 3.288\n",
      "  timestamp: 1634849703\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 248\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   248</td><td style=\"text-align: right;\">         6478.37</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\"> -3.3638</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -5.61</td><td style=\"text-align: right;\">            336.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 249000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-55-33\n",
      "  done: false\n",
      "  episode_len_mean: 332.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.3287999999999727\n",
      "  episode_reward_min: -5.609999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 654\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4160549667146471\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014877600932297301\n",
      "          policy_loss: -0.14525322222875225\n",
      "          total_loss: -0.14921402157180838\n",
      "          vf_explained_var: 0.8074633479118347\n",
      "          vf_loss: 0.0038448064445724918\n",
      "    num_agent_steps_sampled: 249000\n",
      "    num_agent_steps_trained: 249000\n",
      "    num_steps_sampled: 249000\n",
      "    num_steps_trained: 249000\n",
      "  iterations_since_restore: 249\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.84418604651164\n",
      "    ram_util_percent: 47.027906976744184\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03955968009748776\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.321474033636054\n",
      "    mean_inference_ms: 1.734760983777913\n",
      "    mean_raw_obs_processing_ms: 1.6596429925382\n",
      "  time_since_restore: 6508.583354234695\n",
      "  time_this_iter_s: 30.211362838745117\n",
      "  time_total_s: 6508.583354234695\n",
      "  timers:\n",
      "    learn_throughput: 1125.056\n",
      "    learn_time_ms: 888.844\n",
      "    load_throughput: 41729.469\n",
      "    load_time_ms: 23.964\n",
      "    sample_throughput: 29.314\n",
      "    sample_time_ms: 34113.913\n",
      "    update_time_ms: 3.276\n",
      "  timestamp: 1634849733\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 249000\n",
      "  training_iteration: 249\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   249</td><td style=\"text-align: right;\">         6508.58</td><td style=\"text-align: right;\">249000</td><td style=\"text-align: right;\"> -3.3288</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -5.61</td><td style=\"text-align: right;\">            332.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 250000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-56-04\n",
      "  done: false\n",
      "  episode_len_mean: 328.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.286999999999974\n",
      "  episode_reward_min: -5.609999999999925\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 658\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1526883032586839\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023332908584468213\n",
      "          policy_loss: 0.036353026330471036\n",
      "          total_loss: 0.03994199534257253\n",
      "          vf_explained_var: 0.8905958533287048\n",
      "          vf_loss: 0.005149238750648996\n",
      "    num_agent_steps_sampled: 250000\n",
      "    num_agent_steps_trained: 250000\n",
      "    num_steps_sampled: 250000\n",
      "    num_steps_trained: 250000\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.66000000000001\n",
      "    ram_util_percent: 47.10222222222222\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03956530169817103\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.349062992532144\n",
      "    mean_inference_ms: 1.734928148294224\n",
      "    mean_raw_obs_processing_ms: 1.6603826078168995\n",
      "  time_since_restore: 6539.545894861221\n",
      "  time_this_iter_s: 30.96254062652588\n",
      "  time_total_s: 6539.545894861221\n",
      "  timers:\n",
      "    learn_throughput: 1123.541\n",
      "    learn_time_ms: 890.043\n",
      "    load_throughput: 41834.772\n",
      "    load_time_ms: 23.904\n",
      "    sample_throughput: 29.695\n",
      "    sample_time_ms: 33675.825\n",
      "    update_time_ms: 3.194\n",
      "  timestamp: 1634849764\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 250000\n",
      "  training_iteration: 250\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   250</td><td style=\"text-align: right;\">         6539.55</td><td style=\"text-align: right;\">250000</td><td style=\"text-align: right;\">  -3.287</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -5.61</td><td style=\"text-align: right;\">             328.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 251000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-56-52\n",
      "  done: false\n",
      "  episode_len_mean: 325.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.2563999999999744\n",
      "  episode_reward_min: -5.609999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 661\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.620264462629954\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01534862649478374\n",
      "          policy_loss: -0.04499028639660941\n",
      "          total_loss: -0.04407005144490136\n",
      "          vf_explained_var: 0.10569985955953598\n",
      "          vf_loss: 0.007288670027628541\n",
      "    num_agent_steps_sampled: 251000\n",
      "    num_agent_steps_trained: 251000\n",
      "    num_steps_sampled: 251000\n",
      "    num_steps_trained: 251000\n",
      "  iterations_since_restore: 251\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.18695652173913\n",
      "    ram_util_percent: 47.13188405797102\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039569558436989026\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.37071651324673\n",
      "    mean_inference_ms: 1.7350493294372469\n",
      "    mean_raw_obs_processing_ms: 1.663046588964604\n",
      "  time_since_restore: 6588.164283514023\n",
      "  time_this_iter_s: 48.618388652801514\n",
      "  time_total_s: 6588.164283514023\n",
      "  timers:\n",
      "    learn_throughput: 1123.028\n",
      "    learn_time_ms: 890.45\n",
      "    load_throughput: 41625.149\n",
      "    load_time_ms: 24.024\n",
      "    sample_throughput: 28.577\n",
      "    sample_time_ms: 34993.787\n",
      "    update_time_ms: 3.515\n",
      "  timestamp: 1634849812\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 251000\n",
      "  training_iteration: 251\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   251</td><td style=\"text-align: right;\">         6588.16</td><td style=\"text-align: right;\">251000</td><td style=\"text-align: right;\"> -3.2564</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -5.61</td><td style=\"text-align: right;\">            325.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-57-22\n",
      "  done: false\n",
      "  episode_len_mean: 322.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.223299999999975\n",
      "  episode_reward_min: -5.609999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 664\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5956423388587104\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01170974797741289\n",
      "          policy_loss: 0.0971369120809767\n",
      "          total_loss: 0.09224469653434224\n",
      "          vf_explained_var: 0.7442124485969543\n",
      "          vf_loss: 0.0035615036910813715\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_agent_steps_trained: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "  iterations_since_restore: 252\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.6\n",
      "    ram_util_percent: 47.12619047619047\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039573852957426575\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.39298526759792\n",
      "    mean_inference_ms: 1.7351712965993715\n",
      "    mean_raw_obs_processing_ms: 1.6657940283493762\n",
      "  time_since_restore: 6617.846004247665\n",
      "  time_this_iter_s: 29.681720733642578\n",
      "  time_total_s: 6617.846004247665\n",
      "  timers:\n",
      "    learn_throughput: 1122.758\n",
      "    learn_time_ms: 890.664\n",
      "    load_throughput: 40336.905\n",
      "    load_time_ms: 24.791\n",
      "    sample_throughput: 30.704\n",
      "    sample_time_ms: 32568.721\n",
      "    update_time_ms: 3.442\n",
      "  timestamp: 1634849842\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 252\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   252</td><td style=\"text-align: right;\">         6617.85</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\"> -3.2233</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -5.61</td><td style=\"text-align: right;\">            322.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 253000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-57-53\n",
      "  done: false\n",
      "  episode_len_mean: 316.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.1667999999999767\n",
      "  episode_reward_min: -5.609999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 667\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.516927018430498\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019756342209348632\n",
      "          policy_loss: 0.01201222472720676\n",
      "          total_loss: 0.01376683231857088\n",
      "          vf_explained_var: 0.8382337093353271\n",
      "          vf_loss: 0.004265541729465541\n",
      "    num_agent_steps_sampled: 253000\n",
      "    num_agent_steps_trained: 253000\n",
      "    num_steps_sampled: 253000\n",
      "    num_steps_trained: 253000\n",
      "  iterations_since_restore: 253\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.50444444444443\n",
      "    ram_util_percent: 47.08222222222222\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03957792344946264\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.41672143190691\n",
      "    mean_inference_ms: 1.735290913178611\n",
      "    mean_raw_obs_processing_ms: 1.668687770427807\n",
      "  time_since_restore: 6649.030473947525\n",
      "  time_this_iter_s: 31.18446969985962\n",
      "  time_total_s: 6649.030473947525\n",
      "  timers:\n",
      "    learn_throughput: 1117.893\n",
      "    learn_time_ms: 894.54\n",
      "    load_throughput: 41469.869\n",
      "    load_time_ms: 24.114\n",
      "    sample_throughput: 30.926\n",
      "    sample_time_ms: 32335.175\n",
      "    update_time_ms: 3.353\n",
      "  timestamp: 1634849873\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 253000\n",
      "  training_iteration: 253\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   253</td><td style=\"text-align: right;\">         6649.03</td><td style=\"text-align: right;\">253000</td><td style=\"text-align: right;\"> -3.1668</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -5.61</td><td style=\"text-align: right;\">            316.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 254000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-58-23\n",
      "  done: false\n",
      "  episode_len_mean: 311.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.117899999999978\n",
      "  episode_reward_min: -5.609999999999925\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 670\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3576888190375433\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013373278789402813\n",
      "          policy_loss: -0.0793048806488514\n",
      "          total_loss: -0.07689162641763687\n",
      "          vf_explained_var: 0.5994133353233337\n",
      "          vf_loss: 0.00742158032177637\n",
      "    num_agent_steps_sampled: 254000\n",
      "    num_agent_steps_trained: 254000\n",
      "    num_steps_sampled: 254000\n",
      "    num_steps_trained: 254000\n",
      "  iterations_since_restore: 254\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.90952380952382\n",
      "    ram_util_percent: 47.15714285714286\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03958200416887522\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.441370858639196\n",
      "    mean_inference_ms: 1.7354095419007962\n",
      "    mean_raw_obs_processing_ms: 1.670178879926513\n",
      "  time_since_restore: 6678.242736339569\n",
      "  time_this_iter_s: 29.212262392044067\n",
      "  time_total_s: 6678.242736339569\n",
      "  timers:\n",
      "    learn_throughput: 1163.568\n",
      "    learn_time_ms: 859.426\n",
      "    load_throughput: 43268.707\n",
      "    load_time_ms: 23.111\n",
      "    sample_throughput: 31.46\n",
      "    sample_time_ms: 31786.257\n",
      "    update_time_ms: 3.35\n",
      "  timestamp: 1634849903\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 254000\n",
      "  training_iteration: 254\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   254</td><td style=\"text-align: right;\">         6678.24</td><td style=\"text-align: right;\">254000</td><td style=\"text-align: right;\"> -3.1179</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -5.61</td><td style=\"text-align: right;\">            311.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 255000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-58-54\n",
      "  done: false\n",
      "  episode_len_mean: 304.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.04259999999998\n",
      "  episode_reward_min: -5.34999999999993\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 674\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.518036499288347\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018711977017854528\n",
      "          policy_loss: -0.0018432569172647263\n",
      "          total_loss: -0.0009490220083130731\n",
      "          vf_explained_var: 0.8307228684425354\n",
      "          vf_loss: 0.0040854101625478105\n",
      "    num_agent_steps_sampled: 255000\n",
      "    num_agent_steps_trained: 255000\n",
      "    num_steps_sampled: 255000\n",
      "    num_steps_trained: 255000\n",
      "  iterations_since_restore: 255\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.47954545454546\n",
      "    ram_util_percent: 47.002272727272725\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039587271737157814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.475372997452915\n",
      "    mean_inference_ms: 1.73556649671371\n",
      "    mean_raw_obs_processing_ms: 1.6713015491702805\n",
      "  time_since_restore: 6709.548757553101\n",
      "  time_this_iter_s: 31.306021213531494\n",
      "  time_total_s: 6709.548757553101\n",
      "  timers:\n",
      "    learn_throughput: 1150.647\n",
      "    learn_time_ms: 869.076\n",
      "    load_throughput: 41583.88\n",
      "    load_time_ms: 24.048\n",
      "    sample_throughput: 31.795\n",
      "    sample_time_ms: 31451.681\n",
      "    update_time_ms: 3.418\n",
      "  timestamp: 1634849934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 255000\n",
      "  training_iteration: 255\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   255</td><td style=\"text-align: right;\">         6709.55</td><td style=\"text-align: right;\">255000</td><td style=\"text-align: right;\"> -3.0426</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -5.35</td><td style=\"text-align: right;\">            304.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-59-26\n",
      "  done: false\n",
      "  episode_len_mean: 299.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -2.996499999999981\n",
      "  episode_reward_min: -5.34999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 677\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4496623026000128\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012031641108977167\n",
      "          policy_loss: 0.016635604202747345\n",
      "          total_loss: 0.014195837577184041\n",
      "          vf_explained_var: 0.44905099272727966\n",
      "          vf_loss: 0.004347908024727884\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.19782608695652\n",
      "    ram_util_percent: 47.00869565217391\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039591441945557936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.501981481318932\n",
      "    mean_inference_ms: 1.7356794389566033\n",
      "    mean_raw_obs_processing_ms: 1.672296478239331\n",
      "  time_since_restore: 6741.93935751915\n",
      "  time_this_iter_s: 32.390599966049194\n",
      "  time_total_s: 6741.93935751915\n",
      "  timers:\n",
      "    learn_throughput: 1149.823\n",
      "    learn_time_ms: 869.699\n",
      "    load_throughput: 42550.318\n",
      "    load_time_ms: 23.502\n",
      "    sample_throughput: 31.896\n",
      "    sample_time_ms: 31351.681\n",
      "    update_time_ms: 3.815\n",
      "  timestamp: 1634849966\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 256\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   256</td><td style=\"text-align: right;\">         6741.94</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\"> -2.9965</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -5.35</td><td style=\"text-align: right;\">            299.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 257000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_20-59-59\n",
      "  done: false\n",
      "  episode_len_mean: 298.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -2.9809999999999803\n",
      "  episode_reward_min: -5.34999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 680\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4198435650931465\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010897462578521457\n",
      "          policy_loss: 0.03290600727001826\n",
      "          total_loss: 0.027602917204300564\n",
      "          vf_explained_var: 0.9324609637260437\n",
      "          vf_loss: 0.0019130947889708396\n",
      "    num_agent_steps_sampled: 257000\n",
      "    num_agent_steps_trained: 257000\n",
      "    num_steps_sampled: 257000\n",
      "    num_steps_trained: 257000\n",
      "  iterations_since_restore: 257\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.1212765957447\n",
      "    ram_util_percent: 47.04255319148936\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03959548167420149\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.528938142121074\n",
      "    mean_inference_ms: 1.7357901630003667\n",
      "    mean_raw_obs_processing_ms: 1.673371988889781\n",
      "  time_since_restore: 6774.213427066803\n",
      "  time_this_iter_s: 32.2740695476532\n",
      "  time_total_s: 6774.213427066803\n",
      "  timers:\n",
      "    learn_throughput: 1150.314\n",
      "    learn_time_ms: 869.328\n",
      "    load_throughput: 44443.192\n",
      "    load_time_ms: 22.501\n",
      "    sample_throughput: 31.832\n",
      "    sample_time_ms: 31415.114\n",
      "    update_time_ms: 3.855\n",
      "  timestamp: 1634849999\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 257000\n",
      "  training_iteration: 257\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   257</td><td style=\"text-align: right;\">         6774.21</td><td style=\"text-align: right;\">257000</td><td style=\"text-align: right;\">  -2.981</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -5.35</td><td style=\"text-align: right;\">             298.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 258000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-00-30\n",
      "  done: false\n",
      "  episode_len_mean: 296.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -2.96249999999998\n",
      "  episode_reward_min: -4.9199999999999395\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 683\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4665502044889662\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016956391955995482\n",
      "          policy_loss: -0.06502187351385752\n",
      "          total_loss: -0.0638735079103046\n",
      "          vf_explained_var: 0.8273932337760925\n",
      "          vf_loss: 0.004949526152470045\n",
      "    num_agent_steps_sampled: 258000\n",
      "    num_agent_steps_trained: 258000\n",
      "    num_steps_sampled: 258000\n",
      "    num_steps_trained: 258000\n",
      "  iterations_since_restore: 258\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.93555555555555\n",
      "    ram_util_percent: 47.04888888888888\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039599311497422546\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.55611793433221\n",
      "    mean_inference_ms: 1.7359017776739902\n",
      "    mean_raw_obs_processing_ms: 1.6745255272957216\n",
      "  time_since_restore: 6805.771901607513\n",
      "  time_this_iter_s: 31.55847454071045\n",
      "  time_total_s: 6805.771901607513\n",
      "  timers:\n",
      "    learn_throughput: 1152.097\n",
      "    learn_time_ms: 867.983\n",
      "    load_throughput: 44354.646\n",
      "    load_time_ms: 22.546\n",
      "    sample_throughput: 31.406\n",
      "    sample_time_ms: 31840.926\n",
      "    update_time_ms: 3.133\n",
      "  timestamp: 1634850030\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 258000\n",
      "  training_iteration: 258\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   258</td><td style=\"text-align: right;\">         6805.77</td><td style=\"text-align: right;\">258000</td><td style=\"text-align: right;\"> -2.9625</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -4.92</td><td style=\"text-align: right;\">            296.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 259000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-01-01\n",
      "  done: false\n",
      "  episode_len_mean: 294.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -2.9464999999999817\n",
      "  episode_reward_min: -4.269999999999953\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 687\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3614774081442091\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01148765974175012\n",
      "          policy_loss: 0.04799014702439308\n",
      "          total_loss: 0.046777073459492786\n",
      "          vf_explained_var: 0.8340224623680115\n",
      "          vf_loss: 0.005041295352081458\n",
      "    num_agent_steps_sampled: 259000\n",
      "    num_agent_steps_trained: 259000\n",
      "    num_steps_sampled: 259000\n",
      "    num_steps_trained: 259000\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.98181818181818\n",
      "    ram_util_percent: 47.084090909090904\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03960430061867387\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.59223092081558\n",
      "    mean_inference_ms: 1.736048049411304\n",
      "    mean_raw_obs_processing_ms: 1.676130878400344\n",
      "  time_since_restore: 6836.537102937698\n",
      "  time_this_iter_s: 30.765201330184937\n",
      "  time_total_s: 6836.537102937698\n",
      "  timers:\n",
      "    learn_throughput: 1150.14\n",
      "    learn_time_ms: 869.459\n",
      "    load_throughput: 43978.681\n",
      "    load_time_ms: 22.738\n",
      "    sample_throughput: 31.353\n",
      "    sample_time_ms: 31894.467\n",
      "    update_time_ms: 3.187\n",
      "  timestamp: 1634850061\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 259000\n",
      "  training_iteration: 259\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   259</td><td style=\"text-align: right;\">         6836.54</td><td style=\"text-align: right;\">259000</td><td style=\"text-align: right;\"> -2.9465</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -4.27</td><td style=\"text-align: right;\">            294.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-01-47\n",
      "  done: false\n",
      "  episode_len_mean: 295.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -2.9510999999999807\n",
      "  episode_reward_min: -4.269999999999953\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 690\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.585444864961836\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014631125494773557\n",
      "          policy_loss: 0.019879067440827687\n",
      "          total_loss: 0.01625623471207089\n",
      "          vf_explained_var: 0.8664856553077698\n",
      "          vf_loss: 0.0028571230456388244\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_agent_steps_trained: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "  iterations_since_restore: 260\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.76153846153848\n",
      "    ram_util_percent: 47.04923076923078\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03960797095632045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.618929489135745\n",
      "    mean_inference_ms: 1.7361572820783866\n",
      "    mean_raw_obs_processing_ms: 1.6794198603537966\n",
      "  time_since_restore: 6882.394724607468\n",
      "  time_this_iter_s: 45.85762166976929\n",
      "  time_total_s: 6882.394724607468\n",
      "  timers:\n",
      "    learn_throughput: 1151.259\n",
      "    learn_time_ms: 868.614\n",
      "    load_throughput: 45879.3\n",
      "    load_time_ms: 21.796\n",
      "    sample_throughput: 29.952\n",
      "    sample_time_ms: 33386.353\n",
      "    update_time_ms: 3.15\n",
      "  timestamp: 1634850107\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 260\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   260</td><td style=\"text-align: right;\">         6882.39</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -2.9511</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -4.27</td><td style=\"text-align: right;\">            295.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 261000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-02-17\n",
      "  done: false\n",
      "  episode_len_mean: 294.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -2.9486999999999806\n",
      "  episode_reward_min: -4.129999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 693\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5527704450819226\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012656742950093185\n",
      "          policy_loss: -0.01909328384531869\n",
      "          total_loss: -0.020198637578222486\n",
      "          vf_explained_var: 0.6298106908798218\n",
      "          vf_loss: 0.006312888585186253\n",
      "    num_agent_steps_sampled: 261000\n",
      "    num_agent_steps_trained: 261000\n",
      "    num_steps_sampled: 261000\n",
      "    num_steps_trained: 261000\n",
      "  iterations_since_restore: 261\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.33023255813954\n",
      "    ram_util_percent: 46.92558139534884\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039611696530993175\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.64572626879945\n",
      "    mean_inference_ms: 1.7362696373073927\n",
      "    mean_raw_obs_processing_ms: 1.68271464551349\n",
      "  time_since_restore: 6912.590618133545\n",
      "  time_this_iter_s: 30.19589352607727\n",
      "  time_total_s: 6912.590618133545\n",
      "  timers:\n",
      "    learn_throughput: 1156.33\n",
      "    learn_time_ms: 864.805\n",
      "    load_throughput: 47907.91\n",
      "    load_time_ms: 20.873\n",
      "    sample_throughput: 31.696\n",
      "    sample_time_ms: 31549.364\n",
      "    update_time_ms: 2.758\n",
      "  timestamp: 1634850137\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 261000\n",
      "  training_iteration: 261\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   261</td><td style=\"text-align: right;\">         6912.59</td><td style=\"text-align: right;\">261000</td><td style=\"text-align: right;\"> -2.9487</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -4.13</td><td style=\"text-align: right;\">            294.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 262000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-02-45\n",
      "  done: false\n",
      "  episode_len_mean: 295.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -2.9553999999999796\n",
      "  episode_reward_min: -4.129999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 696\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7321207549836901\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013613820615855914\n",
      "          policy_loss: -0.05470422067575985\n",
      "          total_loss: -0.0582941607468658\n",
      "          vf_explained_var: 0.6804731488227844\n",
      "          vf_loss: 0.0050085841264161796\n",
      "    num_agent_steps_sampled: 262000\n",
      "    num_agent_steps_trained: 262000\n",
      "    num_steps_sampled: 262000\n",
      "    num_steps_trained: 262000\n",
      "  iterations_since_restore: 262\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.1974358974359\n",
      "    ram_util_percent: 46.935897435897445\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0396151868736703\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.672065814078966\n",
      "    mean_inference_ms: 1.7363773080017844\n",
      "    mean_raw_obs_processing_ms: 1.6860155519898263\n",
      "  time_since_restore: 6940.027463197708\n",
      "  time_this_iter_s: 27.436845064163208\n",
      "  time_total_s: 6940.027463197708\n",
      "  timers:\n",
      "    learn_throughput: 1157.368\n",
      "    learn_time_ms: 864.03\n",
      "    load_throughput: 48037.951\n",
      "    load_time_ms: 20.817\n",
      "    sample_throughput: 31.923\n",
      "    sample_time_ms: 31325.674\n",
      "    update_time_ms: 2.798\n",
      "  timestamp: 1634850165\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 262000\n",
      "  training_iteration: 262\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   262</td><td style=\"text-align: right;\">         6940.03</td><td style=\"text-align: right;\">262000</td><td style=\"text-align: right;\"> -2.9554</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -4.13</td><td style=\"text-align: right;\">            295.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 263000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-03-12\n",
      "  done: false\n",
      "  episode_len_mean: 296.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -2.96919999999998\n",
      "  episode_reward_min: -4.129999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 698\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6577457600169712\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016588888502848266\n",
      "          policy_loss: -0.1494906249973509\n",
      "          total_loss: -0.1496820648511251\n",
      "          vf_explained_var: 0.7021847367286682\n",
      "          vf_loss: 0.005757142305891547\n",
      "    num_agent_steps_sampled: 263000\n",
      "    num_agent_steps_trained: 263000\n",
      "    num_steps_sampled: 263000\n",
      "    num_steps_trained: 263000\n",
      "  iterations_since_restore: 263\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.96923076923078\n",
      "    ram_util_percent: 46.88974358974359\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039617557880373966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.689284078661146\n",
      "    mean_inference_ms: 1.7364488905534052\n",
      "    mean_raw_obs_processing_ms: 1.6874714799027533\n",
      "  time_since_restore: 6966.95817899704\n",
      "  time_this_iter_s: 26.930715799331665\n",
      "  time_total_s: 6966.95817899704\n",
      "  timers:\n",
      "    learn_throughput: 1162.788\n",
      "    learn_time_ms: 860.002\n",
      "    load_throughput: 45916.568\n",
      "    load_time_ms: 21.779\n",
      "    sample_throughput: 32.358\n",
      "    sample_time_ms: 30904.055\n",
      "    update_time_ms: 2.944\n",
      "  timestamp: 1634850192\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 263000\n",
      "  training_iteration: 263\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   263</td><td style=\"text-align: right;\">         6966.96</td><td style=\"text-align: right;\">263000</td><td style=\"text-align: right;\"> -2.9692</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -4.13</td><td style=\"text-align: right;\">            296.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-03-41\n",
      "  done: false\n",
      "  episode_len_mean: 298.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -2.9804999999999806\n",
      "  episode_reward_min: -4.129999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 701\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.469935933748881\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01249789187087161\n",
      "          policy_loss: -0.08062229951222738\n",
      "          total_loss: -0.07748456762896644\n",
      "          vf_explained_var: 0.524699330329895\n",
      "          vf_loss: 0.00982940665109911\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "  iterations_since_restore: 264\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.88536585365854\n",
      "    ram_util_percent: 46.81707317073171\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03962098422685142\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.715180129749662\n",
      "    mean_inference_ms: 1.736555762662787\n",
      "    mean_raw_obs_processing_ms: 1.6884834943057365\n",
      "  time_since_restore: 6996.126093387604\n",
      "  time_this_iter_s: 29.167914390563965\n",
      "  time_total_s: 6996.126093387604\n",
      "  timers:\n",
      "    learn_throughput: 1162.857\n",
      "    learn_time_ms: 859.951\n",
      "    load_throughput: 42918.975\n",
      "    load_time_ms: 23.3\n",
      "    sample_throughput: 32.364\n",
      "    sample_time_ms: 30898.085\n",
      "    update_time_ms: 2.899\n",
      "  timestamp: 1634850221\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 264\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   264</td><td style=\"text-align: right;\">         6996.13</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\"> -2.9805</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -4.13</td><td style=\"text-align: right;\">            298.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 265000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-04-11\n",
      "  done: false\n",
      "  episode_len_mean: 299.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -2.9965999999999804\n",
      "  episode_reward_min: -4.129999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 704\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7730783740679423\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012848769880911614\n",
      "          policy_loss: -0.020967580046918656\n",
      "          total_loss: -0.022695865896013048\n",
      "          vf_explained_var: 0.523371696472168\n",
      "          vf_loss: 0.007769994979672548\n",
      "    num_agent_steps_sampled: 265000\n",
      "    num_agent_steps_trained: 265000\n",
      "    num_steps_sampled: 265000\n",
      "    num_steps_trained: 265000\n",
      "  iterations_since_restore: 265\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.55813953488371\n",
      "    ram_util_percent: 46.865116279069774\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039624346616453844\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.740596580920187\n",
      "    mean_inference_ms: 1.7366608415888334\n",
      "    mean_raw_obs_processing_ms: 1.6895135869646773\n",
      "  time_since_restore: 7025.929561376572\n",
      "  time_this_iter_s: 29.803467988967896\n",
      "  time_total_s: 7025.929561376572\n",
      "  timers:\n",
      "    learn_throughput: 1174.161\n",
      "    learn_time_ms: 851.672\n",
      "    load_throughput: 44788.642\n",
      "    load_time_ms: 22.327\n",
      "    sample_throughput: 32.513\n",
      "    sample_time_ms: 30757.016\n",
      "    update_time_ms: 2.923\n",
      "  timestamp: 1634850251\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 265000\n",
      "  training_iteration: 265\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   265</td><td style=\"text-align: right;\">         7025.93</td><td style=\"text-align: right;\">265000</td><td style=\"text-align: right;\"> -2.9966</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -4.13</td><td style=\"text-align: right;\">            299.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 266000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-04-40\n",
      "  done: false\n",
      "  episode_len_mean: 301.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.0177999999999794\n",
      "  episode_reward_min: -4.129999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 707\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9906999150911966\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013247831150755424\n",
      "          policy_loss: 0.015062071879704793\n",
      "          total_loss: 0.010196372204356723\n",
      "          vf_explained_var: 0.6057475805282593\n",
      "          vf_loss: 0.006553115183487534\n",
      "    num_agent_steps_sampled: 266000\n",
      "    num_agent_steps_trained: 266000\n",
      "    num_steps_sampled: 266000\n",
      "    num_steps_trained: 266000\n",
      "  iterations_since_restore: 266\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.02380952380952\n",
      "    ram_util_percent: 46.74285714285715\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03962697592140365\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.765344369008954\n",
      "    mean_inference_ms: 1.7367629757940048\n",
      "    mean_raw_obs_processing_ms: 1.69055429543982\n",
      "  time_since_restore: 7055.39663028717\n",
      "  time_this_iter_s: 29.467068910598755\n",
      "  time_total_s: 7055.39663028717\n",
      "  timers:\n",
      "    learn_throughput: 1173.47\n",
      "    learn_time_ms: 852.174\n",
      "    load_throughput: 44645.76\n",
      "    load_time_ms: 22.399\n",
      "    sample_throughput: 32.826\n",
      "    sample_time_ms: 30463.742\n",
      "    update_time_ms: 2.656\n",
      "  timestamp: 1634850280\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 266000\n",
      "  training_iteration: 266\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   266</td><td style=\"text-align: right;\">          7055.4</td><td style=\"text-align: right;\">266000</td><td style=\"text-align: right;\"> -3.0178</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -4.13</td><td style=\"text-align: right;\">            301.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 267000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-05-09\n",
      "  done: false\n",
      "  episode_len_mean: 303.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.034099999999979\n",
      "  episode_reward_min: -4.129999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 710\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7463588171535067\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009540372585737146\n",
      "          policy_loss: -0.044433846076329546\n",
      "          total_loss: -0.04725041331516372\n",
      "          vf_explained_var: 0.14236782491207123\n",
      "          vf_loss: 0.008534285658970475\n",
      "    num_agent_steps_sampled: 267000\n",
      "    num_agent_steps_trained: 267000\n",
      "    num_steps_sampled: 267000\n",
      "    num_steps_trained: 267000\n",
      "  iterations_since_restore: 267\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.83414634146342\n",
      "    ram_util_percent: 46.8\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03962945623427612\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.78956298046691\n",
      "    mean_inference_ms: 1.736861875355664\n",
      "    mean_raw_obs_processing_ms: 1.6915503582979423\n",
      "  time_since_restore: 7084.455384254456\n",
      "  time_this_iter_s: 29.058753967285156\n",
      "  time_total_s: 7084.455384254456\n",
      "  timers:\n",
      "    learn_throughput: 1172.615\n",
      "    learn_time_ms: 852.795\n",
      "    load_throughput: 44298.666\n",
      "    load_time_ms: 22.574\n",
      "    sample_throughput: 33.177\n",
      "    sample_time_ms: 30141.134\n",
      "    update_time_ms: 3.016\n",
      "  timestamp: 1634850309\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 267000\n",
      "  training_iteration: 267\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   267</td><td style=\"text-align: right;\">         7084.46</td><td style=\"text-align: right;\">267000</td><td style=\"text-align: right;\"> -3.0341</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -4.13</td><td style=\"text-align: right;\">            303.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 268000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-05-37\n",
      "  done: false\n",
      "  episode_len_mean: 306.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.0604999999999785\n",
      "  episode_reward_min: -4.129999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 713\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8357946621047125\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013435178105546402\n",
      "          policy_loss: 0.035626137256622316\n",
      "          total_loss: 0.030522248645623525\n",
      "          vf_explained_var: 0.6005477905273438\n",
      "          vf_loss: 0.004645833168696198\n",
      "    num_agent_steps_sampled: 268000\n",
      "    num_agent_steps_trained: 268000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "  iterations_since_restore: 268\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.75750000000001\n",
      "    ram_util_percent: 46.839999999999996\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039631875779268924\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.812667198939867\n",
      "    mean_inference_ms: 1.736959789086622\n",
      "    mean_raw_obs_processing_ms: 1.6925604531771254\n",
      "  time_since_restore: 7111.9823541641235\n",
      "  time_this_iter_s: 27.52696990966797\n",
      "  time_total_s: 7111.9823541641235\n",
      "  timers:\n",
      "    learn_throughput: 1178.065\n",
      "    learn_time_ms: 848.849\n",
      "    load_throughput: 46213.948\n",
      "    load_time_ms: 21.638\n",
      "    sample_throughput: 33.621\n",
      "    sample_time_ms: 29742.895\n",
      "    update_time_ms: 3.041\n",
      "  timestamp: 1634850337\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 268\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   268</td><td style=\"text-align: right;\">         7111.98</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\"> -3.0605</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -4.13</td><td style=\"text-align: right;\">            306.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 269000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-06-04\n",
      "  done: false\n",
      "  episode_len_mean: 308.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.1266999999999774\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 715\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8541823705037435\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010822248218461814\n",
      "          policy_loss: -0.011483118848668204\n",
      "          total_loss: 0.043032312724325394\n",
      "          vf_explained_var: 0.03686777129769325\n",
      "          vf_loss: 0.06612319498219424\n",
      "    num_agent_steps_sampled: 269000\n",
      "    num_agent_steps_trained: 269000\n",
      "    num_steps_sampled: 269000\n",
      "    num_steps_trained: 269000\n",
      "  iterations_since_restore: 269\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.60769230769233\n",
      "    ram_util_percent: 46.83076923076922\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03963352047605154\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.827932352177132\n",
      "    mean_inference_ms: 1.7370252322702018\n",
      "    mean_raw_obs_processing_ms: 1.693162924458731\n",
      "  time_since_restore: 7139.676814556122\n",
      "  time_this_iter_s: 27.69446039199829\n",
      "  time_total_s: 7139.676814556122\n",
      "  timers:\n",
      "    learn_throughput: 1126.659\n",
      "    learn_time_ms: 887.58\n",
      "    load_throughput: 46532.132\n",
      "    load_time_ms: 21.491\n",
      "    sample_throughput: 34.017\n",
      "    sample_time_ms: 29396.924\n",
      "    update_time_ms: 3.4\n",
      "  timestamp: 1634850364\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 269000\n",
      "  training_iteration: 269\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   269</td><td style=\"text-align: right;\">         7139.68</td><td style=\"text-align: right;\">269000</td><td style=\"text-align: right;\"> -3.1267</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            308.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 270000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-06-32\n",
      "  done: false\n",
      "  episode_len_mean: 311.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.152499999999977\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 718\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.467098335425059\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012135065544775376\n",
      "          policy_loss: -0.14865387909942204\n",
      "          total_loss: -0.14117394089698793\n",
      "          vf_explained_var: 0.32881680130958557\n",
      "          vf_loss: 0.014375711211727725\n",
      "    num_agent_steps_sampled: 270000\n",
      "    num_agent_steps_trained: 270000\n",
      "    num_steps_sampled: 270000\n",
      "    num_steps_trained: 270000\n",
      "  iterations_since_restore: 270\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.9975\n",
      "    ram_util_percent: 46.81\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03963622536898996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.850140437741132\n",
      "    mean_inference_ms: 1.737128343122668\n",
      "    mean_raw_obs_processing_ms: 1.6940822443061754\n",
      "  time_since_restore: 7167.454852819443\n",
      "  time_this_iter_s: 27.778038263320923\n",
      "  time_total_s: 7167.454852819443\n",
      "  timers:\n",
      "    learn_throughput: 1125.779\n",
      "    learn_time_ms: 888.274\n",
      "    load_throughput: 44306.902\n",
      "    load_time_ms: 22.57\n",
      "    sample_throughput: 36.249\n",
      "    sample_time_ms: 27586.812\n",
      "    update_time_ms: 3.476\n",
      "  timestamp: 1634850392\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 270000\n",
      "  training_iteration: 270\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   270</td><td style=\"text-align: right;\">         7167.45</td><td style=\"text-align: right;\">270000</td><td style=\"text-align: right;\"> -3.1525</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            311.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 271000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-07-17\n",
      "  done: false\n",
      "  episode_len_mean: 313.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.1771999999999765\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 721\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.796246752474043\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01463260618769244\n",
      "          policy_loss: -0.1400898616347048\n",
      "          total_loss: -0.136786651197407\n",
      "          vf_explained_var: 0.6711161136627197\n",
      "          vf_loss: 0.011890235106046828\n",
      "    num_agent_steps_sampled: 271000\n",
      "    num_agent_steps_trained: 271000\n",
      "    num_steps_sampled: 271000\n",
      "    num_steps_trained: 271000\n",
      "  iterations_since_restore: 271\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.896875\n",
      "    ram_util_percent: 46.721875\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03963883697217894\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.87179017265194\n",
      "    mean_inference_ms: 1.737228024953764\n",
      "    mean_raw_obs_processing_ms: 1.696808229554019\n",
      "  time_since_restore: 7212.413456439972\n",
      "  time_this_iter_s: 44.958603620529175\n",
      "  time_total_s: 7212.413456439972\n",
      "  timers:\n",
      "    learn_throughput: 1122.759\n",
      "    learn_time_ms: 890.663\n",
      "    load_throughput: 42736.79\n",
      "    load_time_ms: 23.399\n",
      "    sample_throughput: 34.412\n",
      "    sample_time_ms: 29059.676\n",
      "    update_time_ms: 3.659\n",
      "  timestamp: 1634850437\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 271000\n",
      "  training_iteration: 271\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   271</td><td style=\"text-align: right;\">         7212.41</td><td style=\"text-align: right;\">271000</td><td style=\"text-align: right;\"> -3.1772</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">             313.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-07-46\n",
      "  done: false\n",
      "  episode_len_mean: 316.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.2349999999999763\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 724\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8123409933514065\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009573542592819074\n",
      "          policy_loss: -0.04034592600332366\n",
      "          total_loss: -0.02681163822611173\n",
      "          vf_explained_var: 0.7057963609695435\n",
      "          vf_loss: 0.025523709210877616\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "  iterations_since_restore: 272\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.88780487804878\n",
      "    ram_util_percent: 46.49024390243902\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03964142672090948\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.892898717488166\n",
      "    mean_inference_ms: 1.7373313064364035\n",
      "    mean_raw_obs_processing_ms: 1.699485278680696\n",
      "  time_since_restore: 7240.92885518074\n",
      "  time_this_iter_s: 28.515398740768433\n",
      "  time_total_s: 7240.92885518074\n",
      "  timers:\n",
      "    learn_throughput: 1125.737\n",
      "    learn_time_ms: 888.307\n",
      "    load_throughput: 42563.402\n",
      "    load_time_ms: 23.494\n",
      "    sample_throughput: 34.282\n",
      "    sample_time_ms: 29169.877\n",
      "    update_time_ms: 3.618\n",
      "  timestamp: 1634850466\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 272\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   272</td><td style=\"text-align: right;\">         7240.93</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\">  -3.235</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            316.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 273000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-08-15\n",
      "  done: false\n",
      "  episode_len_mean: 318.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.489999999999991\n",
      "  episode_reward_mean: -3.2567999999999757\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 727\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.565128129058414\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013661257458955746\n",
      "          policy_loss: 0.008251845836639404\n",
      "          total_loss: 0.009514699793524213\n",
      "          vf_explained_var: 0.5824495553970337\n",
      "          vf_loss: 0.008161058760662046\n",
      "    num_agent_steps_sampled: 273000\n",
      "    num_agent_steps_trained: 273000\n",
      "    num_steps_sampled: 273000\n",
      "    num_steps_trained: 273000\n",
      "  iterations_since_restore: 273\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.78536585365855\n",
      "    ram_util_percent: 46.37073170731708\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039644093813784205\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.913519845137266\n",
      "    mean_inference_ms: 1.737435234443194\n",
      "    mean_raw_obs_processing_ms: 1.7021148334543392\n",
      "  time_since_restore: 7270.024707555771\n",
      "  time_this_iter_s: 29.095852375030518\n",
      "  time_total_s: 7270.024707555771\n",
      "  timers:\n",
      "    learn_throughput: 1117.904\n",
      "    learn_time_ms: 894.531\n",
      "    load_throughput: 42223.956\n",
      "    load_time_ms: 23.683\n",
      "    sample_throughput: 34.037\n",
      "    sample_time_ms: 29379.657\n",
      "    update_time_ms: 3.872\n",
      "  timestamp: 1634850495\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 273000\n",
      "  training_iteration: 273\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   273</td><td style=\"text-align: right;\">         7270.02</td><td style=\"text-align: right;\">273000</td><td style=\"text-align: right;\"> -3.2568</td><td style=\"text-align: right;\">               -2.49</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            318.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 274000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-08-48\n",
      "  done: false\n",
      "  episode_len_mean: 321.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.569999999999989\n",
      "  episode_reward_mean: -3.280299999999975\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 730\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4725760592354669\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010059884743299028\n",
      "          policy_loss: -0.012086815138657888\n",
      "          total_loss: -0.009767567697498534\n",
      "          vf_explained_var: 0.1474376767873764\n",
      "          vf_loss: 0.010599412616445786\n",
      "    num_agent_steps_sampled: 274000\n",
      "    num_agent_steps_trained: 274000\n",
      "    num_steps_sampled: 274000\n",
      "    num_steps_trained: 274000\n",
      "  iterations_since_restore: 274\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.30833333333334\n",
      "    ram_util_percent: 46.56041666666667\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03964704404231234\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.933417980232644\n",
      "    mean_inference_ms: 1.7375440821929538\n",
      "    mean_raw_obs_processing_ms: 1.7026397637100381\n",
      "  time_since_restore: 7303.089594364166\n",
      "  time_this_iter_s: 33.064886808395386\n",
      "  time_total_s: 7303.089594364166\n",
      "  timers:\n",
      "    learn_throughput: 1066.973\n",
      "    learn_time_ms: 937.231\n",
      "    load_throughput: 44512.1\n",
      "    load_time_ms: 22.466\n",
      "    sample_throughput: 33.639\n",
      "    sample_time_ms: 29727.492\n",
      "    update_time_ms: 4.315\n",
      "  timestamp: 1634850528\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 274000\n",
      "  training_iteration: 274\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   274</td><td style=\"text-align: right;\">         7303.09</td><td style=\"text-align: right;\">274000</td><td style=\"text-align: right;\"> -3.2803</td><td style=\"text-align: right;\">               -2.57</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">             321.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 275000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-09-19\n",
      "  done: false\n",
      "  episode_len_mean: 322.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.569999999999989\n",
      "  episode_reward_mean: -3.2910999999999744\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 733\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3883892112308078\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00805689490648456\n",
      "          policy_loss: 0.008709041277567546\n",
      "          total_loss: 0.008752305308977764\n",
      "          vf_explained_var: 0.3578372895717621\n",
      "          vf_loss: 0.008764921899677978\n",
      "    num_agent_steps_sampled: 275000\n",
      "    num_agent_steps_trained: 275000\n",
      "    num_steps_sampled: 275000\n",
      "    num_steps_trained: 275000\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.36363636363636\n",
      "    ram_util_percent: 46.60681818181818\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03965012489076187\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.95281945740795\n",
      "    mean_inference_ms: 1.7376559835543572\n",
      "    mean_raw_obs_processing_ms: 1.703179874078197\n",
      "  time_since_restore: 7333.851113319397\n",
      "  time_this_iter_s: 30.761518955230713\n",
      "  time_total_s: 7333.851113319397\n",
      "  timers:\n",
      "    learn_throughput: 1070.62\n",
      "    learn_time_ms: 934.038\n",
      "    load_throughput: 44420.176\n",
      "    load_time_ms: 22.512\n",
      "    sample_throughput: 33.528\n",
      "    sample_time_ms: 29825.632\n",
      "    update_time_ms: 5.114\n",
      "  timestamp: 1634850559\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 275000\n",
      "  training_iteration: 275\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   275</td><td style=\"text-align: right;\">         7333.85</td><td style=\"text-align: right;\">275000</td><td style=\"text-align: right;\"> -3.2911</td><td style=\"text-align: right;\">               -2.57</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            322.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 276000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-09-48\n",
      "  done: false\n",
      "  episode_len_mean: 324.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.569999999999989\n",
      "  episode_reward_mean: -3.3126999999999747\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 736\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4157610045539009\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008747492581847007\n",
      "          policy_loss: -0.014920526122053464\n",
      "          total_loss: -0.01664260799686114\n",
      "          vf_explained_var: 0.09726561605930328\n",
      "          vf_loss: 0.006830812047701329\n",
      "    num_agent_steps_sampled: 276000\n",
      "    num_agent_steps_trained: 276000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "  iterations_since_restore: 276\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.16666666666667\n",
      "    ram_util_percent: 46.66428571428572\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039653204652496665\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.971848136770458\n",
      "    mean_inference_ms: 1.7377645005861937\n",
      "    mean_raw_obs_processing_ms: 1.703676576638891\n",
      "  time_since_restore: 7363.262917757034\n",
      "  time_this_iter_s: 29.41180443763733\n",
      "  time_total_s: 7363.262917757034\n",
      "  timers:\n",
      "    learn_throughput: 1065.095\n",
      "    learn_time_ms: 938.883\n",
      "    load_throughput: 45348.877\n",
      "    load_time_ms: 22.051\n",
      "    sample_throughput: 33.539\n",
      "    sample_time_ms: 29816.233\n",
      "    update_time_ms: 5.047\n",
      "  timestamp: 1634850588\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 276\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   276</td><td style=\"text-align: right;\">         7363.26</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\"> -3.3127</td><td style=\"text-align: right;\">               -2.57</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            324.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 277000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-10-16\n",
      "  done: false\n",
      "  episode_len_mean: 327.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.569999999999989\n",
      "  episode_reward_mean: -3.338799999999975\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 739\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.850521116786533\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01628001660770965\n",
      "          policy_loss: 0.02481877696182993\n",
      "          total_loss: 0.02554545799891154\n",
      "          vf_explained_var: 0.5993639826774597\n",
      "          vf_loss: 0.008800917061873609\n",
      "    num_agent_steps_sampled: 277000\n",
      "    num_agent_steps_trained: 277000\n",
      "    num_steps_sampled: 277000\n",
      "    num_steps_trained: 277000\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.17948717948717\n",
      "    ram_util_percent: 46.76666666666667\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039656267128970725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.990338915170327\n",
      "    mean_inference_ms: 1.7378721490799365\n",
      "    mean_raw_obs_processing_ms: 1.7041303667584016\n",
      "  time_since_restore: 7391.049065113068\n",
      "  time_this_iter_s: 27.786147356033325\n",
      "  time_total_s: 7391.049065113068\n",
      "  timers:\n",
      "    learn_throughput: 1066.437\n",
      "    learn_time_ms: 937.702\n",
      "    load_throughput: 45041.247\n",
      "    load_time_ms: 22.202\n",
      "    sample_throughput: 33.681\n",
      "    sample_time_ms: 29690.141\n",
      "    update_time_ms: 4.786\n",
      "  timestamp: 1634850616\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 277000\n",
      "  training_iteration: 277\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   277</td><td style=\"text-align: right;\">         7391.05</td><td style=\"text-align: right;\">277000</td><td style=\"text-align: right;\"> -3.3388</td><td style=\"text-align: right;\">               -2.57</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            327.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 278000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-10-48\n",
      "  done: false\n",
      "  episode_len_mean: 328.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.569999999999989\n",
      "  episode_reward_mean: -3.3498999999999732\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 742\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.373229710261027\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007930608962792057\n",
      "          policy_loss: 0.01615444463160303\n",
      "          total_loss: 0.014580546650621626\n",
      "          vf_explained_var: 0.5721988081932068\n",
      "          vf_loss: 0.007077080217034866\n",
      "    num_agent_steps_sampled: 278000\n",
      "    num_agent_steps_trained: 278000\n",
      "    num_steps_sampled: 278000\n",
      "    num_steps_trained: 278000\n",
      "  iterations_since_restore: 278\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.49787234042553\n",
      "    ram_util_percent: 46.71489361702127\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03965928924590864\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.00843693276771\n",
      "    mean_inference_ms: 1.7379830019826474\n",
      "    mean_raw_obs_processing_ms: 1.7046031723294386\n",
      "  time_since_restore: 7423.4987449646\n",
      "  time_this_iter_s: 32.44967985153198\n",
      "  time_total_s: 7423.4987449646\n",
      "  timers:\n",
      "    learn_throughput: 1059.958\n",
      "    learn_time_ms: 943.433\n",
      "    load_throughput: 44884.933\n",
      "    load_time_ms: 22.279\n",
      "    sample_throughput: 33.139\n",
      "    sample_time_ms: 30176.248\n",
      "    update_time_ms: 5.051\n",
      "  timestamp: 1634850648\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 278000\n",
      "  training_iteration: 278\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   278</td><td style=\"text-align: right;\">          7423.5</td><td style=\"text-align: right;\">278000</td><td style=\"text-align: right;\"> -3.3499</td><td style=\"text-align: right;\">               -2.57</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            328.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 279000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-11-19\n",
      "  done: false\n",
      "  episode_len_mean: 329.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.679999999999987\n",
      "  episode_reward_mean: -3.362699999999973\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 745\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3692772375212776\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012191516055602373\n",
      "          policy_loss: -0.12205986086693076\n",
      "          total_loss: -0.12241309214797284\n",
      "          vf_explained_var: 0.7226249575614929\n",
      "          vf_loss: 0.00552816156996414\n",
      "    num_agent_steps_sampled: 279000\n",
      "    num_agent_steps_trained: 279000\n",
      "    num_steps_sampled: 279000\n",
      "    num_steps_trained: 279000\n",
      "  iterations_since_restore: 279\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.46744186046512\n",
      "    ram_util_percent: 46.75116279069767\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03966246625633673\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.02635309496332\n",
      "    mean_inference_ms: 1.7380955484311194\n",
      "    mean_raw_obs_processing_ms: 1.705034840528692\n",
      "  time_since_restore: 7453.637450456619\n",
      "  time_this_iter_s: 30.138705492019653\n",
      "  time_total_s: 7453.637450456619\n",
      "  timers:\n",
      "    learn_throughput: 1110.856\n",
      "    learn_time_ms: 900.207\n",
      "    load_throughput: 47340.629\n",
      "    load_time_ms: 21.124\n",
      "    sample_throughput: 32.824\n",
      "    sample_time_ms: 30465.507\n",
      "    update_time_ms: 4.653\n",
      "  timestamp: 1634850679\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 279000\n",
      "  training_iteration: 279\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   279</td><td style=\"text-align: right;\">         7453.64</td><td style=\"text-align: right;\">279000</td><td style=\"text-align: right;\"> -3.3627</td><td style=\"text-align: right;\">               -2.68</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            329.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-11-44\n",
      "  done: false\n",
      "  episode_len_mean: 331.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.679999999999987\n",
      "  episode_reward_mean: -3.3833999999999738\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 748\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.948102241092258\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007833891337425344\n",
      "          policy_loss: 0.018256872726811302\n",
      "          total_loss: 0.013264289498329163\n",
      "          vf_explained_var: -0.0873676985502243\n",
      "          vf_loss: 0.009469087718429768\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "  iterations_since_restore: 280\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.5388888888889\n",
      "    ram_util_percent: 46.71666666666666\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039665525969219136\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.043477867427132\n",
      "    mean_inference_ms: 1.7382015919369675\n",
      "    mean_raw_obs_processing_ms: 1.7054854155043757\n",
      "  time_since_restore: 7479.371648073196\n",
      "  time_this_iter_s: 25.73419761657715\n",
      "  time_total_s: 7479.371648073196\n",
      "  timers:\n",
      "    learn_throughput: 1111.26\n",
      "    learn_time_ms: 899.879\n",
      "    load_throughput: 49578.003\n",
      "    load_time_ms: 20.17\n",
      "    sample_throughput: 33.045\n",
      "    sample_time_ms: 30261.919\n",
      "    update_time_ms: 5.442\n",
      "  timestamp: 1634850704\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 280\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   280</td><td style=\"text-align: right;\">         7479.37</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\"> -3.3834</td><td style=\"text-align: right;\">               -2.68</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            331.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 281000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-12-31\n",
      "  done: false\n",
      "  episode_len_mean: 330.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.679999999999987\n",
      "  episode_reward_mean: -3.376099999999974\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 751\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.578714538945092\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014365917875733509\n",
      "          policy_loss: -0.010263150102562374\n",
      "          total_loss: -0.01270482838153839\n",
      "          vf_explained_var: 0.409286767244339\n",
      "          vf_loss: 0.004140900230009316\n",
      "    num_agent_steps_sampled: 281000\n",
      "    num_agent_steps_trained: 281000\n",
      "    num_steps_sampled: 281000\n",
      "    num_steps_trained: 281000\n",
      "  iterations_since_restore: 281\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.80294117647058\n",
      "    ram_util_percent: 46.71029411764706\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0396687142975897\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.060762771550426\n",
      "    mean_inference_ms: 1.7383091787848997\n",
      "    mean_raw_obs_processing_ms: 1.7077845633052444\n",
      "  time_since_restore: 7526.452048540115\n",
      "  time_this_iter_s: 47.080400466918945\n",
      "  time_total_s: 7526.452048540115\n",
      "  timers:\n",
      "    learn_throughput: 1086.04\n",
      "    learn_time_ms: 920.777\n",
      "    load_throughput: 51465.621\n",
      "    load_time_ms: 19.43\n",
      "    sample_throughput: 32.837\n",
      "    sample_time_ms: 30453.814\n",
      "    update_time_ms: 5.426\n",
      "  timestamp: 1634850751\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 281000\n",
      "  training_iteration: 281\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   281</td><td style=\"text-align: right;\">         7526.45</td><td style=\"text-align: right;\">281000</td><td style=\"text-align: right;\"> -3.3761</td><td style=\"text-align: right;\">               -2.68</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            330.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 282000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-12-59\n",
      "  done: false\n",
      "  episode_len_mean: 333.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.679999999999987\n",
      "  episode_reward_mean: -3.399099999999973\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 753\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9421794162856207\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008995474225817625\n",
      "          policy_loss: -0.12896346069044537\n",
      "          total_loss: -0.13373240364922417\n",
      "          vf_explained_var: -0.015845773741602898\n",
      "          vf_loss: 0.008889243377941764\n",
      "    num_agent_steps_sampled: 282000\n",
      "    num_agent_steps_trained: 282000\n",
      "    num_steps_sampled: 282000\n",
      "    num_steps_trained: 282000\n",
      "  iterations_since_restore: 282\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.82894736842105\n",
      "    ram_util_percent: 46.58684210526316\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03967110506412992\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.07197899308678\n",
      "    mean_inference_ms: 1.7383828334662814\n",
      "    mean_raw_obs_processing_ms: 1.7093223178463046\n",
      "  time_since_restore: 7553.637179374695\n",
      "  time_this_iter_s: 27.185130834579468\n",
      "  time_total_s: 7553.637179374695\n",
      "  timers:\n",
      "    learn_throughput: 1083.484\n",
      "    learn_time_ms: 922.949\n",
      "    load_throughput: 54448.216\n",
      "    load_time_ms: 18.366\n",
      "    sample_throughput: 32.982\n",
      "    sample_time_ms: 30319.42\n",
      "    update_time_ms: 5.431\n",
      "  timestamp: 1634850779\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 282000\n",
      "  training_iteration: 282\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   282</td><td style=\"text-align: right;\">         7553.64</td><td style=\"text-align: right;\">282000</td><td style=\"text-align: right;\"> -3.3991</td><td style=\"text-align: right;\">               -2.68</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            333.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 283000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-13-35\n",
      "  done: false\n",
      "  episode_len_mean: 332.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.6099999999999883\n",
      "  episode_reward_mean: -3.3933999999999735\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 757\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1454590479532878\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005224501048995044\n",
      "          policy_loss: 0.017067270891533957\n",
      "          total_loss: 0.019233280917008717\n",
      "          vf_explained_var: 0.3753167986869812\n",
      "          vf_loss: 0.010273143649101257\n",
      "    num_agent_steps_sampled: 283000\n",
      "    num_agent_steps_trained: 283000\n",
      "    num_steps_sampled: 283000\n",
      "    num_steps_trained: 283000\n",
      "  iterations_since_restore: 283\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.62692307692306\n",
      "    ram_util_percent: 46.52884615384615\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03967592476501689\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.095148688662594\n",
      "    mean_inference_ms: 1.738536442418133\n",
      "    mean_raw_obs_processing_ms: 1.7123654870846394\n",
      "  time_since_restore: 7589.555650234222\n",
      "  time_this_iter_s: 35.91847085952759\n",
      "  time_total_s: 7589.555650234222\n",
      "  timers:\n",
      "    learn_throughput: 1091.326\n",
      "    learn_time_ms: 916.317\n",
      "    load_throughput: 57954.869\n",
      "    load_time_ms: 17.255\n",
      "    sample_throughput: 32.248\n",
      "    sample_time_ms: 31009.491\n",
      "    update_time_ms: 5.443\n",
      "  timestamp: 1634850815\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 283000\n",
      "  training_iteration: 283\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   283</td><td style=\"text-align: right;\">         7589.56</td><td style=\"text-align: right;\">283000</td><td style=\"text-align: right;\"> -3.3934</td><td style=\"text-align: right;\">               -2.61</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            332.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 284000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-14-07\n",
      "  done: false\n",
      "  episode_len_mean: 332.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.6099999999999883\n",
      "  episode_reward_mean: -3.394299999999973\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 760\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6066556493441264\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006275431287740081\n",
      "          policy_loss: -0.06345277693536547\n",
      "          total_loss: -0.0705740244852172\n",
      "          vf_explained_var: 0.05929991975426674\n",
      "          vf_loss: 0.00492449689643561\n",
      "    num_agent_steps_sampled: 284000\n",
      "    num_agent_steps_trained: 284000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "  iterations_since_restore: 284\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.54782608695653\n",
      "    ram_util_percent: 46.62826086956522\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03967983779928739\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.11252133076081\n",
      "    mean_inference_ms: 1.73865570014021\n",
      "    mean_raw_obs_processing_ms: 1.713300549670842\n",
      "  time_since_restore: 7621.9070744514465\n",
      "  time_this_iter_s: 32.35142421722412\n",
      "  time_total_s: 7621.9070744514465\n",
      "  timers:\n",
      "    learn_throughput: 1146.755\n",
      "    learn_time_ms: 872.026\n",
      "    load_throughput: 58693.363\n",
      "    load_time_ms: 17.038\n",
      "    sample_throughput: 32.276\n",
      "    sample_time_ms: 30982.874\n",
      "    update_time_ms: 4.911\n",
      "  timestamp: 1634850847\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 284\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   284</td><td style=\"text-align: right;\">         7621.91</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\"> -3.3943</td><td style=\"text-align: right;\">               -2.61</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">             332.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 285000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-14-40\n",
      "  done: false\n",
      "  episode_len_mean: 330.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4999999999999907\n",
      "  episode_reward_mean: -3.375299999999973\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 764\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1637053728103637\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00819273623897973\n",
      "          policy_loss: -0.07917409266034763\n",
      "          total_loss: -0.07325171712372038\n",
      "          vf_explained_var: 0.3362971544265747\n",
      "          vf_loss: 0.012310155708756711\n",
      "    num_agent_steps_sampled: 285000\n",
      "    num_agent_steps_trained: 285000\n",
      "    num_steps_sampled: 285000\n",
      "    num_steps_trained: 285000\n",
      "  iterations_since_restore: 285\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.61304347826085\n",
      "    ram_util_percent: 46.547826086956526\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03968484835084911\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.13575387708993\n",
      "    mean_inference_ms: 1.738808018915079\n",
      "    mean_raw_obs_processing_ms: 1.713696636882687\n",
      "  time_since_restore: 7654.526791810989\n",
      "  time_this_iter_s: 32.61971735954285\n",
      "  time_total_s: 7654.526791810989\n",
      "  timers:\n",
      "    learn_throughput: 1142.741\n",
      "    learn_time_ms: 875.089\n",
      "    load_throughput: 59017.598\n",
      "    load_time_ms: 16.944\n",
      "    sample_throughput: 32.086\n",
      "    sample_time_ms: 31166.217\n",
      "    update_time_ms: 4.41\n",
      "  timestamp: 1634850880\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 285000\n",
      "  training_iteration: 285\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   285</td><td style=\"text-align: right;\">         7654.53</td><td style=\"text-align: right;\">285000</td><td style=\"text-align: right;\"> -3.3753</td><td style=\"text-align: right;\">                -2.5</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">             330.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 286000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-15-17\n",
      "  done: false\n",
      "  episode_len_mean: 328.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4999999999999907\n",
      "  episode_reward_mean: -3.3532999999999737\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 768\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0905536399947273\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0024369474895286833\n",
      "          policy_loss: -0.012503964164190822\n",
      "          total_loss: -0.010541941225528716\n",
      "          vf_explained_var: 0.24119041860103607\n",
      "          vf_loss: 0.011306150702552663\n",
      "    num_agent_steps_sampled: 286000\n",
      "    num_agent_steps_trained: 286000\n",
      "    num_steps_sampled: 286000\n",
      "    num_steps_trained: 286000\n",
      "  iterations_since_restore: 286\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.00754716981132\n",
      "    ram_util_percent: 46.75094339622642\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03968968240316478\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.159257305162555\n",
      "    mean_inference_ms: 1.7389603551971653\n",
      "    mean_raw_obs_processing_ms: 1.7142406571629667\n",
      "  time_since_restore: 7691.670013666153\n",
      "  time_this_iter_s: 37.143221855163574\n",
      "  time_total_s: 7691.670013666153\n",
      "  timers:\n",
      "    learn_throughput: 1145.917\n",
      "    learn_time_ms: 872.663\n",
      "    load_throughput: 59343.925\n",
      "    load_time_ms: 16.851\n",
      "    sample_throughput: 31.307\n",
      "    sample_time_ms: 31942.112\n",
      "    update_time_ms: 4.452\n",
      "  timestamp: 1634850917\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 286000\n",
      "  training_iteration: 286\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   286</td><td style=\"text-align: right;\">         7691.67</td><td style=\"text-align: right;\">286000</td><td style=\"text-align: right;\"> -3.3533</td><td style=\"text-align: right;\">                -2.5</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">             328.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 287000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-15-53\n",
      "  done: false\n",
      "  episode_len_mean: 327.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4999999999999907\n",
      "  episode_reward_mean: -3.339499999999974\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 771\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0944621390766567\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01017910994796662\n",
      "          policy_loss: -0.11358155724075106\n",
      "          total_loss: -0.11031875726249483\n",
      "          vf_explained_var: 0.2574424147605896\n",
      "          vf_loss: 0.010946424667619997\n",
      "    num_agent_steps_sampled: 287000\n",
      "    num_agent_steps_trained: 287000\n",
      "    num_steps_sampled: 287000\n",
      "    num_steps_trained: 287000\n",
      "  iterations_since_restore: 287\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.6903846153846\n",
      "    ram_util_percent: 46.66346153846154\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03969328619600345\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.177401642449365\n",
      "    mean_inference_ms: 1.739072733168871\n",
      "    mean_raw_obs_processing_ms: 1.7146790372197114\n",
      "  time_since_restore: 7728.02495932579\n",
      "  time_this_iter_s: 36.35494565963745\n",
      "  time_total_s: 7728.02495932579\n",
      "  timers:\n",
      "    learn_throughput: 1150.927\n",
      "    learn_time_ms: 868.865\n",
      "    load_throughput: 60708.094\n",
      "    load_time_ms: 16.472\n",
      "    sample_throughput: 30.485\n",
      "    sample_time_ms: 32803.368\n",
      "    update_time_ms: 4.366\n",
      "  timestamp: 1634850953\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 287000\n",
      "  training_iteration: 287\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   287</td><td style=\"text-align: right;\">         7728.02</td><td style=\"text-align: right;\">287000</td><td style=\"text-align: right;\"> -3.3395</td><td style=\"text-align: right;\">                -2.5</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            327.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-16-24\n",
      "  done: false\n",
      "  episode_len_mean: 325.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4999999999999907\n",
      "  episode_reward_mean: -3.325799999999974\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 775\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.078201158841451\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006309516664135057\n",
      "          policy_loss: -0.029136978917651706\n",
      "          total_loss: -0.027523238129085966\n",
      "          vf_explained_var: 0.30406567454338074\n",
      "          vf_loss: 0.010374430111712879\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "  iterations_since_restore: 288\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.1659090909091\n",
      "    ram_util_percent: 46.60681818181818\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03969776402971013\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.201383152367427\n",
      "    mean_inference_ms: 1.7392154171183438\n",
      "    mean_raw_obs_processing_ms: 1.7152663736158513\n",
      "  time_since_restore: 7758.81430721283\n",
      "  time_this_iter_s: 30.789347887039185\n",
      "  time_total_s: 7758.81430721283\n",
      "  timers:\n",
      "    learn_throughput: 1149.772\n",
      "    learn_time_ms: 869.738\n",
      "    load_throughput: 57486.774\n",
      "    load_time_ms: 17.395\n",
      "    sample_throughput: 30.641\n",
      "    sample_time_ms: 32635.67\n",
      "    update_time_ms: 4.176\n",
      "  timestamp: 1634850984\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 288\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   288</td><td style=\"text-align: right;\">         7758.81</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\"> -3.3258</td><td style=\"text-align: right;\">                -2.5</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            325.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 289000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-17-00\n",
      "  done: false\n",
      "  episode_len_mean: 323.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4999999999999907\n",
      "  episode_reward_mean: -3.3044999999999747\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 779\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036132812500007\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0401826712820266\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003203019873986119\n",
      "          policy_loss: 0.019210163586669497\n",
      "          total_loss: 0.021973281684848998\n",
      "          vf_explained_var: 0.13968850672245026\n",
      "          vf_loss: 0.01213882120533122\n",
      "    num_agent_steps_sampled: 289000\n",
      "    num_agent_steps_trained: 289000\n",
      "    num_steps_sampled: 289000\n",
      "    num_steps_trained: 289000\n",
      "  iterations_since_restore: 289\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.12692307692305\n",
      "    ram_util_percent: 46.63653846153846\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03970187955606439\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.22540677069867\n",
      "    mean_inference_ms: 1.7393526231987497\n",
      "    mean_raw_obs_processing_ms: 1.7159400256139656\n",
      "  time_since_restore: 7795.12401843071\n",
      "  time_this_iter_s: 36.30971121788025\n",
      "  time_total_s: 7795.12401843071\n",
      "  timers:\n",
      "    learn_throughput: 1144.738\n",
      "    learn_time_ms: 873.563\n",
      "    load_throughput: 57174.264\n",
      "    load_time_ms: 17.49\n",
      "    sample_throughput: 30.076\n",
      "    sample_time_ms: 33248.732\n",
      "    update_time_ms: 4.202\n",
      "  timestamp: 1634851020\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 289000\n",
      "  training_iteration: 289\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   289</td><td style=\"text-align: right;\">         7795.12</td><td style=\"text-align: right;\">289000</td><td style=\"text-align: right;\"> -3.3045</td><td style=\"text-align: right;\">                -2.5</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            323.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 290000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-17-54\n",
      "  done: false\n",
      "  episode_len_mean: 321.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -3.280799999999975\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 783\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.16018066406250003\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0232589324315389\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020714469460836802\n",
      "          policy_loss: 0.006007636338472367\n",
      "          total_loss: 0.00913621327943272\n",
      "          vf_explained_var: 0.39452460408210754\n",
      "          vf_loss: 0.010043108866860469\n",
      "    num_agent_steps_sampled: 290000\n",
      "    num_agent_steps_trained: 290000\n",
      "    num_steps_sampled: 290000\n",
      "    num_steps_trained: 290000\n",
      "  iterations_since_restore: 290\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.73116883116883\n",
      "    ram_util_percent: 46.64805194805196\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03970589215837756\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.24957852406756\n",
      "    mean_inference_ms: 1.7394829356707588\n",
      "    mean_raw_obs_processing_ms: 1.719126524984145\n",
      "  time_since_restore: 7849.073392391205\n",
      "  time_this_iter_s: 53.949373960494995\n",
      "  time_total_s: 7849.073392391205\n",
      "  timers:\n",
      "    learn_throughput: 1143.764\n",
      "    learn_time_ms: 874.306\n",
      "    load_throughput: 55716.047\n",
      "    load_time_ms: 17.948\n",
      "    sample_throughput: 27.724\n",
      "    sample_time_ms: 36069.463\n",
      "    update_time_ms: 3.424\n",
      "  timestamp: 1634851074\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 290000\n",
      "  training_iteration: 290\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   290</td><td style=\"text-align: right;\">         7849.07</td><td style=\"text-align: right;\">290000</td><td style=\"text-align: right;\"> -3.2808</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            321.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 291000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-18-30\n",
      "  done: false\n",
      "  episode_len_mean: 320.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -3.2702999999999753\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 787\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2402709960937499\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0322499129507277\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01026957247009924\n",
      "          policy_loss: 0.07285118790136444\n",
      "          total_loss: 0.07401459001832539\n",
      "          vf_explained_var: 0.5816954970359802\n",
      "          vf_loss: 0.009018422541622486\n",
      "    num_agent_steps_sampled: 291000\n",
      "    num_agent_steps_trained: 291000\n",
      "    num_steps_sampled: 291000\n",
      "    num_steps_trained: 291000\n",
      "  iterations_since_restore: 291\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.7607843137255\n",
      "    ram_util_percent: 46.641176470588235\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03970983156049737\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.27397035185641\n",
      "    mean_inference_ms: 1.7396111291670713\n",
      "    mean_raw_obs_processing_ms: 1.7223789308509874\n",
      "  time_since_restore: 7884.552628517151\n",
      "  time_this_iter_s: 35.479236125946045\n",
      "  time_total_s: 7884.552628517151\n",
      "  timers:\n",
      "    learn_throughput: 1172.229\n",
      "    learn_time_ms: 853.076\n",
      "    load_throughput: 56195.063\n",
      "    load_time_ms: 17.795\n",
      "    sample_throughput: 28.628\n",
      "    sample_time_ms: 34930.969\n",
      "    update_time_ms: 3.247\n",
      "  timestamp: 1634851110\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 291000\n",
      "  training_iteration: 291\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   291</td><td style=\"text-align: right;\">         7884.55</td><td style=\"text-align: right;\">291000</td><td style=\"text-align: right;\"> -3.2703</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">             320.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 292000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-19-04\n",
      "  done: false\n",
      "  episode_len_mean: 318.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -3.2518999999999756\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 790\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2402709960937499\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0767634524239433\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.022721735017923253\n",
      "          policy_loss: -0.012645408552553918\n",
      "          total_loss: -0.015473645884129736\n",
      "          vf_explained_var: 0.9146797060966492\n",
      "          vf_loss: 0.0024800252324591082\n",
      "    num_agent_steps_sampled: 292000\n",
      "    num_agent_steps_trained: 292000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "  iterations_since_restore: 292\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.24489795918367\n",
      "    ram_util_percent: 46.673469387755105\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0397129265669214\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.29279821234196\n",
      "    mean_inference_ms: 1.7397079749134303\n",
      "    mean_raw_obs_processing_ms: 1.7227893405025665\n",
      "  time_since_restore: 7918.968297243118\n",
      "  time_this_iter_s: 34.41566872596741\n",
      "  time_total_s: 7918.968297243118\n",
      "  timers:\n",
      "    learn_throughput: 1175.754\n",
      "    learn_time_ms: 850.518\n",
      "    load_throughput: 53490.315\n",
      "    load_time_ms: 18.695\n",
      "    sample_throughput: 28.046\n",
      "    sample_time_ms: 35655.889\n",
      "    update_time_ms: 3.263\n",
      "  timestamp: 1634851144\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 292\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   292</td><td style=\"text-align: right;\">         7918.97</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\"> -3.2519</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            318.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 293000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-19-39\n",
      "  done: false\n",
      "  episode_len_mean: 315.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -3.2265999999999764\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 794\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.176761751042472\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02066325189655542\n",
      "          policy_loss: -0.0056845486991935305\n",
      "          total_loss: -0.0014237641460365718\n",
      "          vf_explained_var: 0.5528036952018738\n",
      "          vf_loss: 0.008581230259086524\n",
      "    num_agent_steps_sampled: 293000\n",
      "    num_agent_steps_trained: 293000\n",
      "    num_steps_sampled: 293000\n",
      "    num_steps_trained: 293000\n",
      "  iterations_since_restore: 293\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.09387755102041\n",
      "    ram_util_percent: 46.679591836734694\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03971748428190356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.318183018476525\n",
      "    mean_inference_ms: 1.7398390877710737\n",
      "    mean_raw_obs_processing_ms: 1.7234249519126368\n",
      "  time_since_restore: 7953.297147512436\n",
      "  time_this_iter_s: 34.32885026931763\n",
      "  time_total_s: 7953.297147512436\n",
      "  timers:\n",
      "    learn_throughput: 1175.766\n",
      "    learn_time_ms: 850.509\n",
      "    load_throughput: 53582.43\n",
      "    load_time_ms: 18.663\n",
      "    sample_throughput: 28.171\n",
      "    sample_time_ms: 35497.455\n",
      "    update_time_ms: 2.862\n",
      "  timestamp: 1634851179\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 293000\n",
      "  training_iteration: 293\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   293</td><td style=\"text-align: right;\">          7953.3</td><td style=\"text-align: right;\">293000</td><td style=\"text-align: right;\"> -3.2266</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            315.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 294000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-20-16\n",
      "  done: false\n",
      "  episode_len_mean: 312.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -3.188799999999978\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 798\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9619937658309936\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013581750782638045\n",
      "          policy_loss: -0.019934512343671586\n",
      "          total_loss: -0.015186323556635115\n",
      "          vf_explained_var: 0.6518568992614746\n",
      "          vf_loss: 0.007025699004427426\n",
      "    num_agent_steps_sampled: 294000\n",
      "    num_agent_steps_trained: 294000\n",
      "    num_steps_sampled: 294000\n",
      "    num_steps_trained: 294000\n",
      "  iterations_since_restore: 294\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.88490566037736\n",
      "    ram_util_percent: 46.860377358490574\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03972213802796003\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.34464470922459\n",
      "    mean_inference_ms: 1.739977651060667\n",
      "    mean_raw_obs_processing_ms: 1.7241479067747036\n",
      "  time_since_restore: 7990.164638757706\n",
      "  time_this_iter_s: 36.867491245269775\n",
      "  time_total_s: 7990.164638757706\n",
      "  timers:\n",
      "    learn_throughput: 1165.566\n",
      "    learn_time_ms: 857.952\n",
      "    load_throughput: 52958.384\n",
      "    load_time_ms: 18.883\n",
      "    sample_throughput: 27.823\n",
      "    sample_time_ms: 35941.33\n",
      "    update_time_ms: 3.18\n",
      "  timestamp: 1634851216\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 294000\n",
      "  training_iteration: 294\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   294</td><td style=\"text-align: right;\">         7990.16</td><td style=\"text-align: right;\">294000</td><td style=\"text-align: right;\"> -3.1888</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            312.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 295000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-20-45\n",
      "  done: false\n",
      "  episode_len_mean: 311.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -3.181399999999978\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 801\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3927111056115893\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017168410417261547\n",
      "          policy_loss: 0.00931897179947959\n",
      "          total_loss: 0.013405130141311221\n",
      "          vf_explained_var: 0.5204965472221375\n",
      "          vf_loss: 0.00873185485187504\n",
      "    num_agent_steps_sampled: 295000\n",
      "    num_agent_steps_trained: 295000\n",
      "    num_steps_sampled: 295000\n",
      "    num_steps_trained: 295000\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.37142857142857\n",
      "    ram_util_percent: 46.97857142857143\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039725464562169834\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.364305726109805\n",
      "    mean_inference_ms: 1.740076491719085\n",
      "    mean_raw_obs_processing_ms: 1.7247918085757399\n",
      "  time_since_restore: 8019.625451087952\n",
      "  time_this_iter_s: 29.46081233024597\n",
      "  time_total_s: 8019.625451087952\n",
      "  timers:\n",
      "    learn_throughput: 1171.47\n",
      "    learn_time_ms: 853.629\n",
      "    load_throughput: 50604.385\n",
      "    load_time_ms: 19.761\n",
      "    sample_throughput: 28.067\n",
      "    sample_time_ms: 35629.433\n",
      "    update_time_ms: 2.789\n",
      "  timestamp: 1634851245\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 295000\n",
      "  training_iteration: 295\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   295</td><td style=\"text-align: right;\">         8019.63</td><td style=\"text-align: right;\">295000</td><td style=\"text-align: right;\"> -3.1814</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            311.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-21-17\n",
      "  done: false\n",
      "  episode_len_mean: 309.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -3.1618999999999784\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 804\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2508955895900726\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.026882467414564813\n",
      "          policy_loss: -0.12941215203868017\n",
      "          total_loss: -0.12159367824594179\n",
      "          vf_explained_var: 0.7117484211921692\n",
      "          vf_loss: 0.005794506278147714\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "  iterations_since_restore: 296\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.96304347826086\n",
      "    ram_util_percent: 46.89782608695653\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039728865594081196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.384106255733286\n",
      "    mean_inference_ms: 1.7401751273728434\n",
      "    mean_raw_obs_processing_ms: 1.72545107197881\n",
      "  time_since_restore: 8052.012899637222\n",
      "  time_this_iter_s: 32.38744854927063\n",
      "  time_total_s: 8052.012899637222\n",
      "  timers:\n",
      "    learn_throughput: 1174.501\n",
      "    learn_time_ms: 851.425\n",
      "    load_throughput: 50410.308\n",
      "    load_time_ms: 19.837\n",
      "    sample_throughput: 28.445\n",
      "    sample_time_ms: 35156.048\n",
      "    update_time_ms: 2.705\n",
      "  timestamp: 1634851277\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 296\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   296</td><td style=\"text-align: right;\">         8052.01</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\"> -3.1619</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            309.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 297000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-21-50\n",
      "  done: false\n",
      "  episode_len_mean: 308.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -3.149199999999978\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 808\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2948958026038275\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007797701125865212\n",
      "          policy_loss: -0.030437209871080187\n",
      "          total_loss: -0.0347113495071729\n",
      "          vf_explained_var: 0.8853574395179749\n",
      "          vf_loss: 0.002351550429335071\n",
      "    num_agent_steps_sampled: 297000\n",
      "    num_agent_steps_trained: 297000\n",
      "    num_steps_sampled: 297000\n",
      "    num_steps_trained: 297000\n",
      "  iterations_since_restore: 297\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.1804347826087\n",
      "    ram_util_percent: 46.873913043478254\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039733680272363986\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.410549742446804\n",
      "    mean_inference_ms: 1.7403061070654717\n",
      "    mean_raw_obs_processing_ms: 1.726409068433243\n",
      "  time_since_restore: 8084.104749917984\n",
      "  time_this_iter_s: 32.09185028076172\n",
      "  time_total_s: 8084.104749917984\n",
      "  timers:\n",
      "    learn_throughput: 1168.409\n",
      "    learn_time_ms: 855.864\n",
      "    load_throughput: 50145.008\n",
      "    load_time_ms: 19.942\n",
      "    sample_throughput: 28.798\n",
      "    sample_time_ms: 34725.034\n",
      "    update_time_ms: 2.871\n",
      "  timestamp: 1634851310\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 297000\n",
      "  training_iteration: 297\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   297</td><td style=\"text-align: right;\">          8084.1</td><td style=\"text-align: right;\">297000</td><td style=\"text-align: right;\"> -3.1492</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            308.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 298000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-22-41\n",
      "  done: false\n",
      "  episode_len_mean: 305.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -3.127199999999979\n",
      "  episode_reward_min: -7.769999999999964\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 811\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1572086334228515\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0065841474838862265\n",
      "          policy_loss: -0.05700188188089265\n",
      "          total_loss: -0.059004035964608195\n",
      "          vf_explained_var: 0.5533361434936523\n",
      "          vf_loss: 0.004230750375427306\n",
      "    num_agent_steps_sampled: 298000\n",
      "    num_agent_steps_trained: 298000\n",
      "    num_steps_sampled: 298000\n",
      "    num_steps_trained: 298000\n",
      "  iterations_since_restore: 298\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.04520547945205\n",
      "    ram_util_percent: 46.85205479452055\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03973730588381215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.430833704148803\n",
      "    mean_inference_ms: 1.7404044595698798\n",
      "    mean_raw_obs_processing_ms: 1.7288668115147479\n",
      "  time_since_restore: 8135.566212654114\n",
      "  time_this_iter_s: 51.46146273612976\n",
      "  time_total_s: 8135.566212654114\n",
      "  timers:\n",
      "    learn_throughput: 1171.163\n",
      "    learn_time_ms: 853.852\n",
      "    load_throughput: 52697.16\n",
      "    load_time_ms: 18.976\n",
      "    sample_throughput: 27.177\n",
      "    sample_time_ms: 36795.324\n",
      "    update_time_ms: 2.819\n",
      "  timestamp: 1634851361\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 298000\n",
      "  training_iteration: 298\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   298</td><td style=\"text-align: right;\">         8135.57</td><td style=\"text-align: right;\">298000</td><td style=\"text-align: right;\"> -3.1272</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -7.77</td><td style=\"text-align: right;\">            305.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 299000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-23-13\n",
      "  done: false\n",
      "  episode_len_mean: 302.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -3.050299999999979\n",
      "  episode_reward_min: -6.459999999999971\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 815\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2381215042538114\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011895364512649111\n",
      "          policy_loss: 0.0317370298008124\n",
      "          total_loss: 0.03458746928307745\n",
      "          vf_explained_var: 0.6327933073043823\n",
      "          vf_loss: 0.005585525519887192\n",
      "    num_agent_steps_sampled: 299000\n",
      "    num_agent_steps_trained: 299000\n",
      "    num_steps_sampled: 299000\n",
      "    num_steps_trained: 299000\n",
      "  iterations_since_restore: 299\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.47391304347825\n",
      "    ram_util_percent: 46.91521739130435\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039742157092792656\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.458332614921147\n",
      "    mean_inference_ms: 1.7405371554312865\n",
      "    mean_raw_obs_processing_ms: 1.7322002182955978\n",
      "  time_since_restore: 8167.864958763123\n",
      "  time_this_iter_s: 32.29874610900879\n",
      "  time_total_s: 8167.864958763123\n",
      "  timers:\n",
      "    learn_throughput: 1172.256\n",
      "    learn_time_ms: 853.056\n",
      "    load_throughput: 52371.453\n",
      "    load_time_ms: 19.094\n",
      "    sample_throughput: 27.476\n",
      "    sample_time_ms: 36394.901\n",
      "    update_time_ms: 2.898\n",
      "  timestamp: 1634851393\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 299000\n",
      "  training_iteration: 299\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         8167.86</td><td style=\"text-align: right;\">299000</td><td style=\"text-align: right;\"> -3.0503</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -6.46</td><td style=\"text-align: right;\">            302.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-23-45\n",
      "  done: false\n",
      "  episode_len_mean: 301.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -3.0389999999999793\n",
      "  episode_reward_min: -6.459999999999971\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 818\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.363813665178087\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009560474184163522\n",
      "          policy_loss: 0.0643673246105512\n",
      "          total_loss: 0.06233562781578965\n",
      "          vf_explained_var: 0.7926248908042908\n",
      "          vf_loss: 0.003853712678473029\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_agent_steps_trained: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "  iterations_since_restore: 300\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.22\n",
      "    ram_util_percent: 46.87333333333333\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039746114680799506\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.479102431168887\n",
      "    mean_inference_ms: 1.7406389526141095\n",
      "    mean_raw_obs_processing_ms: 1.734791031152007\n",
      "  time_since_restore: 8199.357833623886\n",
      "  time_this_iter_s: 31.49287486076355\n",
      "  time_total_s: 8199.357833623886\n",
      "  timers:\n",
      "    learn_throughput: 1171.899\n",
      "    learn_time_ms: 853.316\n",
      "    load_throughput: 52178.344\n",
      "    load_time_ms: 19.165\n",
      "    sample_throughput: 29.283\n",
      "    sample_time_ms: 34149.279\n",
      "    update_time_ms: 2.853\n",
      "  timestamp: 1634851425\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 300\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   300</td><td style=\"text-align: right;\">         8199.36</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">  -3.039</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -6.46</td><td style=\"text-align: right;\">            301.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 301000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-24-14\n",
      "  done: false\n",
      "  episode_len_mean: 300.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -3.0374999999999788\n",
      "  episode_reward_min: -6.459999999999971\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 821\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6898035128911337\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013681478136569246\n",
      "          policy_loss: 0.06834509621063868\n",
      "          total_loss: 0.06535969566967752\n",
      "          vf_explained_var: 0.7381706237792969\n",
      "          vf_loss: 0.002818122254878593\n",
      "    num_agent_steps_sampled: 301000\n",
      "    num_agent_steps_trained: 301000\n",
      "    num_steps_sampled: 301000\n",
      "    num_steps_trained: 301000\n",
      "  iterations_since_restore: 301\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.15853658536584\n",
      "    ram_util_percent: 46.90975609756098\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03975018526181091\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.49980573376917\n",
      "    mean_inference_ms: 1.7407442263412287\n",
      "    mean_raw_obs_processing_ms: 1.7355386573800302\n",
      "  time_since_restore: 8228.059525489807\n",
      "  time_this_iter_s: 28.70169186592102\n",
      "  time_total_s: 8228.059525489807\n",
      "  timers:\n",
      "    learn_throughput: 1170.532\n",
      "    learn_time_ms: 854.313\n",
      "    load_throughput: 52373.481\n",
      "    load_time_ms: 19.094\n",
      "    sample_throughput: 29.879\n",
      "    sample_time_ms: 33468.758\n",
      "    update_time_ms: 4.649\n",
      "  timestamp: 1634851454\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 301000\n",
      "  training_iteration: 301\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   301</td><td style=\"text-align: right;\">         8228.06</td><td style=\"text-align: right;\">301000</td><td style=\"text-align: right;\"> -3.0375</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -6.46</td><td style=\"text-align: right;\">            300.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 302000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-24-47\n",
      "  done: false\n",
      "  episode_len_mean: 298.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.98959999999998\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 824\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2343876361846924\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006845069893120974\n",
      "          policy_loss: -0.14365775105026032\n",
      "          total_loss: -0.14459694359037611\n",
      "          vf_explained_var: 0.7170701026916504\n",
      "          vf_loss: 0.00585391553144695\n",
      "    num_agent_steps_sampled: 302000\n",
      "    num_agent_steps_trained: 302000\n",
      "    num_steps_sampled: 302000\n",
      "    num_steps_trained: 302000\n",
      "  iterations_since_restore: 302\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.33958333333334\n",
      "    ram_util_percent: 46.95416666666667\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03975436450619108\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.52089824125985\n",
      "    mean_inference_ms: 1.740843770183445\n",
      "    mean_raw_obs_processing_ms: 1.736299115421399\n",
      "  time_since_restore: 8261.488906860352\n",
      "  time_this_iter_s: 33.429381370544434\n",
      "  time_total_s: 8261.488906860352\n",
      "  timers:\n",
      "    learn_throughput: 1168.965\n",
      "    learn_time_ms: 855.458\n",
      "    load_throughput: 54836.965\n",
      "    load_time_ms: 18.236\n",
      "    sample_throughput: 29.968\n",
      "    sample_time_ms: 33369.401\n",
      "    update_time_ms: 4.762\n",
      "  timestamp: 1634851487\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 302000\n",
      "  training_iteration: 302\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   302</td><td style=\"text-align: right;\">         8261.49</td><td style=\"text-align: right;\">302000</td><td style=\"text-align: right;\"> -2.9896</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            298.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 303000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-25-18\n",
      "  done: false\n",
      "  episode_len_mean: 297.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.9711999999999796\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 828\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3194588210847642\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01898811891064827\n",
      "          policy_loss: -0.022168080343140495\n",
      "          total_loss: -0.016928562190797594\n",
      "          vf_explained_var: 0.8884050250053406\n",
      "          vf_loss: 0.0030363631182505437\n",
      "    num_agent_steps_sampled: 303000\n",
      "    num_agent_steps_trained: 303000\n",
      "    num_steps_sampled: 303000\n",
      "    num_steps_trained: 303000\n",
      "  iterations_since_restore: 303\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.41136363636362\n",
      "    ram_util_percent: 46.975\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03975962915844864\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.548830352368224\n",
      "    mean_inference_ms: 1.74097320985198\n",
      "    mean_raw_obs_processing_ms: 1.737388395504677\n",
      "  time_since_restore: 8292.260997772217\n",
      "  time_this_iter_s: 30.772090911865234\n",
      "  time_total_s: 8292.260997772217\n",
      "  timers:\n",
      "    learn_throughput: 1166.426\n",
      "    learn_time_ms: 857.32\n",
      "    load_throughput: 52611.625\n",
      "    load_time_ms: 19.007\n",
      "    sample_throughput: 30.293\n",
      "    sample_time_ms: 33011.151\n",
      "    update_time_ms: 4.712\n",
      "  timestamp: 1634851518\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 303000\n",
      "  training_iteration: 303\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   303</td><td style=\"text-align: right;\">         8292.26</td><td style=\"text-align: right;\">303000</td><td style=\"text-align: right;\"> -2.9712</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            297.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-25-48\n",
      "  done: false\n",
      "  episode_len_mean: 296.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.9633999999999805\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 831\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2760806931389703\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009248785766547604\n",
      "          policy_loss: 0.06275261259741254\n",
      "          total_loss: 0.060184645363026194\n",
      "          vf_explained_var: 0.9279566407203674\n",
      "          vf_loss: 0.0026928661915007978\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "  iterations_since_restore: 304\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.35348837209303\n",
      "    ram_util_percent: 47.01627906976744\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039763209793683744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.569416947916046\n",
      "    mean_inference_ms: 1.741060578686742\n",
      "    mean_raw_obs_processing_ms: 1.7382306795176923\n",
      "  time_since_restore: 8322.61201930046\n",
      "  time_this_iter_s: 30.35102152824402\n",
      "  time_total_s: 8322.61201930046\n",
      "  timers:\n",
      "    learn_throughput: 1175.636\n",
      "    learn_time_ms: 850.603\n",
      "    load_throughput: 52113.707\n",
      "    load_time_ms: 19.189\n",
      "    sample_throughput: 30.897\n",
      "    sample_time_ms: 32365.536\n",
      "    update_time_ms: 5.243\n",
      "  timestamp: 1634851548\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 304\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   304</td><td style=\"text-align: right;\">         8322.61</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\"> -2.9634</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            296.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 305000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-26-16\n",
      "  done: false\n",
      "  episode_len_mean: 296.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.96809999999998\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 834\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.580295745531718\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012089758593724772\n",
      "          policy_loss: 0.06429478915201293\n",
      "          total_loss: 0.06077870196766323\n",
      "          vf_explained_var: 0.9103429913520813\n",
      "          vf_loss: 0.0024831107235513627\n",
      "    num_agent_steps_sampled: 305000\n",
      "    num_agent_steps_trained: 305000\n",
      "    num_steps_sampled: 305000\n",
      "    num_steps_trained: 305000\n",
      "  iterations_since_restore: 305\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.32750000000001\n",
      "    ram_util_percent: 46.95\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03976740966099781\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.589614670167883\n",
      "    mean_inference_ms: 1.7411463159209728\n",
      "    mean_raw_obs_processing_ms: 1.739085926089619\n",
      "  time_since_restore: 8350.213198661804\n",
      "  time_this_iter_s: 27.601179361343384\n",
      "  time_total_s: 8350.213198661804\n",
      "  timers:\n",
      "    learn_throughput: 1169.099\n",
      "    learn_time_ms: 855.359\n",
      "    load_throughput: 52035.412\n",
      "    load_time_ms: 19.218\n",
      "    sample_throughput: 31.081\n",
      "    sample_time_ms: 32174.03\n",
      "    update_time_ms: 5.862\n",
      "  timestamp: 1634851576\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 305000\n",
      "  training_iteration: 305\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   305</td><td style=\"text-align: right;\">         8350.21</td><td style=\"text-align: right;\">305000</td><td style=\"text-align: right;\"> -2.9681</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            296.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 306000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-26-47\n",
      "  done: false\n",
      "  episode_len_mean: 295.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.9568999999999805\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 837\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2403627283043333\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013449091478628992\n",
      "          policy_loss: -0.03034251059095065\n",
      "          total_loss: -0.02939416691660881\n",
      "          vf_explained_var: 0.953696608543396\n",
      "          vf_loss: 0.002445902286045667\n",
      "    num_agent_steps_sampled: 306000\n",
      "    num_agent_steps_trained: 306000\n",
      "    num_steps_sampled: 306000\n",
      "    num_steps_trained: 306000\n",
      "  iterations_since_restore: 306\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.09545454545454\n",
      "    ram_util_percent: 46.97727272727273\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039771609919314777\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.609947661724718\n",
      "    mean_inference_ms: 1.741234415430392\n",
      "    mean_raw_obs_processing_ms: 1.7399539547668925\n",
      "  time_since_restore: 8381.536230564117\n",
      "  time_this_iter_s: 31.323031902313232\n",
      "  time_total_s: 8381.536230564117\n",
      "  timers:\n",
      "    learn_throughput: 1167.013\n",
      "    learn_time_ms: 856.888\n",
      "    load_throughput: 52256.745\n",
      "    load_time_ms: 19.136\n",
      "    sample_throughput: 31.186\n",
      "    sample_time_ms: 32065.963\n",
      "    update_time_ms: 5.75\n",
      "  timestamp: 1634851607\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 306000\n",
      "  training_iteration: 306\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   306</td><td style=\"text-align: right;\">         8381.54</td><td style=\"text-align: right;\">306000</td><td style=\"text-align: right;\"> -2.9569</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            295.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 307000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-27-33\n",
      "  done: false\n",
      "  episode_len_mean: 295.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.955899999999981\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 840\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8538995613654454\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01431179000605047\n",
      "          policy_loss: -0.01693024800883399\n",
      "          total_loss: -0.0014539561337894864\n",
      "          vf_explained_var: 0.8281078934669495\n",
      "          vf_loss: 0.01240964699536562\n",
      "    num_agent_steps_sampled: 307000\n",
      "    num_agent_steps_trained: 307000\n",
      "    num_steps_sampled: 307000\n",
      "    num_steps_trained: 307000\n",
      "  iterations_since_restore: 307\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.57076923076923\n",
      "    ram_util_percent: 46.89846153846154\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03977600098462839\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.63009356182798\n",
      "    mean_inference_ms: 1.7413238535809097\n",
      "    mean_raw_obs_processing_ms: 1.7424820567653243\n",
      "  time_since_restore: 8426.951412200928\n",
      "  time_this_iter_s: 45.4151816368103\n",
      "  time_total_s: 8426.951412200928\n",
      "  timers:\n",
      "    learn_throughput: 1166.649\n",
      "    learn_time_ms: 857.156\n",
      "    load_throughput: 51062.309\n",
      "    load_time_ms: 19.584\n",
      "    sample_throughput: 29.942\n",
      "    sample_time_ms: 33397.699\n",
      "    update_time_ms: 5.632\n",
      "  timestamp: 1634851653\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 307000\n",
      "  training_iteration: 307\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   307</td><td style=\"text-align: right;\">         8426.95</td><td style=\"text-align: right;\">307000</td><td style=\"text-align: right;\"> -2.9559</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            295.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-28-02\n",
      "  done: false\n",
      "  episode_len_mean: 296.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.97619999999998\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 843\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0937622043821547\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009915604890210419\n",
      "          policy_loss: 0.014823492450846566\n",
      "          total_loss: 0.023056350234482024\n",
      "          vf_explained_var: 0.7109197974205017\n",
      "          vf_loss: 0.011129769853626688\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_agent_steps_trained: 308000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "  iterations_since_restore: 308\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.26097560975609\n",
      "    ram_util_percent: 46.81219512195122\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039780378443482534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.64983440256952\n",
      "    mean_inference_ms: 1.7414137051199299\n",
      "    mean_raw_obs_processing_ms: 1.7450178235537697\n",
      "  time_since_restore: 8455.83562040329\n",
      "  time_this_iter_s: 28.88420820236206\n",
      "  time_total_s: 8455.83562040329\n",
      "  timers:\n",
      "    learn_throughput: 1167.816\n",
      "    learn_time_ms: 856.299\n",
      "    load_throughput: 51526.253\n",
      "    load_time_ms: 19.408\n",
      "    sample_throughput: 32.112\n",
      "    sample_time_ms: 31140.969\n",
      "    update_time_ms: 5.7\n",
      "  timestamp: 1634851682\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 308\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   308</td><td style=\"text-align: right;\">         8455.84</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\"> -2.9762</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            296.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 309000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-28-32\n",
      "  done: false\n",
      "  episode_len_mean: 296.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.9771999999999803\n",
      "  episode_reward_min: -4.699999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 846\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2249585893419055\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0071623482110859445\n",
      "          policy_loss: -0.13328664186928008\n",
      "          total_loss: -0.13307495137883557\n",
      "          vf_explained_var: 0.6323071718215942\n",
      "          vf_loss: 0.0066532234363775285\n",
      "    num_agent_steps_sampled: 309000\n",
      "    num_agent_steps_trained: 309000\n",
      "    num_steps_sampled: 309000\n",
      "    num_steps_trained: 309000\n",
      "  iterations_since_restore: 309\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.28181818181818\n",
      "    ram_util_percent: 46.79318181818182\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03978472830636978\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.669649704235525\n",
      "    mean_inference_ms: 1.7415066772155787\n",
      "    mean_raw_obs_processing_ms: 1.7475612367043183\n",
      "  time_since_restore: 8486.378004789352\n",
      "  time_this_iter_s: 30.542384386062622\n",
      "  time_total_s: 8486.378004789352\n",
      "  timers:\n",
      "    learn_throughput: 1169.751\n",
      "    learn_time_ms: 854.883\n",
      "    load_throughput: 50641.289\n",
      "    load_time_ms: 19.747\n",
      "    sample_throughput: 32.293\n",
      "    sample_time_ms: 30966.428\n",
      "    update_time_ms: 5.588\n",
      "  timestamp: 1634851712\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 309000\n",
      "  training_iteration: 309\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   309</td><td style=\"text-align: right;\">         8486.38</td><td style=\"text-align: right;\">309000</td><td style=\"text-align: right;\"> -2.9772</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">                -4.7</td><td style=\"text-align: right;\">            296.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 310000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-29-02\n",
      "  done: false\n",
      "  episode_len_mean: 294.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.9525999999999812\n",
      "  episode_reward_min: -4.6399999999999455\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 850\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0895236796802945\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0031402144478515403\n",
      "          policy_loss: -0.015089941355917189\n",
      "          total_loss: -0.013827117946412828\n",
      "          vf_explained_var: 0.5172232389450073\n",
      "          vf_loss: 0.009611615502379007\n",
      "    num_agent_steps_sampled: 310000\n",
      "    num_agent_steps_trained: 310000\n",
      "    num_steps_sampled: 310000\n",
      "    num_steps_trained: 310000\n",
      "  iterations_since_restore: 310\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.21428571428571\n",
      "    ram_util_percent: 46.73809523809524\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03979050753334144\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.696161447286894\n",
      "    mean_inference_ms: 1.741629782302212\n",
      "    mean_raw_obs_processing_ms: 1.7497814510194574\n",
      "  time_since_restore: 8515.859395980835\n",
      "  time_this_iter_s: 29.481391191482544\n",
      "  time_total_s: 8515.859395980835\n",
      "  timers:\n",
      "    learn_throughput: 1171.279\n",
      "    learn_time_ms: 853.767\n",
      "    load_throughput: 52472.089\n",
      "    load_time_ms: 19.058\n",
      "    sample_throughput: 32.503\n",
      "    sample_time_ms: 30766.842\n",
      "    update_time_ms: 5.636\n",
      "  timestamp: 1634851742\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 310000\n",
      "  training_iteration: 310\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   310</td><td style=\"text-align: right;\">         8515.86</td><td style=\"text-align: right;\">310000</td><td style=\"text-align: right;\"> -2.9526</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -4.64</td><td style=\"text-align: right;\">            294.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 311000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-29-31\n",
      "  done: false\n",
      "  episode_len_mean: 292.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.933399999999982\n",
      "  episode_reward_min: -4.6399999999999455\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 853\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4164256082640754\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010277689355187074\n",
      "          policy_loss: 0.020749402542908985\n",
      "          total_loss: 0.01641637103425132\n",
      "          vf_explained_var: 0.7027444839477539\n",
      "          vf_loss: 0.005664060769292215\n",
      "    num_agent_steps_sampled: 311000\n",
      "    num_agent_steps_trained: 311000\n",
      "    num_steps_sampled: 311000\n",
      "    num_steps_trained: 311000\n",
      "  iterations_since_restore: 311\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.4904761904762\n",
      "    ram_util_percent: 46.738095238095234\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039794567698688665\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.716015083793508\n",
      "    mean_inference_ms: 1.7417205448711686\n",
      "    mean_raw_obs_processing_ms: 1.750569882177084\n",
      "  time_since_restore: 8545.426176548004\n",
      "  time_this_iter_s: 29.56678056716919\n",
      "  time_total_s: 8545.426176548004\n",
      "  timers:\n",
      "    learn_throughput: 1164.404\n",
      "    learn_time_ms: 858.808\n",
      "    load_throughput: 52343.611\n",
      "    load_time_ms: 19.105\n",
      "    sample_throughput: 32.416\n",
      "    sample_time_ms: 30849.341\n",
      "    update_time_ms: 4.496\n",
      "  timestamp: 1634851771\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 311000\n",
      "  training_iteration: 311\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   311</td><td style=\"text-align: right;\">         8545.43</td><td style=\"text-align: right;\">311000</td><td style=\"text-align: right;\"> -2.9334</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -4.64</td><td style=\"text-align: right;\">            292.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-30-00\n",
      "  done: false\n",
      "  episode_len_mean: 293.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.9436999999999807\n",
      "  episode_reward_min: -4.6399999999999455\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 856\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2109958781136407\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010498441068580415\n",
      "          policy_loss: 0.060502823462916744\n",
      "          total_loss: 0.05520292458434899\n",
      "          vf_explained_var: 0.8882578611373901\n",
      "          vf_loss: 0.0025533883159773217\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "  iterations_since_restore: 312\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.01904761904763\n",
      "    ram_util_percent: 46.81666666666666\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03979849121004184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.734969789965525\n",
      "    mean_inference_ms: 1.741804686636242\n",
      "    mean_raw_obs_processing_ms: 1.7514183321890762\n",
      "  time_since_restore: 8574.282767772675\n",
      "  time_this_iter_s: 28.85659122467041\n",
      "  time_total_s: 8574.282767772675\n",
      "  timers:\n",
      "    learn_throughput: 1165.342\n",
      "    learn_time_ms: 858.117\n",
      "    load_throughput: 50326.112\n",
      "    load_time_ms: 19.87\n",
      "    sample_throughput: 32.903\n",
      "    sample_time_ms: 30392.516\n",
      "    update_time_ms: 4.365\n",
      "  timestamp: 1634851800\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 312\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   312</td><td style=\"text-align: right;\">         8574.28</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\"> -2.9437</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -4.64</td><td style=\"text-align: right;\">            293.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 313000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-30-25\n",
      "  done: false\n",
      "  episode_len_mean: 295.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.960899999999981\n",
      "  episode_reward_min: -4.6399999999999455\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 858\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3819529241985744\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011733548786856634\n",
      "          policy_loss: -0.08093480169773101\n",
      "          total_loss: -0.0845103296968672\n",
      "          vf_explained_var: 0.7239720821380615\n",
      "          vf_loss: 0.005486549209389422\n",
      "    num_agent_steps_sampled: 313000\n",
      "    num_agent_steps_trained: 313000\n",
      "    num_steps_sampled: 313000\n",
      "    num_steps_trained: 313000\n",
      "  iterations_since_restore: 313\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.33142857142857\n",
      "    ram_util_percent: 46.777142857142856\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03980095007844181\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.74728122268272\n",
      "    mean_inference_ms: 1.7418582877426922\n",
      "    mean_raw_obs_processing_ms: 1.751935794469556\n",
      "  time_since_restore: 8598.911605596542\n",
      "  time_this_iter_s: 24.628837823867798\n",
      "  time_total_s: 8598.911605596542\n",
      "  timers:\n",
      "    learn_throughput: 1171.064\n",
      "    learn_time_ms: 853.924\n",
      "    load_throughput: 52228.113\n",
      "    load_time_ms: 19.147\n",
      "    sample_throughput: 33.577\n",
      "    sample_time_ms: 29782.717\n",
      "    update_time_ms: 4.463\n",
      "  timestamp: 1634851825\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 313000\n",
      "  training_iteration: 313\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   313</td><td style=\"text-align: right;\">         8598.91</td><td style=\"text-align: right;\">313000</td><td style=\"text-align: right;\"> -2.9609</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -4.64</td><td style=\"text-align: right;\">             295.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 314000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-30-46\n",
      "  done: false\n",
      "  episode_len_mean: 298.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.99589999999998\n",
      "  episode_reward_min: -5.069999999999936\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 861\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0226263086001077\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010874095570927316\n",
      "          policy_loss: 0.02948655237754186\n",
      "          total_loss: 0.02957114941544003\n",
      "          vf_explained_var: 0.5070223212242126\n",
      "          vf_loss: 0.005901878768215991\n",
      "    num_agent_steps_sampled: 314000\n",
      "    num_agent_steps_trained: 314000\n",
      "    num_steps_sampled: 314000\n",
      "    num_steps_trained: 314000\n",
      "  iterations_since_restore: 314\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.76333333333332\n",
      "    ram_util_percent: 46.69666666666667\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03980448441625804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.76470211813114\n",
      "    mean_inference_ms: 1.741936528766435\n",
      "    mean_raw_obs_processing_ms: 1.7526959581936645\n",
      "  time_since_restore: 8620.42190361023\n",
      "  time_this_iter_s: 21.510298013687134\n",
      "  time_total_s: 8620.42190361023\n",
      "  timers:\n",
      "    learn_throughput: 1172.713\n",
      "    learn_time_ms: 852.723\n",
      "    load_throughput: 53453.163\n",
      "    load_time_ms: 18.708\n",
      "    sample_throughput: 34.601\n",
      "    sample_time_ms: 28900.742\n",
      "    update_time_ms: 3.703\n",
      "  timestamp: 1634851846\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 314000\n",
      "  training_iteration: 314\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   314</td><td style=\"text-align: right;\">         8620.42</td><td style=\"text-align: right;\">314000</td><td style=\"text-align: right;\"> -2.9959</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -5.07</td><td style=\"text-align: right;\">             298.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 315000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-31-14\n",
      "  done: false\n",
      "  episode_len_mean: 300.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -3.0125999999999795\n",
      "  episode_reward_min: -5.069999999999936\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 864\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2012132710880703\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007939605981435468\n",
      "          policy_loss: 0.00013562109735276964\n",
      "          total_loss: 0.0008402135637071398\n",
      "          vf_explained_var: 0.24707698822021484\n",
      "          vf_loss: 0.009497555955830548\n",
      "    num_agent_steps_sampled: 315000\n",
      "    num_agent_steps_trained: 315000\n",
      "    num_steps_sampled: 315000\n",
      "    num_steps_trained: 315000\n",
      "  iterations_since_restore: 315\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.635\n",
      "    ram_util_percent: 46.677499999999995\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039808057995239246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.78177023662243\n",
      "    mean_inference_ms: 1.7420157292207161\n",
      "    mean_raw_obs_processing_ms: 1.7534135650842573\n",
      "  time_since_restore: 8647.980060338974\n",
      "  time_this_iter_s: 27.558156728744507\n",
      "  time_total_s: 8647.980060338974\n",
      "  timers:\n",
      "    learn_throughput: 1173.644\n",
      "    learn_time_ms: 852.047\n",
      "    load_throughput: 56197.397\n",
      "    load_time_ms: 17.794\n",
      "    sample_throughput: 34.604\n",
      "    sample_time_ms: 28898.697\n",
      "    update_time_ms: 3.131\n",
      "  timestamp: 1634851874\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 315000\n",
      "  training_iteration: 315\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   315</td><td style=\"text-align: right;\">         8647.98</td><td style=\"text-align: right;\">315000</td><td style=\"text-align: right;\"> -3.0126</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -5.07</td><td style=\"text-align: right;\">            300.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 316000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-31-40\n",
      "  done: false\n",
      "  episode_len_mean: 302.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -3.031199999999979\n",
      "  episode_reward_min: -5.069999999999936\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 866\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3057360728581746\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008013010160482163\n",
      "          policy_loss: -0.1228052039941152\n",
      "          total_loss: -0.12313801878028446\n",
      "          vf_explained_var: 0.30643606185913086\n",
      "          vf_loss: 0.009475615921999431\n",
      "    num_agent_steps_sampled: 316000\n",
      "    num_agent_steps_trained: 316000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "  iterations_since_restore: 316\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.6842105263158\n",
      "    ram_util_percent: 46.60263157894736\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039810388570870554\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.792372481941065\n",
      "    mean_inference_ms: 1.7420672415341065\n",
      "    mean_raw_obs_processing_ms: 1.7538951438180985\n",
      "  time_since_restore: 8674.541693210602\n",
      "  time_this_iter_s: 26.561632871627808\n",
      "  time_total_s: 8674.541693210602\n",
      "  timers:\n",
      "    learn_throughput: 1176.041\n",
      "    learn_time_ms: 850.31\n",
      "    load_throughput: 56122.578\n",
      "    load_time_ms: 17.818\n",
      "    sample_throughput: 35.181\n",
      "    sample_time_ms: 28424.134\n",
      "    update_time_ms: 3.54\n",
      "  timestamp: 1634851900\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 316\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   316</td><td style=\"text-align: right;\">         8674.54</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\"> -3.0312</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -5.07</td><td style=\"text-align: right;\">            302.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 317000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-32-06\n",
      "  done: false\n",
      "  episode_len_mean: 304.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -3.058799999999979\n",
      "  episode_reward_min: -5.069999999999936\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 869\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2229444622993468\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011451663469699665\n",
      "          policy_loss: -0.07591082296437687\n",
      "          total_loss: -0.07412432341112031\n",
      "          vf_explained_var: 0.6017182469367981\n",
      "          vf_loss: 0.0093727853278526\n",
      "    num_agent_steps_sampled: 317000\n",
      "    num_agent_steps_trained: 317000\n",
      "    num_steps_sampled: 317000\n",
      "    num_steps_trained: 317000\n",
      "  iterations_since_restore: 317\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.64722222222221\n",
      "    ram_util_percent: 46.65833333333333\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03981389656559252\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.807821299191854\n",
      "    mean_inference_ms: 1.7421444918017022\n",
      "    mean_raw_obs_processing_ms: 1.7545265709129247\n",
      "  time_since_restore: 8699.729551315308\n",
      "  time_this_iter_s: 25.18785810470581\n",
      "  time_total_s: 8699.729551315308\n",
      "  timers:\n",
      "    learn_throughput: 1175.477\n",
      "    learn_time_ms: 850.719\n",
      "    load_throughput: 57762.837\n",
      "    load_time_ms: 17.312\n",
      "    sample_throughput: 37.877\n",
      "    sample_time_ms: 26401.492\n",
      "    update_time_ms: 3.479\n",
      "  timestamp: 1634851926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 317000\n",
      "  training_iteration: 317\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   317</td><td style=\"text-align: right;\">         8699.73</td><td style=\"text-align: right;\">317000</td><td style=\"text-align: right;\"> -3.0588</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -5.07</td><td style=\"text-align: right;\">            304.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 318000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-32-50\n",
      "  done: false\n",
      "  episode_len_mean: 308.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -3.104699999999978\n",
      "  episode_reward_min: -5.269999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 872\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2920142610867817\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0187914397284458\n",
      "          policy_loss: 0.02753941458132532\n",
      "          total_loss: 0.03388769432074494\n",
      "          vf_explained_var: 0.5501846671104431\n",
      "          vf_loss: 0.011649296295622157\n",
      "    num_agent_steps_sampled: 318000\n",
      "    num_agent_steps_trained: 318000\n",
      "    num_steps_sampled: 318000\n",
      "    num_steps_trained: 318000\n",
      "  iterations_since_restore: 318\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.7203125\n",
      "    ram_util_percent: 46.740625\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03981755190266261\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.822523088975778\n",
      "    mean_inference_ms: 1.7422289301758702\n",
      "    mean_raw_obs_processing_ms: 1.7567394103959548\n",
      "  time_since_restore: 8744.550989627838\n",
      "  time_this_iter_s: 44.82143831253052\n",
      "  time_total_s: 8744.550989627838\n",
      "  timers:\n",
      "    learn_throughput: 1175.323\n",
      "    learn_time_ms: 850.83\n",
      "    load_throughput: 56942.025\n",
      "    load_time_ms: 17.562\n",
      "    sample_throughput: 35.721\n",
      "    sample_time_ms: 27994.573\n",
      "    update_time_ms: 3.498\n",
      "  timestamp: 1634851970\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 318000\n",
      "  training_iteration: 318\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   318</td><td style=\"text-align: right;\">         8744.55</td><td style=\"text-align: right;\">318000</td><td style=\"text-align: right;\"> -3.1047</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -5.27</td><td style=\"text-align: right;\">            308.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 319000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-33-16\n",
      "  done: false\n",
      "  episode_len_mean: 310.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -3.1264999999999774\n",
      "  episode_reward_min: -5.269999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 874\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.409136970837911\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014998990265704344\n",
      "          policy_loss: -0.034872738189167446\n",
      "          total_loss: -0.03637973103258345\n",
      "          vf_explained_var: 0.32101887464523315\n",
      "          vf_loss: 0.006502917970323728\n",
      "    num_agent_steps_sampled: 319000\n",
      "    num_agent_steps_trained: 319000\n",
      "    num_steps_sampled: 319000\n",
      "    num_steps_trained: 319000\n",
      "  iterations_since_restore: 319\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.08333333333331\n",
      "    ram_util_percent: 46.66111111111111\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0398200608368279\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.83203719810047\n",
      "    mean_inference_ms: 1.7422876732162749\n",
      "    mean_raw_obs_processing_ms: 1.7581827745217509\n",
      "  time_since_restore: 8769.82441997528\n",
      "  time_this_iter_s: 25.273430347442627\n",
      "  time_total_s: 8769.82441997528\n",
      "  timers:\n",
      "    learn_throughput: 1175.054\n",
      "    learn_time_ms: 851.025\n",
      "    load_throughput: 58354.884\n",
      "    load_time_ms: 17.137\n",
      "    sample_throughput: 36.407\n",
      "    sample_time_ms: 27467.259\n",
      "    update_time_ms: 3.913\n",
      "  timestamp: 1634851996\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 319000\n",
      "  training_iteration: 319\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   319</td><td style=\"text-align: right;\">         8769.82</td><td style=\"text-align: right;\">319000</td><td style=\"text-align: right;\"> -3.1265</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -5.27</td><td style=\"text-align: right;\">            310.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-33-42\n",
      "  done: false\n",
      "  episode_len_mean: 314.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -3.1619999999999764\n",
      "  episode_reward_min: -5.269999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 877\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3642896321084765\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009900075483426068\n",
      "          policy_loss: -0.03150347053176827\n",
      "          total_loss: -0.032538952057560286\n",
      "          vf_explained_var: 0.6264523267745972\n",
      "          vf_loss: 0.008593357336293492\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "  iterations_since_restore: 320\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.35945945945946\n",
      "    ram_util_percent: 46.678378378378376\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03982381491350405\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.84559077343252\n",
      "    mean_inference_ms: 1.7423762507455225\n",
      "    mean_raw_obs_processing_ms: 1.7603022778909274\n",
      "  time_since_restore: 8796.088540792465\n",
      "  time_this_iter_s: 26.26412081718445\n",
      "  time_total_s: 8796.088540792465\n",
      "  timers:\n",
      "    learn_throughput: 1178.049\n",
      "    learn_time_ms: 848.861\n",
      "    load_throughput: 57350.161\n",
      "    load_time_ms: 17.437\n",
      "    sample_throughput: 36.836\n",
      "    sample_time_ms: 27147.368\n",
      "    update_time_ms: 4.016\n",
      "  timestamp: 1634852022\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 320\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 21.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   320</td><td style=\"text-align: right;\">         8796.09</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\">  -3.162</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -5.27</td><td style=\"text-align: right;\">            314.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 321000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-34-10\n",
      "  done: false\n",
      "  episode_len_mean: 317.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.189699999999976\n",
      "  episode_reward_min: -5.269999999999944\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 880\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0946609093083275\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009543697316785248\n",
      "          policy_loss: -0.00766311999824312\n",
      "          total_loss: -0.009105650087197622\n",
      "          vf_explained_var: 0.48395320773124695\n",
      "          vf_loss: 0.005634521156187273\n",
      "    num_agent_steps_sampled: 321000\n",
      "    num_agent_steps_trained: 321000\n",
      "    num_steps_sampled: 321000\n",
      "    num_steps_trained: 321000\n",
      "  iterations_since_restore: 321\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.2125\n",
      "    ram_util_percent: 46.71\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03982760538208709\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.85856850445886\n",
      "    mean_inference_ms: 1.7424640875502018\n",
      "    mean_raw_obs_processing_ms: 1.761768911260941\n",
      "  time_since_restore: 8823.856803417206\n",
      "  time_this_iter_s: 27.7682626247406\n",
      "  time_total_s: 8823.856803417206\n",
      "  timers:\n",
      "    learn_throughput: 1184.365\n",
      "    learn_time_ms: 844.334\n",
      "    load_throughput: 57539.612\n",
      "    load_time_ms: 17.379\n",
      "    sample_throughput: 37.074\n",
      "    sample_time_ms: 26972.79\n",
      "    update_time_ms: 3.367\n",
      "  timestamp: 1634852050\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 321000\n",
      "  training_iteration: 321\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   321</td><td style=\"text-align: right;\">         8823.86</td><td style=\"text-align: right;\">321000</td><td style=\"text-align: right;\"> -3.1897</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">               -5.27</td><td style=\"text-align: right;\">            317.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 322000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-34-33\n",
      "  done: false\n",
      "  episode_len_mean: 319.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.2165999999999757\n",
      "  episode_reward_min: -5.269999999999944\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 882\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4330005367596945\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017210444181611547\n",
      "          policy_loss: -0.11327589452266693\n",
      "          total_loss: -0.11398755841785008\n",
      "          vf_explained_var: 0.696868360042572\n",
      "          vf_loss: 0.006640241260174662\n",
      "    num_agent_steps_sampled: 322000\n",
      "    num_agent_steps_trained: 322000\n",
      "    num_steps_sampled: 322000\n",
      "    num_steps_trained: 322000\n",
      "  iterations_since_restore: 322\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.20625\n",
      "    ram_util_percent: 46.728125\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03983011801014331\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.866534150724636\n",
      "    mean_inference_ms: 1.7425228335025498\n",
      "    mean_raw_obs_processing_ms: 1.7619044990952046\n",
      "  time_since_restore: 8846.669024944305\n",
      "  time_this_iter_s: 22.81222152709961\n",
      "  time_total_s: 8846.669024944305\n",
      "  timers:\n",
      "    learn_throughput: 1186.488\n",
      "    learn_time_ms: 842.824\n",
      "    load_throughput: 60736.929\n",
      "    load_time_ms: 16.464\n",
      "    sample_throughput: 37.921\n",
      "    sample_time_ms: 26370.787\n",
      "    update_time_ms: 3.356\n",
      "  timestamp: 1634852073\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 322000\n",
      "  training_iteration: 322\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   322</td><td style=\"text-align: right;\">         8846.67</td><td style=\"text-align: right;\">322000</td><td style=\"text-align: right;\"> -3.2166</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">               -5.27</td><td style=\"text-align: right;\">             319.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 323000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-34-52\n",
      "  done: false\n",
      "  episode_len_mean: 325.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.2755999999999745\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 885\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4695157514678108\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013375091775530161\n",
      "          policy_loss: -0.018913811859157352\n",
      "          total_loss: -0.021783715652094946\n",
      "          vf_explained_var: 0.734582245349884\n",
      "          vf_loss: 0.00640222601052503\n",
      "    num_agent_steps_sampled: 323000\n",
      "    num_agent_steps_trained: 323000\n",
      "    num_steps_sampled: 323000\n",
      "    num_steps_trained: 323000\n",
      "  iterations_since_restore: 323\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.60357142857143\n",
      "    ram_util_percent: 46.782142857142865\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03983374185910537\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.87717041098193\n",
      "    mean_inference_ms: 1.7426063343374125\n",
      "    mean_raw_obs_processing_ms: 1.7620682979835853\n",
      "  time_since_restore: 8865.77483868599\n",
      "  time_this_iter_s: 19.10581374168396\n",
      "  time_total_s: 8865.77483868599\n",
      "  timers:\n",
      "    learn_throughput: 1183.155\n",
      "    learn_time_ms: 845.198\n",
      "    load_throughput: 57738.505\n",
      "    load_time_ms: 17.319\n",
      "    sample_throughput: 38.736\n",
      "    sample_time_ms: 25815.607\n",
      "    update_time_ms: 3.241\n",
      "  timestamp: 1634852092\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 323000\n",
      "  training_iteration: 323\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   323</td><td style=\"text-align: right;\">         8865.77</td><td style=\"text-align: right;\">323000</td><td style=\"text-align: right;\"> -3.2756</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">             325.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 324000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-35-15\n",
      "  done: false\n",
      "  episode_len_mean: 327.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.2948999999999744\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 887\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5257480727301704\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011539214842330224\n",
      "          policy_loss: -0.09241306318177117\n",
      "          total_loss: -0.09356405122412575\n",
      "          vf_explained_var: 0.6208950877189636\n",
      "          vf_loss: 0.00942783248093393\n",
      "    num_agent_steps_sampled: 324000\n",
      "    num_agent_steps_trained: 324000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "  iterations_since_restore: 324\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.88181818181818\n",
      "    ram_util_percent: 46.75757575757576\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03983615469567299\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.883883807773763\n",
      "    mean_inference_ms: 1.7426628342679504\n",
      "    mean_raw_obs_processing_ms: 1.7621173167628212\n",
      "  time_since_restore: 8889.269488811493\n",
      "  time_this_iter_s: 23.49465012550354\n",
      "  time_total_s: 8889.269488811493\n",
      "  timers:\n",
      "    learn_throughput: 1181.993\n",
      "    learn_time_ms: 846.029\n",
      "    load_throughput: 57723.566\n",
      "    load_time_ms: 17.324\n",
      "    sample_throughput: 38.442\n",
      "    sample_time_ms: 26013.344\n",
      "    update_time_ms: 3.207\n",
      "  timestamp: 1634852115\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 324\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   324</td><td style=\"text-align: right;\">         8889.27</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\"> -3.2949</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            327.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 325000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-35-41\n",
      "  done: false\n",
      "  episode_len_mean: 330.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.328099999999974\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 890\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.436710602707333\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008708640516126357\n",
      "          policy_loss: 0.02125318327711688\n",
      "          total_loss: 0.019312538703282676\n",
      "          vf_explained_var: 0.5520676374435425\n",
      "          vf_loss: 0.008895477649962737\n",
      "    num_agent_steps_sampled: 325000\n",
      "    num_agent_steps_trained: 325000\n",
      "    num_steps_sampled: 325000\n",
      "    num_steps_trained: 325000\n",
      "  iterations_since_restore: 325\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.82162162162163\n",
      "    ram_util_percent: 46.78108108108109\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03983972684999944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.893033394276404\n",
      "    mean_inference_ms: 1.7427466894228598\n",
      "    mean_raw_obs_processing_ms: 1.762204315446357\n",
      "  time_since_restore: 8914.998876810074\n",
      "  time_this_iter_s: 25.729387998580933\n",
      "  time_total_s: 8914.998876810074\n",
      "  timers:\n",
      "    learn_throughput: 1186.303\n",
      "    learn_time_ms: 842.955\n",
      "    load_throughput: 57843.374\n",
      "    load_time_ms: 17.288\n",
      "    sample_throughput: 38.71\n",
      "    sample_time_ms: 25833.12\n",
      "    update_time_ms: 3.574\n",
      "  timestamp: 1634852141\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 325000\n",
      "  training_iteration: 325\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   325</td><td style=\"text-align: right;\">            8915</td><td style=\"text-align: right;\">325000</td><td style=\"text-align: right;\"> -3.3281</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            330.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 326000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-36-08\n",
      "  done: false\n",
      "  episode_len_mean: 332.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.3546999999999736\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 892\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.429672156439887\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019412547492332895\n",
      "          policy_loss: -0.09342139926221636\n",
      "          total_loss: -0.09149770306216345\n",
      "          vf_explained_var: 0.7837722897529602\n",
      "          vf_loss: 0.008349451060510344\n",
      "    num_agent_steps_sampled: 326000\n",
      "    num_agent_steps_trained: 326000\n",
      "    num_steps_sampled: 326000\n",
      "    num_steps_trained: 326000\n",
      "  iterations_since_restore: 326\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.95384615384616\n",
      "    ram_util_percent: 46.78717948717948\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03984201942220636\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.898604167854003\n",
      "    mean_inference_ms: 1.7428019788696423\n",
      "    mean_raw_obs_processing_ms: 1.7622673957979145\n",
      "  time_since_restore: 8941.974351406097\n",
      "  time_this_iter_s: 26.97547459602356\n",
      "  time_total_s: 8941.974351406097\n",
      "  timers:\n",
      "    learn_throughput: 1176.975\n",
      "    learn_time_ms: 849.636\n",
      "    load_throughput: 54126.826\n",
      "    load_time_ms: 18.475\n",
      "    sample_throughput: 38.66\n",
      "    sample_time_ms: 25866.238\n",
      "    update_time_ms: 4.006\n",
      "  timestamp: 1634852168\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 326000\n",
      "  training_iteration: 326\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   326</td><td style=\"text-align: right;\">         8941.97</td><td style=\"text-align: right;\">326000</td><td style=\"text-align: right;\"> -3.3547</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            332.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 327000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-36-38\n",
      "  done: false\n",
      "  episode_len_mean: 335.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.387299999999972\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 896\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2342731608284845\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010284074044148649\n",
      "          policy_loss: -0.04817491604222192\n",
      "          total_loss: -0.04569757166835997\n",
      "          vf_explained_var: 0.5050758719444275\n",
      "          vf_loss: 0.01065032106772479\n",
      "    num_agent_steps_sampled: 327000\n",
      "    num_agent_steps_trained: 327000\n",
      "    num_steps_sampled: 327000\n",
      "    num_steps_trained: 327000\n",
      "  iterations_since_restore: 327\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.91428571428571\n",
      "    ram_util_percent: 46.80238095238095\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039846624202724154\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.909454071036503\n",
      "    mean_inference_ms: 1.7429147838782328\n",
      "    mean_raw_obs_processing_ms: 1.7623193627366882\n",
      "  time_since_restore: 8971.777106523514\n",
      "  time_this_iter_s: 29.802755117416382\n",
      "  time_total_s: 8971.777106523514\n",
      "  timers:\n",
      "    learn_throughput: 1179.689\n",
      "    learn_time_ms: 847.681\n",
      "    load_throughput: 53616.883\n",
      "    load_time_ms: 18.651\n",
      "    sample_throughput: 37.981\n",
      "    sample_time_ms: 26329.15\n",
      "    update_time_ms: 4.391\n",
      "  timestamp: 1634852198\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 327000\n",
      "  training_iteration: 327\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   327</td><td style=\"text-align: right;\">         8971.78</td><td style=\"text-align: right;\">327000</td><td style=\"text-align: right;\"> -3.3873</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            335.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-37-07\n",
      "  done: false\n",
      "  episode_len_mean: 337.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.4026999999999714\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 899\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2402507325013479\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008875823534445972\n",
      "          policy_loss: 0.015279563764731089\n",
      "          total_loss: 0.015336665014425913\n",
      "          vf_explained_var: 0.5774902105331421\n",
      "          vf_loss: 0.008860842359717935\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "  iterations_since_restore: 328\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.90714285714286\n",
      "    ram_util_percent: 46.86904761904762\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03985019959384359\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.917208742866052\n",
      "    mean_inference_ms: 1.7430007603637188\n",
      "    mean_raw_obs_processing_ms: 1.7623468209447546\n",
      "  time_since_restore: 9001.173038721085\n",
      "  time_this_iter_s: 29.3959321975708\n",
      "  time_total_s: 9001.173038721085\n",
      "  timers:\n",
      "    learn_throughput: 1178.451\n",
      "    learn_time_ms: 848.572\n",
      "    load_throughput: 53597.219\n",
      "    load_time_ms: 18.658\n",
      "    sample_throughput: 40.346\n",
      "    sample_time_ms: 24785.83\n",
      "    update_time_ms: 4.438\n",
      "  timestamp: 1634852227\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 328\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   328</td><td style=\"text-align: right;\">         9001.17</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\"> -3.4027</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            337.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 329000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-37-55\n",
      "  done: false\n",
      "  episode_len_mean: 337.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.4017999999999717\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 902\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5080928405125935\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010861433720001814\n",
      "          policy_loss: 0.05220806681447559\n",
      "          total_loss: 0.046377605862087674\n",
      "          vf_explained_var: 0.6988080143928528\n",
      "          vf_loss: 0.004846617849802392\n",
      "    num_agent_steps_sampled: 329000\n",
      "    num_agent_steps_trained: 329000\n",
      "    num_steps_sampled: 329000\n",
      "    num_steps_trained: 329000\n",
      "  iterations_since_restore: 329\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.52352941176471\n",
      "    ram_util_percent: 46.81617647058823\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03985379296353456\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.924883695502235\n",
      "    mean_inference_ms: 1.7430894210654764\n",
      "    mean_raw_obs_processing_ms: 1.7639447660930887\n",
      "  time_since_restore: 9048.646478652954\n",
      "  time_this_iter_s: 47.47343993186951\n",
      "  time_total_s: 9048.646478652954\n",
      "  timers:\n",
      "    learn_throughput: 1177.816\n",
      "    learn_time_ms: 849.029\n",
      "    load_throughput: 53802.927\n",
      "    load_time_ms: 18.586\n",
      "    sample_throughput: 37.029\n",
      "    sample_time_ms: 27006.099\n",
      "    update_time_ms: 4.168\n",
      "  timestamp: 1634852275\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 329000\n",
      "  training_iteration: 329\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   329</td><td style=\"text-align: right;\">         9048.65</td><td style=\"text-align: right;\">329000</td><td style=\"text-align: right;\"> -3.4018</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            337.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 330000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-38-22\n",
      "  done: false\n",
      "  episode_len_mean: 339.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.423099999999971\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 904\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.798514511850145\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013334595929105022\n",
      "          policy_loss: -0.11157961885134379\n",
      "          total_loss: -0.11340343985292646\n",
      "          vf_explained_var: -0.05144280940294266\n",
      "          vf_loss: 0.01075471485964954\n",
      "    num_agent_steps_sampled: 330000\n",
      "    num_agent_steps_trained: 330000\n",
      "    num_steps_sampled: 330000\n",
      "    num_steps_trained: 330000\n",
      "  iterations_since_restore: 330\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.3763157894737\n",
      "    ram_util_percent: 46.73157894736841\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03985617229062564\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.929749075748592\n",
      "    mean_inference_ms: 1.7431497389484047\n",
      "    mean_raw_obs_processing_ms: 1.7649812348856964\n",
      "  time_since_restore: 9075.417277812958\n",
      "  time_this_iter_s: 26.770799160003662\n",
      "  time_total_s: 9075.417277812958\n",
      "  timers:\n",
      "    learn_throughput: 1172.845\n",
      "    learn_time_ms: 852.627\n",
      "    load_throughput: 54049.822\n",
      "    load_time_ms: 18.501\n",
      "    sample_throughput: 36.964\n",
      "    sample_time_ms: 27053.327\n",
      "    update_time_ms: 3.986\n",
      "  timestamp: 1634852302\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 330000\n",
      "  training_iteration: 330\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   330</td><td style=\"text-align: right;\">         9075.42</td><td style=\"text-align: right;\">330000</td><td style=\"text-align: right;\"> -3.4231</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            339.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 331000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-38-45\n",
      "  done: false\n",
      "  episode_len_mean: 341.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.445199999999971\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 907\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.983708205487993\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014230805910726039\n",
      "          policy_loss: -0.028458768874406813\n",
      "          total_loss: -0.03513456703060203\n",
      "          vf_explained_var: 0.4812021851539612\n",
      "          vf_loss: 0.007391295544544442\n",
      "    num_agent_steps_sampled: 331000\n",
      "    num_agent_steps_trained: 331000\n",
      "    num_steps_sampled: 331000\n",
      "    num_steps_trained: 331000\n",
      "  iterations_since_restore: 331\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.41176470588235\n",
      "    ram_util_percent: 46.71470588235294\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03985960338221334\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.936186866298012\n",
      "    mean_inference_ms: 1.7432403308246986\n",
      "    mean_raw_obs_processing_ms: 1.7665434590733509\n",
      "  time_since_restore: 9098.963078975677\n",
      "  time_this_iter_s: 23.545801162719727\n",
      "  time_total_s: 9098.963078975677\n",
      "  timers:\n",
      "    learn_throughput: 1173.611\n",
      "    learn_time_ms: 852.071\n",
      "    load_throughput: 53450.166\n",
      "    load_time_ms: 18.709\n",
      "    sample_throughput: 37.55\n",
      "    sample_time_ms: 26631.488\n",
      "    update_time_ms: 4.058\n",
      "  timestamp: 1634852325\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 331000\n",
      "  training_iteration: 331\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   331</td><td style=\"text-align: right;\">         9098.96</td><td style=\"text-align: right;\">331000</td><td style=\"text-align: right;\"> -3.4452</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            341.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 332000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-39-14\n",
      "  done: false\n",
      "  episode_len_mean: 342.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.4573999999999705\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 910\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3914832578765022\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014686410065614395\n",
      "          policy_loss: 0.014687371088398828\n",
      "          total_loss: 0.018723080886734855\n",
      "          vf_explained_var: 0.524298369884491\n",
      "          vf_loss: 0.011995828570798039\n",
      "    num_agent_steps_sampled: 332000\n",
      "    num_agent_steps_trained: 332000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "  iterations_since_restore: 332\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.84\n",
      "    ram_util_percent: 46.7025\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03986296303247983\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.942263710812245\n",
      "    mean_inference_ms: 1.7433285015866422\n",
      "    mean_raw_obs_processing_ms: 1.7669244165147098\n",
      "  time_since_restore: 9127.362809181213\n",
      "  time_this_iter_s: 28.39973020553589\n",
      "  time_total_s: 9127.362809181213\n",
      "  timers:\n",
      "    learn_throughput: 1168.625\n",
      "    learn_time_ms: 855.707\n",
      "    load_throughput: 50882.296\n",
      "    load_time_ms: 19.653\n",
      "    sample_throughput: 36.784\n",
      "    sample_time_ms: 27185.636\n",
      "    update_time_ms: 4.055\n",
      "  timestamp: 1634852354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 332\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   332</td><td style=\"text-align: right;\">         9127.36</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\"> -3.4574</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            342.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 333000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-39-44\n",
      "  done: false\n",
      "  episode_len_mean: 343.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.4631999999999703\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 913\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0088512268331316\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009193398411223146\n",
      "          policy_loss: -0.12648266876737277\n",
      "          total_loss: -0.12469499599602488\n",
      "          vf_explained_var: 0.7472435235977173\n",
      "          vf_loss: 0.0081486546408592\n",
      "    num_agent_steps_sampled: 333000\n",
      "    num_agent_steps_trained: 333000\n",
      "    num_steps_sampled: 333000\n",
      "    num_steps_trained: 333000\n",
      "  iterations_since_restore: 333\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.34772727272728\n",
      "    ram_util_percent: 46.75681818181818\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039866479737431126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.94801038273368\n",
      "    mean_inference_ms: 1.7434151633453845\n",
      "    mean_raw_obs_processing_ms: 1.7667455936860768\n",
      "  time_since_restore: 9157.701083421707\n",
      "  time_this_iter_s: 30.338274240493774\n",
      "  time_total_s: 9157.701083421707\n",
      "  timers:\n",
      "    learn_throughput: 1169.906\n",
      "    learn_time_ms: 854.77\n",
      "    load_throughput: 53855.707\n",
      "    load_time_ms: 18.568\n",
      "    sample_throughput: 35.322\n",
      "    sample_time_ms: 28310.858\n",
      "    update_time_ms: 4.133\n",
      "  timestamp: 1634852384\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 333000\n",
      "  training_iteration: 333\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   333</td><td style=\"text-align: right;\">          9157.7</td><td style=\"text-align: right;\">333000</td><td style=\"text-align: right;\"> -3.4632</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            343.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 334000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-40-18\n",
      "  done: false\n",
      "  episode_len_mean: 343.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.4595999999999707\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 917\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0525923483901554\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01599063486273514\n",
      "          policy_loss: -0.05769971116549439\n",
      "          total_loss: -0.05444010976288054\n",
      "          vf_explained_var: 0.4986712634563446\n",
      "          vf_loss: 0.007302005534681181\n",
      "    num_agent_steps_sampled: 334000\n",
      "    num_agent_steps_trained: 334000\n",
      "    num_steps_sampled: 334000\n",
      "    num_steps_trained: 334000\n",
      "  iterations_since_restore: 334\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.46666666666665\n",
      "    ram_util_percent: 46.76666666666667\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039870870702923895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.955986356068514\n",
      "    mean_inference_ms: 1.7435307208142694\n",
      "    mean_raw_obs_processing_ms: 1.7664984493940437\n",
      "  time_since_restore: 9191.933517217636\n",
      "  time_this_iter_s: 34.232433795928955\n",
      "  time_total_s: 9191.933517217636\n",
      "  timers:\n",
      "    learn_throughput: 1169.555\n",
      "    learn_time_ms: 855.026\n",
      "    load_throughput: 53886.082\n",
      "    load_time_ms: 18.558\n",
      "    sample_throughput: 34.032\n",
      "    sample_time_ms: 29384.432\n",
      "    update_time_ms: 4.08\n",
      "  timestamp: 1634852418\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 334000\n",
      "  training_iteration: 334\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   334</td><td style=\"text-align: right;\">         9191.93</td><td style=\"text-align: right;\">334000</td><td style=\"text-align: right;\"> -3.4596</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            343.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 335000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-40-52\n",
      "  done: false\n",
      "  episode_len_mean: 340.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.4375999999999713\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 921\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1293354372183482\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010361592473793103\n",
      "          policy_loss: -0.018448143038484785\n",
      "          total_loss: -0.014210426145129733\n",
      "          vf_explained_var: 0.2097989022731781\n",
      "          vf_loss: 0.011329886249990926\n",
      "    num_agent_steps_sampled: 335000\n",
      "    num_agent_steps_trained: 335000\n",
      "    num_steps_sampled: 335000\n",
      "    num_steps_trained: 335000\n",
      "  iterations_since_restore: 335\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.51666666666665\n",
      "    ram_util_percent: 46.72708333333333\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0398749796353383\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.96417345088245\n",
      "    mean_inference_ms: 1.7436445630481976\n",
      "    mean_raw_obs_processing_ms: 1.7663260069779767\n",
      "  time_since_restore: 9225.247046232224\n",
      "  time_this_iter_s: 33.3135290145874\n",
      "  time_total_s: 9225.247046232224\n",
      "  timers:\n",
      "    learn_throughput: 1168.698\n",
      "    learn_time_ms: 855.653\n",
      "    load_throughput: 53648.499\n",
      "    load_time_ms: 18.64\n",
      "    sample_throughput: 33.176\n",
      "    sample_time_ms: 30141.988\n",
      "    update_time_ms: 4.076\n",
      "  timestamp: 1634852452\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 335000\n",
      "  training_iteration: 335\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   335</td><td style=\"text-align: right;\">         9225.25</td><td style=\"text-align: right;\">335000</td><td style=\"text-align: right;\"> -3.4376</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            340.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-41-23\n",
      "  done: false\n",
      "  episode_len_mean: 341.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.440399999999971\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 924\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.194424174229304\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.027247279554757142\n",
      "          policy_loss: 0.03731817644503382\n",
      "          total_loss: 0.043570098363690905\n",
      "          vf_explained_var: 0.6174387335777283\n",
      "          vf_loss: 0.007148554544740667\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_agent_steps_trained: 336000\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "  iterations_since_restore: 336\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.4521739130435\n",
      "    ram_util_percent: 46.800000000000004\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03987805679747562\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.970008551682277\n",
      "    mean_inference_ms: 1.7437322308846195\n",
      "    mean_raw_obs_processing_ms: 1.766249995818861\n",
      "  time_since_restore: 9257.163452386856\n",
      "  time_this_iter_s: 31.91640615463257\n",
      "  time_total_s: 9257.163452386856\n",
      "  timers:\n",
      "    learn_throughput: 1174.794\n",
      "    learn_time_ms: 851.213\n",
      "    load_throughput: 57042.854\n",
      "    load_time_ms: 17.531\n",
      "    sample_throughput: 32.636\n",
      "    sample_time_ms: 30641.414\n",
      "    update_time_ms: 3.692\n",
      "  timestamp: 1634852483\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 336\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   336</td><td style=\"text-align: right;\">         9257.16</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\"> -3.4404</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            341.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 337000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-41-56\n",
      "  done: false\n",
      "  episode_len_mean: 340.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.438199999999971\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 927\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1649912814299266\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006568423031185135\n",
      "          policy_loss: -0.09400231788555781\n",
      "          total_loss: -0.09186833682987425\n",
      "          vf_explained_var: 0.3990946412086487\n",
      "          vf_loss: 0.009789067279133533\n",
      "    num_agent_steps_sampled: 337000\n",
      "    num_agent_steps_trained: 337000\n",
      "    num_steps_sampled: 337000\n",
      "    num_steps_trained: 337000\n",
      "  iterations_since_restore: 337\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.27777777777777\n",
      "    ram_util_percent: 46.81333333333333\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039881144354487794\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.975898381232522\n",
      "    mean_inference_ms: 1.7438181109501287\n",
      "    mean_raw_obs_processing_ms: 1.7661850626460656\n",
      "  time_since_restore: 9289.230194330215\n",
      "  time_this_iter_s: 32.066741943359375\n",
      "  time_total_s: 9289.230194330215\n",
      "  timers:\n",
      "    learn_throughput: 1169.268\n",
      "    learn_time_ms: 855.236\n",
      "    load_throughput: 54953.645\n",
      "    load_time_ms: 18.197\n",
      "    sample_throughput: 32.401\n",
      "    sample_time_ms: 30863.571\n",
      "    update_time_ms: 3.312\n",
      "  timestamp: 1634852516\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 337000\n",
      "  training_iteration: 337\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   337</td><td style=\"text-align: right;\">         9289.23</td><td style=\"text-align: right;\">337000</td><td style=\"text-align: right;\"> -3.4382</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            340.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 338000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-42-43\n",
      "  done: false\n",
      "  episode_len_mean: 341.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.44269999999997\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 931\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0984430611133575\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01030487636126046\n",
      "          policy_loss: -0.07684328166974916\n",
      "          total_loss: -0.0721837941557169\n",
      "          vf_explained_var: 0.583359956741333\n",
      "          vf_loss: 0.009376640007313754\n",
      "    num_agent_steps_sampled: 338000\n",
      "    num_agent_steps_trained: 338000\n",
      "    num_steps_sampled: 338000\n",
      "    num_steps_trained: 338000\n",
      "  iterations_since_restore: 338\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.78382352941173\n",
      "    ram_util_percent: 46.88382352941177\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03988536619413873\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.983778954341087\n",
      "    mean_inference_ms: 1.74393762454166\n",
      "    mean_raw_obs_processing_ms: 1.7681196581745546\n",
      "  time_since_restore: 9336.616415262222\n",
      "  time_this_iter_s: 47.386220932006836\n",
      "  time_total_s: 9336.616415262222\n",
      "  timers:\n",
      "    learn_throughput: 1168.847\n",
      "    learn_time_ms: 855.544\n",
      "    load_throughput: 55017.728\n",
      "    load_time_ms: 18.176\n",
      "    sample_throughput: 30.616\n",
      "    sample_time_ms: 32662.441\n",
      "    update_time_ms: 3.301\n",
      "  timestamp: 1634852563\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 338000\n",
      "  training_iteration: 338\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   338</td><td style=\"text-align: right;\">         9336.62</td><td style=\"text-align: right;\">338000</td><td style=\"text-align: right;\"> -3.4427</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            341.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 339000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-43-17\n",
      "  done: false\n",
      "  episode_len_mean: 338.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.4169999999999714\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 934\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8610538250870174\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0046991680183574485\n",
      "          policy_loss: -0.10261861152119106\n",
      "          total_loss: -0.09744293085402912\n",
      "          vf_explained_var: 0.34421423077583313\n",
      "          vf_loss: 0.010928252913678686\n",
      "    num_agent_steps_sampled: 339000\n",
      "    num_agent_steps_trained: 339000\n",
      "    num_steps_sampled: 339000\n",
      "    num_steps_trained: 339000\n",
      "  iterations_since_restore: 339\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.55102040816327\n",
      "    ram_util_percent: 46.89795918367346\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03988801545151519\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.990135783262104\n",
      "    mean_inference_ms: 1.7440268517876176\n",
      "    mean_raw_obs_processing_ms: 1.7696182948450643\n",
      "  time_since_restore: 9370.855227708817\n",
      "  time_this_iter_s: 34.23881244659424\n",
      "  time_total_s: 9370.855227708817\n",
      "  timers:\n",
      "    learn_throughput: 1166.816\n",
      "    learn_time_ms: 857.033\n",
      "    load_throughput: 55044.371\n",
      "    load_time_ms: 18.167\n",
      "    sample_throughput: 31.911\n",
      "    sample_time_ms: 31337.585\n",
      "    update_time_ms: 3.156\n",
      "  timestamp: 1634852597\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 339000\n",
      "  training_iteration: 339\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   339</td><td style=\"text-align: right;\">         9370.86</td><td style=\"text-align: right;\">339000</td><td style=\"text-align: right;\">  -3.417</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            338.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 340000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-43-48\n",
      "  done: false\n",
      "  episode_len_mean: 338.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -3.4111999999999716\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 938\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9537416120370229\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013219693581600547\n",
      "          policy_loss: -0.12248225005136595\n",
      "          total_loss: -0.11670995520220863\n",
      "          vf_explained_var: 0.6230820417404175\n",
      "          vf_loss: 0.011289693576852895\n",
      "    num_agent_steps_sampled: 340000\n",
      "    num_agent_steps_trained: 340000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "  iterations_since_restore: 340\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.51136363636364\n",
      "    ram_util_percent: 46.934090909090905\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039891350319790624\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.99835793878789\n",
      "    mean_inference_ms: 1.744136181021351\n",
      "    mean_raw_obs_processing_ms: 1.7711324846967327\n",
      "  time_since_restore: 9401.46988272667\n",
      "  time_this_iter_s: 30.614655017852783\n",
      "  time_total_s: 9401.46988272667\n",
      "  timers:\n",
      "    learn_throughput: 1172.059\n",
      "    learn_time_ms: 853.2\n",
      "    load_throughput: 55750.113\n",
      "    load_time_ms: 17.937\n",
      "    sample_throughput: 31.52\n",
      "    sample_time_ms: 31726.121\n",
      "    update_time_ms: 3.207\n",
      "  timestamp: 1634852628\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 340\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   340</td><td style=\"text-align: right;\">         9401.47</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\"> -3.4112</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            338.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 341000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-44-21\n",
      "  done: false\n",
      "  episode_len_mean: 335.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.3712999999999713\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 941\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8557697799470689\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007480162698881701\n",
      "          policy_loss: -0.15405980779065026\n",
      "          total_loss: -0.14852215291725265\n",
      "          vf_explained_var: 0.38827595114707947\n",
      "          vf_loss: 0.011820688973077469\n",
      "    num_agent_steps_sampled: 341000\n",
      "    num_agent_steps_trained: 341000\n",
      "    num_steps_sampled: 341000\n",
      "    num_steps_trained: 341000\n",
      "  iterations_since_restore: 341\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.5468085106383\n",
      "    ram_util_percent: 46.84893617021277\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039893635143145645\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.004875895653676\n",
      "    mean_inference_ms: 1.7442168548560335\n",
      "    mean_raw_obs_processing_ms: 1.7710559509279844\n",
      "  time_since_restore: 9434.871722221375\n",
      "  time_this_iter_s: 33.4018394947052\n",
      "  time_total_s: 9434.871722221375\n",
      "  timers:\n",
      "    learn_throughput: 1171.538\n",
      "    learn_time_ms: 853.578\n",
      "    load_throughput: 55383.949\n",
      "    load_time_ms: 18.056\n",
      "    sample_throughput: 30.571\n",
      "    sample_time_ms: 32711.13\n",
      "    update_time_ms: 3.301\n",
      "  timestamp: 1634852661\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 341000\n",
      "  training_iteration: 341\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   341</td><td style=\"text-align: right;\">         9434.87</td><td style=\"text-align: right;\">341000</td><td style=\"text-align: right;\"> -3.3713</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            335.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 342000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-44-53\n",
      "  done: false\n",
      "  episode_len_mean: 334.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.3684999999999725\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 945\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1228176249398125\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013672689686084519\n",
      "          policy_loss: 0.07109919295956692\n",
      "          total_loss: 0.07318654412196743\n",
      "          vf_explained_var: 0.7643261551856995\n",
      "          vf_loss: 0.009157758263043232\n",
      "    num_agent_steps_sampled: 342000\n",
      "    num_agent_steps_trained: 342000\n",
      "    num_steps_sampled: 342000\n",
      "    num_steps_trained: 342000\n",
      "  iterations_since_restore: 342\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.34\n",
      "    ram_util_percent: 46.78888888888889\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039896720690998776\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.013642893574602\n",
      "    mean_inference_ms: 1.744322772015911\n",
      "    mean_raw_obs_processing_ms: 1.7710085932427668\n",
      "  time_since_restore: 9466.428514242172\n",
      "  time_this_iter_s: 31.55679202079773\n",
      "  time_total_s: 9466.428514242172\n",
      "  timers:\n",
      "    learn_throughput: 1174.198\n",
      "    learn_time_ms: 851.645\n",
      "    load_throughput: 58022.214\n",
      "    load_time_ms: 17.235\n",
      "    sample_throughput: 30.276\n",
      "    sample_time_ms: 33029.416\n",
      "    update_time_ms: 3.414\n",
      "  timestamp: 1634852693\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 342000\n",
      "  training_iteration: 342\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   342</td><td style=\"text-align: right;\">         9466.43</td><td style=\"text-align: right;\">342000</td><td style=\"text-align: right;\"> -3.3685</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            334.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 343000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-45-28\n",
      "  done: false\n",
      "  episode_len_mean: 333.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.355399999999973\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 948\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9255236042870416\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010275567179540385\n",
      "          policy_loss: -0.09963979456159804\n",
      "          total_loss: -0.09431957370705074\n",
      "          vf_explained_var: 0.3437243700027466\n",
      "          vf_loss: 0.011450726721280564\n",
      "    num_agent_steps_sampled: 343000\n",
      "    num_agent_steps_trained: 343000\n",
      "    num_steps_sampled: 343000\n",
      "    num_steps_trained: 343000\n",
      "  iterations_since_restore: 343\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.142\n",
      "    ram_util_percent: 46.867999999999995\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03989901049559921\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.020539487519738\n",
      "    mean_inference_ms: 1.7444018076886723\n",
      "    mean_raw_obs_processing_ms: 1.7710096768022896\n",
      "  time_since_restore: 9501.450459241867\n",
      "  time_this_iter_s: 35.021944999694824\n",
      "  time_total_s: 9501.450459241867\n",
      "  timers:\n",
      "    learn_throughput: 1168.213\n",
      "    learn_time_ms: 856.008\n",
      "    load_throughput: 57572.073\n",
      "    load_time_ms: 17.37\n",
      "    sample_throughput: 29.857\n",
      "    sample_time_ms: 33493.369\n",
      "    update_time_ms: 3.333\n",
      "  timestamp: 1634852728\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 343000\n",
      "  training_iteration: 343\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   343</td><td style=\"text-align: right;\">         9501.45</td><td style=\"text-align: right;\">343000</td><td style=\"text-align: right;\"> -3.3554</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            333.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-45-54\n",
      "  done: false\n",
      "  episode_len_mean: 334.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.3685999999999727\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 951\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4651929828855725\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025614157063205942\n",
      "          policy_loss: -0.044789055776264936\n",
      "          total_loss: -0.0469636963473426\n",
      "          vf_explained_var: 0.7472261190414429\n",
      "          vf_loss: 0.004688202822580934\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "  iterations_since_restore: 344\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.3756756756757\n",
      "    ram_util_percent: 46.96756756756756\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0399013271519316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.027177227983767\n",
      "    mean_inference_ms: 1.7444808597388735\n",
      "    mean_raw_obs_processing_ms: 1.7709747034014192\n",
      "  time_since_restore: 9527.272088766098\n",
      "  time_this_iter_s: 25.821629524230957\n",
      "  time_total_s: 9527.272088766098\n",
      "  timers:\n",
      "    learn_throughput: 1167.084\n",
      "    learn_time_ms: 856.836\n",
      "    load_throughput: 57401.727\n",
      "    load_time_ms: 17.421\n",
      "    sample_throughput: 30.627\n",
      "    sample_time_ms: 32651.385\n",
      "    update_time_ms: 3.642\n",
      "  timestamp: 1634852754\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 344\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   344</td><td style=\"text-align: right;\">         9527.27</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\"> -3.3686</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">             334.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 345000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-46-25\n",
      "  done: false\n",
      "  episode_len_mean: 333.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.3594999999999726\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 955\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0617035574383207\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016386414635920863\n",
      "          policy_loss: -0.039530085668795636\n",
      "          total_loss: -0.032094215653422804\n",
      "          vf_explained_var: 0.4099594056606293\n",
      "          vf_loss: 0.010578416487098568\n",
      "    num_agent_steps_sampled: 345000\n",
      "    num_agent_steps_trained: 345000\n",
      "    num_steps_sampled: 345000\n",
      "    num_steps_trained: 345000\n",
      "  iterations_since_restore: 345\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.32272727272728\n",
      "    ram_util_percent: 47.061363636363645\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03990422809484075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.036093052315195\n",
      "    mean_inference_ms: 1.7445835100728766\n",
      "    mean_raw_obs_processing_ms: 1.7709826327505727\n",
      "  time_since_restore: 9558.257446289062\n",
      "  time_this_iter_s: 30.985357522964478\n",
      "  time_total_s: 9558.257446289062\n",
      "  timers:\n",
      "    learn_throughput: 1166.31\n",
      "    learn_time_ms: 857.405\n",
      "    load_throughput: 57554.851\n",
      "    load_time_ms: 17.375\n",
      "    sample_throughput: 30.847\n",
      "    sample_time_ms: 32418.319\n",
      "    update_time_ms: 3.466\n",
      "  timestamp: 1634852785\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 345000\n",
      "  training_iteration: 345\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   345</td><td style=\"text-align: right;\">         9558.26</td><td style=\"text-align: right;\">345000</td><td style=\"text-align: right;\"> -3.3595</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            333.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 346000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-46-49\n",
      "  done: false\n",
      "  episode_len_mean: 335.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.375799999999972\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 957\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7475789533721076\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01745595190900049\n",
      "          policy_loss: 0.07261300616794163\n",
      "          total_loss: 0.06791443477074305\n",
      "          vf_explained_var: -0.16059976816177368\n",
      "          vf_loss: 0.004814864795965453\n",
      "    num_agent_steps_sampled: 346000\n",
      "    num_agent_steps_trained: 346000\n",
      "    num_steps_sampled: 346000\n",
      "    num_steps_trained: 346000\n",
      "  iterations_since_restore: 346\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.34857142857142\n",
      "    ram_util_percent: 47.07999999999999\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03990568392235857\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.040374286441043\n",
      "    mean_inference_ms: 1.7446357156044663\n",
      "    mean_raw_obs_processing_ms: 1.7709939532657593\n",
      "  time_since_restore: 9582.265473604202\n",
      "  time_this_iter_s: 24.00802731513977\n",
      "  time_total_s: 9582.265473604202\n",
      "  timers:\n",
      "    learn_throughput: 1167.191\n",
      "    learn_time_ms: 856.758\n",
      "    load_throughput: 58159.713\n",
      "    load_time_ms: 17.194\n",
      "    sample_throughput: 31.616\n",
      "    sample_time_ms: 31629.149\n",
      "    update_time_ms: 3.038\n",
      "  timestamp: 1634852809\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 346000\n",
      "  training_iteration: 346\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   346</td><td style=\"text-align: right;\">         9582.27</td><td style=\"text-align: right;\">346000</td><td style=\"text-align: right;\"> -3.3758</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            335.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 347000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-47-38\n",
      "  done: false\n",
      "  episode_len_mean: 332.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.348699999999973\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 960\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.047866079542372\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012751078207787082\n",
      "          policy_loss: -0.15923282214336926\n",
      "          total_loss: -0.15421761100490888\n",
      "          vf_explained_var: 0.4146115481853485\n",
      "          vf_loss: 0.009677601083078318\n",
      "    num_agent_steps_sampled: 347000\n",
      "    num_agent_steps_trained: 347000\n",
      "    num_steps_sampled: 347000\n",
      "    num_steps_trained: 347000\n",
      "  iterations_since_restore: 347\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.65142857142858\n",
      "    ram_util_percent: 47.03571428571429\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039908147155877406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.04768243903685\n",
      "    mean_inference_ms: 1.7447167787846327\n",
      "    mean_raw_obs_processing_ms: 1.772510981955506\n",
      "  time_since_restore: 9631.646364688873\n",
      "  time_this_iter_s: 49.38089108467102\n",
      "  time_total_s: 9631.646364688873\n",
      "  timers:\n",
      "    learn_throughput: 1168.553\n",
      "    learn_time_ms: 855.759\n",
      "    load_throughput: 61268.639\n",
      "    load_time_ms: 16.322\n",
      "    sample_throughput: 29.975\n",
      "    sample_time_ms: 33361.501\n",
      "    update_time_ms: 3.821\n",
      "  timestamp: 1634852858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 347000\n",
      "  training_iteration: 347\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   347</td><td style=\"text-align: right;\">         9631.65</td><td style=\"text-align: right;\">347000</td><td style=\"text-align: right;\"> -3.3487</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            332.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 348000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-48-04\n",
      "  done: false\n",
      "  episode_len_mean: 330.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.3271999999999737\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 963\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1476800342400868\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014543908172338913\n",
      "          policy_loss: -0.13320568510227734\n",
      "          total_loss: -0.1264062395526303\n",
      "          vf_explained_var: 0.23501046001911163\n",
      "          vf_loss: 0.011642196458867854\n",
      "    num_agent_steps_sampled: 348000\n",
      "    num_agent_steps_trained: 348000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "  iterations_since_restore: 348\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.3837837837838\n",
      "    ram_util_percent: 46.97027027027027\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03991063408304609\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.054972065290404\n",
      "    mean_inference_ms: 1.7447965474658969\n",
      "    mean_raw_obs_processing_ms: 1.7740367545674522\n",
      "  time_since_restore: 9657.139284133911\n",
      "  time_this_iter_s: 25.492919445037842\n",
      "  time_total_s: 9657.139284133911\n",
      "  timers:\n",
      "    learn_throughput: 1169.239\n",
      "    learn_time_ms: 855.257\n",
      "    load_throughput: 60023.67\n",
      "    load_time_ms: 16.66\n",
      "    sample_throughput: 32.08\n",
      "    sample_time_ms: 31172.224\n",
      "    update_time_ms: 3.989\n",
      "  timestamp: 1634852884\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 348\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   348</td><td style=\"text-align: right;\">         9657.14</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\"> -3.3272</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            330.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 349000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-48-25\n",
      "  done: false\n",
      "  episode_len_mean: 332.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.344299999999973\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 965\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.771375717057122\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013865855428334722\n",
      "          policy_loss: -0.1332534952296151\n",
      "          total_loss: -0.13684205752280024\n",
      "          vf_explained_var: 0.16048772633075714\n",
      "          vf_loss: 0.007800426709258722\n",
      "    num_agent_steps_sampled: 349000\n",
      "    num_agent_steps_trained: 349000\n",
      "    num_steps_sampled: 349000\n",
      "    num_steps_trained: 349000\n",
      "  iterations_since_restore: 349\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.83666666666669\n",
      "    ram_util_percent: 46.98333333333334\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039912316716663544\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.059518240539184\n",
      "    mean_inference_ms: 1.7448489863386143\n",
      "    mean_raw_obs_processing_ms: 1.7750436784463026\n",
      "  time_since_restore: 9678.774234056473\n",
      "  time_this_iter_s: 21.634949922561646\n",
      "  time_total_s: 9678.774234056473\n",
      "  timers:\n",
      "    learn_throughput: 1169.713\n",
      "    learn_time_ms: 854.911\n",
      "    load_throughput: 56398.705\n",
      "    load_time_ms: 17.731\n",
      "    sample_throughput: 33.432\n",
      "    sample_time_ms: 29911.036\n",
      "    update_time_ms: 4.024\n",
      "  timestamp: 1634852905\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 349000\n",
      "  training_iteration: 349\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   349</td><td style=\"text-align: right;\">         9678.77</td><td style=\"text-align: right;\">349000</td><td style=\"text-align: right;\"> -3.3443</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            332.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 350000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-48-57\n",
      "  done: false\n",
      "  episode_len_mean: 331.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.3381999999999734\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 969\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2218883792559305\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013127618049308154\n",
      "          policy_loss: -0.043039207119080755\n",
      "          total_loss: -0.04088989744583766\n",
      "          vf_explained_var: 0.578287661075592\n",
      "          vf_loss: 0.008380167005169723\n",
      "    num_agent_steps_sampled: 350000\n",
      "    num_agent_steps_trained: 350000\n",
      "    num_steps_sampled: 350000\n",
      "    num_steps_trained: 350000\n",
      "  iterations_since_restore: 350\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.5911111111111\n",
      "    ram_util_percent: 46.846666666666664\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039915925817993504\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.069234197513\n",
      "    mean_inference_ms: 1.7449571259731127\n",
      "    mean_raw_obs_processing_ms: 1.777124932519584\n",
      "  time_since_restore: 9710.24100780487\n",
      "  time_this_iter_s: 31.466773748397827\n",
      "  time_total_s: 9710.24100780487\n",
      "  timers:\n",
      "    learn_throughput: 1165.145\n",
      "    learn_time_ms: 858.263\n",
      "    load_throughput: 52880.399\n",
      "    load_time_ms: 18.911\n",
      "    sample_throughput: 33.342\n",
      "    sample_time_ms: 29991.795\n",
      "    update_time_ms: 4.02\n",
      "  timestamp: 1634852937\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 350000\n",
      "  training_iteration: 350\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   350</td><td style=\"text-align: right;\">         9710.24</td><td style=\"text-align: right;\">350000</td><td style=\"text-align: right;\"> -3.3382</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            331.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 351000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-49-24\n",
      "  done: false\n",
      "  episode_len_mean: 330.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.3196999999999743\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 972\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3623899532688988\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017341226248647836\n",
      "          policy_loss: 0.002655214940508207\n",
      "          total_loss: 0.003195451572537422\n",
      "          vf_explained_var: 0.4543473422527313\n",
      "          vf_loss: 0.006254116833830873\n",
      "    num_agent_steps_sampled: 351000\n",
      "    num_agent_steps_trained: 351000\n",
      "    num_steps_sampled: 351000\n",
      "    num_steps_trained: 351000\n",
      "  iterations_since_restore: 351\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.44358974358975\n",
      "    ram_util_percent: 46.76923076923078\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03991861529182649\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.0764305571968\n",
      "    mean_inference_ms: 1.7450376113621544\n",
      "    mean_raw_obs_processing_ms: 1.7771608737852196\n",
      "  time_since_restore: 9737.628118991852\n",
      "  time_this_iter_s: 27.3871111869812\n",
      "  time_total_s: 9737.628118991852\n",
      "  timers:\n",
      "    learn_throughput: 1166.787\n",
      "    learn_time_ms: 857.055\n",
      "    load_throughput: 51204.688\n",
      "    load_time_ms: 19.529\n",
      "    sample_throughput: 34.024\n",
      "    sample_time_ms: 29391.044\n",
      "    update_time_ms: 3.914\n",
      "  timestamp: 1634852964\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 351000\n",
      "  training_iteration: 351\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   351</td><td style=\"text-align: right;\">         9737.63</td><td style=\"text-align: right;\">351000</td><td style=\"text-align: right;\"> -3.3197</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            330.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-49-50\n",
      "  done: false\n",
      "  episode_len_mean: 330.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.316499999999974\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 974\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4733549310101404\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010941470831471081\n",
      "          policy_loss: -0.09887265976932313\n",
      "          total_loss: -0.10077911507752206\n",
      "          vf_explained_var: 0.0012162593193352222\n",
      "          vf_loss: 0.007836260126593213\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_agent_steps_trained: 352000\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "  iterations_since_restore: 352\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.27297297297295\n",
      "    ram_util_percent: 46.78648648648648\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03992046495219666\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.08124727954026\n",
      "    mean_inference_ms: 1.745090530072286\n",
      "    mean_raw_obs_processing_ms: 1.7771939989088752\n",
      "  time_since_restore: 9763.453682899475\n",
      "  time_this_iter_s: 25.82556390762329\n",
      "  time_total_s: 9763.453682899475\n",
      "  timers:\n",
      "    learn_throughput: 1158.96\n",
      "    learn_time_ms: 862.842\n",
      "    load_throughput: 51473.263\n",
      "    load_time_ms: 19.428\n",
      "    sample_throughput: 34.707\n",
      "    sample_time_ms: 28812.243\n",
      "    update_time_ms: 3.814\n",
      "  timestamp: 1634852990\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 352\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   352</td><td style=\"text-align: right;\">         9763.45</td><td style=\"text-align: right;\">352000</td><td style=\"text-align: right;\"> -3.3165</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            330.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 353000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-50-12\n",
      "  done: false\n",
      "  episode_len_mean: 331.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.3281999999999727\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 977\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8361553880903456\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012903140541214499\n",
      "          policy_loss: -0.0059027277761035495\n",
      "          total_loss: -0.008488632159100638\n",
      "          vf_explained_var: -0.08298470824956894\n",
      "          vf_loss: 0.009890016702572918\n",
      "    num_agent_steps_sampled: 353000\n",
      "    num_agent_steps_trained: 353000\n",
      "    num_steps_sampled: 353000\n",
      "    num_steps_trained: 353000\n",
      "  iterations_since_restore: 353\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.34687500000001\n",
      "    ram_util_percent: 46.8625\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039923196973408834\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.08810938512352\n",
      "    mean_inference_ms: 1.7451695639153098\n",
      "    mean_raw_obs_processing_ms: 1.7772571895691416\n",
      "  time_since_restore: 9785.72312450409\n",
      "  time_this_iter_s: 22.269441604614258\n",
      "  time_total_s: 9785.72312450409\n",
      "  timers:\n",
      "    learn_throughput: 1165.141\n",
      "    learn_time_ms: 858.265\n",
      "    load_throughput: 51649.41\n",
      "    load_time_ms: 19.361\n",
      "    sample_throughput: 36.31\n",
      "    sample_time_ms: 27540.716\n",
      "    update_time_ms: 4.662\n",
      "  timestamp: 1634853012\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 353000\n",
      "  training_iteration: 353\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   353</td><td style=\"text-align: right;\">         9785.72</td><td style=\"text-align: right;\">353000</td><td style=\"text-align: right;\"> -3.3282</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            331.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 354000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-50-42\n",
      "  done: false\n",
      "  episode_len_mean: 331.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.325499999999973\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 980\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.350160410669115\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00973897077089517\n",
      "          policy_loss: 0.027253664367728764\n",
      "          total_loss: 0.02922171817885505\n",
      "          vf_explained_var: 0.16518177092075348\n",
      "          vf_loss: 0.011027326414154635\n",
      "    num_agent_steps_sampled: 354000\n",
      "    num_agent_steps_trained: 354000\n",
      "    num_steps_sampled: 354000\n",
      "    num_steps_trained: 354000\n",
      "  iterations_since_restore: 354\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.34761904761903\n",
      "    ram_util_percent: 46.95952380952381\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03992597275870533\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.095022923919636\n",
      "    mean_inference_ms: 1.7452506502779865\n",
      "    mean_raw_obs_processing_ms: 1.7773337609870878\n",
      "  time_since_restore: 9814.792882442474\n",
      "  time_this_iter_s: 29.06975793838501\n",
      "  time_total_s: 9814.792882442474\n",
      "  timers:\n",
      "    learn_throughput: 1131.195\n",
      "    learn_time_ms: 884.021\n",
      "    load_throughput: 47417.322\n",
      "    load_time_ms: 21.089\n",
      "    sample_throughput: 35.922\n",
      "    sample_time_ms: 27838.329\n",
      "    update_time_ms: 4.355\n",
      "  timestamp: 1634853042\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 354000\n",
      "  training_iteration: 354\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   354</td><td style=\"text-align: right;\">         9814.79</td><td style=\"text-align: right;\">354000</td><td style=\"text-align: right;\"> -3.3255</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            331.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 355000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-51-10\n",
      "  done: false\n",
      "  episode_len_mean: 329.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.3004999999999733\n",
      "  episode_reward_min: -5.499999999999927\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 983\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.161882722377777\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017141569132248808\n",
      "          policy_loss: 0.043041669494575927\n",
      "          total_loss: 0.04787308126688004\n",
      "          vf_explained_var: 0.29985079169273376\n",
      "          vf_loss: 0.008631292989270555\n",
      "    num_agent_steps_sampled: 355000\n",
      "    num_agent_steps_trained: 355000\n",
      "    num_steps_sampled: 355000\n",
      "    num_steps_trained: 355000\n",
      "  iterations_since_restore: 355\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.39756097560975\n",
      "    ram_util_percent: 47.05853658536585\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03992882830327333\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.10264029322202\n",
      "    mean_inference_ms: 1.7453338550730648\n",
      "    mean_raw_obs_processing_ms: 1.7774762076379185\n",
      "  time_since_restore: 9843.466186285019\n",
      "  time_this_iter_s: 28.673303842544556\n",
      "  time_total_s: 9843.466186285019\n",
      "  timers:\n",
      "    learn_throughput: 1132.1\n",
      "    learn_time_ms: 883.314\n",
      "    load_throughput: 45086.178\n",
      "    load_time_ms: 22.18\n",
      "    sample_throughput: 36.223\n",
      "    sample_time_ms: 27606.874\n",
      "    update_time_ms: 4.16\n",
      "  timestamp: 1634853070\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 355000\n",
      "  training_iteration: 355\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   355</td><td style=\"text-align: right;\">         9843.47</td><td style=\"text-align: right;\">355000</td><td style=\"text-align: right;\"> -3.3005</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">                -5.5</td><td style=\"text-align: right;\">            329.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 356000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-51-41\n",
      "  done: false\n",
      "  episode_len_mean: 325.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.2618999999999745\n",
      "  episode_reward_min: -5.199999999999934\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 986\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.011851237879859\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006888389333127836\n",
      "          policy_loss: 0.02797636083430714\n",
      "          total_loss: 0.02814661951528655\n",
      "          vf_explained_var: 0.5239356160163879\n",
      "          vf_loss: 0.007146703712512843\n",
      "    num_agent_steps_sampled: 356000\n",
      "    num_agent_steps_trained: 356000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "  iterations_since_restore: 356\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.10232558139533\n",
      "    ram_util_percent: 47.069767441860456\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03993180171729366\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.111099009855575\n",
      "    mean_inference_ms: 1.7454191615486374\n",
      "    mean_raw_obs_processing_ms: 1.7776315280586965\n",
      "  time_since_restore: 9873.75313091278\n",
      "  time_this_iter_s: 30.28694462776184\n",
      "  time_total_s: 9873.75313091278\n",
      "  timers:\n",
      "    learn_throughput: 1134.456\n",
      "    learn_time_ms: 881.479\n",
      "    load_throughput: 44605.687\n",
      "    load_time_ms: 22.419\n",
      "    sample_throughput: 35.416\n",
      "    sample_time_ms: 28236.125\n",
      "    update_time_ms: 4.451\n",
      "  timestamp: 1634853101\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 356\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   356</td><td style=\"text-align: right;\">         9873.75</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\"> -3.2619</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">                -5.2</td><td style=\"text-align: right;\">             325.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 357000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-52-14\n",
      "  done: false\n",
      "  episode_len_mean: 322.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3799999999999932\n",
      "  episode_reward_mean: -3.2394999999999743\n",
      "  episode_reward_min: -5.199999999999934\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 989\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0398213764031727\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011271496498671495\n",
      "          policy_loss: -0.024538877937528823\n",
      "          total_loss: -0.02061371256907781\n",
      "          vf_explained_var: 0.6520624756813049\n",
      "          vf_loss: 0.009182006968573356\n",
      "    num_agent_steps_sampled: 357000\n",
      "    num_agent_steps_trained: 357000\n",
      "    num_steps_sampled: 357000\n",
      "    num_steps_trained: 357000\n",
      "  iterations_since_restore: 357\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.29999999999998\n",
      "    ram_util_percent: 47.0595744680851\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03993472864540273\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.120293585615023\n",
      "    mean_inference_ms: 1.745504091455615\n",
      "    mean_raw_obs_processing_ms: 1.7778469089741338\n",
      "  time_since_restore: 9906.813942193985\n",
      "  time_this_iter_s: 33.060811281204224\n",
      "  time_total_s: 9906.813942193985\n",
      "  timers:\n",
      "    learn_throughput: 1136.126\n",
      "    learn_time_ms: 880.184\n",
      "    load_throughput: 44412.744\n",
      "    load_time_ms: 22.516\n",
      "    sample_throughput: 37.585\n",
      "    sample_time_ms: 26606.077\n",
      "    update_time_ms: 3.631\n",
      "  timestamp: 1634853134\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 357000\n",
      "  training_iteration: 357\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   357</td><td style=\"text-align: right;\">         9906.81</td><td style=\"text-align: right;\">357000</td><td style=\"text-align: right;\"> -3.2395</td><td style=\"text-align: right;\">               -2.38</td><td style=\"text-align: right;\">                -5.2</td><td style=\"text-align: right;\">            322.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 358000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-53-03\n",
      "  done: false\n",
      "  episode_len_mean: 317.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.177099999999976\n",
      "  episode_reward_min: -5.199999999999934\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 993\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6374208622508579\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003593342920429047\n",
      "          policy_loss: -0.01382885438700517\n",
      "          total_loss: -0.0036408468253082697\n",
      "          vf_explained_var: 0.15759685635566711\n",
      "          vf_loss: 0.014923147039694919\n",
      "    num_agent_steps_sampled: 358000\n",
      "    num_agent_steps_trained: 358000\n",
      "    num_steps_sampled: 358000\n",
      "    num_steps_trained: 358000\n",
      "  iterations_since_restore: 358\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.056338028169\n",
      "    ram_util_percent: 46.918309859154945\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03993833849254425\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.13308995848768\n",
      "    mean_inference_ms: 1.7456148593782905\n",
      "    mean_raw_obs_processing_ms: 1.7801058254935946\n",
      "  time_since_restore: 9956.58298921585\n",
      "  time_this_iter_s: 49.769047021865845\n",
      "  time_total_s: 9956.58298921585\n",
      "  timers:\n",
      "    learn_throughput: 1136.704\n",
      "    learn_time_ms: 879.736\n",
      "    load_throughput: 44979.18\n",
      "    load_time_ms: 22.233\n",
      "    sample_throughput: 34.442\n",
      "    sample_time_ms: 29034.497\n",
      "    update_time_ms: 3.549\n",
      "  timestamp: 1634853183\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 358000\n",
      "  training_iteration: 358\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   358</td><td style=\"text-align: right;\">         9956.58</td><td style=\"text-align: right;\">358000</td><td style=\"text-align: right;\"> -3.1771</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">                -5.2</td><td style=\"text-align: right;\">            317.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 359000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-53-30\n",
      "  done: false\n",
      "  episode_len_mean: 319.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.1970999999999754\n",
      "  episode_reward_min: -5.199999999999934\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 996\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22806973457336427\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.758842294745975\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.04794174996805549\n",
      "          policy_loss: 0.051814133591122094\n",
      "          total_loss: 0.051006108687983616\n",
      "          vf_explained_var: 0.31700146198272705\n",
      "          vf_loss: 0.005846337783926477\n",
      "    num_agent_steps_sampled: 359000\n",
      "    num_agent_steps_trained: 359000\n",
      "    num_steps_sampled: 359000\n",
      "    num_steps_trained: 359000\n",
      "  iterations_since_restore: 359\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.18684210526315\n",
      "    ram_util_percent: 46.8421052631579\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03994096959440223\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.142437620173595\n",
      "    mean_inference_ms: 1.745697393002829\n",
      "    mean_raw_obs_processing_ms: 1.7818029923224281\n",
      "  time_since_restore: 9983.259984493256\n",
      "  time_this_iter_s: 26.676995277404785\n",
      "  time_total_s: 9983.259984493256\n",
      "  timers:\n",
      "    learn_throughput: 1138.329\n",
      "    learn_time_ms: 878.481\n",
      "    load_throughput: 45073.241\n",
      "    load_time_ms: 22.186\n",
      "    sample_throughput: 33.852\n",
      "    sample_time_ms: 29540.148\n",
      "    update_time_ms: 3.512\n",
      "  timestamp: 1634853210\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 359000\n",
      "  training_iteration: 359\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   359</td><td style=\"text-align: right;\">         9983.26</td><td style=\"text-align: right;\">359000</td><td style=\"text-align: right;\"> -3.1971</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">                -5.2</td><td style=\"text-align: right;\">            319.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-53-55\n",
      "  done: false\n",
      "  episode_len_mean: 321.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.2137999999999756\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 998\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3421046018600464\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5319822967052459\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02883338304693643\n",
      "          policy_loss: -0.05878048340479533\n",
      "          total_loss: -0.05844889316293928\n",
      "          vf_explained_var: -0.008597508072853088\n",
      "          vf_loss: 0.005787375852297474\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "  iterations_since_restore: 360\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.17222222222223\n",
      "    ram_util_percent: 46.86388888888888\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039942611335455534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.148398765789565\n",
      "    mean_inference_ms: 1.745751742989857\n",
      "    mean_raw_obs_processing_ms: 1.7829370369825979\n",
      "  time_since_restore: 10008.364990234375\n",
      "  time_this_iter_s: 25.105005741119385\n",
      "  time_total_s: 10008.364990234375\n",
      "  timers:\n",
      "    learn_throughput: 1138.542\n",
      "    learn_time_ms: 878.316\n",
      "    load_throughput: 45236.58\n",
      "    load_time_ms: 22.106\n",
      "    sample_throughput: 34.597\n",
      "    sample_time_ms: 28904.074\n",
      "    update_time_ms: 3.605\n",
      "  timestamp: 1634853235\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 360\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   360</td><td style=\"text-align: right;\">         10008.4</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\"> -3.2138</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            321.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 361000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-54-17\n",
      "  done: false\n",
      "  episode_len_mean: 324.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.243699999999975\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1000\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7353051874372694\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01802390665044046\n",
      "          policy_loss: -0.12728707277112536\n",
      "          total_loss: -0.1313728561831845\n",
      "          vf_explained_var: 0.8474992513656616\n",
      "          vf_loss: 0.004018171116088828\n",
      "    num_agent_steps_sampled: 361000\n",
      "    num_agent_steps_trained: 361000\n",
      "    num_steps_sampled: 361000\n",
      "    num_steps_trained: 361000\n",
      "  iterations_since_restore: 361\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.184375\n",
      "    ram_util_percent: 46.86875\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039944344001981505\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.153993591991483\n",
      "    mean_inference_ms: 1.7458074270505366\n",
      "    mean_raw_obs_processing_ms: 1.7835113672278395\n",
      "  time_since_restore: 10030.472137212753\n",
      "  time_this_iter_s: 22.107146978378296\n",
      "  time_total_s: 10030.472137212753\n",
      "  timers:\n",
      "    learn_throughput: 1136.35\n",
      "    learn_time_ms: 880.011\n",
      "    load_throughput: 47363.616\n",
      "    load_time_ms: 21.113\n",
      "    sample_throughput: 35.242\n",
      "    sample_time_ms: 28375.274\n",
      "    update_time_ms: 3.601\n",
      "  timestamp: 1634853257\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 361000\n",
      "  training_iteration: 361\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   361</td><td style=\"text-align: right;\">         10030.5</td><td style=\"text-align: right;\">361000</td><td style=\"text-align: right;\"> -3.2437</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            324.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 362000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-54-41\n",
      "  done: false\n",
      "  episode_len_mean: 326.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.2686999999999746\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1003\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.789025154378679\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012841800778680421\n",
      "          policy_loss: 0.02944375549753507\n",
      "          total_loss: 0.021978873759508133\n",
      "          vf_explained_var: 0.5245177149772644\n",
      "          vf_loss: 0.0038355132202721304\n",
      "    num_agent_steps_sampled: 362000\n",
      "    num_agent_steps_trained: 362000\n",
      "    num_steps_sampled: 362000\n",
      "    num_steps_trained: 362000\n",
      "  iterations_since_restore: 362\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.31818181818183\n",
      "    ram_util_percent: 46.936363636363645\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039946996747890075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.16192409690573\n",
      "    mean_inference_ms: 1.7458924380688259\n",
      "    mean_raw_obs_processing_ms: 1.7835855759506742\n",
      "  time_since_restore: 10053.952615976334\n",
      "  time_this_iter_s: 23.480478763580322\n",
      "  time_total_s: 10053.952615976334\n",
      "  timers:\n",
      "    learn_throughput: 1141.675\n",
      "    learn_time_ms: 875.906\n",
      "    load_throughput: 47537.436\n",
      "    load_time_ms: 21.036\n",
      "    sample_throughput: 35.53\n",
      "    sample_time_ms: 28144.857\n",
      "    update_time_ms: 3.739\n",
      "  timestamp: 1634853281\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 362000\n",
      "  training_iteration: 362\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   362</td><td style=\"text-align: right;\">           10054</td><td style=\"text-align: right;\">362000</td><td style=\"text-align: right;\"> -3.2687</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            326.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 363000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-55-05\n",
      "  done: false\n",
      "  episode_len_mean: 328.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.2815999999999743\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1005\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9505633605851067\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012025919190300351\n",
      "          policy_loss: 0.09254027207692464\n",
      "          total_loss: 0.08326670941379335\n",
      "          vf_explained_var: 0.21532867848873138\n",
      "          vf_loss: 0.004060889455851995\n",
      "    num_agent_steps_sampled: 363000\n",
      "    num_agent_steps_trained: 363000\n",
      "    num_steps_sampled: 363000\n",
      "    num_steps_trained: 363000\n",
      "  iterations_since_restore: 363\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.33235294117647\n",
      "    ram_util_percent: 47.07058823529411\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0399487252574703\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.167121122895132\n",
      "    mean_inference_ms: 1.7459486326989144\n",
      "    mean_raw_obs_processing_ms: 1.7836590702699773\n",
      "  time_since_restore: 10077.82548713684\n",
      "  time_this_iter_s: 23.872871160507202\n",
      "  time_total_s: 10077.82548713684\n",
      "  timers:\n",
      "    learn_throughput: 1139.718\n",
      "    learn_time_ms: 877.41\n",
      "    load_throughput: 45155.832\n",
      "    load_time_ms: 22.146\n",
      "    sample_throughput: 35.332\n",
      "    sample_time_ms: 28303.348\n",
      "    update_time_ms: 3.048\n",
      "  timestamp: 1634853305\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 363000\n",
      "  training_iteration: 363\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   363</td><td style=\"text-align: right;\">         10077.8</td><td style=\"text-align: right;\">363000</td><td style=\"text-align: right;\"> -3.2816</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            328.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 364000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-55-27\n",
      "  done: false\n",
      "  episode_len_mean: 328.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.2891999999999735\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1007\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.677135243680742\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019719988514965082\n",
      "          policy_loss: -0.1331629925303989\n",
      "          total_loss: -0.13009208531843292\n",
      "          vf_explained_var: -0.1095890998840332\n",
      "          vf_loss: 0.009722811620870036\n",
      "    num_agent_steps_sampled: 364000\n",
      "    num_agent_steps_trained: 364000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "  iterations_since_restore: 364\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.125\n",
      "    ram_util_percent: 47.140625\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03995045119594692\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.17216708310053\n",
      "    mean_inference_ms: 1.7460054110036918\n",
      "    mean_raw_obs_processing_ms: 1.7836924193804302\n",
      "  time_since_restore: 10099.93845963478\n",
      "  time_this_iter_s: 22.112972497940063\n",
      "  time_total_s: 10099.93845963478\n",
      "  timers:\n",
      "    learn_throughput: 1177.546\n",
      "    learn_time_ms: 849.224\n",
      "    load_throughput: 46656.981\n",
      "    load_time_ms: 21.433\n",
      "    sample_throughput: 36.184\n",
      "    sample_time_ms: 27636.588\n",
      "    update_time_ms: 3.052\n",
      "  timestamp: 1634853327\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 364\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   364</td><td style=\"text-align: right;\">         10099.9</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\"> -3.2892</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            328.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 365000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-55-50\n",
      "  done: false\n",
      "  episode_len_mean: 330.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.3041999999999736\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1009\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8332008282343546\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013227931024691546\n",
      "          policy_loss: -0.11760586533281538\n",
      "          total_loss: -0.1259878362218539\n",
      "          vf_explained_var: 0.8281993269920349\n",
      "          vf_loss: 0.0031620332764254674\n",
      "    num_agent_steps_sampled: 365000\n",
      "    num_agent_steps_trained: 365000\n",
      "    num_steps_sampled: 365000\n",
      "    num_steps_trained: 365000\n",
      "  iterations_since_restore: 365\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.09375\n",
      "    ram_util_percent: 47.178125\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03995231019265362\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.17687462432179\n",
      "    mean_inference_ms: 1.7460635071599853\n",
      "    mean_raw_obs_processing_ms: 1.7837338312262432\n",
      "  time_since_restore: 10122.683064222336\n",
      "  time_this_iter_s: 22.74460458755493\n",
      "  time_total_s: 10122.683064222336\n",
      "  timers:\n",
      "    learn_throughput: 1176.158\n",
      "    learn_time_ms: 850.226\n",
      "    load_throughput: 47884.72\n",
      "    load_time_ms: 20.883\n",
      "    sample_throughput: 36.978\n",
      "    sample_time_ms: 27043.379\n",
      "    update_time_ms: 3.055\n",
      "  timestamp: 1634853350\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 365000\n",
      "  training_iteration: 365\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   365</td><td style=\"text-align: right;\">         10122.7</td><td style=\"text-align: right;\">365000</td><td style=\"text-align: right;\"> -3.3042</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            330.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 366000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-56-11\n",
      "  done: false\n",
      "  episode_len_mean: 335.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.3596999999999726\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1012\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7909862690501743\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01110204158926336\n",
      "          policy_loss: -0.10747741783658664\n",
      "          total_loss: -0.11217475607991219\n",
      "          vf_explained_var: 0.5785658359527588\n",
      "          vf_loss: 0.007515434462887546\n",
      "    num_agent_steps_sampled: 366000\n",
      "    num_agent_steps_trained: 366000\n",
      "    num_steps_sampled: 366000\n",
      "    num_steps_trained: 366000\n",
      "  iterations_since_restore: 366\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.28387096774195\n",
      "    ram_util_percent: 47.13870967741936\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039954989815082616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.18330046978622\n",
      "    mean_inference_ms: 1.746152461246445\n",
      "    mean_raw_obs_processing_ms: 1.7837632015359446\n",
      "  time_since_restore: 10144.390634775162\n",
      "  time_this_iter_s: 21.707570552825928\n",
      "  time_total_s: 10144.390634775162\n",
      "  timers:\n",
      "    learn_throughput: 1174.223\n",
      "    learn_time_ms: 851.627\n",
      "    load_throughput: 48160.348\n",
      "    load_time_ms: 20.764\n",
      "    sample_throughput: 38.191\n",
      "    sample_time_ms: 26184.25\n",
      "    update_time_ms: 2.948\n",
      "  timestamp: 1634853371\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 366000\n",
      "  training_iteration: 366\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   366</td><td style=\"text-align: right;\">         10144.4</td><td style=\"text-align: right;\">366000</td><td style=\"text-align: right;\"> -3.3597</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            335.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 367000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-56-37\n",
      "  done: false\n",
      "  episode_len_mean: 336.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.3685999999999723\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1014\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8634723875257704\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011972779468555473\n",
      "          policy_loss: -0.08696017033523984\n",
      "          total_loss: -0.09091276534729534\n",
      "          vf_explained_var: 0.046721745282411575\n",
      "          vf_loss: 0.008538212290214788\n",
      "    num_agent_steps_sampled: 367000\n",
      "    num_agent_steps_trained: 367000\n",
      "    num_steps_sampled: 367000\n",
      "    num_steps_trained: 367000\n",
      "  iterations_since_restore: 367\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.1972972972973\n",
      "    ram_util_percent: 47.14324324324324\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03995673074187084\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.18721507376043\n",
      "    mean_inference_ms: 1.746211328520738\n",
      "    mean_raw_obs_processing_ms: 1.783773302240934\n",
      "  time_since_restore: 10169.843074798584\n",
      "  time_this_iter_s: 25.45244002342224\n",
      "  time_total_s: 10169.843074798584\n",
      "  timers:\n",
      "    learn_throughput: 1180.441\n",
      "    learn_time_ms: 847.141\n",
      "    load_throughput: 48464.915\n",
      "    load_time_ms: 20.633\n",
      "    sample_throughput: 39.327\n",
      "    sample_time_ms: 25428.037\n",
      "    update_time_ms: 3.189\n",
      "  timestamp: 1634853397\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 367000\n",
      "  training_iteration: 367\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   367</td><td style=\"text-align: right;\">         10169.8</td><td style=\"text-align: right;\">367000</td><td style=\"text-align: right;\"> -3.3686</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            336.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 368000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-57-07\n",
      "  done: false\n",
      "  episode_len_mean: 338.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.385299999999972\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1018\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9617548644542694\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004198214123569106\n",
      "          policy_loss: 0.014708476430839963\n",
      "          total_loss: 0.020844938192102643\n",
      "          vf_explained_var: 0.38654497265815735\n",
      "          vf_loss: 0.013599665406056576\n",
      "    num_agent_steps_sampled: 368000\n",
      "    num_agent_steps_trained: 368000\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "  iterations_since_restore: 368\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.97441860465116\n",
      "    ram_util_percent: 47.15348837209303\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03996027429947951\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.19476740266706\n",
      "    mean_inference_ms: 1.7463270857638462\n",
      "    mean_raw_obs_processing_ms: 1.7837638896330652\n",
      "  time_since_restore: 10199.98448753357\n",
      "  time_this_iter_s: 30.14141273498535\n",
      "  time_total_s: 10199.98448753357\n",
      "  timers:\n",
      "    learn_throughput: 1183.421\n",
      "    learn_time_ms: 845.008\n",
      "    load_throughput: 46787.252\n",
      "    load_time_ms: 21.373\n",
      "    sample_throughput: 42.613\n",
      "    sample_time_ms: 23466.88\n",
      "    update_time_ms: 2.938\n",
      "  timestamp: 1634853427\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 368\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   368</td><td style=\"text-align: right;\">           10200</td><td style=\"text-align: right;\">368000</td><td style=\"text-align: right;\"> -3.3853</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            338.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 369000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-57-45\n",
      "  done: false\n",
      "  episode_len_mean: 341.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.4126999999999708\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1020\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2565784513950349\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.038007416990068\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.028142136680134003\n",
      "          policy_loss: -0.007060736500554615\n",
      "          total_loss: -0.01635024468931887\n",
      "          vf_explained_var: 0.7999665141105652\n",
      "          vf_loss: 0.003869898792537343\n",
      "    num_agent_steps_sampled: 369000\n",
      "    num_agent_steps_trained: 369000\n",
      "    num_steps_sampled: 369000\n",
      "    num_steps_trained: 369000\n",
      "  iterations_since_restore: 369\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.18888888888888\n",
      "    ram_util_percent: 47.11851851851852\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039962011666036974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.19794883225237\n",
      "    mean_inference_ms: 1.7463831943295158\n",
      "    mean_raw_obs_processing_ms: 1.7846651850847746\n",
      "  time_since_restore: 10238.220136880875\n",
      "  time_this_iter_s: 38.2356493473053\n",
      "  time_total_s: 10238.220136880875\n",
      "  timers:\n",
      "    learn_throughput: 1180.316\n",
      "    learn_time_ms: 847.231\n",
      "    load_throughput: 46755.75\n",
      "    load_time_ms: 21.388\n",
      "    sample_throughput: 40.617\n",
      "    sample_time_ms: 24620.238\n",
      "    update_time_ms: 3.129\n",
      "  timestamp: 1634853465\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 369000\n",
      "  training_iteration: 369\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   369</td><td style=\"text-align: right;\">         10238.2</td><td style=\"text-align: right;\">369000</td><td style=\"text-align: right;\"> -3.4127</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            341.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 370000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-58-10\n",
      "  done: false\n",
      "  episode_len_mean: 343.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.43989999999997\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1022\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8311692039171854\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012947255092100641\n",
      "          policy_loss: -0.06448501886592971\n",
      "          total_loss: -0.06849954790539212\n",
      "          vf_explained_var: 0.4730869233608246\n",
      "          vf_loss: 0.009314185522574311\n",
      "    num_agent_steps_sampled: 370000\n",
      "    num_agent_steps_trained: 370000\n",
      "    num_steps_sampled: 370000\n",
      "    num_steps_trained: 370000\n",
      "  iterations_since_restore: 370\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.39166666666667\n",
      "    ram_util_percent: 46.833333333333336\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03996383338956962\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.20088849216328\n",
      "    mean_inference_ms: 1.7464404414106058\n",
      "    mean_raw_obs_processing_ms: 1.7855267765292944\n",
      "  time_since_restore: 10263.19778251648\n",
      "  time_this_iter_s: 24.97764563560486\n",
      "  time_total_s: 10263.19778251648\n",
      "  timers:\n",
      "    learn_throughput: 1181.746\n",
      "    learn_time_ms: 846.206\n",
      "    load_throughput: 46864.308\n",
      "    load_time_ms: 21.338\n",
      "    sample_throughput: 40.638\n",
      "    sample_time_ms: 24607.348\n",
      "    update_time_ms: 4.454\n",
      "  timestamp: 1634853490\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 370000\n",
      "  training_iteration: 370\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   370</td><td style=\"text-align: right;\">         10263.2</td><td style=\"text-align: right;\">370000</td><td style=\"text-align: right;\"> -3.4399</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            343.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 371000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-58-35\n",
      "  done: false\n",
      "  episode_len_mean: 347.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.4766999999999695\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1025\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6092768086327447\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011068262007375217\n",
      "          policy_loss: 0.07396415852838092\n",
      "          total_loss: 0.07248595216208034\n",
      "          vf_explained_var: 0.02868177555501461\n",
      "          vf_loss: 0.010354747475745777\n",
      "    num_agent_steps_sampled: 371000\n",
      "    num_agent_steps_trained: 371000\n",
      "    num_steps_sampled: 371000\n",
      "    num_steps_trained: 371000\n",
      "  iterations_since_restore: 371\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.24000000000001\n",
      "    ram_util_percent: 46.82\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03996650754919012\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.204706613154308\n",
      "    mean_inference_ms: 1.7465270501418806\n",
      "    mean_raw_obs_processing_ms: 1.7868054985023176\n",
      "  time_since_restore: 10287.491669178009\n",
      "  time_this_iter_s: 24.29388666152954\n",
      "  time_total_s: 10287.491669178009\n",
      "  timers:\n",
      "    learn_throughput: 1185.857\n",
      "    learn_time_ms: 843.272\n",
      "    load_throughput: 45760.071\n",
      "    load_time_ms: 21.853\n",
      "    sample_throughput: 40.276\n",
      "    sample_time_ms: 24828.502\n",
      "    update_time_ms: 4.495\n",
      "  timestamp: 1634853515\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 371000\n",
      "  training_iteration: 371\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   371</td><td style=\"text-align: right;\">         10287.5</td><td style=\"text-align: right;\">371000</td><td style=\"text-align: right;\"> -3.4767</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            347.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 372000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-59-02\n",
      "  done: false\n",
      "  episode_len_mean: 348.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.488499999999969\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1028\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5740687555736965\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009836129940675854\n",
      "          policy_loss: 0.08676622187097867\n",
      "          total_loss: 0.08227472574346595\n",
      "          vf_explained_var: 0.6724661588668823\n",
      "          vf_loss: 0.00746358080391979\n",
      "    num_agent_steps_sampled: 372000\n",
      "    num_agent_steps_trained: 372000\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "  iterations_since_restore: 372\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.47948717948718\n",
      "    ram_util_percent: 46.8076923076923\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039969347450809455\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.208161227313358\n",
      "    mean_inference_ms: 1.7466129800357706\n",
      "    mean_raw_obs_processing_ms: 1.7875849900541143\n",
      "  time_since_restore: 10315.113581418991\n",
      "  time_this_iter_s: 27.621912240982056\n",
      "  time_total_s: 10315.113581418991\n",
      "  timers:\n",
      "    learn_throughput: 1186.29\n",
      "    learn_time_ms: 842.964\n",
      "    load_throughput: 44626.332\n",
      "    load_time_ms: 22.408\n",
      "    sample_throughput: 39.617\n",
      "    sample_time_ms: 25241.596\n",
      "    update_time_ms: 5.291\n",
      "  timestamp: 1634853542\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 372\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   372</td><td style=\"text-align: right;\">         10315.1</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\"> -3.4885</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            348.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 373000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-59-31\n",
      "  done: false\n",
      "  episode_len_mean: 350.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.5034999999999696\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1031\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7095542828241983\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012819360034454164\n",
      "          policy_loss: 0.034422899782657626\n",
      "          total_loss: 0.02986902048190435\n",
      "          vf_explained_var: 0.6594528555870056\n",
      "          vf_loss: 0.007607906410056684\n",
      "    num_agent_steps_sampled: 373000\n",
      "    num_agent_steps_trained: 373000\n",
      "    num_steps_sampled: 373000\n",
      "    num_steps_trained: 373000\n",
      "  iterations_since_restore: 373\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.45365853658535\n",
      "    ram_util_percent: 46.90731707317073\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03997226427572674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.21156598232361\n",
      "    mean_inference_ms: 1.7466994252724002\n",
      "    mean_raw_obs_processing_ms: 1.7873052396799194\n",
      "  time_since_restore: 10343.630897760391\n",
      "  time_this_iter_s: 28.517316341400146\n",
      "  time_total_s: 10343.630897760391\n",
      "  timers:\n",
      "    learn_throughput: 1189.913\n",
      "    learn_time_ms: 840.397\n",
      "    load_throughput: 46955.492\n",
      "    load_time_ms: 21.297\n",
      "    sample_throughput: 38.896\n",
      "    sample_time_ms: 25709.787\n",
      "    update_time_ms: 5.15\n",
      "  timestamp: 1634853571\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 373000\n",
      "  training_iteration: 373\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   373</td><td style=\"text-align: right;\">         10343.6</td><td style=\"text-align: right;\">373000</td><td style=\"text-align: right;\"> -3.5035</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            350.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 374000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_21-59-55\n",
      "  done: false\n",
      "  episode_len_mean: 353.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.5325999999999693\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1033\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8771362278196546\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011820098370184015\n",
      "          policy_loss: -0.05915937076012293\n",
      "          total_loss: -0.06470738483799829\n",
      "          vf_explained_var: 0.6784968972206116\n",
      "          vf_loss: 0.008674168882943275\n",
      "    num_agent_steps_sampled: 374000\n",
      "    num_agent_steps_trained: 374000\n",
      "    num_steps_sampled: 374000\n",
      "    num_steps_trained: 374000\n",
      "  iterations_since_restore: 374\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.20882352941176\n",
      "    ram_util_percent: 47.002941176470586\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039974094958227034\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.21323120749012\n",
      "    mean_inference_ms: 1.746757698540295\n",
      "    mean_raw_obs_processing_ms: 1.7871263443852525\n",
      "  time_since_restore: 10367.49719119072\n",
      "  time_this_iter_s: 23.86629343032837\n",
      "  time_total_s: 10367.49719119072\n",
      "  timers:\n",
      "    learn_throughput: 1189.148\n",
      "    learn_time_ms: 840.938\n",
      "    load_throughput: 49414.398\n",
      "    load_time_ms: 20.237\n",
      "    sample_throughput: 38.632\n",
      "    sample_time_ms: 25885.058\n",
      "    update_time_ms: 5.559\n",
      "  timestamp: 1634853595\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 374000\n",
      "  training_iteration: 374\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   374</td><td style=\"text-align: right;\">         10367.5</td><td style=\"text-align: right;\">374000</td><td style=\"text-align: right;\"> -3.5326</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            353.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 375000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-00-21\n",
      "  done: false\n",
      "  episode_len_mean: 356.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.569199999999968\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1036\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.929867864979638\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012369130159260472\n",
      "          policy_loss: 0.0397644837697347\n",
      "          total_loss: 0.033049558848142625\n",
      "          vf_explained_var: 0.5478871464729309\n",
      "          vf_loss: 0.00782327333258258\n",
      "    num_agent_steps_sampled: 375000\n",
      "    num_agent_steps_trained: 375000\n",
      "    num_steps_sampled: 375000\n",
      "    num_steps_trained: 375000\n",
      "  iterations_since_restore: 375\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.36486486486487\n",
      "    ram_util_percent: 47.03783783783783\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039976848427460375\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.21543164719139\n",
      "    mean_inference_ms: 1.7468486613216936\n",
      "    mean_raw_obs_processing_ms: 1.7868248103914783\n",
      "  time_since_restore: 10393.350989341736\n",
      "  time_this_iter_s: 25.853798151016235\n",
      "  time_total_s: 10393.350989341736\n",
      "  timers:\n",
      "    learn_throughput: 1189.083\n",
      "    learn_time_ms: 840.984\n",
      "    load_throughput: 48552.041\n",
      "    load_time_ms: 20.596\n",
      "    sample_throughput: 38.174\n",
      "    sample_time_ms: 26195.639\n",
      "    update_time_ms: 5.567\n",
      "  timestamp: 1634853621\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 375000\n",
      "  training_iteration: 375\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   375</td><td style=\"text-align: right;\">         10393.4</td><td style=\"text-align: right;\">375000</td><td style=\"text-align: right;\"> -3.5692</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            356.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-00-47\n",
      "  done: false\n",
      "  episode_len_mean: 357.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.578899999999967\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1038\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6077462156613669\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018115235309962764\n",
      "          policy_loss: -0.07655779756605625\n",
      "          total_loss: -0.07806090033716626\n",
      "          vf_explained_var: 0.7099959254264832\n",
      "          vf_loss: 0.007602388955031832\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_agent_steps_trained: 376000\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "  iterations_since_restore: 376\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.32894736842105\n",
      "    ram_util_percent: 47.1157894736842\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03997868089800269\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.216834270932416\n",
      "    mean_inference_ms: 1.746911354910468\n",
      "    mean_raw_obs_processing_ms: 1.7865707012776368\n",
      "  time_since_restore: 10420.003589391708\n",
      "  time_this_iter_s: 26.652600049972534\n",
      "  time_total_s: 10420.003589391708\n",
      "  timers:\n",
      "    learn_throughput: 1190.573\n",
      "    learn_time_ms: 839.932\n",
      "    load_throughput: 46101.385\n",
      "    load_time_ms: 21.691\n",
      "    sample_throughput: 37.467\n",
      "    sample_time_ms: 26690.418\n",
      "    update_time_ms: 5.361\n",
      "  timestamp: 1634853647\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 376\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   376</td><td style=\"text-align: right;\">           10420</td><td style=\"text-align: right;\">376000</td><td style=\"text-align: right;\"> -3.5789</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            357.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 377000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-01-12\n",
      "  done: false\n",
      "  episode_len_mean: 362.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.621999999999966\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1041\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3848676770925521\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7913573768403794\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02444350347084817\n",
      "          policy_loss: -0.05725339021947649\n",
      "          total_loss: -0.054696999821397994\n",
      "          vf_explained_var: 0.5283967852592468\n",
      "          vf_loss: 0.011062446962265918\n",
      "    num_agent_steps_sampled: 377000\n",
      "    num_agent_steps_trained: 377000\n",
      "    num_steps_sampled: 377000\n",
      "    num_steps_trained: 377000\n",
      "  iterations_since_restore: 377\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.32499999999999\n",
      "    ram_util_percent: 47.144444444444446\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03998178798132358\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.218220110666742\n",
      "    mean_inference_ms: 1.747003071967538\n",
      "    mean_raw_obs_processing_ms: 1.786203271969366\n",
      "  time_since_restore: 10445.21423625946\n",
      "  time_this_iter_s: 25.210646867752075\n",
      "  time_total_s: 10445.21423625946\n",
      "  timers:\n",
      "    learn_throughput: 1183.74\n",
      "    learn_time_ms: 844.78\n",
      "    load_throughput: 44069.156\n",
      "    load_time_ms: 22.692\n",
      "    sample_throughput: 37.509\n",
      "    sample_time_ms: 26660.355\n",
      "    update_time_ms: 5.257\n",
      "  timestamp: 1634853672\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 377000\n",
      "  training_iteration: 377\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   377</td><td style=\"text-align: right;\">         10445.2</td><td style=\"text-align: right;\">377000</td><td style=\"text-align: right;\">  -3.622</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">             362.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 378000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-01-32\n",
      "  done: false\n",
      "  episode_len_mean: 365.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.654999999999966\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1043\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5773015156388283\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.949320826265547\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012143018773627043\n",
      "          policy_loss: 0.09334961589839723\n",
      "          total_loss: 0.08249318185779783\n",
      "          vf_explained_var: 0.039603959769010544\n",
      "          vf_loss: 0.0016265882996473616\n",
      "    num_agent_steps_sampled: 378000\n",
      "    num_agent_steps_trained: 378000\n",
      "    num_steps_sampled: 378000\n",
      "    num_steps_trained: 378000\n",
      "  iterations_since_restore: 378\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.60714285714286\n",
      "    ram_util_percent: 47.135714285714286\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0399837786955128\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.218493662558263\n",
      "    mean_inference_ms: 1.747061244311653\n",
      "    mean_raw_obs_processing_ms: 1.7859633171255838\n",
      "  time_since_restore: 10464.963916063309\n",
      "  time_this_iter_s: 19.749679803848267\n",
      "  time_total_s: 10464.963916063309\n",
      "  timers:\n",
      "    learn_throughput: 1181.533\n",
      "    learn_time_ms: 846.358\n",
      "    load_throughput: 44019.298\n",
      "    load_time_ms: 22.717\n",
      "    sample_throughput: 39.034\n",
      "    sample_time_ms: 25618.494\n",
      "    update_time_ms: 6.153\n",
      "  timestamp: 1634853692\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 378000\n",
      "  training_iteration: 378\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   378</td><td style=\"text-align: right;\">           10465</td><td style=\"text-align: right;\">378000</td><td style=\"text-align: right;\">  -3.655</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">             365.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 379000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-02-00\n",
      "  done: false\n",
      "  episode_len_mean: 367.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.6701999999999657\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1046\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5773015156388283\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.621554426352183\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012747177627307633\n",
      "          policy_loss: 0.02003323518567615\n",
      "          total_loss: 0.014426064325703515\n",
      "          vf_explained_var: 0.8454005718231201\n",
      "          vf_loss: 0.003249409624064962\n",
      "    num_agent_steps_sampled: 379000\n",
      "    num_agent_steps_trained: 379000\n",
      "    num_steps_sampled: 379000\n",
      "    num_steps_trained: 379000\n",
      "  iterations_since_restore: 379\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 83.6051282051282\n",
      "    ram_util_percent: 47.07692307692307\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039986829021134464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.218753186508597\n",
      "    mean_inference_ms: 1.7471496554580324\n",
      "    mean_raw_obs_processing_ms: 1.7855276062719796\n",
      "  time_since_restore: 10492.532967805862\n",
      "  time_this_iter_s: 27.56905174255371\n",
      "  time_total_s: 10492.532967805862\n",
      "  timers:\n",
      "    learn_throughput: 1180.466\n",
      "    learn_time_ms: 847.123\n",
      "    load_throughput: 42729.171\n",
      "    load_time_ms: 23.403\n",
      "    sample_throughput: 40.733\n",
      "    sample_time_ms: 24550.251\n",
      "    update_time_ms: 5.999\n",
      "  timestamp: 1634853720\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 379000\n",
      "  training_iteration: 379\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   379</td><td style=\"text-align: right;\">         10492.5</td><td style=\"text-align: right;\">379000</td><td style=\"text-align: right;\"> -3.6702</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            367.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 380000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-02-23\n",
      "  done: false\n",
      "  episode_len_mean: 370.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.704799999999965\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1048\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5773015156388283\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8552795052528381\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012630466313919264\n",
      "          policy_loss: -0.10213351382149591\n",
      "          total_loss: -0.10257253862089581\n",
      "          vf_explained_var: -0.06017925962805748\n",
      "          vf_loss: 0.010822186266563626\n",
      "    num_agent_steps_sampled: 380000\n",
      "    num_agent_steps_trained: 380000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "  iterations_since_restore: 380\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.20909090909092\n",
      "    ram_util_percent: 47.00303030303031\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03998884008650947\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.218404341204703\n",
      "    mean_inference_ms: 1.747209378357161\n",
      "    mean_raw_obs_processing_ms: 1.7852143324516094\n",
      "  time_since_restore: 10515.296978473663\n",
      "  time_this_iter_s: 22.764010667800903\n",
      "  time_total_s: 10515.296978473663\n",
      "  timers:\n",
      "    learn_throughput: 1180.262\n",
      "    learn_time_ms: 847.269\n",
      "    load_throughput: 40465.679\n",
      "    load_time_ms: 24.712\n",
      "    sample_throughput: 41.104\n",
      "    sample_time_ms: 24328.371\n",
      "    update_time_ms: 5.006\n",
      "  timestamp: 1634853743\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 380\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   380</td><td style=\"text-align: right;\">         10515.3</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\"> -3.7048</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            370.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 381000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-03-02\n",
      "  done: false\n",
      "  episode_len_mean: 373.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.738799999999965\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1050\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5773015156388283\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6712515619066026\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01003045331401727\n",
      "          policy_loss: -0.09730363504754172\n",
      "          total_loss: -0.09493458304140302\n",
      "          vf_explained_var: -0.09139148890972137\n",
      "          vf_loss: 0.013290979322563443\n",
      "    num_agent_steps_sampled: 381000\n",
      "    num_agent_steps_trained: 381000\n",
      "    num_steps_sampled: 381000\n",
      "    num_steps_trained: 381000\n",
      "  iterations_since_restore: 381\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.38214285714285\n",
      "    ram_util_percent: 47.017857142857146\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03999081340409843\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.217866733284882\n",
      "    mean_inference_ms: 1.7472701770075576\n",
      "    mean_raw_obs_processing_ms: 1.7857919270078018\n",
      "  time_since_restore: 10554.268176317215\n",
      "  time_this_iter_s: 38.971197843551636\n",
      "  time_total_s: 10554.268176317215\n",
      "  timers:\n",
      "    learn_throughput: 1179.794\n",
      "    learn_time_ms: 847.606\n",
      "    load_throughput: 40978.889\n",
      "    load_time_ms: 24.403\n",
      "    sample_throughput: 38.767\n",
      "    sample_time_ms: 25795.4\n",
      "    update_time_ms: 5.205\n",
      "  timestamp: 1634853782\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 381000\n",
      "  training_iteration: 381\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   381</td><td style=\"text-align: right;\">         10554.3</td><td style=\"text-align: right;\">381000</td><td style=\"text-align: right;\"> -3.7388</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            373.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 382000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-03-28\n",
      "  done: false\n",
      "  episode_len_mean: 374.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.7492999999999643\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1052\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5773015156388283\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6363167193200854\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010197643857074942\n",
      "          policy_loss: -0.08649659852186839\n",
      "          total_loss: -0.08597833630111483\n",
      "          vf_explained_var: 0.40708300471305847\n",
      "          vf_loss: 0.010994315730770015\n",
      "    num_agent_steps_sampled: 382000\n",
      "    num_agent_steps_trained: 382000\n",
      "    num_steps_sampled: 382000\n",
      "    num_steps_trained: 382000\n",
      "  iterations_since_restore: 382\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.27027027027026\n",
      "    ram_util_percent: 46.83513513513514\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039992797656712445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.217177990737138\n",
      "    mean_inference_ms: 1.7473326295728566\n",
      "    mean_raw_obs_processing_ms: 1.7863297027870835\n",
      "  time_since_restore: 10580.173643112183\n",
      "  time_this_iter_s: 25.90546679496765\n",
      "  time_total_s: 10580.173643112183\n",
      "  timers:\n",
      "    learn_throughput: 1184.328\n",
      "    learn_time_ms: 844.361\n",
      "    load_throughput: 41013.55\n",
      "    load_time_ms: 24.382\n",
      "    sample_throughput: 39.021\n",
      "    sample_time_ms: 25627.505\n",
      "    update_time_ms: 4.669\n",
      "  timestamp: 1634853808\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 382000\n",
      "  training_iteration: 382\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   382</td><td style=\"text-align: right;\">         10580.2</td><td style=\"text-align: right;\">382000</td><td style=\"text-align: right;\"> -3.7493</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            374.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 383000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-03-53\n",
      "  done: false\n",
      "  episode_len_mean: 378.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.7895999999999628\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1055\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5773015156388283\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6594468818770514\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011685769907709068\n",
      "          policy_loss: -0.03559779359234704\n",
      "          total_loss: -0.03367801043722365\n",
      "          vf_explained_var: -0.02389957383275032\n",
      "          vf_loss: 0.0117680392616118\n",
      "    num_agent_steps_sampled: 383000\n",
      "    num_agent_steps_trained: 383000\n",
      "    num_steps_sampled: 383000\n",
      "    num_steps_trained: 383000\n",
      "  iterations_since_restore: 383\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.01142857142857\n",
      "    ram_util_percent: 46.92857142857142\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039995892485700166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.215824533571887\n",
      "    mean_inference_ms: 1.7474284505964233\n",
      "    mean_raw_obs_processing_ms: 1.7870801340355207\n",
      "  time_since_restore: 10605.096381425858\n",
      "  time_this_iter_s: 24.922738313674927\n",
      "  time_total_s: 10605.096381425858\n",
      "  timers:\n",
      "    learn_throughput: 1159.933\n",
      "    learn_time_ms: 862.119\n",
      "    load_throughput: 40506.673\n",
      "    load_time_ms: 24.687\n",
      "    sample_throughput: 39.604\n",
      "    sample_time_ms: 25249.903\n",
      "    update_time_ms: 4.73\n",
      "  timestamp: 1634853833\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 383000\n",
      "  training_iteration: 383\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   383</td><td style=\"text-align: right;\">         10605.1</td><td style=\"text-align: right;\">383000</td><td style=\"text-align: right;\"> -3.7896</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            378.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 384000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-04-17\n",
      "  done: false\n",
      "  episode_len_mean: 378.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.7805999999999638\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1057\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5773015156388283\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4860945463180542\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011010705778383133\n",
      "          policy_loss: -0.13094610141383278\n",
      "          total_loss: -0.1321941163804796\n",
      "          vf_explained_var: 0.23250643908977509\n",
      "          vf_loss: 0.007256427986430935\n",
      "    num_agent_steps_sampled: 384000\n",
      "    num_agent_steps_trained: 384000\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "  iterations_since_restore: 384\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.33142857142857\n",
      "    ram_util_percent: 46.931428571428576\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03999797882746003\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.214956337417416\n",
      "    mean_inference_ms: 1.7474934986945159\n",
      "    mean_raw_obs_processing_ms: 1.7875885600945538\n",
      "  time_since_restore: 10629.546489238739\n",
      "  time_this_iter_s: 24.45010781288147\n",
      "  time_total_s: 10629.546489238739\n",
      "  timers:\n",
      "    learn_throughput: 1155.579\n",
      "    learn_time_ms: 865.367\n",
      "    load_throughput: 39004.989\n",
      "    load_time_ms: 25.638\n",
      "    sample_throughput: 39.519\n",
      "    sample_time_ms: 25304.176\n",
      "    update_time_ms: 4.734\n",
      "  timestamp: 1634853857\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 384\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   384</td><td style=\"text-align: right;\">         10629.5</td><td style=\"text-align: right;\">384000</td><td style=\"text-align: right;\"> -3.7806</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            378.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 385000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-04-39\n",
      "  done: false\n",
      "  episode_len_mean: 382.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.8285999999999625\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1059\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5773015156388283\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.468342161178589\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006869423040476224\n",
      "          policy_loss: -0.12043556206756167\n",
      "          total_loss: -0.12321148051155938\n",
      "          vf_explained_var: -0.061335258185863495\n",
      "          vf_loss: 0.007941774864674598\n",
      "    num_agent_steps_sampled: 385000\n",
      "    num_agent_steps_trained: 385000\n",
      "    num_steps_sampled: 385000\n",
      "    num_steps_trained: 385000\n",
      "  iterations_since_restore: 385\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.21935483870968\n",
      "    ram_util_percent: 47.00322580645161\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03999988009392541\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.213501848302293\n",
      "    mean_inference_ms: 1.7475583620770903\n",
      "    mean_raw_obs_processing_ms: 1.787129227366413\n",
      "  time_since_restore: 10651.388324022293\n",
      "  time_this_iter_s: 21.841834783554077\n",
      "  time_total_s: 10651.388324022293\n",
      "  timers:\n",
      "    learn_throughput: 1152.747\n",
      "    learn_time_ms: 867.493\n",
      "    load_throughput: 39045.512\n",
      "    load_time_ms: 25.611\n",
      "    sample_throughput: 40.16\n",
      "    sample_time_ms: 24900.687\n",
      "    update_time_ms: 4.952\n",
      "  timestamp: 1634853879\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 385000\n",
      "  training_iteration: 385\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   385</td><td style=\"text-align: right;\">         10651.4</td><td style=\"text-align: right;\">385000</td><td style=\"text-align: right;\"> -3.8286</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            382.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 386000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-05-02\n",
      "  done: false\n",
      "  episode_len_mean: 386.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.8630999999999616\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1061\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5773015156388283\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3435692164633009\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009354767362464243\n",
      "          policy_loss: -0.239135159076088\n",
      "          total_loss: -0.23569788922452264\n",
      "          vf_explained_var: -0.15688303112983704\n",
      "          vf_loss: 0.011472440076371033\n",
      "    num_agent_steps_sampled: 386000\n",
      "    num_agent_steps_trained: 386000\n",
      "    num_steps_sampled: 386000\n",
      "    num_steps_trained: 386000\n",
      "  iterations_since_restore: 386\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.42058823529412\n",
      "    ram_util_percent: 47.067647058823525\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04000191653819733\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.21193437532691\n",
      "    mean_inference_ms: 1.7476253318196258\n",
      "    mean_raw_obs_processing_ms: 1.7866337674338593\n",
      "  time_since_restore: 10674.969763040543\n",
      "  time_this_iter_s: 23.58143901824951\n",
      "  time_total_s: 10674.969763040543\n",
      "  timers:\n",
      "    learn_throughput: 1152.041\n",
      "    learn_time_ms: 868.025\n",
      "    load_throughput: 38807.653\n",
      "    load_time_ms: 25.768\n",
      "    sample_throughput: 40.662\n",
      "    sample_time_ms: 24592.916\n",
      "    update_time_ms: 4.956\n",
      "  timestamp: 1634853902\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 386000\n",
      "  training_iteration: 386\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   386</td><td style=\"text-align: right;\">           10675</td><td style=\"text-align: right;\">386000</td><td style=\"text-align: right;\"> -3.8631</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            386.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 387000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-05-27\n",
      "  done: false\n",
      "  episode_len_mean: 386.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.8664999999999616\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1064\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5773015156388283\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5553367959128486\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01515815708834512\n",
      "          policy_loss: -0.049435293757253224\n",
      "          total_loss: -0.048533051047060224\n",
      "          vf_explained_var: 0.7703943848609924\n",
      "          vf_loss: 0.007704782537702057\n",
      "    num_agent_steps_sampled: 387000\n",
      "    num_agent_steps_trained: 387000\n",
      "    num_steps_sampled: 387000\n",
      "    num_steps_trained: 387000\n",
      "  iterations_since_restore: 387\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.05714285714286\n",
      "    ram_util_percent: 47.091428571428565\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040004946152810764\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.209597877500126\n",
      "    mean_inference_ms: 1.7477284426566038\n",
      "    mean_raw_obs_processing_ms: 1.7858850250332374\n",
      "  time_since_restore: 10699.244752168655\n",
      "  time_this_iter_s: 24.274989128112793\n",
      "  time_total_s: 10699.244752168655\n",
      "  timers:\n",
      "    learn_throughput: 1154.436\n",
      "    learn_time_ms: 866.224\n",
      "    load_throughput: 40241.122\n",
      "    load_time_ms: 24.85\n",
      "    sample_throughput: 40.813\n",
      "    sample_time_ms: 24502.207\n",
      "    update_time_ms: 4.866\n",
      "  timestamp: 1634853927\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 387000\n",
      "  training_iteration: 387\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   387</td><td style=\"text-align: right;\">         10699.2</td><td style=\"text-align: right;\">387000</td><td style=\"text-align: right;\"> -3.8665</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            386.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 388000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-05-50\n",
      "  done: false\n",
      "  episode_len_mean: 387.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.8757999999999617\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1066\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5773015156388283\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4398066679636636\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00353152709143632\n",
      "          policy_loss: 0.054120354188813106\n",
      "          total_loss: 0.0487799185845587\n",
      "          vf_explained_var: 0.05956033989787102\n",
      "          vf_loss: 0.007018875993607152\n",
      "    num_agent_steps_sampled: 388000\n",
      "    num_agent_steps_trained: 388000\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "  iterations_since_restore: 388\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.334375\n",
      "    ram_util_percent: 47.184375\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04000689508148803\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.207897690118788\n",
      "    mean_inference_ms: 1.7477981119023043\n",
      "    mean_raw_obs_processing_ms: 1.785409054173015\n",
      "  time_since_restore: 10722.091357707977\n",
      "  time_this_iter_s: 22.8466055393219\n",
      "  time_total_s: 10722.091357707977\n",
      "  timers:\n",
      "    learn_throughput: 1157.934\n",
      "    learn_time_ms: 863.607\n",
      "    load_throughput: 41920.906\n",
      "    load_time_ms: 23.854\n",
      "    sample_throughput: 40.296\n",
      "    sample_time_ms: 24816.661\n",
      "    update_time_ms: 3.959\n",
      "  timestamp: 1634853950\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 388\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   388</td><td style=\"text-align: right;\">         10722.1</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\"> -3.8758</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            387.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 389000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-06-11\n",
      "  done: false\n",
      "  episode_len_mean: 390.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.905799999999961\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1068\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3859256426493327\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015130186540230758\n",
      "          policy_loss: -0.0911030489537451\n",
      "          total_loss: -0.09164729615052541\n",
      "          vf_explained_var: 0.29312267899513245\n",
      "          vf_loss: 0.008947667681301634\n",
      "    num_agent_steps_sampled: 389000\n",
      "    num_agent_steps_trained: 389000\n",
      "    num_steps_sampled: 389000\n",
      "    num_steps_trained: 389000\n",
      "  iterations_since_restore: 389\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.74838709677418\n",
      "    ram_util_percent: 47.22903225806452\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040008724129814904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.205835187163093\n",
      "    mean_inference_ms: 1.7478652579650742\n",
      "    mean_raw_obs_processing_ms: 1.7848949718738236\n",
      "  time_since_restore: 10743.928352355957\n",
      "  time_this_iter_s: 21.836994647979736\n",
      "  time_total_s: 10743.928352355957\n",
      "  timers:\n",
      "    learn_throughput: 1161.726\n",
      "    learn_time_ms: 860.788\n",
      "    load_throughput: 42964.83\n",
      "    load_time_ms: 23.275\n",
      "    sample_throughput: 41.243\n",
      "    sample_time_ms: 24246.821\n",
      "    update_time_ms: 4.003\n",
      "  timestamp: 1634853971\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 389000\n",
      "  training_iteration: 389\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   389</td><td style=\"text-align: right;\">         10743.9</td><td style=\"text-align: right;\">389000</td><td style=\"text-align: right;\"> -3.9058</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            390.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 390000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-06-36\n",
      "  done: false\n",
      "  episode_len_mean: 393.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.934199999999961\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1070\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5792150484191048\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013628601795348376\n",
      "          policy_loss: -0.09877026933762763\n",
      "          total_loss: -0.10183658541904556\n",
      "          vf_explained_var: 0.5471389889717102\n",
      "          vf_loss: 0.008791923819161537\n",
      "    num_agent_steps_sampled: 390000\n",
      "    num_agent_steps_trained: 390000\n",
      "    num_steps_sampled: 390000\n",
      "    num_steps_trained: 390000\n",
      "  iterations_since_restore: 390\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.38285714285715\n",
      "    ram_util_percent: 47.231428571428566\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04001060078801844\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.203638914796766\n",
      "    mean_inference_ms: 1.7479321245450223\n",
      "    mean_raw_obs_processing_ms: 1.7843456594124394\n",
      "  time_since_restore: 10768.040661334991\n",
      "  time_this_iter_s: 24.112308979034424\n",
      "  time_total_s: 10768.040661334991\n",
      "  timers:\n",
      "    learn_throughput: 1163.506\n",
      "    learn_time_ms: 859.471\n",
      "    load_throughput: 47515.625\n",
      "    load_time_ms: 21.046\n",
      "    sample_throughput: 41.008\n",
      "    sample_time_ms: 24385.621\n",
      "    update_time_ms: 3.631\n",
      "  timestamp: 1634853996\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 390000\n",
      "  training_iteration: 390\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   390</td><td style=\"text-align: right;\">           10768</td><td style=\"text-align: right;\">390000</td><td style=\"text-align: right;\"> -3.9342</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            393.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 391000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-07-02\n",
      "  done: false\n",
      "  episode_len_mean: 395.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.9546999999999604\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1073\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4270703064070807\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014043321441481875\n",
      "          policy_loss: -0.07405723796950446\n",
      "          total_loss: -0.07356790279348692\n",
      "          vf_explained_var: 0.3984074890613556\n",
      "          vf_loss: 0.01070642231627264\n",
      "    num_agent_steps_sampled: 391000\n",
      "    num_agent_steps_trained: 391000\n",
      "    num_steps_sampled: 391000\n",
      "    num_steps_trained: 391000\n",
      "  iterations_since_restore: 391\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.35000000000001\n",
      "    ram_util_percent: 47.14210526315789\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04001337859959741\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.20032256284843\n",
      "    mean_inference_ms: 1.7480316985488753\n",
      "    mean_raw_obs_processing_ms: 1.783519467786482\n",
      "  time_since_restore: 10794.420164108276\n",
      "  time_this_iter_s: 26.379502773284912\n",
      "  time_total_s: 10794.420164108276\n",
      "  timers:\n",
      "    learn_throughput: 1158.055\n",
      "    learn_time_ms: 863.517\n",
      "    load_throughput: 46663.314\n",
      "    load_time_ms: 21.43\n",
      "    sample_throughput: 43.248\n",
      "    sample_time_ms: 23122.516\n",
      "    update_time_ms: 3.566\n",
      "  timestamp: 1634854022\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 391000\n",
      "  training_iteration: 391\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   391</td><td style=\"text-align: right;\">         10794.4</td><td style=\"text-align: right;\">391000</td><td style=\"text-align: right;\"> -3.9547</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            395.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-07-29\n",
      "  done: false\n",
      "  episode_len_mean: 392.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.92469999999996\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1076\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7226728399594624\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018748460727526232\n",
      "          policy_loss: 0.009821708169248369\n",
      "          total_loss: 0.0038175636695490945\n",
      "          vf_explained_var: 0.6509382128715515\n",
      "          vf_loss: 0.005810822158431013\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "  iterations_since_restore: 392\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.16578947368421\n",
      "    ram_util_percent: 47.1\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04001607130335931\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.197335478155047\n",
      "    mean_inference_ms: 1.7481316888762783\n",
      "    mean_raw_obs_processing_ms: 1.7827538909372462\n",
      "  time_since_restore: 10821.609463691711\n",
      "  time_this_iter_s: 27.18929958343506\n",
      "  time_total_s: 10821.609463691711\n",
      "  timers:\n",
      "    learn_throughput: 1156.382\n",
      "    learn_time_ms: 864.766\n",
      "    load_throughput: 47587.919\n",
      "    load_time_ms: 21.014\n",
      "    sample_throughput: 43.01\n",
      "    sample_time_ms: 23250.276\n",
      "    update_time_ms: 3.388\n",
      "  timestamp: 1634854049\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 392\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   392</td><td style=\"text-align: right;\">         10821.6</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\"> -3.9247</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            392.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 393000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-07-54\n",
      "  done: false\n",
      "  episode_len_mean: 393.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.9327999999999603\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1078\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3890980376137627\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01774346840804605\n",
      "          policy_loss: -0.08109326064586639\n",
      "          total_loss: -0.08414348214864731\n",
      "          vf_explained_var: 0.7805584669113159\n",
      "          vf_loss: 0.0057190936019954584\n",
      "    num_agent_steps_sampled: 393000\n",
      "    num_agent_steps_trained: 393000\n",
      "    num_steps_sampled: 393000\n",
      "    num_steps_trained: 393000\n",
      "  iterations_since_restore: 393\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.7888888888889\n",
      "    ram_util_percent: 47.04722222222222\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040017833884061264\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.195290419164976\n",
      "    mean_inference_ms: 1.7481980506480597\n",
      "    mean_raw_obs_processing_ms: 1.7822368051547925\n",
      "  time_since_restore: 10846.58495593071\n",
      "  time_this_iter_s: 24.975492238998413\n",
      "  time_total_s: 10846.58495593071\n",
      "  timers:\n",
      "    learn_throughput: 1177.553\n",
      "    learn_time_ms: 849.219\n",
      "    load_throughput: 48294.101\n",
      "    load_time_ms: 20.706\n",
      "    sample_throughput: 42.971\n",
      "    sample_time_ms: 23271.542\n",
      "    update_time_ms: 3.339\n",
      "  timestamp: 1634854074\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 393000\n",
      "  training_iteration: 393\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   393</td><td style=\"text-align: right;\">         10846.6</td><td style=\"text-align: right;\">393000</td><td style=\"text-align: right;\"> -3.9328</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            393.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 394000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-08-40\n",
      "  done: false\n",
      "  episode_len_mean: 394.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.9435999999999605\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1081\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.416101172235277\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012884707000500623\n",
      "          policy_loss: -0.08852002148826917\n",
      "          total_loss: -0.0919800022410022\n",
      "          vf_explained_var: 0.7608373165130615\n",
      "          vf_loss: 0.006981849931697879\n",
      "    num_agent_steps_sampled: 394000\n",
      "    num_agent_steps_trained: 394000\n",
      "    num_steps_sampled: 394000\n",
      "    num_steps_trained: 394000\n",
      "  iterations_since_restore: 394\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.58923076923075\n",
      "    ram_util_percent: 47.0076923076923\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04002043041541623\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.19220604302205\n",
      "    mean_inference_ms: 1.74829659516736\n",
      "    mean_raw_obs_processing_ms: 1.7827527883366028\n",
      "  time_since_restore: 10891.870923042297\n",
      "  time_this_iter_s: 45.285967111587524\n",
      "  time_total_s: 10891.870923042297\n",
      "  timers:\n",
      "    learn_throughput: 1180.564\n",
      "    learn_time_ms: 847.053\n",
      "    load_throughput: 50190.973\n",
      "    load_time_ms: 19.924\n",
      "    sample_throughput: 39.435\n",
      "    sample_time_ms: 25358.197\n",
      "    update_time_ms: 3.059\n",
      "  timestamp: 1634854120\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 394000\n",
      "  training_iteration: 394\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   394</td><td style=\"text-align: right;\">         10891.9</td><td style=\"text-align: right;\">394000</td><td style=\"text-align: right;\"> -3.9436</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            394.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 395000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-09-06\n",
      "  done: false\n",
      "  episode_len_mean: 395.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.959199999999959\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1084\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3496727877193027\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011434135812122362\n",
      "          policy_loss: 0.05441803832848867\n",
      "          total_loss: 0.053306380907694496\n",
      "          vf_explained_var: -0.03438631445169449\n",
      "          vf_loss: 0.009084597944618307\n",
      "    num_agent_steps_sampled: 395000\n",
      "    num_agent_steps_trained: 395000\n",
      "    num_steps_sampled: 395000\n",
      "    num_steps_trained: 395000\n",
      "  iterations_since_restore: 395\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.02105263157897\n",
      "    ram_util_percent: 46.997368421052634\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040022997443054056\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.188923968655082\n",
      "    mean_inference_ms: 1.7483961917873156\n",
      "    mean_raw_obs_processing_ms: 1.7832796300196743\n",
      "  time_since_restore: 10918.71556687355\n",
      "  time_this_iter_s: 26.84464383125305\n",
      "  time_total_s: 10918.71556687355\n",
      "  timers:\n",
      "    learn_throughput: 1184.18\n",
      "    learn_time_ms: 844.466\n",
      "    load_throughput: 50832.408\n",
      "    load_time_ms: 19.672\n",
      "    sample_throughput: 38.667\n",
      "    sample_time_ms: 25861.523\n",
      "    update_time_ms: 2.864\n",
      "  timestamp: 1634854146\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 395000\n",
      "  training_iteration: 395\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   395</td><td style=\"text-align: right;\">         10918.7</td><td style=\"text-align: right;\">395000</td><td style=\"text-align: right;\"> -3.9592</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            395.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 396000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-09-36\n",
      "  done: false\n",
      "  episode_len_mean: 395.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.3199999999999945\n",
      "  episode_reward_mean: -3.95649999999996\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1087\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.466530481974284\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015238381199595372\n",
      "          policy_loss: -0.06667860274513562\n",
      "          total_loss: -0.07143800622887081\n",
      "          vf_explained_var: 0.7940515279769897\n",
      "          vf_loss: 0.005507331888657064\n",
      "    num_agent_steps_sampled: 396000\n",
      "    num_agent_steps_trained: 396000\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "  iterations_since_restore: 396\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.90238095238095\n",
      "    ram_util_percent: 47.01428571428572\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0400256088550586\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.185473259374543\n",
      "    mean_inference_ms: 1.7484964064914548\n",
      "    mean_raw_obs_processing_ms: 1.7838183589851042\n",
      "  time_since_restore: 10948.148602247238\n",
      "  time_this_iter_s: 29.433035373687744\n",
      "  time_total_s: 10948.148602247238\n",
      "  timers:\n",
      "    learn_throughput: 1186.637\n",
      "    learn_time_ms: 842.718\n",
      "    load_throughput: 54434.296\n",
      "    load_time_ms: 18.371\n",
      "    sample_throughput: 37.808\n",
      "    sample_time_ms: 26449.266\n",
      "    update_time_ms: 3.26\n",
      "  timestamp: 1634854176\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 396\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   396</td><td style=\"text-align: right;\">         10948.1</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\"> -3.9565</td><td style=\"text-align: right;\">               -2.32</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            395.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 397000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-10-04\n",
      "  done: false\n",
      "  episode_len_mean: 398.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5199999999999902\n",
      "  episode_reward_mean: -3.98399999999996\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1090\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6275327112939624\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017214298784183294\n",
      "          policy_loss: -0.04628779101702902\n",
      "          total_loss: -0.05286631625559595\n",
      "          vf_explained_var: 0.8291057348251343\n",
      "          vf_loss: 0.004727884090971202\n",
      "    num_agent_steps_sampled: 397000\n",
      "    num_agent_steps_trained: 397000\n",
      "    num_steps_sampled: 397000\n",
      "    num_steps_trained: 397000\n",
      "  iterations_since_restore: 397\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.8875\n",
      "    ram_util_percent: 46.96\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040028190945012726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.181596909738474\n",
      "    mean_inference_ms: 1.7485955121051635\n",
      "    mean_raw_obs_processing_ms: 1.7838955428001497\n",
      "  time_since_restore: 10976.12563419342\n",
      "  time_this_iter_s: 27.97703194618225\n",
      "  time_total_s: 10976.12563419342\n",
      "  timers:\n",
      "    learn_throughput: 1185.462\n",
      "    learn_time_ms: 843.553\n",
      "    load_throughput: 54642.433\n",
      "    load_time_ms: 18.301\n",
      "    sample_throughput: 37.287\n",
      "    sample_time_ms: 26818.784\n",
      "    update_time_ms: 3.237\n",
      "  timestamp: 1634854204\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 397000\n",
      "  training_iteration: 397\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   397</td><td style=\"text-align: right;\">         10976.1</td><td style=\"text-align: right;\">397000</td><td style=\"text-align: right;\">  -3.984</td><td style=\"text-align: right;\">               -2.52</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">             398.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 398000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-10-29\n",
      "  done: false\n",
      "  episode_len_mean: 401.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.619999999999988\n",
      "  episode_reward_mean: -4.010899999999959\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1093\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2886507578194141\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6719603604740567\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020698033137818723\n",
      "          policy_loss: -0.023839137620396085\n",
      "          total_loss: -0.026444232794973585\n",
      "          vf_explained_var: 0.5039684772491455\n",
      "          vf_loss: 0.00814001028193161\n",
      "    num_agent_steps_sampled: 398000\n",
      "    num_agent_steps_trained: 398000\n",
      "    num_steps_sampled: 398000\n",
      "    num_steps_trained: 398000\n",
      "  iterations_since_restore: 398\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.78333333333333\n",
      "    ram_util_percent: 46.975\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040030694927525125\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.177274224327643\n",
      "    mean_inference_ms: 1.7486906863349063\n",
      "    mean_raw_obs_processing_ms: 1.7829930471893949\n",
      "  time_since_restore: 11001.132286787033\n",
      "  time_this_iter_s: 25.00665259361267\n",
      "  time_total_s: 11001.132286787033\n",
      "  timers:\n",
      "    learn_throughput: 1180.093\n",
      "    learn_time_ms: 847.391\n",
      "    load_throughput: 51850.024\n",
      "    load_time_ms: 19.286\n",
      "    sample_throughput: 36.997\n",
      "    sample_time_ms: 27029.062\n",
      "    update_time_ms: 4.058\n",
      "  timestamp: 1634854229\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 398000\n",
      "  training_iteration: 398\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   398</td><td style=\"text-align: right;\">         11001.1</td><td style=\"text-align: right;\">398000</td><td style=\"text-align: right;\"> -4.0109</td><td style=\"text-align: right;\">               -2.62</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            401.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 399000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-10-55\n",
      "  done: false\n",
      "  episode_len_mean: 399.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.619999999999988\n",
      "  episode_reward_mean: -3.993699999999959\n",
      "  episode_reward_min: -5.35999999999993\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1095\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4329761367291214\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6331710232628716\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013505706245603101\n",
      "          policy_loss: -0.10825737234618929\n",
      "          total_loss: -0.11060592813624276\n",
      "          vf_explained_var: 0.5191429853439331\n",
      "          vf_loss: 0.008135502945838704\n",
      "    num_agent_steps_sampled: 399000\n",
      "    num_agent_steps_trained: 399000\n",
      "    num_steps_sampled: 399000\n",
      "    num_steps_trained: 399000\n",
      "  iterations_since_restore: 399\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.2861111111111\n",
      "    ram_util_percent: 47.036111111111104\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040032334374644984\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.174353066824533\n",
      "    mean_inference_ms: 1.7487521847333327\n",
      "    mean_raw_obs_processing_ms: 1.7823990995866177\n",
      "  time_since_restore: 11026.851419448853\n",
      "  time_this_iter_s: 25.719132661819458\n",
      "  time_total_s: 11026.851419448853\n",
      "  timers:\n",
      "    learn_throughput: 1177.418\n",
      "    learn_time_ms: 849.316\n",
      "    load_throughput: 54835.603\n",
      "    load_time_ms: 18.236\n",
      "    sample_throughput: 36.475\n",
      "    sample_time_ms: 27416.22\n",
      "    update_time_ms: 4.464\n",
      "  timestamp: 1634854255\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 399000\n",
      "  training_iteration: 399\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">         11026.9</td><td style=\"text-align: right;\">399000</td><td style=\"text-align: right;\"> -3.9937</td><td style=\"text-align: right;\">               -2.62</td><td style=\"text-align: right;\">               -5.36</td><td style=\"text-align: right;\">            399.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-11-22\n",
      "  done: false\n",
      "  episode_len_mean: 399.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7899999999999845\n",
      "  episode_reward_mean: -3.99029999999996\n",
      "  episode_reward_min: -5.279999999999932\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1098\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4329761367291214\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3312071561813354\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009756885246773175\n",
      "          policy_loss: -0.11729112010863092\n",
      "          total_loss: -0.11893166510595216\n",
      "          vf_explained_var: 0.7531275153160095\n",
      "          vf_loss: 0.007447028621875991\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_agent_steps_trained: 400000\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "  iterations_since_restore: 400\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.8102564102564\n",
      "    ram_util_percent: 47.00769230769231\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040034875438077384\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.17012499642991\n",
      "    mean_inference_ms: 1.7488444941241854\n",
      "    mean_raw_obs_processing_ms: 1.781482034628704\n",
      "  time_since_restore: 11054.16553735733\n",
      "  time_this_iter_s: 27.314117908477783\n",
      "  time_total_s: 11054.16553735733\n",
      "  timers:\n",
      "    learn_throughput: 1177.404\n",
      "    learn_time_ms: 849.326\n",
      "    load_throughput: 54742.634\n",
      "    load_time_ms: 18.267\n",
      "    sample_throughput: 36.055\n",
      "    sample_time_ms: 27735.389\n",
      "    update_time_ms: 5.323\n",
      "  timestamp: 1634854282\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 400\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   400</td><td style=\"text-align: right;\">         11054.2</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\"> -3.9903</td><td style=\"text-align: right;\">               -2.79</td><td style=\"text-align: right;\">               -5.28</td><td style=\"text-align: right;\">            399.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 401000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-11-50\n",
      "  done: false\n",
      "  episode_len_mean: 396.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7899999999999845\n",
      "  episode_reward_mean: -3.963399999999959\n",
      "  episode_reward_min: -5.279999999999932\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1101\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4329761367291214\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5281688226593866\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011814995008175956\n",
      "          policy_loss: -0.12278973346369135\n",
      "          total_loss: -0.12580101461046272\n",
      "          vf_explained_var: 0.7717520594596863\n",
      "          vf_loss: 0.007154795889639192\n",
      "    num_agent_steps_sampled: 401000\n",
      "    num_agent_steps_trained: 401000\n",
      "    num_steps_sampled: 401000\n",
      "    num_steps_trained: 401000\n",
      "  iterations_since_restore: 401\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.8\n",
      "    ram_util_percent: 46.94500000000001\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04003721065569287\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.166483251679598\n",
      "    mean_inference_ms: 1.7489309693930803\n",
      "    mean_raw_obs_processing_ms: 1.7806670173072043\n",
      "  time_since_restore: 11081.997693777084\n",
      "  time_this_iter_s: 27.83215641975403\n",
      "  time_total_s: 11081.997693777084\n",
      "  timers:\n",
      "    learn_throughput: 1179.883\n",
      "    learn_time_ms: 847.542\n",
      "    load_throughput: 55513.035\n",
      "    load_time_ms: 18.014\n",
      "    sample_throughput: 35.865\n",
      "    sample_time_ms: 27882.416\n",
      "    update_time_ms: 5.495\n",
      "  timestamp: 1634854310\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 401000\n",
      "  training_iteration: 401\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   401</td><td style=\"text-align: right;\">           11082</td><td style=\"text-align: right;\">401000</td><td style=\"text-align: right;\"> -3.9634</td><td style=\"text-align: right;\">               -2.79</td><td style=\"text-align: right;\">               -5.28</td><td style=\"text-align: right;\">            396.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 402000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-12-13\n",
      "  done: false\n",
      "  episode_len_mean: 395.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7899999999999845\n",
      "  episode_reward_mean: -3.9517999999999605\n",
      "  episode_reward_min: -5.279999999999932\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1104\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4329761367291214\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.660051989555359\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02009639559066114\n",
      "          policy_loss: -0.03610424689120716\n",
      "          total_loss: -0.03734600692987442\n",
      "          vf_explained_var: 0.7176819443702698\n",
      "          vf_loss: 0.0066575024276971815\n",
      "    num_agent_steps_sampled: 402000\n",
      "    num_agent_steps_trained: 402000\n",
      "    num_steps_sampled: 402000\n",
      "    num_steps_trained: 402000\n",
      "  iterations_since_restore: 402\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.84848484848484\n",
      "    ram_util_percent: 46.9969696969697\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04003952317156241\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.16281886748579\n",
      "    mean_inference_ms: 1.7490146746481319\n",
      "    mean_raw_obs_processing_ms: 1.7798648611019798\n",
      "  time_since_restore: 11104.81418800354\n",
      "  time_this_iter_s: 22.81649422645569\n",
      "  time_total_s: 11104.81418800354\n",
      "  timers:\n",
      "    learn_throughput: 1179.305\n",
      "    learn_time_ms: 847.957\n",
      "    load_throughput: 55589.552\n",
      "    load_time_ms: 17.989\n",
      "    sample_throughput: 36.437\n",
      "    sample_time_ms: 27444.592\n",
      "    update_time_ms: 5.373\n",
      "  timestamp: 1634854333\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 402000\n",
      "  training_iteration: 402\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   402</td><td style=\"text-align: right;\">         11104.8</td><td style=\"text-align: right;\">402000</td><td style=\"text-align: right;\"> -3.9518</td><td style=\"text-align: right;\">               -2.79</td><td style=\"text-align: right;\">               -5.28</td><td style=\"text-align: right;\">            395.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 403000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-12-36\n",
      "  done: false\n",
      "  episode_len_mean: 395.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7899999999999845\n",
      "  episode_reward_mean: -3.95819999999996\n",
      "  episode_reward_min: -5.279999999999932\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1106\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6494642050936816\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6152593149079217\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01154371346467021\n",
      "          policy_loss: -0.02511364064282841\n",
      "          total_loss: -0.026310678157541487\n",
      "          vf_explained_var: 0.651487410068512\n",
      "          vf_loss: 0.007458328861846692\n",
      "    num_agent_steps_sampled: 403000\n",
      "    num_agent_steps_trained: 403000\n",
      "    num_steps_sampled: 403000\n",
      "    num_steps_trained: 403000\n",
      "  iterations_since_restore: 403\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.07575757575756\n",
      "    ram_util_percent: 47.15454545454545\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04004103990119203\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.160434758990146\n",
      "    mean_inference_ms: 1.749068863856549\n",
      "    mean_raw_obs_processing_ms: 1.7793540135836952\n",
      "  time_since_restore: 11128.108217000961\n",
      "  time_this_iter_s: 23.294028997421265\n",
      "  time_total_s: 11128.108217000961\n",
      "  timers:\n",
      "    learn_throughput: 1176.696\n",
      "    learn_time_ms: 849.837\n",
      "    load_throughput: 54699.442\n",
      "    load_time_ms: 18.282\n",
      "    sample_throughput: 36.665\n",
      "    sample_time_ms: 27273.715\n",
      "    update_time_ms: 5.737\n",
      "  timestamp: 1634854356\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 403000\n",
      "  training_iteration: 403\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   403</td><td style=\"text-align: right;\">         11128.1</td><td style=\"text-align: right;\">403000</td><td style=\"text-align: right;\"> -3.9582</td><td style=\"text-align: right;\">               -2.79</td><td style=\"text-align: right;\">               -5.28</td><td style=\"text-align: right;\">            395.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 404000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-13-06\n",
      "  done: false\n",
      "  episode_len_mean: 392.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7899999999999845\n",
      "  episode_reward_mean: -3.9227999999999605\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1109\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6494642050936816\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5279923571480645\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012666191587752944\n",
      "          policy_loss: -0.008661607570118374\n",
      "          total_loss: -0.009957802461253272\n",
      "          vf_explained_var: 0.7605920433998108\n",
      "          vf_loss: 0.005757489288872522\n",
      "    num_agent_steps_sampled: 404000\n",
      "    num_agent_steps_trained: 404000\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "  iterations_since_restore: 404\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.10697674418604\n",
      "    ram_util_percent: 47.32325581395349\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04004329830007621\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.157503777860747\n",
      "    mean_inference_ms: 1.7491498419426086\n",
      "    mean_raw_obs_processing_ms: 1.7786272575775095\n",
      "  time_since_restore: 11158.095880508423\n",
      "  time_this_iter_s: 29.987663507461548\n",
      "  time_total_s: 11158.095880508423\n",
      "  timers:\n",
      "    learn_throughput: 1178.862\n",
      "    learn_time_ms: 848.275\n",
      "    load_throughput: 55212.838\n",
      "    load_time_ms: 18.112\n",
      "    sample_throughput: 38.841\n",
      "    sample_time_ms: 25745.761\n",
      "    update_time_ms: 5.806\n",
      "  timestamp: 1634854386\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 404\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   404</td><td style=\"text-align: right;\">         11158.1</td><td style=\"text-align: right;\">404000</td><td style=\"text-align: right;\"> -3.9228</td><td style=\"text-align: right;\">               -2.79</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            392.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 405000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-13-49\n",
      "  done: false\n",
      "  episode_len_mean: 389.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7899999999999845\n",
      "  episode_reward_mean: -3.8974999999999613\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1112\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6494642050936816\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.793137182129754\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016598476173539982\n",
      "          policy_loss: 0.006780349918537669\n",
      "          total_loss: 0.006793334417872958\n",
      "          vf_explained_var: 0.5144118666648865\n",
      "          vf_loss: 0.00716424110190322\n",
      "    num_agent_steps_sampled: 405000\n",
      "    num_agent_steps_trained: 405000\n",
      "    num_steps_sampled: 405000\n",
      "    num_steps_trained: 405000\n",
      "  iterations_since_restore: 405\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.59032258064514\n",
      "    ram_util_percent: 47.09516129032257\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040045537987658565\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.155065797054007\n",
      "    mean_inference_ms: 1.7492276000936005\n",
      "    mean_raw_obs_processing_ms: 1.7791963566939006\n",
      "  time_since_restore: 11201.322880506516\n",
      "  time_this_iter_s: 43.22699999809265\n",
      "  time_total_s: 11201.322880506516\n",
      "  timers:\n",
      "    learn_throughput: 1175.417\n",
      "    learn_time_ms: 850.762\n",
      "    load_throughput: 54215.611\n",
      "    load_time_ms: 18.445\n",
      "    sample_throughput: 36.522\n",
      "    sample_time_ms: 27380.607\n",
      "    update_time_ms: 6.232\n",
      "  timestamp: 1634854429\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 405000\n",
      "  training_iteration: 405\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   405</td><td style=\"text-align: right;\">         11201.3</td><td style=\"text-align: right;\">405000</td><td style=\"text-align: right;\"> -3.8975</td><td style=\"text-align: right;\">               -2.79</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            389.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 406000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-14-15\n",
      "  done: false\n",
      "  episode_len_mean: 389.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7899999999999845\n",
      "  episode_reward_mean: -3.8918999999999615\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1114\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6494642050936816\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5696984105639988\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02107269998600166\n",
      "          policy_loss: -0.17205120258861117\n",
      "          total_loss: -0.16724808282322354\n",
      "          vf_explained_var: 0.15819677710533142\n",
      "          vf_loss: 0.006814139190181676\n",
      "    num_agent_steps_sampled: 406000\n",
      "    num_agent_steps_trained: 406000\n",
      "    num_steps_sampled: 406000\n",
      "    num_steps_trained: 406000\n",
      "  iterations_since_restore: 406\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.8081081081081\n",
      "    ram_util_percent: 46.91351351351352\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04004703981081363\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.153468043643503\n",
      "    mean_inference_ms: 1.749279011713338\n",
      "    mean_raw_obs_processing_ms: 1.7795825017435651\n",
      "  time_since_restore: 11227.24011516571\n",
      "  time_this_iter_s: 25.917234659194946\n",
      "  time_total_s: 11227.24011516571\n",
      "  timers:\n",
      "    learn_throughput: 1170.783\n",
      "    learn_time_ms: 854.129\n",
      "    load_throughput: 51446.746\n",
      "    load_time_ms: 19.438\n",
      "    sample_throughput: 37.003\n",
      "    sample_time_ms: 27024.873\n",
      "    update_time_ms: 6.024\n",
      "  timestamp: 1634854455\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 406000\n",
      "  training_iteration: 406\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   406</td><td style=\"text-align: right;\">         11227.2</td><td style=\"text-align: right;\">406000</td><td style=\"text-align: right;\"> -3.8919</td><td style=\"text-align: right;\">               -2.79</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            389.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 407000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-14-38\n",
      "  done: false\n",
      "  episode_len_mean: 391.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7899999999999845\n",
      "  episode_reward_mean: -3.919699999999961\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1117\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9741963076405227\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8103092537985908\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009283671862460154\n",
      "          policy_loss: 0.010093056451943186\n",
      "          total_loss: 0.009929919325643115\n",
      "          vf_explained_var: 0.47491776943206787\n",
      "          vf_loss: 0.008895840858652566\n",
      "    num_agent_steps_sampled: 407000\n",
      "    num_agent_steps_trained: 407000\n",
      "    num_steps_sampled: 407000\n",
      "    num_steps_trained: 407000\n",
      "  iterations_since_restore: 407\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.69062500000001\n",
      "    ram_util_percent: 46.943749999999994\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04004919083822967\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.150506314436644\n",
      "    mean_inference_ms: 1.7493540183194844\n",
      "    mean_raw_obs_processing_ms: 1.780168836898043\n",
      "  time_since_restore: 11250.008329868317\n",
      "  time_this_iter_s: 22.7682147026062\n",
      "  time_total_s: 11250.008329868317\n",
      "  timers:\n",
      "    learn_throughput: 1173.611\n",
      "    learn_time_ms: 852.071\n",
      "    load_throughput: 51647.121\n",
      "    load_time_ms: 19.362\n",
      "    sample_throughput: 37.73\n",
      "    sample_time_ms: 26504.344\n",
      "    update_time_ms: 6.685\n",
      "  timestamp: 1634854478\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 407000\n",
      "  training_iteration: 407\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   407</td><td style=\"text-align: right;\">           11250</td><td style=\"text-align: right;\">407000</td><td style=\"text-align: right;\"> -3.9197</td><td style=\"text-align: right;\">               -2.79</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            391.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-15-04\n",
      "  done: false\n",
      "  episode_len_mean: 390.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7899999999999845\n",
      "  episode_reward_mean: -3.9075999999999613\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1120\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9741963076405227\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6019232233365377\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007990530397572628\n",
      "          policy_loss: 0.05771012554566066\n",
      "          total_loss: 0.0575630519953039\n",
      "          vf_explained_var: 0.4138670265674591\n",
      "          vf_loss: 0.008087815290006498\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "  iterations_since_restore: 408\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.14473684210525\n",
      "    ram_util_percent: 46.9842105263158\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04005125617814987\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.14783649173223\n",
      "    mean_inference_ms: 1.7494274955427915\n",
      "    mean_raw_obs_processing_ms: 1.7798019529320193\n",
      "  time_since_restore: 11276.36737704277\n",
      "  time_this_iter_s: 26.359047174453735\n",
      "  time_total_s: 11276.36737704277\n",
      "  timers:\n",
      "    learn_throughput: 1174.786\n",
      "    learn_time_ms: 851.219\n",
      "    load_throughput: 51948.922\n",
      "    load_time_ms: 19.25\n",
      "    sample_throughput: 37.536\n",
      "    sample_time_ms: 26641.042\n",
      "    update_time_ms: 6.192\n",
      "  timestamp: 1634854504\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 408\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   408</td><td style=\"text-align: right;\">         11276.4</td><td style=\"text-align: right;\">408000</td><td style=\"text-align: right;\"> -3.9076</td><td style=\"text-align: right;\">               -2.79</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            390.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 409000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-15-32\n",
      "  done: false\n",
      "  episode_len_mean: 387.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7899999999999845\n",
      "  episode_reward_mean: -3.878799999999961\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1123\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9741963076405227\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.520823389954037\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006645298153758805\n",
      "          policy_loss: 0.09272933320866691\n",
      "          total_loss: 0.08682916946709156\n",
      "          vf_explained_var: 0.8092954754829407\n",
      "          vf_loss: 0.0028342414970716667\n",
      "    num_agent_steps_sampled: 409000\n",
      "    num_agent_steps_trained: 409000\n",
      "    num_steps_sampled: 409000\n",
      "    num_steps_trained: 409000\n",
      "  iterations_since_restore: 409\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.23846153846154\n",
      "    ram_util_percent: 46.96923076923077\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04005309989447304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.145577175540417\n",
      "    mean_inference_ms: 1.749497092331025\n",
      "    mean_raw_obs_processing_ms: 1.7790709946876433\n",
      "  time_since_restore: 11304.157657623291\n",
      "  time_this_iter_s: 27.79028058052063\n",
      "  time_total_s: 11304.157657623291\n",
      "  timers:\n",
      "    learn_throughput: 1173.262\n",
      "    learn_time_ms: 852.324\n",
      "    load_throughput: 49569.273\n",
      "    load_time_ms: 20.174\n",
      "    sample_throughput: 37.249\n",
      "    sample_time_ms: 26846.642\n",
      "    update_time_ms: 5.748\n",
      "  timestamp: 1634854532\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 409000\n",
      "  training_iteration: 409\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   409</td><td style=\"text-align: right;\">         11304.2</td><td style=\"text-align: right;\">409000</td><td style=\"text-align: right;\"> -3.8788</td><td style=\"text-align: right;\">               -2.79</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            387.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 410000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-16-00\n",
      "  done: false\n",
      "  episode_len_mean: 387.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.959999999999981\n",
      "  episode_reward_mean: -3.876499999999962\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1126\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9741963076405227\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7364676356315614\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010694609342301181\n",
      "          policy_loss: -0.02069987195233504\n",
      "          total_loss: -0.02385178026225832\n",
      "          vf_explained_var: 0.3945409059524536\n",
      "          vf_loss: 0.0037941213942960733\n",
      "    num_agent_steps_sampled: 410000\n",
      "    num_agent_steps_trained: 410000\n",
      "    num_steps_sampled: 410000\n",
      "    num_steps_trained: 410000\n",
      "  iterations_since_restore: 410\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.2\n",
      "    ram_util_percent: 47.0025\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040054885651624295\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.143503331619314\n",
      "    mean_inference_ms: 1.749565370166447\n",
      "    mean_raw_obs_processing_ms: 1.7783534999841342\n",
      "  time_since_restore: 11331.925786495209\n",
      "  time_this_iter_s: 27.768128871917725\n",
      "  time_total_s: 11331.925786495209\n",
      "  timers:\n",
      "    learn_throughput: 1171.609\n",
      "    learn_time_ms: 853.527\n",
      "    load_throughput: 47438.775\n",
      "    load_time_ms: 21.08\n",
      "    sample_throughput: 37.187\n",
      "    sample_time_ms: 26890.941\n",
      "    update_time_ms: 4.797\n",
      "  timestamp: 1634854560\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 410000\n",
      "  training_iteration: 410\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   410</td><td style=\"text-align: right;\">         11331.9</td><td style=\"text-align: right;\">410000</td><td style=\"text-align: right;\"> -3.8765</td><td style=\"text-align: right;\">               -2.96</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            387.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 411000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-16-25\n",
      "  done: false\n",
      "  episode_len_mean: 388.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.959999999999981\n",
      "  episode_reward_mean: -3.880099999999962\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1128\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9741963076405227\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5157760447925992\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010690689024477134\n",
      "          policy_loss: -0.10504874322149489\n",
      "          total_loss: -0.10532485362556246\n",
      "          vf_explained_var: 0.7032318115234375\n",
      "          vf_loss: 0.0044668230900747905\n",
      "    num_agent_steps_sampled: 411000\n",
      "    num_agent_steps_trained: 411000\n",
      "    num_steps_sampled: 411000\n",
      "    num_steps_trained: 411000\n",
      "  iterations_since_restore: 411\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.15277777777777\n",
      "    ram_util_percent: 47.094444444444434\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040055979092970914\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.14203022287058\n",
      "    mean_inference_ms: 1.7496112216271467\n",
      "    mean_raw_obs_processing_ms: 1.7778526737842097\n",
      "  time_since_restore: 11357.167351961136\n",
      "  time_this_iter_s: 25.241565465927124\n",
      "  time_total_s: 11357.167351961136\n",
      "  timers:\n",
      "    learn_throughput: 1170.402\n",
      "    learn_time_ms: 854.407\n",
      "    load_throughput: 48197.87\n",
      "    load_time_ms: 20.748\n",
      "    sample_throughput: 37.549\n",
      "    sample_time_ms: 26631.725\n",
      "    update_time_ms: 4.478\n",
      "  timestamp: 1634854585\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 411000\n",
      "  training_iteration: 411\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   411</td><td style=\"text-align: right;\">         11357.2</td><td style=\"text-align: right;\">411000</td><td style=\"text-align: right;\"> -3.8801</td><td style=\"text-align: right;\">               -2.96</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            388.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 412000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-16-52\n",
      "  done: false\n",
      "  episode_len_mean: 388.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.959999999999981\n",
      "  episode_reward_mean: -3.8832999999999624\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1131\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9741963076405227\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.497170122464498\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008980361919019783\n",
      "          policy_loss: 0.0005157628407080968\n",
      "          total_loss: -0.0005216562085681492\n",
      "          vf_explained_var: 0.8352591395378113\n",
      "          vf_loss: 0.005185643190311061\n",
      "    num_agent_steps_sampled: 412000\n",
      "    num_agent_steps_trained: 412000\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "  iterations_since_restore: 412\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.11794871794872\n",
      "    ram_util_percent: 47.12564102564102\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040057569528523115\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.139716645412108\n",
      "    mean_inference_ms: 1.7496777214609849\n",
      "    mean_raw_obs_processing_ms: 1.7771132371540856\n",
      "  time_since_restore: 11384.394990205765\n",
      "  time_this_iter_s: 27.227638244628906\n",
      "  time_total_s: 11384.394990205765\n",
      "  timers:\n",
      "    learn_throughput: 1168.868\n",
      "    learn_time_ms: 855.529\n",
      "    load_throughput: 48237.615\n",
      "    load_time_ms: 20.731\n",
      "    sample_throughput: 36.938\n",
      "    sample_time_ms: 27072.17\n",
      "    update_time_ms: 4.379\n",
      "  timestamp: 1634854612\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 412\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   412</td><td style=\"text-align: right;\">         11384.4</td><td style=\"text-align: right;\">412000</td><td style=\"text-align: right;\"> -3.8833</td><td style=\"text-align: right;\">               -2.96</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            388.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 413000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-17-18\n",
      "  done: false\n",
      "  episode_len_mean: 387.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.959999999999981\n",
      "  episode_reward_mean: -3.871199999999962\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1134\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9741963076405227\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5711576342582703\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00609474953299297\n",
      "          policy_loss: 0.10412561487820414\n",
      "          total_loss: 0.09960334954990281\n",
      "          vf_explained_var: 0.6852079629898071\n",
      "          vf_loss: 0.005251826168710573\n",
      "    num_agent_steps_sampled: 413000\n",
      "    num_agent_steps_trained: 413000\n",
      "    num_steps_sampled: 413000\n",
      "    num_steps_trained: 413000\n",
      "  iterations_since_restore: 413\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.04722222222223\n",
      "    ram_util_percent: 47.12777777777777\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04005907407707862\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.137545907300996\n",
      "    mean_inference_ms: 1.7497428901001146\n",
      "    mean_raw_obs_processing_ms: 1.7764285843382945\n",
      "  time_since_restore: 11409.760201692581\n",
      "  time_this_iter_s: 25.365211486816406\n",
      "  time_total_s: 11409.760201692581\n",
      "  timers:\n",
      "    learn_throughput: 1173.159\n",
      "    learn_time_ms: 852.399\n",
      "    load_throughput: 48675.155\n",
      "    load_time_ms: 20.544\n",
      "    sample_throughput: 36.653\n",
      "    sample_time_ms: 27283.135\n",
      "    update_time_ms: 4.056\n",
      "  timestamp: 1634854638\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 413000\n",
      "  training_iteration: 413\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   413</td><td style=\"text-align: right;\">         11409.8</td><td style=\"text-align: right;\">413000</td><td style=\"text-align: right;\"> -3.8712</td><td style=\"text-align: right;\">               -2.96</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            387.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 414000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-17-44\n",
      "  done: false\n",
      "  episode_len_mean: 386.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.959999999999981\n",
      "  episode_reward_mean: -3.866399999999962\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1137\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9741963076405227\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5219557841618856\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009017301836953944\n",
      "          policy_loss: -0.04734666595856349\n",
      "          total_loss: -0.04308869805600908\n",
      "          vf_explained_var: 0.2341344654560089\n",
      "          vf_loss: 0.010692903875476785\n",
      "    num_agent_steps_sampled: 414000\n",
      "    num_agent_steps_trained: 414000\n",
      "    num_steps_sampled: 414000\n",
      "    num_steps_trained: 414000\n",
      "  iterations_since_restore: 414\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.04473684210528\n",
      "    ram_util_percent: 47.16315789473684\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04006088026817127\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.135386588444554\n",
      "    mean_inference_ms: 1.7498077423523042\n",
      "    mean_raw_obs_processing_ms: 1.775757613028435\n",
      "  time_since_restore: 11435.96103978157\n",
      "  time_this_iter_s: 26.200838088989258\n",
      "  time_total_s: 11435.96103978157\n",
      "  timers:\n",
      "    learn_throughput: 1171.449\n",
      "    learn_time_ms: 853.644\n",
      "    load_throughput: 48694.82\n",
      "    load_time_ms: 20.536\n",
      "    sample_throughput: 37.17\n",
      "    sample_time_ms: 26903.102\n",
      "    update_time_ms: 4.19\n",
      "  timestamp: 1634854664\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 414000\n",
      "  training_iteration: 414\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   414</td><td style=\"text-align: right;\">           11436</td><td style=\"text-align: right;\">414000</td><td style=\"text-align: right;\"> -3.8664</td><td style=\"text-align: right;\">               -2.96</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            386.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 415000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-18-09\n",
      "  done: false\n",
      "  episode_len_mean: 386.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.959999999999981\n",
      "  episode_reward_mean: -3.8603999999999616\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1139\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9741963076405227\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5088947839207119\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009015391587318868\n",
      "          policy_loss: -0.08357774416605632\n",
      "          total_loss: -0.08432927661471896\n",
      "          vf_explained_var: 0.7150930762290955\n",
      "          vf_loss: 0.005554652360216197\n",
      "    num_agent_steps_sampled: 415000\n",
      "    num_agent_steps_trained: 415000\n",
      "    num_steps_sampled: 415000\n",
      "    num_steps_trained: 415000\n",
      "  iterations_since_restore: 415\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.92777777777778\n",
      "    ram_util_percent: 47.23055555555556\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04006197632240041\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.133903966492344\n",
      "    mean_inference_ms: 1.749850999522862\n",
      "    mean_raw_obs_processing_ms: 1.7753316294421218\n",
      "  time_since_restore: 11461.217638015747\n",
      "  time_this_iter_s: 25.256598234176636\n",
      "  time_total_s: 11461.217638015747\n",
      "  timers:\n",
      "    learn_throughput: 1142.606\n",
      "    learn_time_ms: 875.192\n",
      "    load_throughput: 49171.38\n",
      "    load_time_ms: 20.337\n",
      "    sample_throughput: 39.865\n",
      "    sample_time_ms: 25084.874\n",
      "    update_time_ms: 4.107\n",
      "  timestamp: 1634854689\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 415000\n",
      "  training_iteration: 415\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   415</td><td style=\"text-align: right;\">         11461.2</td><td style=\"text-align: right;\">415000</td><td style=\"text-align: right;\"> -3.8604</td><td style=\"text-align: right;\">               -2.96</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            386.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-18-52\n",
      "  done: false\n",
      "  episode_len_mean: 384.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8299999999999836\n",
      "  episode_reward_mean: -3.8473999999999613\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1142\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9741963076405227\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7393254823154873\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008995811958347592\n",
      "          policy_loss: 0.05460517820384767\n",
      "          total_loss: 0.04870737658606635\n",
      "          vf_explained_var: 0.8530375361442566\n",
      "          vf_loss: 0.0027317655615560297\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_agent_steps_trained: 416000\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "  iterations_since_restore: 416\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.53934426229507\n",
      "    ram_util_percent: 47.11803278688525\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040063461246995766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.131890454180965\n",
      "    mean_inference_ms: 1.7499176285726714\n",
      "    mean_raw_obs_processing_ms: 1.7759003572495144\n",
      "  time_since_restore: 11504.207587003708\n",
      "  time_this_iter_s: 42.989948987960815\n",
      "  time_total_s: 11504.207587003708\n",
      "  timers:\n",
      "    learn_throughput: 1145.965\n",
      "    learn_time_ms: 872.627\n",
      "    load_throughput: 51874.457\n",
      "    load_time_ms: 19.277\n",
      "    sample_throughput: 37.319\n",
      "    sample_time_ms: 26796.074\n",
      "    update_time_ms: 3.913\n",
      "  timestamp: 1634854732\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 416\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   416</td><td style=\"text-align: right;\">         11504.2</td><td style=\"text-align: right;\">416000</td><td style=\"text-align: right;\"> -3.8474</td><td style=\"text-align: right;\">               -2.83</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            384.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 417000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-19-16\n",
      "  done: false\n",
      "  episode_len_mean: 384.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8299999999999836\n",
      "  episode_reward_mean: -3.847899999999963\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1144\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9741963076405227\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7232695089446173\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014493394196347669\n",
      "          policy_loss: -0.13865379575226042\n",
      "          total_loss: -0.13539068198038473\n",
      "          vf_explained_var: 0.7005143165588379\n",
      "          vf_loss: 0.006376397629113247\n",
      "    num_agent_steps_sampled: 417000\n",
      "    num_agent_steps_trained: 417000\n",
      "    num_steps_sampled: 417000\n",
      "    num_steps_trained: 417000\n",
      "  iterations_since_restore: 417\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.83823529411765\n",
      "    ram_util_percent: 47.02058823529411\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04006437069488324\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.130619902515868\n",
      "    mean_inference_ms: 1.7499598286508007\n",
      "    mean_raw_obs_processing_ms: 1.7762992241267088\n",
      "  time_since_restore: 11527.804368972778\n",
      "  time_this_iter_s: 23.596781969070435\n",
      "  time_total_s: 11527.804368972778\n",
      "  timers:\n",
      "    learn_throughput: 1140.553\n",
      "    learn_time_ms: 876.767\n",
      "    load_throughput: 49057.449\n",
      "    load_time_ms: 20.384\n",
      "    sample_throughput: 37.209\n",
      "    sample_time_ms: 26875.202\n",
      "    update_time_ms: 3.244\n",
      "  timestamp: 1634854756\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 417000\n",
      "  training_iteration: 417\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   417</td><td style=\"text-align: right;\">         11527.8</td><td style=\"text-align: right;\">417000</td><td style=\"text-align: right;\"> -3.8479</td><td style=\"text-align: right;\">               -2.83</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            384.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 418000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-19-42\n",
      "  done: false\n",
      "  episode_len_mean: 383.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8299999999999836\n",
      "  episode_reward_mean: -3.8382999999999616\n",
      "  episode_reward_min: -5.259999999999932\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1147\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9741963076405227\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5852522492408752\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009328727105116405\n",
      "          policy_loss: -0.1440320298075676\n",
      "          total_loss: -0.14371977587127024\n",
      "          vf_explained_var: 0.7044475674629211\n",
      "          vf_loss: 0.00707676112651825\n",
      "    num_agent_steps_sampled: 418000\n",
      "    num_agent_steps_trained: 418000\n",
      "    num_steps_sampled: 418000\n",
      "    num_steps_trained: 418000\n",
      "  iterations_since_restore: 418\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.01944444444445\n",
      "    ram_util_percent: 47.01111111111111\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04006571959174596\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.12871838266834\n",
      "    mean_inference_ms: 1.7500233879747387\n",
      "    mean_raw_obs_processing_ms: 1.776888388502775\n",
      "  time_since_restore: 11553.480993509293\n",
      "  time_this_iter_s: 25.676624536514282\n",
      "  time_total_s: 11553.480993509293\n",
      "  timers:\n",
      "    learn_throughput: 1139.667\n",
      "    learn_time_ms: 877.449\n",
      "    load_throughput: 51077.108\n",
      "    load_time_ms: 19.578\n",
      "    sample_throughput: 37.303\n",
      "    sample_time_ms: 26807.416\n",
      "    update_time_ms: 2.918\n",
      "  timestamp: 1634854782\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 418000\n",
      "  training_iteration: 418\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   418</td><td style=\"text-align: right;\">         11553.5</td><td style=\"text-align: right;\">418000</td><td style=\"text-align: right;\"> -3.8383</td><td style=\"text-align: right;\">               -2.83</td><td style=\"text-align: right;\">               -5.26</td><td style=\"text-align: right;\">            383.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 419000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-20-10\n",
      "  done: false\n",
      "  episode_len_mean: 380.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8299999999999836\n",
      "  episode_reward_mean: -3.803299999999963\n",
      "  episode_reward_min: -5.139999999999935\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1150\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9741963076405227\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1824290302064684\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0039791678305682076\n",
      "          policy_loss: -0.07847999160488446\n",
      "          total_loss: -0.07590942540102535\n",
      "          vf_explained_var: 0.5657562613487244\n",
      "          vf_loss: 0.010518365958705544\n",
      "    num_agent_steps_sampled: 419000\n",
      "    num_agent_steps_trained: 419000\n",
      "    num_steps_sampled: 419000\n",
      "    num_steps_trained: 419000\n",
      "  iterations_since_restore: 419\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.92999999999999\n",
      "    ram_util_percent: 46.92\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04006707927520644\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.127334603856912\n",
      "    mean_inference_ms: 1.750083836228899\n",
      "    mean_raw_obs_processing_ms: 1.7766484027254839\n",
      "  time_since_restore: 11581.377923488617\n",
      "  time_this_iter_s: 27.89692997932434\n",
      "  time_total_s: 11581.377923488617\n",
      "  timers:\n",
      "    learn_throughput: 1143.046\n",
      "    learn_time_ms: 874.855\n",
      "    load_throughput: 53870.855\n",
      "    load_time_ms: 18.563\n",
      "    sample_throughput: 37.283\n",
      "    sample_time_ms: 26821.598\n",
      "    update_time_ms: 2.961\n",
      "  timestamp: 1634854810\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 419000\n",
      "  training_iteration: 419\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   419</td><td style=\"text-align: right;\">         11581.4</td><td style=\"text-align: right;\">419000</td><td style=\"text-align: right;\"> -3.8033</td><td style=\"text-align: right;\">               -2.83</td><td style=\"text-align: right;\">               -5.14</td><td style=\"text-align: right;\">            380.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 420000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-20-38\n",
      "  done: false\n",
      "  episode_len_mean: 377.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8299999999999836\n",
      "  episode_reward_mean: -3.770699999999963\n",
      "  episode_reward_min: -5.139999999999935\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1153\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4196227219369677\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009463759137233692\n",
      "          policy_loss: -0.002592930446068446\n",
      "          total_loss: -0.002605556903613938\n",
      "          vf_explained_var: 0.42003992199897766\n",
      "          vf_loss: 0.009573818911384377\n",
      "    num_agent_steps_sampled: 420000\n",
      "    num_agent_steps_trained: 420000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 420\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.1525\n",
      "    ram_util_percent: 46.8975\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04006840554628628\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.126249495919268\n",
      "    mean_inference_ms: 1.7501393868742532\n",
      "    mean_raw_obs_processing_ms: 1.776062561516246\n",
      "  time_since_restore: 11609.402230739594\n",
      "  time_this_iter_s: 28.024307250976562\n",
      "  time_total_s: 11609.402230739594\n",
      "  timers:\n",
      "    learn_throughput: 1141.655\n",
      "    learn_time_ms: 875.921\n",
      "    load_throughput: 55688.75\n",
      "    load_time_ms: 17.957\n",
      "    sample_throughput: 37.249\n",
      "    sample_time_ms: 26846.549\n",
      "    update_time_ms: 3.099\n",
      "  timestamp: 1634854838\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 420\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   420</td><td style=\"text-align: right;\">         11609.4</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\"> -3.7707</td><td style=\"text-align: right;\">               -2.83</td><td style=\"text-align: right;\">               -5.14</td><td style=\"text-align: right;\">            377.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 421000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-21-09\n",
      "  done: false\n",
      "  episode_len_mean: 375.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8299999999999836\n",
      "  episode_reward_mean: -3.750399999999964\n",
      "  episode_reward_min: -5.139999999999935\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1156\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1444092717435626\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007804191222687212\n",
      "          policy_loss: -0.10401055862506231\n",
      "          total_loss: -0.09565842217869229\n",
      "          vf_explained_var: 0.36695072054862976\n",
      "          vf_loss: 0.015994823465330734\n",
      "    num_agent_steps_sampled: 421000\n",
      "    num_agent_steps_trained: 421000\n",
      "    num_steps_sampled: 421000\n",
      "    num_steps_trained: 421000\n",
      "  iterations_since_restore: 421\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.09333333333332\n",
      "    ram_util_percent: 47.03555555555556\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04006959973281529\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.125653105418564\n",
      "    mean_inference_ms: 1.7501919682814304\n",
      "    mean_raw_obs_processing_ms: 1.775490075916853\n",
      "  time_since_restore: 11640.695222377777\n",
      "  time_this_iter_s: 31.292991638183594\n",
      "  time_total_s: 11640.695222377777\n",
      "  timers:\n",
      "    learn_throughput: 1141.771\n",
      "    learn_time_ms: 875.832\n",
      "    load_throughput: 55261.578\n",
      "    load_time_ms: 18.096\n",
      "    sample_throughput: 36.428\n",
      "    sample_time_ms: 27451.725\n",
      "    update_time_ms: 3.04\n",
      "  timestamp: 1634854869\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 421000\n",
      "  training_iteration: 421\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   421</td><td style=\"text-align: right;\">         11640.7</td><td style=\"text-align: right;\">421000</td><td style=\"text-align: right;\"> -3.7504</td><td style=\"text-align: right;\">               -2.83</td><td style=\"text-align: right;\">               -5.14</td><td style=\"text-align: right;\">            375.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 422000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-21-37\n",
      "  done: false\n",
      "  episode_len_mean: 368.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.8299999999999836\n",
      "  episode_reward_mean: -3.6839999999999655\n",
      "  episode_reward_min: -4.939999999999939\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1160\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2989673150910273\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008653277122870624\n",
      "          policy_loss: -0.04061622661021021\n",
      "          total_loss: -0.038931510514683194\n",
      "          vf_explained_var: 0.47122621536254883\n",
      "          vf_loss: 0.01045939154509041\n",
      "    num_agent_steps_sampled: 422000\n",
      "    num_agent_steps_trained: 422000\n",
      "    num_steps_sampled: 422000\n",
      "    num_steps_trained: 422000\n",
      "  iterations_since_restore: 422\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.055\n",
      "    ram_util_percent: 47.12\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04007102234819056\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.125563415532657\n",
      "    mean_inference_ms: 1.750258153746629\n",
      "    mean_raw_obs_processing_ms: 1.774858927748374\n",
      "  time_since_restore: 11668.738084077835\n",
      "  time_this_iter_s: 28.042861700057983\n",
      "  time_total_s: 11668.738084077835\n",
      "  timers:\n",
      "    learn_throughput: 1147.512\n",
      "    learn_time_ms: 871.451\n",
      "    load_throughput: 55381.17\n",
      "    load_time_ms: 18.057\n",
      "    sample_throughput: 36.315\n",
      "    sample_time_ms: 27536.954\n",
      "    update_time_ms: 3.614\n",
      "  timestamp: 1634854897\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 422000\n",
      "  training_iteration: 422\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   422</td><td style=\"text-align: right;\">         11668.7</td><td style=\"text-align: right;\">422000</td><td style=\"text-align: right;\">  -3.684</td><td style=\"text-align: right;\">               -2.83</td><td style=\"text-align: right;\">               -4.94</td><td style=\"text-align: right;\">             368.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 423000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-22-06\n",
      "  done: false\n",
      "  episode_len_mean: 364.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.809999999999984\n",
      "  episode_reward_mean: -3.647599999999967\n",
      "  episode_reward_min: -4.9199999999999395\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1163\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.225237316555447\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008702836826098409\n",
      "          policy_loss: -0.021457589831617142\n",
      "          total_loss: -0.02144487574696541\n",
      "          vf_explained_var: 0.6262929439544678\n",
      "          vf_loss: 0.008025949643666132\n",
      "    num_agent_steps_sampled: 423000\n",
      "    num_agent_steps_trained: 423000\n",
      "    num_steps_sampled: 423000\n",
      "    num_steps_trained: 423000\n",
      "  iterations_since_restore: 423\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.27317073170731\n",
      "    ram_util_percent: 47.21463414634147\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04007197977710695\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.126012431279563\n",
      "    mean_inference_ms: 1.7503063338495333\n",
      "    mean_raw_obs_processing_ms: 1.7744788836799135\n",
      "  time_since_restore: 11697.544481039047\n",
      "  time_this_iter_s: 28.806396961212158\n",
      "  time_total_s: 11697.544481039047\n",
      "  timers:\n",
      "    learn_throughput: 1147.218\n",
      "    learn_time_ms: 871.674\n",
      "    load_throughput: 52519.33\n",
      "    load_time_ms: 19.041\n",
      "    sample_throughput: 35.869\n",
      "    sample_time_ms: 27878.884\n",
      "    update_time_ms: 4.336\n",
      "  timestamp: 1634854926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 423000\n",
      "  training_iteration: 423\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   423</td><td style=\"text-align: right;\">         11697.5</td><td style=\"text-align: right;\">423000</td><td style=\"text-align: right;\"> -3.6476</td><td style=\"text-align: right;\">               -2.81</td><td style=\"text-align: right;\">               -4.92</td><td style=\"text-align: right;\">            364.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-22-36\n",
      "  done: false\n",
      "  episode_len_mean: 360.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7399999999999856\n",
      "  episode_reward_mean: -3.6070999999999676\n",
      "  episode_reward_min: -4.809999999999942\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1166\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9952199041843415\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008630949778782416\n",
      "          policy_loss: 0.03898694159256087\n",
      "          total_loss: 0.0433150514960289\n",
      "          vf_explained_var: 0.39541423320770264\n",
      "          vf_loss: 0.01007618967236744\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "  iterations_since_restore: 424\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.10454545454546\n",
      "    ram_util_percent: 47.31136363636363\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04007291711420567\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.126986955693365\n",
      "    mean_inference_ms: 1.7503527911931256\n",
      "    mean_raw_obs_processing_ms: 1.7741111378389187\n",
      "  time_since_restore: 11728.099323034286\n",
      "  time_this_iter_s: 30.554841995239258\n",
      "  time_total_s: 11728.099323034286\n",
      "  timers:\n",
      "    learn_throughput: 1148.275\n",
      "    learn_time_ms: 870.871\n",
      "    load_throughput: 49881.478\n",
      "    load_time_ms: 20.048\n",
      "    sample_throughput: 35.318\n",
      "    sample_time_ms: 28314.252\n",
      "    update_time_ms: 4.008\n",
      "  timestamp: 1634854956\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 424\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   424</td><td style=\"text-align: right;\">         11728.1</td><td style=\"text-align: right;\">424000</td><td style=\"text-align: right;\"> -3.6071</td><td style=\"text-align: right;\">               -2.74</td><td style=\"text-align: right;\">               -4.81</td><td style=\"text-align: right;\">            360.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 425000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-23-07\n",
      "  done: false\n",
      "  episode_len_mean: 356.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.7399999999999856\n",
      "  episode_reward_mean: -3.562299999999967\n",
      "  episode_reward_min: -4.809999999999942\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1169\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.156344375345442\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00807222981902274\n",
      "          policy_loss: 0.0006366989678806728\n",
      "          total_loss: 0.0002690894736184014\n",
      "          vf_explained_var: 0.6509788036346436\n",
      "          vf_loss: 0.007263866849502342\n",
      "    num_agent_steps_sampled: 425000\n",
      "    num_agent_steps_trained: 425000\n",
      "    num_steps_sampled: 425000\n",
      "    num_steps_trained: 425000\n",
      "  iterations_since_restore: 425\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.03181818181818\n",
      "    ram_util_percent: 47.325\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04007397548773783\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.128740564978912\n",
      "    mean_inference_ms: 1.7504031149592987\n",
      "    mean_raw_obs_processing_ms: 1.7738382446845384\n",
      "  time_since_restore: 11758.911428928375\n",
      "  time_this_iter_s: 30.812105894088745\n",
      "  time_total_s: 11758.911428928375\n",
      "  timers:\n",
      "    learn_throughput: 1177.426\n",
      "    learn_time_ms: 849.31\n",
      "    load_throughput: 49381.993\n",
      "    load_time_ms: 20.25\n",
      "    sample_throughput: 34.613\n",
      "    sample_time_ms: 28890.826\n",
      "    update_time_ms: 4.037\n",
      "  timestamp: 1634854987\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 425000\n",
      "  training_iteration: 425\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   425</td><td style=\"text-align: right;\">         11758.9</td><td style=\"text-align: right;\">425000</td><td style=\"text-align: right;\"> -3.5623</td><td style=\"text-align: right;\">               -2.74</td><td style=\"text-align: right;\">               -4.81</td><td style=\"text-align: right;\">            356.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 426000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-23-56\n",
      "  done: false\n",
      "  episode_len_mean: 351.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5899999999999888\n",
      "  episode_reward_mean: -3.512799999999969\n",
      "  episode_reward_min: -4.809999999999942\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1173\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48709815382026134\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8263409621185727\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004287530380674396\n",
      "          policy_loss: 0.007660446771317058\n",
      "          total_loss: 0.015215627724925677\n",
      "          vf_explained_var: 0.3905709683895111\n",
      "          vf_loss: 0.013730139595766862\n",
      "    num_agent_steps_sampled: 426000\n",
      "    num_agent_steps_trained: 426000\n",
      "    num_steps_sampled: 426000\n",
      "    num_steps_trained: 426000\n",
      "  iterations_since_restore: 426\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.23333333333333\n",
      "    ram_util_percent: 47.12028985507246\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04007524571597549\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.13181676814922\n",
      "    mean_inference_ms: 1.750467804964462\n",
      "    mean_raw_obs_processing_ms: 1.7750630811975783\n",
      "  time_since_restore: 11807.74336194992\n",
      "  time_this_iter_s: 48.83193302154541\n",
      "  time_total_s: 11807.74336194992\n",
      "  timers:\n",
      "    learn_throughput: 1177.76\n",
      "    learn_time_ms: 849.069\n",
      "    load_throughput: 48181.648\n",
      "    load_time_ms: 20.755\n",
      "    sample_throughput: 33.928\n",
      "    sample_time_ms: 29474.289\n",
      "    update_time_ms: 4.154\n",
      "  timestamp: 1634855036\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 426000\n",
      "  training_iteration: 426\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   426</td><td style=\"text-align: right;\">         11807.7</td><td style=\"text-align: right;\">426000</td><td style=\"text-align: right;\"> -3.5128</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">               -4.81</td><td style=\"text-align: right;\">            351.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 427000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-24-27\n",
      "  done: false\n",
      "  episode_len_mean: 350.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5899999999999888\n",
      "  episode_reward_mean: -3.5030999999999692\n",
      "  episode_reward_min: -4.809999999999942\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1176\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1870447450213961\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011607034473406625\n",
      "          policy_loss: 0.007334587474664052\n",
      "          total_loss: 0.00774523549609714\n",
      "          vf_explained_var: 0.5539177060127258\n",
      "          vf_loss: 0.009454213656459211\n",
      "    num_agent_steps_sampled: 427000\n",
      "    num_agent_steps_trained: 427000\n",
      "    num_steps_sampled: 427000\n",
      "    num_steps_trained: 427000\n",
      "  iterations_since_restore: 427\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.92727272727274\n",
      "    ram_util_percent: 46.95454545454547\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04007620731097255\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.134343600107826\n",
      "    mean_inference_ms: 1.7505142233865696\n",
      "    mean_raw_obs_processing_ms: 1.7760170562630404\n",
      "  time_since_restore: 11838.300328969955\n",
      "  time_this_iter_s: 30.55696702003479\n",
      "  time_total_s: 11838.300328969955\n",
      "  timers:\n",
      "    learn_throughput: 1177.86\n",
      "    learn_time_ms: 848.997\n",
      "    load_throughput: 48289.819\n",
      "    load_time_ms: 20.708\n",
      "    sample_throughput: 33.145\n",
      "    sample_time_ms: 30170.462\n",
      "    update_time_ms: 4.332\n",
      "  timestamp: 1634855067\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 427000\n",
      "  training_iteration: 427\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   427</td><td style=\"text-align: right;\">         11838.3</td><td style=\"text-align: right;\">427000</td><td style=\"text-align: right;\"> -3.5031</td><td style=\"text-align: right;\">               -2.59</td><td style=\"text-align: right;\">               -4.81</td><td style=\"text-align: right;\">            350.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 428000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-25-01\n",
      "  done: false\n",
      "  episode_len_mean: 345.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5199999999999902\n",
      "  episode_reward_mean: -3.45269999999997\n",
      "  episode_reward_min: -4.809999999999942\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1180\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24354907691013067\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7614274144172668\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004922427422919718\n",
      "          policy_loss: 0.0005965046584606171\n",
      "          total_loss: 0.007272072633107503\n",
      "          vf_explained_var: 0.3896647095680237\n",
      "          vf_loss: 0.013090987306916052\n",
      "    num_agent_steps_sampled: 428000\n",
      "    num_agent_steps_trained: 428000\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "  iterations_since_restore: 428\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.08541666666667\n",
      "    ram_util_percent: 46.93958333333334\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04007750891398583\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.138436756844243\n",
      "    mean_inference_ms: 1.7505742680405125\n",
      "    mean_raw_obs_processing_ms: 1.7765181436341169\n",
      "  time_since_restore: 11872.206848621368\n",
      "  time_this_iter_s: 33.906519651412964\n",
      "  time_total_s: 11872.206848621368\n",
      "  timers:\n",
      "    learn_throughput: 1184.07\n",
      "    learn_time_ms: 844.545\n",
      "    load_throughput: 48382.678\n",
      "    load_time_ms: 20.669\n",
      "    sample_throughput: 32.261\n",
      "    sample_time_ms: 30997.603\n",
      "    update_time_ms: 4.726\n",
      "  timestamp: 1634855101\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 428\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   428</td><td style=\"text-align: right;\">         11872.2</td><td style=\"text-align: right;\">428000</td><td style=\"text-align: right;\"> -3.4527</td><td style=\"text-align: right;\">               -2.52</td><td style=\"text-align: right;\">               -4.81</td><td style=\"text-align: right;\">            345.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 429000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-25-31\n",
      "  done: false\n",
      "  episode_len_mean: 344.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5199999999999902\n",
      "  episode_reward_mean: -3.4470999999999696\n",
      "  episode_reward_min: -4.809999999999942\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1183\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12177453845506533\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5523459759023455\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02316316518752678\n",
      "          policy_loss: -0.023178248604138692\n",
      "          total_loss: -0.02835364821884367\n",
      "          vf_explained_var: 0.15645886957645416\n",
      "          vf_loss: 0.007527375733479858\n",
      "    num_agent_steps_sampled: 429000\n",
      "    num_agent_steps_trained: 429000\n",
      "    num_steps_sampled: 429000\n",
      "    num_steps_trained: 429000\n",
      "  iterations_since_restore: 429\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.02790697674418\n",
      "    ram_util_percent: 47.03488372093023\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04007844115979386\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.141659078375707\n",
      "    mean_inference_ms: 1.7506185371019913\n",
      "    mean_raw_obs_processing_ms: 1.7762719375924079\n",
      "  time_since_restore: 11902.235654115677\n",
      "  time_this_iter_s: 30.02880549430847\n",
      "  time_total_s: 11902.235654115677\n",
      "  timers:\n",
      "    learn_throughput: 1189.451\n",
      "    learn_time_ms: 840.724\n",
      "    load_throughput: 48670.354\n",
      "    load_time_ms: 20.546\n",
      "    sample_throughput: 32.037\n",
      "    sample_time_ms: 31214.2\n",
      "    update_time_ms: 5.076\n",
      "  timestamp: 1634855131\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 429000\n",
      "  training_iteration: 429\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   429</td><td style=\"text-align: right;\">         11902.2</td><td style=\"text-align: right;\">429000</td><td style=\"text-align: right;\"> -3.4471</td><td style=\"text-align: right;\">               -2.52</td><td style=\"text-align: right;\">               -4.81</td><td style=\"text-align: right;\">            344.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 430000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-26-02\n",
      "  done: false\n",
      "  episode_len_mean: 343.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5199999999999902\n",
      "  episode_reward_mean: -3.4375999999999705\n",
      "  episode_reward_min: -4.809999999999942\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1186\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1826618076825981\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2187496953540378\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015281210031935277\n",
      "          policy_loss: 0.016789075318309997\n",
      "          total_loss: 0.015294231474399567\n",
      "          vf_explained_var: 0.6016200184822083\n",
      "          vf_loss: 0.007901361196612318\n",
      "    num_agent_steps_sampled: 430000\n",
      "    num_agent_steps_trained: 430000\n",
      "    num_steps_sampled: 430000\n",
      "    num_steps_trained: 430000\n",
      "  iterations_since_restore: 430\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.00909090909092\n",
      "    ram_util_percent: 47.16818181818182\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0400793320715238\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.145001398380924\n",
      "    mean_inference_ms: 1.7506609306168741\n",
      "    mean_raw_obs_processing_ms: 1.7760358936992149\n",
      "  time_since_restore: 11933.085276603699\n",
      "  time_this_iter_s: 30.84962248802185\n",
      "  time_total_s: 11933.085276603699\n",
      "  timers:\n",
      "    learn_throughput: 1157.888\n",
      "    learn_time_ms: 863.642\n",
      "    load_throughput: 49419.347\n",
      "    load_time_ms: 20.235\n",
      "    sample_throughput: 31.772\n",
      "    sample_time_ms: 31473.89\n",
      "    update_time_ms: 5.378\n",
      "  timestamp: 1634855162\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 430000\n",
      "  training_iteration: 430\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   430</td><td style=\"text-align: right;\">         11933.1</td><td style=\"text-align: right;\">430000</td><td style=\"text-align: right;\"> -3.4376</td><td style=\"text-align: right;\">               -2.52</td><td style=\"text-align: right;\">               -4.81</td><td style=\"text-align: right;\">            343.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 431000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-26-35\n",
      "  done: false\n",
      "  episode_len_mean: 341.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5199999999999902\n",
      "  episode_reward_mean: -3.412999999999971\n",
      "  episode_reward_min: -4.809999999999942\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1190\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1826618076825981\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9876407338513269\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.04300268733537678\n",
      "          policy_loss: 0.021580598917272355\n",
      "          total_loss: 0.02752041783597734\n",
      "          vf_explained_var: 0.8317999243736267\n",
      "          vf_loss: 0.007961281174276438\n",
      "    num_agent_steps_sampled: 431000\n",
      "    num_agent_steps_trained: 431000\n",
      "    num_steps_sampled: 431000\n",
      "    num_steps_trained: 431000\n",
      "  iterations_since_restore: 431\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.96734693877552\n",
      "    ram_util_percent: 47.22857142857143\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040080451082517954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.14990224550593\n",
      "    mean_inference_ms: 1.7507164195839748\n",
      "    mean_raw_obs_processing_ms: 1.7757502149234272\n",
      "  time_since_restore: 11966.869938373566\n",
      "  time_this_iter_s: 33.78466176986694\n",
      "  time_total_s: 11966.869938373566\n",
      "  timers:\n",
      "    learn_throughput: 1159.15\n",
      "    learn_time_ms: 862.701\n",
      "    load_throughput: 47709.604\n",
      "    load_time_ms: 20.96\n",
      "    sample_throughput: 31.523\n",
      "    sample_time_ms: 31723.222\n",
      "    update_time_ms: 5.402\n",
      "  timestamp: 1634855195\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 431000\n",
      "  training_iteration: 431\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   431</td><td style=\"text-align: right;\">         11966.9</td><td style=\"text-align: right;\">431000</td><td style=\"text-align: right;\">  -3.413</td><td style=\"text-align: right;\">               -2.52</td><td style=\"text-align: right;\">               -4.81</td><td style=\"text-align: right;\">             341.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-27-05\n",
      "  done: false\n",
      "  episode_len_mean: 339.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5199999999999902\n",
      "  episode_reward_mean: -3.3987999999999716\n",
      "  episode_reward_min: -4.809999999999942\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1193\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2739927115238969\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0331809090243445\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011340450838933218\n",
      "          policy_loss: 0.03472166731953621\n",
      "          total_loss: 0.03970944508910179\n",
      "          vf_explained_var: 0.31229156255722046\n",
      "          vf_loss: 0.012212382479467326\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_agent_steps_trained: 432000\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "  iterations_since_restore: 432\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.1547619047619\n",
      "    ram_util_percent: 47.27619047619047\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040081390388109484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.15388982011536\n",
      "    mean_inference_ms: 1.7507614885643616\n",
      "    mean_raw_obs_processing_ms: 1.7755766296133415\n",
      "  time_since_restore: 11996.930061101913\n",
      "  time_this_iter_s: 30.06012272834778\n",
      "  time_total_s: 11996.930061101913\n",
      "  timers:\n",
      "    learn_throughput: 1159.498\n",
      "    learn_time_ms: 862.442\n",
      "    load_throughput: 46421.971\n",
      "    load_time_ms: 21.542\n",
      "    sample_throughput: 31.323\n",
      "    sample_time_ms: 31925.35\n",
      "    update_time_ms: 4.832\n",
      "  timestamp: 1634855225\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 432\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   432</td><td style=\"text-align: right;\">         11996.9</td><td style=\"text-align: right;\">432000</td><td style=\"text-align: right;\"> -3.3988</td><td style=\"text-align: right;\">               -2.52</td><td style=\"text-align: right;\">               -4.81</td><td style=\"text-align: right;\">            339.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 433000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-27-37\n",
      "  done: false\n",
      "  episode_len_mean: 337.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.5199999999999902\n",
      "  episode_reward_mean: -3.371199999999972\n",
      "  episode_reward_min: -4.809999999999942\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1196\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2739927115238969\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8376344548331367\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0049426997258580525\n",
      "          policy_loss: -0.10571380588743422\n",
      "          total_loss: -0.09941818027032746\n",
      "          vf_explained_var: 0.41927745938301086\n",
      "          vf_loss: 0.013317705846081178\n",
      "    num_agent_steps_sampled: 433000\n",
      "    num_agent_steps_trained: 433000\n",
      "    num_steps_sampled: 433000\n",
      "    num_steps_trained: 433000\n",
      "  iterations_since_restore: 433\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.90666666666667\n",
      "    ram_util_percent: 47.24444444444445\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04008236199970153\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.158231643583953\n",
      "    mean_inference_ms: 1.7508038833308204\n",
      "    mean_raw_obs_processing_ms: 1.775454859517606\n",
      "  time_since_restore: 12028.261237859726\n",
      "  time_this_iter_s: 31.3311767578125\n",
      "  time_total_s: 12028.261237859726\n",
      "  timers:\n",
      "    learn_throughput: 1165.335\n",
      "    learn_time_ms: 858.123\n",
      "    load_throughput: 46664.197\n",
      "    load_time_ms: 21.43\n",
      "    sample_throughput: 31.072\n",
      "    sample_time_ms: 32183.272\n",
      "    update_time_ms: 4.042\n",
      "  timestamp: 1634855257\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 433000\n",
      "  training_iteration: 433\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   433</td><td style=\"text-align: right;\">         12028.3</td><td style=\"text-align: right;\">433000</td><td style=\"text-align: right;\"> -3.3712</td><td style=\"text-align: right;\">               -2.52</td><td style=\"text-align: right;\">               -4.81</td><td style=\"text-align: right;\">            337.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 434000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-28-24\n",
      "  done: false\n",
      "  episode_len_mean: 334.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.340999999999972\n",
      "  episode_reward_min: -4.809999999999942\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1200\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.13699635576194846\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.813706229130427\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007789994898910279\n",
      "          policy_loss: -0.007865459637509451\n",
      "          total_loss: -0.0023662043942345515\n",
      "          vf_explained_var: 0.4818163216114044\n",
      "          vf_loss: 0.012569116935547854\n",
      "    num_agent_steps_sampled: 434000\n",
      "    num_agent_steps_trained: 434000\n",
      "    num_steps_sampled: 434000\n",
      "    num_steps_trained: 434000\n",
      "  iterations_since_restore: 434\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.3764705882353\n",
      "    ram_util_percent: 47.21176470588235\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040083631579782494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.164268853029906\n",
      "    mean_inference_ms: 1.7508571690140957\n",
      "    mean_raw_obs_processing_ms: 1.7768550663990064\n",
      "  time_since_restore: 12075.572001457214\n",
      "  time_this_iter_s: 47.3107635974884\n",
      "  time_total_s: 12075.572001457214\n",
      "  timers:\n",
      "    learn_throughput: 1160.313\n",
      "    learn_time_ms: 861.836\n",
      "    load_throughput: 49016.401\n",
      "    load_time_ms: 20.401\n",
      "    sample_throughput: 29.537\n",
      "    sample_time_ms: 33855.293\n",
      "    update_time_ms: 5.049\n",
      "  timestamp: 1634855304\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 434000\n",
      "  training_iteration: 434\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   434</td><td style=\"text-align: right;\">         12075.6</td><td style=\"text-align: right;\">434000</td><td style=\"text-align: right;\">  -3.341</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.81</td><td style=\"text-align: right;\">             334.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 435000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-28-55\n",
      "  done: false\n",
      "  episode_len_mean: 333.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.3326999999999725\n",
      "  episode_reward_min: -4.809999999999942\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1203\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.13699635576194846\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4457876755131616\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012740687669897601\n",
      "          policy_loss: 0.03116162030233277\n",
      "          total_loss: 0.02484204351074166\n",
      "          vf_explained_var: 0.6075571775436401\n",
      "          vf_loss: 0.006392868623758356\n",
      "    num_agent_steps_sampled: 435000\n",
      "    num_agent_steps_trained: 435000\n",
      "    num_steps_sampled: 435000\n",
      "    num_steps_trained: 435000\n",
      "  iterations_since_restore: 435\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.33255813953488\n",
      "    ram_util_percent: 47.01162790697675\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04008462248505376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.16917852269848\n",
      "    mean_inference_ms: 1.7508974374762667\n",
      "    mean_raw_obs_processing_ms: 1.7779307608673958\n",
      "  time_since_restore: 12105.944638967514\n",
      "  time_this_iter_s: 30.372637510299683\n",
      "  time_total_s: 12105.944638967514\n",
      "  timers:\n",
      "    learn_throughput: 1162.199\n",
      "    learn_time_ms: 860.438\n",
      "    load_throughput: 51347.738\n",
      "    load_time_ms: 19.475\n",
      "    sample_throughput: 29.573\n",
      "    sample_time_ms: 33814.081\n",
      "    update_time_ms: 4.988\n",
      "  timestamp: 1634855335\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 435000\n",
      "  training_iteration: 435\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   435</td><td style=\"text-align: right;\">         12105.9</td><td style=\"text-align: right;\">435000</td><td style=\"text-align: right;\"> -3.3327</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.81</td><td style=\"text-align: right;\">            333.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 436000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-29-27\n",
      "  done: false\n",
      "  episode_len_mean: 329.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.2927999999999735\n",
      "  episode_reward_min: -4.329999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1206\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.13699635576194846\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9561462978521983\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02084809448773248\n",
      "          policy_loss: -0.12689340429173576\n",
      "          total_loss: -0.12236380709542169\n",
      "          vf_explained_var: 0.5563479661941528\n",
      "          vf_loss: 0.011234945576224063\n",
      "    num_agent_steps_sampled: 436000\n",
      "    num_agent_steps_trained: 436000\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "  iterations_since_restore: 436\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.17659574468085\n",
      "    ram_util_percent: 47.008510638297864\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04008566641440482\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.174760155112807\n",
      "    mean_inference_ms: 1.7509397999425451\n",
      "    mean_raw_obs_processing_ms: 1.7790141248065867\n",
      "  time_since_restore: 12138.789057731628\n",
      "  time_this_iter_s: 32.84441876411438\n",
      "  time_total_s: 12138.789057731628\n",
      "  timers:\n",
      "    learn_throughput: 1162.183\n",
      "    learn_time_ms: 860.45\n",
      "    load_throughput: 49552.524\n",
      "    load_time_ms: 20.181\n",
      "    sample_throughput: 31.041\n",
      "    sample_time_ms: 32214.989\n",
      "    update_time_ms: 4.911\n",
      "  timestamp: 1634855367\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 436\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   436</td><td style=\"text-align: right;\">         12138.8</td><td style=\"text-align: right;\">436000</td><td style=\"text-align: right;\"> -3.2928</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.33</td><td style=\"text-align: right;\">            329.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 437000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-29-59\n",
      "  done: false\n",
      "  episode_len_mean: 327.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.2744999999999744\n",
      "  episode_reward_min: -4.329999999999952\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1210\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20549453364292275\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1651942100789812\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011809656292446943\n",
      "          policy_loss: 0.01266386496524016\n",
      "          total_loss: 0.011169285823901495\n",
      "          vf_explained_var: 0.5958056449890137\n",
      "          vf_loss: 0.007730548091543218\n",
      "    num_agent_steps_sampled: 437000\n",
      "    num_agent_steps_trained: 437000\n",
      "    num_steps_sampled: 437000\n",
      "    num_steps_trained: 437000\n",
      "  iterations_since_restore: 437\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.15217391304348\n",
      "    ram_util_percent: 47.047826086956526\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0400869705695475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.182421238654612\n",
      "    mean_inference_ms: 1.7509932186535087\n",
      "    mean_raw_obs_processing_ms: 1.7801429900052625\n",
      "  time_since_restore: 12170.753394126892\n",
      "  time_this_iter_s: 31.964336395263672\n",
      "  time_total_s: 12170.753394126892\n",
      "  timers:\n",
      "    learn_throughput: 1162.996\n",
      "    learn_time_ms: 859.848\n",
      "    load_throughput: 49463.349\n",
      "    load_time_ms: 20.217\n",
      "    sample_throughput: 30.906\n",
      "    sample_time_ms: 32356.438\n",
      "    update_time_ms: 4.724\n",
      "  timestamp: 1634855399\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 437000\n",
      "  training_iteration: 437\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   437</td><td style=\"text-align: right;\">         12170.8</td><td style=\"text-align: right;\">437000</td><td style=\"text-align: right;\"> -3.2745</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.33</td><td style=\"text-align: right;\">            327.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 438000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-30-33\n",
      "  done: false\n",
      "  episode_len_mean: 324.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.242499999999975\n",
      "  episode_reward_min: -4.329999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1213\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20549453364292275\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.15067683590783\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019228333749869965\n",
      "          policy_loss: -0.1234299456079801\n",
      "          total_loss: -0.11593474853369924\n",
      "          vf_explained_var: 0.4088244140148163\n",
      "          vf_loss: 0.015050650263826052\n",
      "    num_agent_steps_sampled: 438000\n",
      "    num_agent_steps_trained: 438000\n",
      "    num_steps_sampled: 438000\n",
      "    num_steps_trained: 438000\n",
      "  iterations_since_restore: 438\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.20638297872341\n",
      "    ram_util_percent: 47.12553191489362\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04008789871633744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.188621815947943\n",
      "    mean_inference_ms: 1.7510340495380499\n",
      "    mean_raw_obs_processing_ms: 1.780076214500073\n",
      "  time_since_restore: 12203.920973062515\n",
      "  time_this_iter_s: 33.16757893562317\n",
      "  time_total_s: 12203.920973062515\n",
      "  timers:\n",
      "    learn_throughput: 1158.103\n",
      "    learn_time_ms: 863.481\n",
      "    load_throughput: 47210.505\n",
      "    load_time_ms: 21.182\n",
      "    sample_throughput: 30.981\n",
      "    sample_time_ms: 32277.996\n",
      "    update_time_ms: 4.583\n",
      "  timestamp: 1634855433\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 438000\n",
      "  training_iteration: 438\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   438</td><td style=\"text-align: right;\">         12203.9</td><td style=\"text-align: right;\">438000</td><td style=\"text-align: right;\"> -3.2425</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.33</td><td style=\"text-align: right;\">            324.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 439000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-31-04\n",
      "  done: false\n",
      "  episode_len_mean: 320.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.204999999999975\n",
      "  episode_reward_min: -4.329999999999952\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1217\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20549453364292275\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2980803668498992\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.05308394840052026\n",
      "          policy_loss: -0.023279474965400167\n",
      "          total_loss: -0.012553546701868375\n",
      "          vf_explained_var: 0.4393017292022705\n",
      "          vf_loss: 0.01279827132821083\n",
      "    num_agent_steps_sampled: 439000\n",
      "    num_agent_steps_trained: 439000\n",
      "    num_steps_sampled: 439000\n",
      "    num_steps_trained: 439000\n",
      "  iterations_since_restore: 439\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.07499999999999\n",
      "    ram_util_percent: 47.16818181818182\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04008923090141643\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.19757404743599\n",
      "    mean_inference_ms: 1.7510885478689522\n",
      "    mean_raw_obs_processing_ms: 1.7800697989260816\n",
      "  time_since_restore: 12235.003253221512\n",
      "  time_this_iter_s: 31.082280158996582\n",
      "  time_total_s: 12235.003253221512\n",
      "  timers:\n",
      "    learn_throughput: 1154.109\n",
      "    learn_time_ms: 866.469\n",
      "    load_throughput: 44882.964\n",
      "    load_time_ms: 22.28\n",
      "    sample_throughput: 30.884\n",
      "    sample_time_ms: 32379.338\n",
      "    update_time_ms: 4.555\n",
      "  timestamp: 1634855464\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 439000\n",
      "  training_iteration: 439\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   439</td><td style=\"text-align: right;\">           12235</td><td style=\"text-align: right;\">439000</td><td style=\"text-align: right;\">  -3.205</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.33</td><td style=\"text-align: right;\">             320.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-31-35\n",
      "  done: false\n",
      "  episode_len_mean: 319.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.1993999999999754\n",
      "  episode_reward_min: -4.329999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1220\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3082418004643842\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6722670396169026\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.024902167510962296\n",
      "          policy_loss: -0.0109074166458514\n",
      "          total_loss: -0.014194357146819433\n",
      "          vf_explained_var: 0.36409637331962585\n",
      "          vf_loss: 0.005759838343753169\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_agent_steps_trained: 440000\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "  iterations_since_restore: 440\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.8088888888889\n",
      "    ram_util_percent: 47.20888888888889\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040090318226725936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.204644496692364\n",
      "    mean_inference_ms: 1.7511321444718417\n",
      "    mean_raw_obs_processing_ms: 1.7801031770662243\n",
      "  time_since_restore: 12266.036676168442\n",
      "  time_this_iter_s: 31.03342294692993\n",
      "  time_total_s: 12266.036676168442\n",
      "  timers:\n",
      "    learn_throughput: 1184.302\n",
      "    learn_time_ms: 844.379\n",
      "    load_throughput: 43224.606\n",
      "    load_time_ms: 23.135\n",
      "    sample_throughput: 30.846\n",
      "    sample_time_ms: 32419.271\n",
      "    update_time_ms: 4.299\n",
      "  timestamp: 1634855495\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 440\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   440</td><td style=\"text-align: right;\">           12266</td><td style=\"text-align: right;\">440000</td><td style=\"text-align: right;\"> -3.1994</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.33</td><td style=\"text-align: right;\">            319.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 441000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-32-05\n",
      "  done: false\n",
      "  episode_len_mean: 317.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.179699999999976\n",
      "  episode_reward_min: -4.329999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1223\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.46236270069657603\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0016867644257015\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005376563818115758\n",
      "          policy_loss: -0.10338095103700956\n",
      "          total_loss: -0.09521839179926449\n",
      "          vf_explained_var: 0.28843963146209717\n",
      "          vf_loss: 0.01569350299735864\n",
      "    num_agent_steps_sampled: 441000\n",
      "    num_agent_steps_trained: 441000\n",
      "    num_steps_sampled: 441000\n",
      "    num_steps_trained: 441000\n",
      "  iterations_since_restore: 441\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.59302325581395\n",
      "    ram_util_percent: 47.230232558139534\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04009133458083003\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.211861954645666\n",
      "    mean_inference_ms: 1.7511734401544954\n",
      "    mean_raw_obs_processing_ms: 1.7801447623787636\n",
      "  time_since_restore: 12296.308636903763\n",
      "  time_this_iter_s: 30.271960735321045\n",
      "  time_total_s: 12296.308636903763\n",
      "  timers:\n",
      "    learn_throughput: 1184.098\n",
      "    learn_time_ms: 844.525\n",
      "    load_throughput: 43119.424\n",
      "    load_time_ms: 23.191\n",
      "    sample_throughput: 31.185\n",
      "    sample_time_ms: 32067.205\n",
      "    update_time_ms: 4.663\n",
      "  timestamp: 1634855525\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 441000\n",
      "  training_iteration: 441\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   441</td><td style=\"text-align: right;\">         12296.3</td><td style=\"text-align: right;\">441000</td><td style=\"text-align: right;\"> -3.1797</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.33</td><td style=\"text-align: right;\">            317.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 442000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-32-33\n",
      "  done: false\n",
      "  episode_len_mean: 318.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.1804999999999755\n",
      "  episode_reward_min: -4.329999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1226\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.46236270069657603\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7208070000012716\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012531063640388614\n",
      "          policy_loss: -0.01702045632733239\n",
      "          total_loss: -0.01754109346204334\n",
      "          vf_explained_var: 0.2029591202735901\n",
      "          vf_loss: 0.010893535466554265\n",
      "    num_agent_steps_sampled: 442000\n",
      "    num_agent_steps_trained: 442000\n",
      "    num_steps_sampled: 442000\n",
      "    num_steps_trained: 442000\n",
      "  iterations_since_restore: 442\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.24499999999999\n",
      "    ram_util_percent: 47.17999999999999\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04009231645038147\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.21905037812742\n",
      "    mean_inference_ms: 1.7512146816881549\n",
      "    mean_raw_obs_processing_ms: 1.7801933483488985\n",
      "  time_since_restore: 12324.033998012543\n",
      "  time_this_iter_s: 27.725361108779907\n",
      "  time_total_s: 12324.033998012543\n",
      "  timers:\n",
      "    learn_throughput: 1176.367\n",
      "    learn_time_ms: 850.075\n",
      "    load_throughput: 44075.871\n",
      "    load_time_ms: 22.688\n",
      "    sample_throughput: 31.419\n",
      "    sample_time_ms: 31827.7\n",
      "    update_time_ms: 5.264\n",
      "  timestamp: 1634855553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 442000\n",
      "  training_iteration: 442\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   442</td><td style=\"text-align: right;\">           12324</td><td style=\"text-align: right;\">442000</td><td style=\"text-align: right;\"> -3.1805</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.33</td><td style=\"text-align: right;\">            318.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 443000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-33-04\n",
      "  done: false\n",
      "  episode_len_mean: 316.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.1637999999999766\n",
      "  episode_reward_min: -4.329999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1229\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.46236270069657603\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2772181835439471\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009744214735453562\n",
      "          policy_loss: -0.11493428266710705\n",
      "          total_loss: -0.11391929313540458\n",
      "          vf_explained_var: 0.632897675037384\n",
      "          vf_loss: 0.00928180881568955\n",
      "    num_agent_steps_sampled: 443000\n",
      "    num_agent_steps_trained: 443000\n",
      "    num_steps_sampled: 443000\n",
      "    num_steps_trained: 443000\n",
      "  iterations_since_restore: 443\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.12272727272726\n",
      "    ram_util_percent: 47.13863636363637\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04009326844856747\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.226593844383164\n",
      "    mean_inference_ms: 1.7512558021766673\n",
      "    mean_raw_obs_processing_ms: 1.780291094281494\n",
      "  time_since_restore: 12355.106487989426\n",
      "  time_this_iter_s: 31.072489976882935\n",
      "  time_total_s: 12355.106487989426\n",
      "  timers:\n",
      "    learn_throughput: 1176.05\n",
      "    learn_time_ms: 850.304\n",
      "    load_throughput: 45099.849\n",
      "    load_time_ms: 22.173\n",
      "    sample_throughput: 31.445\n",
      "    sample_time_ms: 31801.676\n",
      "    update_time_ms: 5.675\n",
      "  timestamp: 1634855584\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 443000\n",
      "  training_iteration: 443\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   443</td><td style=\"text-align: right;\">         12355.1</td><td style=\"text-align: right;\">443000</td><td style=\"text-align: right;\"> -3.1638</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.33</td><td style=\"text-align: right;\">            316.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 444000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-33-52\n",
      "  done: false\n",
      "  episode_len_mean: 316.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.162299999999977\n",
      "  episode_reward_min: -4.329999999999952\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1232\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.46236270069657603\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7661660101678636\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016520461960052933\n",
      "          policy_loss: -0.10122596240705914\n",
      "          total_loss: -0.10405580202738444\n",
      "          vf_explained_var: 0.8095064759254456\n",
      "          vf_loss: 0.007193375914357603\n",
      "    num_agent_steps_sampled: 444000\n",
      "    num_agent_steps_trained: 444000\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "  iterations_since_restore: 444\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.20882352941176\n",
      "    ram_util_percent: 47.09705882352941\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040094213675992176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.234408146156582\n",
      "    mean_inference_ms: 1.7512978234526988\n",
      "    mean_raw_obs_processing_ms: 1.7815536959798184\n",
      "  time_since_restore: 12403.167332649231\n",
      "  time_this_iter_s: 48.0608446598053\n",
      "  time_total_s: 12403.167332649231\n",
      "  timers:\n",
      "    learn_throughput: 1178.85\n",
      "    learn_time_ms: 848.285\n",
      "    load_throughput: 42852.28\n",
      "    load_time_ms: 23.336\n",
      "    sample_throughput: 31.369\n",
      "    sample_time_ms: 31878.415\n",
      "    update_time_ms: 4.804\n",
      "  timestamp: 1634855632\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 444\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   444</td><td style=\"text-align: right;\">         12403.2</td><td style=\"text-align: right;\">444000</td><td style=\"text-align: right;\"> -3.1623</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.33</td><td style=\"text-align: right;\">            316.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 445000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-34-24\n",
      "  done: false\n",
      "  episode_len_mean: 313.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.131599999999977\n",
      "  episode_reward_min: -4.239999999999954\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1236\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.46236270069657603\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2056650982962713\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010494582102834235\n",
      "          policy_loss: -0.0650447525911861\n",
      "          total_loss: -0.059938069101836945\n",
      "          vf_explained_var: 0.6174274682998657\n",
      "          vf_loss: 0.01231103129684925\n",
      "    num_agent_steps_sampled: 445000\n",
      "    num_agent_steps_trained: 445000\n",
      "    num_steps_sampled: 445000\n",
      "    num_steps_trained: 445000\n",
      "  iterations_since_restore: 445\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.05869565217392\n",
      "    ram_util_percent: 47.130434782608695\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040095338440988504\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.24536404675042\n",
      "    mean_inference_ms: 1.7513511972154547\n",
      "    mean_raw_obs_processing_ms: 1.7832741345402092\n",
      "  time_since_restore: 12435.100466012955\n",
      "  time_this_iter_s: 31.933133363723755\n",
      "  time_total_s: 12435.100466012955\n",
      "  timers:\n",
      "    learn_throughput: 1177.012\n",
      "    learn_time_ms: 849.609\n",
      "    load_throughput: 41244.685\n",
      "    load_time_ms: 24.246\n",
      "    sample_throughput: 31.219\n",
      "    sample_time_ms: 32031.476\n",
      "    update_time_ms: 5.596\n",
      "  timestamp: 1634855664\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 445000\n",
      "  training_iteration: 445\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   445</td><td style=\"text-align: right;\">         12435.1</td><td style=\"text-align: right;\">445000</td><td style=\"text-align: right;\"> -3.1316</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.24</td><td style=\"text-align: right;\">            313.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 446000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-34-53\n",
      "  done: false\n",
      "  episode_len_mean: 312.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.1221999999999768\n",
      "  episode_reward_min: -4.239999999999954\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1239\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.46236270069657603\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.746115861998664\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020483428074177665\n",
      "          policy_loss: -0.04089505275090535\n",
      "          total_loss: -0.040331804255644484\n",
      "          vf_explained_var: 0.6039376854896545\n",
      "          vf_loss: 0.008553629989425342\n",
      "    num_agent_steps_sampled: 446000\n",
      "    num_agent_steps_trained: 446000\n",
      "    num_steps_sampled: 446000\n",
      "    num_steps_trained: 446000\n",
      "  iterations_since_restore: 446\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.09761904761906\n",
      "    ram_util_percent: 47.171428571428564\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04009601024097842\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.25383000260215\n",
      "    mean_inference_ms: 1.7513881183251752\n",
      "    mean_raw_obs_processing_ms: 1.7845892785244462\n",
      "  time_since_restore: 12464.412460327148\n",
      "  time_this_iter_s: 29.311994314193726\n",
      "  time_total_s: 12464.412460327148\n",
      "  timers:\n",
      "    learn_throughput: 1177.72\n",
      "    learn_time_ms: 849.098\n",
      "    load_throughput: 41446.879\n",
      "    load_time_ms: 24.127\n",
      "    sample_throughput: 31.567\n",
      "    sample_time_ms: 31678.518\n",
      "    update_time_ms: 5.932\n",
      "  timestamp: 1634855693\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 446000\n",
      "  training_iteration: 446\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   446</td><td style=\"text-align: right;\">         12464.4</td><td style=\"text-align: right;\">446000</td><td style=\"text-align: right;\"> -3.1222</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.24</td><td style=\"text-align: right;\">            312.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 447000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-35-22\n",
      "  done: false\n",
      "  episode_len_mean: 311.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.117799999999978\n",
      "  episode_reward_min: -4.239999999999954\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1241\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6935440510448645\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6600804381900363\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.022143036372251897\n",
      "          policy_loss: -0.13830969168080223\n",
      "          total_loss: -0.13460495852761797\n",
      "          vf_explained_var: 0.8990553021430969\n",
      "          vf_loss: 0.004948367875638521\n",
      "    num_agent_steps_sampled: 447000\n",
      "    num_agent_steps_trained: 447000\n",
      "    num_steps_sampled: 447000\n",
      "    num_steps_trained: 447000\n",
      "  iterations_since_restore: 447\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.0048780487805\n",
      "    ram_util_percent: 47.19756097560976\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04009648270107613\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.259585494220147\n",
      "    mean_inference_ms: 1.751412074843624\n",
      "    mean_raw_obs_processing_ms: 1.7846832292761328\n",
      "  time_since_restore: 12492.945022583008\n",
      "  time_this_iter_s: 28.532562255859375\n",
      "  time_total_s: 12492.945022583008\n",
      "  timers:\n",
      "    learn_throughput: 1177.258\n",
      "    learn_time_ms: 849.432\n",
      "    load_throughput: 41209.389\n",
      "    load_time_ms: 24.266\n",
      "    sample_throughput: 31.913\n",
      "    sample_time_ms: 31334.702\n",
      "    update_time_ms: 6.195\n",
      "  timestamp: 1634855722\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 447000\n",
      "  training_iteration: 447\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   447</td><td style=\"text-align: right;\">         12492.9</td><td style=\"text-align: right;\">447000</td><td style=\"text-align: right;\"> -3.1178</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.24</td><td style=\"text-align: right;\">            311.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-35-53\n",
      "  done: false\n",
      "  episode_len_mean: 309.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.0936999999999784\n",
      "  episode_reward_min: -4.179999999999955\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1245\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0403160765672967\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5153109749158225\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008331229300657918\n",
      "          policy_loss: -0.03489912967714998\n",
      "          total_loss: -0.03475940856668684\n",
      "          vf_explained_var: 0.837005078792572\n",
      "          vf_loss: 0.006625713224315809\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_agent_steps_trained: 448000\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "  iterations_since_restore: 448\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.83409090909092\n",
      "    ram_util_percent: 47.21590909090909\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040097501168499895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.271711887651918\n",
      "    mean_inference_ms: 1.7514659476687178\n",
      "    mean_raw_obs_processing_ms: 1.7848871444990368\n",
      "  time_since_restore: 12524.225858449936\n",
      "  time_this_iter_s: 31.2808358669281\n",
      "  time_total_s: 12524.225858449936\n",
      "  timers:\n",
      "    learn_throughput: 1175.055\n",
      "    learn_time_ms: 851.024\n",
      "    load_throughput: 41056.708\n",
      "    load_time_ms: 24.357\n",
      "    sample_throughput: 32.108\n",
      "    sample_time_ms: 31144.511\n",
      "    update_time_ms: 5.984\n",
      "  timestamp: 1634855753\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 448\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   448</td><td style=\"text-align: right;\">         12524.2</td><td style=\"text-align: right;\">448000</td><td style=\"text-align: right;\"> -3.0937</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.18</td><td style=\"text-align: right;\">            309.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 449000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-36-20\n",
      "  done: false\n",
      "  episode_len_mean: 308.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.086899999999978\n",
      "  episode_reward_min: -4.179999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1248\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0403160765672967\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.608872860007816\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007461438892175288\n",
      "          policy_loss: -0.09532445983754265\n",
      "          total_loss: -0.09834218546748161\n",
      "          vf_explained_var: 0.8581292033195496\n",
      "          vf_loss: 0.005308753360683719\n",
      "    num_agent_steps_sampled: 449000\n",
      "    num_agent_steps_trained: 449000\n",
      "    num_steps_sampled: 449000\n",
      "    num_steps_trained: 449000\n",
      "  iterations_since_restore: 449\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.06052631578947\n",
      "    ram_util_percent: 47.25263157894738\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040098184859810766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.28084457931053\n",
      "    mean_inference_ms: 1.7515029499472985\n",
      "    mean_raw_obs_processing_ms: 1.7850847367197633\n",
      "  time_since_restore: 12550.746351242065\n",
      "  time_this_iter_s: 26.520492792129517\n",
      "  time_total_s: 12550.746351242065\n",
      "  timers:\n",
      "    learn_throughput: 1176.838\n",
      "    learn_time_ms: 849.735\n",
      "    load_throughput: 41027.351\n",
      "    load_time_ms: 24.374\n",
      "    sample_throughput: 32.584\n",
      "    sample_time_ms: 30689.736\n",
      "    update_time_ms: 5.982\n",
      "  timestamp: 1634855780\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 449000\n",
      "  training_iteration: 449\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   449</td><td style=\"text-align: right;\">         12550.7</td><td style=\"text-align: right;\">449000</td><td style=\"text-align: right;\"> -3.0869</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.18</td><td style=\"text-align: right;\">            308.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 450000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-36-51\n",
      "  done: false\n",
      "  episode_len_mean: 308.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.080999999999978\n",
      "  episode_reward_min: -4.179999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1251\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0403160765672967\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6004736688401964\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010814895007101817\n",
      "          policy_loss: 0.07196189736326536\n",
      "          total_loss: 0.07072077501151297\n",
      "          vf_explained_var: 0.8271591067314148\n",
      "          vf_loss: 0.003512704903389224\n",
      "    num_agent_steps_sampled: 450000\n",
      "    num_agent_steps_trained: 450000\n",
      "    num_steps_sampled: 450000\n",
      "    num_steps_trained: 450000\n",
      "  iterations_since_restore: 450\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.30000000000001\n",
      "    ram_util_percent: 47.225000000000016\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040099030414786394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.29013191916869\n",
      "    mean_inference_ms: 1.7515406226473391\n",
      "    mean_raw_obs_processing_ms: 1.7852900654415662\n",
      "  time_since_restore: 12581.52447772026\n",
      "  time_this_iter_s: 30.77812647819519\n",
      "  time_total_s: 12581.52447772026\n",
      "  timers:\n",
      "    learn_throughput: 1172.364\n",
      "    learn_time_ms: 852.977\n",
      "    load_throughput: 41050.479\n",
      "    load_time_ms: 24.36\n",
      "    sample_throughput: 32.615\n",
      "    sample_time_ms: 30660.636\n",
      "    update_time_ms: 6.259\n",
      "  timestamp: 1634855811\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 450000\n",
      "  training_iteration: 450\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   450</td><td style=\"text-align: right;\">         12581.5</td><td style=\"text-align: right;\">450000</td><td style=\"text-align: right;\">  -3.081</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.18</td><td style=\"text-align: right;\">             308.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 451000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-37-22\n",
      "  done: false\n",
      "  episode_len_mean: 307.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.073499999999979\n",
      "  episode_reward_min: -4.179999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1254\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0403160765672967\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4980518672201368\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009928748602917972\n",
      "          policy_loss: 0.09825575066109499\n",
      "          total_loss: 0.09679535436961385\n",
      "          vf_explained_var: 0.9297043085098267\n",
      "          vf_loss: 0.003191084582229248\n",
      "    num_agent_steps_sampled: 451000\n",
      "    num_agent_steps_trained: 451000\n",
      "    num_steps_sampled: 451000\n",
      "    num_steps_trained: 451000\n",
      "  iterations_since_restore: 451\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.17111111111112\n",
      "    ram_util_percent: 47.224444444444444\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0400999247373893\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.29954083897811\n",
      "    mean_inference_ms: 1.7515798361713724\n",
      "    mean_raw_obs_processing_ms: 1.7855021704672835\n",
      "  time_since_restore: 12612.985859155655\n",
      "  time_this_iter_s: 31.461381435394287\n",
      "  time_total_s: 12612.985859155655\n",
      "  timers:\n",
      "    learn_throughput: 1174.841\n",
      "    learn_time_ms: 851.179\n",
      "    load_throughput: 41011.665\n",
      "    load_time_ms: 24.383\n",
      "    sample_throughput: 32.486\n",
      "    sample_time_ms: 30782.02\n",
      "    update_time_ms: 5.895\n",
      "  timestamp: 1634855842\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 451000\n",
      "  training_iteration: 451\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   451</td><td style=\"text-align: right;\">           12613</td><td style=\"text-align: right;\">451000</td><td style=\"text-align: right;\"> -3.0735</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.18</td><td style=\"text-align: right;\">            307.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 452000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-37-50\n",
      "  done: false\n",
      "  episode_len_mean: 308.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.085799999999978\n",
      "  episode_reward_min: -4.179999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1257\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0403160765672967\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.792238676548004\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009981452925648348\n",
      "          policy_loss: 0.022142603165573545\n",
      "          total_loss: 0.022296892437669965\n",
      "          vf_explained_var: 0.7265974283218384\n",
      "          vf_loss: 0.00769281233014125\n",
      "    num_agent_steps_sampled: 452000\n",
      "    num_agent_steps_trained: 452000\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "  iterations_since_restore: 452\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.69500000000001\n",
      "    ram_util_percent: 47.154999999999994\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04010082488867873\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.308746137034156\n",
      "    mean_inference_ms: 1.751619425669532\n",
      "    mean_raw_obs_processing_ms: 1.785720549696112\n",
      "  time_since_restore: 12640.7502617836\n",
      "  time_this_iter_s: 27.764402627944946\n",
      "  time_total_s: 12640.7502617836\n",
      "  timers:\n",
      "    learn_throughput: 1175.375\n",
      "    learn_time_ms: 850.792\n",
      "    load_throughput: 39384.393\n",
      "    load_time_ms: 25.391\n",
      "    sample_throughput: 32.482\n",
      "    sample_time_ms: 30786.239\n",
      "    update_time_ms: 5.373\n",
      "  timestamp: 1634855870\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 452\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   452</td><td style=\"text-align: right;\">         12640.8</td><td style=\"text-align: right;\">452000</td><td style=\"text-align: right;\"> -3.0858</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.18</td><td style=\"text-align: right;\">            308.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 453000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-38-39\n",
      "  done: false\n",
      "  episode_len_mean: 307.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.0725999999999782\n",
      "  episode_reward_min: -4.179999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1260\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0403160765672967\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0173111617565156\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006033162018452703\n",
      "          policy_loss: -0.0962620457013448\n",
      "          total_loss: -0.09264104863007863\n",
      "          vf_explained_var: 0.8282946348190308\n",
      "          vf_loss: 0.007517710333276126\n",
      "    num_agent_steps_sampled: 453000\n",
      "    num_agent_steps_trained: 453000\n",
      "    num_steps_sampled: 453000\n",
      "    num_steps_trained: 453000\n",
      "  iterations_since_restore: 453\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.24857142857144\n",
      "    ram_util_percent: 47.1742857142857\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0401018707099076\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.318265678292537\n",
      "    mean_inference_ms: 1.7516587915719242\n",
      "    mean_raw_obs_processing_ms: 1.7870272804075602\n",
      "  time_since_restore: 12690.191410779953\n",
      "  time_this_iter_s: 49.44114899635315\n",
      "  time_total_s: 12690.191410779953\n",
      "  timers:\n",
      "    learn_throughput: 1170.248\n",
      "    learn_time_ms: 854.52\n",
      "    load_throughput: 40184.333\n",
      "    load_time_ms: 24.885\n",
      "    sample_throughput: 30.656\n",
      "    sample_time_ms: 32620.325\n",
      "    update_time_ms: 4.97\n",
      "  timestamp: 1634855919\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 453000\n",
      "  training_iteration: 453\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   453</td><td style=\"text-align: right;\">         12690.2</td><td style=\"text-align: right;\">453000</td><td style=\"text-align: right;\"> -3.0726</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.18</td><td style=\"text-align: right;\">            307.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 454000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-39-13\n",
      "  done: false\n",
      "  episode_len_mean: 306.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.064999999999978\n",
      "  episode_reward_min: -4.179999999999955\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1264\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0403160765672967\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2559118098682827\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007002751145118364\n",
      "          policy_loss: -0.04059258119927512\n",
      "          total_loss: -0.039012231884731186\n",
      "          vf_explained_var: 0.7982091903686523\n",
      "          vf_loss: 0.0068543953510622185\n",
      "    num_agent_steps_sampled: 454000\n",
      "    num_agent_steps_trained: 454000\n",
      "    num_steps_sampled: 454000\n",
      "    num_steps_trained: 454000\n",
      "  iterations_since_restore: 454\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.22244897959185\n",
      "    ram_util_percent: 47.16530612244899\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040103155380752493\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.33127430857339\n",
      "    mean_inference_ms: 1.751709335088554\n",
      "    mean_raw_obs_processing_ms: 1.7888180572823718\n",
      "  time_since_restore: 12724.314395666122\n",
      "  time_this_iter_s: 34.122984886169434\n",
      "  time_total_s: 12724.314395666122\n",
      "  timers:\n",
      "    learn_throughput: 1172.856\n",
      "    learn_time_ms: 852.62\n",
      "    load_throughput: 42111.909\n",
      "    load_time_ms: 23.746\n",
      "    sample_throughput: 32.021\n",
      "    sample_time_ms: 31229.53\n",
      "    update_time_ms: 5.0\n",
      "  timestamp: 1634855953\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 454000\n",
      "  training_iteration: 454\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   454</td><td style=\"text-align: right;\">         12724.3</td><td style=\"text-align: right;\">454000</td><td style=\"text-align: right;\">  -3.065</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.18</td><td style=\"text-align: right;\">             306.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 455000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-39-44\n",
      "  done: false\n",
      "  episode_len_mean: 306.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.065499999999978\n",
      "  episode_reward_min: -4.179999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1267\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0403160765672967\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1336669637097252\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00588335875159858\n",
      "          policy_loss: 0.09427313949498865\n",
      "          total_loss: 0.09325167689886359\n",
      "          vf_explained_var: 0.809240996837616\n",
      "          vf_loss: 0.00419465494212798\n",
      "    num_agent_steps_sampled: 455000\n",
      "    num_agent_steps_trained: 455000\n",
      "    num_steps_sampled: 455000\n",
      "    num_steps_trained: 455000\n",
      "  iterations_since_restore: 455\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.1046511627907\n",
      "    ram_util_percent: 47.199999999999996\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04010407649280827\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.34094912758674\n",
      "    mean_inference_ms: 1.7517465774642875\n",
      "    mean_raw_obs_processing_ms: 1.7901749464147039\n",
      "  time_since_restore: 12754.72454380989\n",
      "  time_this_iter_s: 30.41014814376831\n",
      "  time_total_s: 12754.72454380989\n",
      "  timers:\n",
      "    learn_throughput: 1176.12\n",
      "    learn_time_ms: 850.253\n",
      "    load_throughput: 43837.751\n",
      "    load_time_ms: 22.811\n",
      "    sample_throughput: 32.174\n",
      "    sample_time_ms: 31080.986\n",
      "    update_time_ms: 4.437\n",
      "  timestamp: 1634855984\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 455000\n",
      "  training_iteration: 455\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   455</td><td style=\"text-align: right;\">         12754.7</td><td style=\"text-align: right;\">455000</td><td style=\"text-align: right;\"> -3.0655</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.18</td><td style=\"text-align: right;\">            306.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-40-17\n",
      "  done: false\n",
      "  episode_len_mean: 306.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.065799999999979\n",
      "  episode_reward_min: -4.179999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1270\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0403160765672967\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.055843115515179\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0054899317385560115\n",
      "          policy_loss: -0.1469858433637354\n",
      "          total_loss: -0.14310964287983047\n",
      "          vf_explained_var: 0.680291473865509\n",
      "          vf_loss: 0.00872336721772121\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_agent_steps_trained: 456000\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "  iterations_since_restore: 456\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.48958333333333\n",
      "    ram_util_percent: 47.1625\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0401048932029728\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.35071198735415\n",
      "    mean_inference_ms: 1.75178065196259\n",
      "    mean_raw_obs_processing_ms: 1.791158594999861\n",
      "  time_since_restore: 12788.121137142181\n",
      "  time_this_iter_s: 33.39659333229065\n",
      "  time_total_s: 12788.121137142181\n",
      "  timers:\n",
      "    learn_throughput: 1175.738\n",
      "    learn_time_ms: 850.529\n",
      "    load_throughput: 45910.034\n",
      "    load_time_ms: 21.782\n",
      "    sample_throughput: 31.755\n",
      "    sample_time_ms: 31490.633\n",
      "    update_time_ms: 4.062\n",
      "  timestamp: 1634856017\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 456\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   456</td><td style=\"text-align: right;\">         12788.1</td><td style=\"text-align: right;\">456000</td><td style=\"text-align: right;\"> -3.0658</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.18</td><td style=\"text-align: right;\">            306.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 457000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-40-44\n",
      "  done: false\n",
      "  episode_len_mean: 308.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.084399999999978\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1273\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0403160765672967\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6289387497637007\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013979418236993071\n",
      "          policy_loss: -0.1056510360704528\n",
      "          total_loss: -0.09956281259655952\n",
      "          vf_explained_var: 0.5968689322471619\n",
      "          vf_loss: 0.007834596342096727\n",
      "    num_agent_steps_sampled: 457000\n",
      "    num_agent_steps_trained: 457000\n",
      "    num_steps_sampled: 457000\n",
      "    num_steps_trained: 457000\n",
      "  iterations_since_restore: 457\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.38421052631578\n",
      "    ram_util_percent: 47.136842105263156\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040105703848685656\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.360153826152217\n",
      "    mean_inference_ms: 1.751811549033359\n",
      "    mean_raw_obs_processing_ms: 1.791354123270207\n",
      "  time_since_restore: 12814.38275051117\n",
      "  time_this_iter_s: 26.261613368988037\n",
      "  time_total_s: 12814.38275051117\n",
      "  timers:\n",
      "    learn_throughput: 1174.419\n",
      "    learn_time_ms: 851.485\n",
      "    load_throughput: 45053.294\n",
      "    load_time_ms: 22.196\n",
      "    sample_throughput: 31.987\n",
      "    sample_time_ms: 31262.227\n",
      "    update_time_ms: 3.982\n",
      "  timestamp: 1634856044\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 457000\n",
      "  training_iteration: 457\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   457</td><td style=\"text-align: right;\">         12814.4</td><td style=\"text-align: right;\">457000</td><td style=\"text-align: right;\"> -3.0844</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            308.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 458000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-41-15\n",
      "  done: false\n",
      "  episode_len_mean: 308.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.0801999999999783\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1277\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0403160765672967\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1583998017840915\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004835823292180165\n",
      "          policy_loss: -0.005378632992506027\n",
      "          total_loss: -0.0029100999236106873\n",
      "          vf_explained_var: 0.6193836331367493\n",
      "          vf_loss: 0.009021747458933128\n",
      "    num_agent_steps_sampled: 458000\n",
      "    num_agent_steps_trained: 458000\n",
      "    num_steps_sampled: 458000\n",
      "    num_steps_trained: 458000\n",
      "  iterations_since_restore: 458\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.17727272727274\n",
      "    ram_util_percent: 47.16818181818181\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04010681902416729\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.372559647935375\n",
      "    mean_inference_ms: 1.751852372213943\n",
      "    mean_raw_obs_processing_ms: 1.7916623572416313\n",
      "  time_since_restore: 12845.568756580353\n",
      "  time_this_iter_s: 31.18600606918335\n",
      "  time_total_s: 12845.568756580353\n",
      "  timers:\n",
      "    learn_throughput: 1174.764\n",
      "    learn_time_ms: 851.235\n",
      "    load_throughput: 44226.825\n",
      "    load_time_ms: 22.611\n",
      "    sample_throughput: 31.997\n",
      "    sample_time_ms: 31252.471\n",
      "    update_time_ms: 4.097\n",
      "  timestamp: 1634856075\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 458000\n",
      "  training_iteration: 458\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   458</td><td style=\"text-align: right;\">         12845.6</td><td style=\"text-align: right;\">458000</td><td style=\"text-align: right;\"> -3.0802</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            308.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 459000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-41-46\n",
      "  done: false\n",
      "  episode_len_mean: 308.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.088799999999978\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1280\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5201580382836484\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2124769614802466\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012880704732044279\n",
      "          policy_loss: 0.026725071999761794\n",
      "          total_loss: 0.027456378440062205\n",
      "          vf_explained_var: 0.6564803123474121\n",
      "          vf_loss: 0.006156075782039099\n",
      "    num_agent_steps_sampled: 459000\n",
      "    num_agent_steps_trained: 459000\n",
      "    num_steps_sampled: 459000\n",
      "    num_steps_trained: 459000\n",
      "  iterations_since_restore: 459\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.96\n",
      "    ram_util_percent: 47.153333333333336\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0401077152672755\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.3817768014269\n",
      "    mean_inference_ms: 1.7518836794904258\n",
      "    mean_raw_obs_processing_ms: 1.791871694185045\n",
      "  time_since_restore: 12876.970608711243\n",
      "  time_this_iter_s: 31.401852130889893\n",
      "  time_total_s: 12876.970608711243\n",
      "  timers:\n",
      "    learn_throughput: 1171.586\n",
      "    learn_time_ms: 853.544\n",
      "    load_throughput: 44407.289\n",
      "    load_time_ms: 22.519\n",
      "    sample_throughput: 31.507\n",
      "    sample_time_ms: 31738.558\n",
      "    update_time_ms: 3.811\n",
      "  timestamp: 1634856106\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 459000\n",
      "  training_iteration: 459\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   459</td><td style=\"text-align: right;\">           12877</td><td style=\"text-align: right;\">459000</td><td style=\"text-align: right;\"> -3.0888</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            308.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 460000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-42-17\n",
      "  done: false\n",
      "  episode_len_mean: 308.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.085699999999978\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1283\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5201580382836484\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1622080915504032\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00651159349274053\n",
      "          policy_loss: -0.08490367945697573\n",
      "          total_loss: -0.08286917573875852\n",
      "          vf_explained_var: 0.5212703943252563\n",
      "          vf_loss: 0.010269523484425412\n",
      "    num_agent_steps_sampled: 460000\n",
      "    num_agent_steps_trained: 460000\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "  iterations_since_restore: 460\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.94999999999999\n",
      "    ram_util_percent: 47.16136363636363\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040108737054415776\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.391000686614245\n",
      "    mean_inference_ms: 1.7519147132055728\n",
      "    mean_raw_obs_processing_ms: 1.792090196111758\n",
      "  time_since_restore: 12907.751122236252\n",
      "  time_this_iter_s: 30.780513525009155\n",
      "  time_total_s: 12907.751122236252\n",
      "  timers:\n",
      "    learn_throughput: 1180.367\n",
      "    learn_time_ms: 847.194\n",
      "    load_throughput: 43842.013\n",
      "    load_time_ms: 22.809\n",
      "    sample_throughput: 31.501\n",
      "    sample_time_ms: 31744.858\n",
      "    update_time_ms: 3.583\n",
      "  timestamp: 1634856137\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 460\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   460</td><td style=\"text-align: right;\">         12907.8</td><td style=\"text-align: right;\">460000</td><td style=\"text-align: right;\"> -3.0857</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            308.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 461000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-42-50\n",
      "  done: false\n",
      "  episode_len_mean: 308.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.083799999999978\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1287\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5201580382836484\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.313297290272183\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008612915569449974\n",
      "          policy_loss: -0.05453703800837199\n",
      "          total_loss: -0.055685503780841826\n",
      "          vf_explained_var: 0.7265231609344482\n",
      "          vf_loss: 0.0075044304235941835\n",
      "    num_agent_steps_sampled: 461000\n",
      "    num_agent_steps_trained: 461000\n",
      "    num_steps_sampled: 461000\n",
      "    num_steps_trained: 461000\n",
      "  iterations_since_restore: 461\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.23617021276597\n",
      "    ram_util_percent: 47.20851063829787\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04011027413625066\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.403253619351464\n",
      "    mean_inference_ms: 1.7519572937219516\n",
      "    mean_raw_obs_processing_ms: 1.7924286491353414\n",
      "  time_since_restore: 12940.679620742798\n",
      "  time_this_iter_s: 32.92849850654602\n",
      "  time_total_s: 12940.679620742798\n",
      "  timers:\n",
      "    learn_throughput: 1157.882\n",
      "    learn_time_ms: 863.646\n",
      "    load_throughput: 43689.301\n",
      "    load_time_ms: 22.889\n",
      "    sample_throughput: 31.373\n",
      "    sample_time_ms: 31874.882\n",
      "    update_time_ms: 3.564\n",
      "  timestamp: 1634856170\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 461000\n",
      "  training_iteration: 461\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   461</td><td style=\"text-align: right;\">         12940.7</td><td style=\"text-align: right;\">461000</td><td style=\"text-align: right;\"> -3.0838</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            308.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 462000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-43-38\n",
      "  done: false\n",
      "  episode_len_mean: 308.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.0842999999999785\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1290\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5201580382836484\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1368724279933506\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006232739143350546\n",
      "          policy_loss: -0.004987177666690615\n",
      "          total_loss: -0.007126401613156001\n",
      "          vf_explained_var: 0.7509344220161438\n",
      "          vf_loss: 0.00598749163457089\n",
      "    num_agent_steps_sampled: 462000\n",
      "    num_agent_steps_trained: 462000\n",
      "    num_steps_sampled: 462000\n",
      "    num_steps_trained: 462000\n",
      "  iterations_since_restore: 462\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.89999999999998\n",
      "    ram_util_percent: 47.23235294117647\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040111456387396494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.412336847345056\n",
      "    mean_inference_ms: 1.7519914626028814\n",
      "    mean_raw_obs_processing_ms: 1.7937648893897093\n",
      "  time_since_restore: 12988.660728931427\n",
      "  time_this_iter_s: 47.98110818862915\n",
      "  time_total_s: 12988.660728931427\n",
      "  timers:\n",
      "    learn_throughput: 1158.231\n",
      "    learn_time_ms: 863.386\n",
      "    load_throughput: 45734.424\n",
      "    load_time_ms: 21.865\n",
      "    sample_throughput: 29.501\n",
      "    sample_time_ms: 33897.555\n",
      "    update_time_ms: 3.845\n",
      "  timestamp: 1634856218\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 462000\n",
      "  training_iteration: 462\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   462</td><td style=\"text-align: right;\">         12988.7</td><td style=\"text-align: right;\">462000</td><td style=\"text-align: right;\"> -3.0843</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            308.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 463000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-44-13\n",
      "  done: false\n",
      "  episode_len_mean: 308.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.0834999999999777\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1294\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5201580382836484\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0575324177742005\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005623848279761192\n",
      "          policy_loss: -0.05300512421462271\n",
      "          total_loss: -0.05208182980616887\n",
      "          vf_explained_var: 0.7046127915382385\n",
      "          vf_loss: 0.008573328170718418\n",
      "    num_agent_steps_sampled: 463000\n",
      "    num_agent_steps_trained: 463000\n",
      "    num_steps_sampled: 463000\n",
      "    num_steps_trained: 463000\n",
      "  iterations_since_restore: 463\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.11176470588235\n",
      "    ram_util_percent: 47.12941176470588\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040113025588346486\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.424724394044304\n",
      "    mean_inference_ms: 1.752035605395961\n",
      "    mean_raw_obs_processing_ms: 1.795591523041948\n",
      "  time_since_restore: 13023.959575414658\n",
      "  time_this_iter_s: 35.29884648323059\n",
      "  time_total_s: 13023.959575414658\n",
      "  timers:\n",
      "    learn_throughput: 1155.899\n",
      "    learn_time_ms: 865.127\n",
      "    load_throughput: 45325.011\n",
      "    load_time_ms: 22.063\n",
      "    sample_throughput: 30.787\n",
      "    sample_time_ms: 32481.324\n",
      "    update_time_ms: 3.865\n",
      "  timestamp: 1634856253\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 463000\n",
      "  training_iteration: 463\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   463</td><td style=\"text-align: right;\">           13024</td><td style=\"text-align: right;\">463000</td><td style=\"text-align: right;\"> -3.0835</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            308.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 464000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-44-45\n",
      "  done: false\n",
      "  episode_len_mean: 307.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.4499999999999917\n",
      "  episode_reward_mean: -3.0771999999999777\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1297\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5201580382836484\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7865579624970754\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007205369935808805\n",
      "          policy_loss: -0.03596275274952253\n",
      "          total_loss: -0.033643842488527295\n",
      "          vf_explained_var: 0.7701460123062134\n",
      "          vf_loss: 0.006436556770414528\n",
      "    num_agent_steps_sampled: 464000\n",
      "    num_agent_steps_trained: 464000\n",
      "    num_steps_sampled: 464000\n",
      "    num_steps_trained: 464000\n",
      "  iterations_since_restore: 464\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.98444444444443\n",
      "    ram_util_percent: 47.09555555555557\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0401141140770291\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.433965770104646\n",
      "    mean_inference_ms: 1.7520690900888867\n",
      "    mean_raw_obs_processing_ms: 1.7965923510046011\n",
      "  time_since_restore: 13055.468603134155\n",
      "  time_this_iter_s: 31.50902771949768\n",
      "  time_total_s: 13055.468603134155\n",
      "  timers:\n",
      "    learn_throughput: 1155.555\n",
      "    learn_time_ms: 865.385\n",
      "    load_throughput: 42753.257\n",
      "    load_time_ms: 23.39\n",
      "    sample_throughput: 31.038\n",
      "    sample_time_ms: 32218.241\n",
      "    update_time_ms: 3.877\n",
      "  timestamp: 1634856285\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 464000\n",
      "  training_iteration: 464\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   464</td><td style=\"text-align: right;\">         13055.5</td><td style=\"text-align: right;\">464000</td><td style=\"text-align: right;\"> -3.0772</td><td style=\"text-align: right;\">               -2.45</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            307.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 465000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-45-18\n",
      "  done: false\n",
      "  episode_len_mean: 307.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.52999999999999\n",
      "  episode_reward_mean: -3.075699999999978\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1301\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5201580382836484\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8639602886305915\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0035652856918110686\n",
      "          policy_loss: -0.0380814082092709\n",
      "          total_loss: -0.035967153931657475\n",
      "          vf_explained_var: 0.6721564531326294\n",
      "          vf_loss: 0.00889934596295158\n",
      "    num_agent_steps_sampled: 465000\n",
      "    num_agent_steps_trained: 465000\n",
      "    num_steps_sampled: 465000\n",
      "    num_steps_trained: 465000\n",
      "  iterations_since_restore: 465\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.55208333333333\n",
      "    ram_util_percent: 46.925000000000004\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04011552099494096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.446511133885583\n",
      "    mean_inference_ms: 1.7521145975565822\n",
      "    mean_raw_obs_processing_ms: 1.7969121170326365\n",
      "  time_since_restore: 13089.038128137589\n",
      "  time_this_iter_s: 33.56952500343323\n",
      "  time_total_s: 13089.038128137589\n",
      "  timers:\n",
      "    learn_throughput: 1152.584\n",
      "    learn_time_ms: 867.616\n",
      "    load_throughput: 41101.89\n",
      "    load_time_ms: 24.33\n",
      "    sample_throughput: 30.739\n",
      "    sample_time_ms: 32531.493\n",
      "    update_time_ms: 3.414\n",
      "  timestamp: 1634856318\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 465000\n",
      "  training_iteration: 465\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   465</td><td style=\"text-align: right;\">           13089</td><td style=\"text-align: right;\">465000</td><td style=\"text-align: right;\"> -3.0757</td><td style=\"text-align: right;\">               -2.53</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            307.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 466000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-45-52\n",
      "  done: false\n",
      "  episode_len_mean: 306.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.52999999999999\n",
      "  episode_reward_mean: -3.0664999999999787\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1304\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2600790191418242\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.732845601770613\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0047103773127485085\n",
      "          policy_loss: 0.041827336533202066\n",
      "          total_loss: 0.042367727309465406\n",
      "          vf_explained_var: 0.6507368087768555\n",
      "          vf_loss: 0.006643776688724757\n",
      "    num_agent_steps_sampled: 466000\n",
      "    num_agent_steps_trained: 466000\n",
      "    num_steps_sampled: 466000\n",
      "    num_steps_trained: 466000\n",
      "  iterations_since_restore: 466\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.00425531914892\n",
      "    ram_util_percent: 46.93829787234044\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040116545973175696\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.455974327782883\n",
      "    mean_inference_ms: 1.7521485793662344\n",
      "    mean_raw_obs_processing_ms: 1.797170063272901\n",
      "  time_since_restore: 13122.157358407974\n",
      "  time_this_iter_s: 33.11923027038574\n",
      "  time_total_s: 13122.157358407974\n",
      "  timers:\n",
      "    learn_throughput: 1149.457\n",
      "    learn_time_ms: 869.976\n",
      "    load_throughput: 39192.949\n",
      "    load_time_ms: 25.515\n",
      "    sample_throughput: 30.769\n",
      "    sample_time_ms: 32500.131\n",
      "    update_time_ms: 3.407\n",
      "  timestamp: 1634856352\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 466000\n",
      "  training_iteration: 466\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   466</td><td style=\"text-align: right;\">         13122.2</td><td style=\"text-align: right;\">466000</td><td style=\"text-align: right;\"> -3.0665</td><td style=\"text-align: right;\">               -2.53</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            306.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 467000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-46-26\n",
      "  done: false\n",
      "  episode_len_mean: 305.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.439999999999992\n",
      "  episode_reward_mean: -3.0565999999999787\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1308\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1300395095709121\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7269428829352061\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0046046379697041095\n",
      "          policy_loss: 0.009234965137309498\n",
      "          total_loss: 0.012644903692934249\n",
      "          vf_explained_var: 0.5642585754394531\n",
      "          vf_loss: 0.010080581510232554\n",
      "    num_agent_steps_sampled: 467000\n",
      "    num_agent_steps_trained: 467000\n",
      "    num_steps_sampled: 467000\n",
      "    num_steps_trained: 467000\n",
      "  iterations_since_restore: 467\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.9795918367347\n",
      "    ram_util_percent: 47.02857142857143\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04011784978011043\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.46856552097553\n",
      "    mean_inference_ms: 1.7521917852782791\n",
      "    mean_raw_obs_processing_ms: 1.7975493436707786\n",
      "  time_since_restore: 13156.183377027512\n",
      "  time_this_iter_s: 34.02601861953735\n",
      "  time_total_s: 13156.183377027512\n",
      "  timers:\n",
      "    learn_throughput: 1153.355\n",
      "    learn_time_ms: 867.036\n",
      "    load_throughput: 39903.872\n",
      "    load_time_ms: 25.06\n",
      "    sample_throughput: 30.049\n",
      "    sample_time_ms: 33279.389\n",
      "    update_time_ms: 3.806\n",
      "  timestamp: 1634856386\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 467000\n",
      "  training_iteration: 467\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   467</td><td style=\"text-align: right;\">         13156.2</td><td style=\"text-align: right;\">467000</td><td style=\"text-align: right;\"> -3.0566</td><td style=\"text-align: right;\">               -2.44</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            305.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 468000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-46-59\n",
      "  done: false\n",
      "  episode_len_mean: 306.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.439999999999992\n",
      "  episode_reward_mean: -3.061799999999978\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1311\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06501975478545605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1974327087402343\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017803939710285217\n",
      "          policy_loss: -0.08530656728479598\n",
      "          total_loss: -0.08592209741473197\n",
      "          vf_explained_var: 0.5506964325904846\n",
      "          vf_loss: 0.01020118647461964\n",
      "    num_agent_steps_sampled: 468000\n",
      "    num_agent_steps_trained: 468000\n",
      "    num_steps_sampled: 468000\n",
      "    num_steps_trained: 468000\n",
      "  iterations_since_restore: 468\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.31458333333335\n",
      "    ram_util_percent: 47.133333333333326\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040118854575460015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.478123630517842\n",
      "    mean_inference_ms: 1.75222423577452\n",
      "    mean_raw_obs_processing_ms: 1.7978210646866566\n",
      "  time_since_restore: 13190.044674158096\n",
      "  time_this_iter_s: 33.86129713058472\n",
      "  time_total_s: 13190.044674158096\n",
      "  timers:\n",
      "    learn_throughput: 1156.602\n",
      "    learn_time_ms: 864.602\n",
      "    load_throughput: 40605.456\n",
      "    load_time_ms: 24.627\n",
      "    sample_throughput: 29.806\n",
      "    sample_time_ms: 33549.91\n",
      "    update_time_ms: 3.74\n",
      "  timestamp: 1634856419\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 468000\n",
      "  training_iteration: 468\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   468</td><td style=\"text-align: right;\">           13190</td><td style=\"text-align: right;\">468000</td><td style=\"text-align: right;\"> -3.0618</td><td style=\"text-align: right;\">               -2.44</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            306.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 469000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-47-32\n",
      "  done: false\n",
      "  episode_len_mean: 305.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.439999999999992\n",
      "  episode_reward_mean: -3.0582999999999787\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1315\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06501975478545605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7020716276433733\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00845798086247426\n",
      "          policy_loss: 0.034832861440049274\n",
      "          total_loss: 0.03753267568018701\n",
      "          vf_explained_var: 0.6805333495140076\n",
      "          vf_loss: 0.009170594945963886\n",
      "    num_agent_steps_sampled: 469000\n",
      "    num_agent_steps_trained: 469000\n",
      "    num_steps_sampled: 469000\n",
      "    num_steps_trained: 469000\n",
      "  iterations_since_restore: 469\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.18695652173913\n",
      "    ram_util_percent: 47.21521739130434\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04012019517949298\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.490707735531846\n",
      "    mean_inference_ms: 1.7522647152951891\n",
      "    mean_raw_obs_processing_ms: 1.798218602934591\n",
      "  time_since_restore: 13222.23571228981\n",
      "  time_this_iter_s: 32.19103813171387\n",
      "  time_total_s: 13222.23571228981\n",
      "  timers:\n",
      "    learn_throughput: 1156.415\n",
      "    learn_time_ms: 864.742\n",
      "    load_throughput: 40498.655\n",
      "    load_time_ms: 24.692\n",
      "    sample_throughput: 29.737\n",
      "    sample_time_ms: 33628.227\n",
      "    update_time_ms: 4.263\n",
      "  timestamp: 1634856452\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 469000\n",
      "  training_iteration: 469\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   469</td><td style=\"text-align: right;\">         13222.2</td><td style=\"text-align: right;\">469000</td><td style=\"text-align: right;\"> -3.0583</td><td style=\"text-align: right;\">               -2.44</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            305.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 470000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-48-07\n",
      "  done: false\n",
      "  episode_len_mean: 303.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.439999999999992\n",
      "  episode_reward_mean: -3.0378999999999787\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1319\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06501975478545605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6556137323379516\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006430490990135398\n",
      "          policy_loss: -0.006381729286577966\n",
      "          total_loss: -0.0021518821517626445\n",
      "          vf_explained_var: 0.6023604869842529\n",
      "          vf_loss: 0.010367876963896884\n",
      "    num_agent_steps_sampled: 470000\n",
      "    num_agent_steps_trained: 470000\n",
      "    num_steps_sampled: 470000\n",
      "    num_steps_trained: 470000\n",
      "  iterations_since_restore: 470\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.926\n",
      "    ram_util_percent: 47.211999999999996\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04012149156351165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.503553986334445\n",
      "    mean_inference_ms: 1.752305114088034\n",
      "    mean_raw_obs_processing_ms: 1.7986270921518939\n",
      "  time_since_restore: 13257.035685062408\n",
      "  time_this_iter_s: 34.79997277259827\n",
      "  time_total_s: 13257.035685062408\n",
      "  timers:\n",
      "    learn_throughput: 1155.247\n",
      "    learn_time_ms: 865.616\n",
      "    load_throughput: 41945.599\n",
      "    load_time_ms: 23.84\n",
      "    sample_throughput: 29.386\n",
      "    sample_time_ms: 34029.968\n",
      "    update_time_ms: 4.717\n",
      "  timestamp: 1634856487\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 470000\n",
      "  training_iteration: 470\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   470</td><td style=\"text-align: right;\">           13257</td><td style=\"text-align: right;\">470000</td><td style=\"text-align: right;\"> -3.0379</td><td style=\"text-align: right;\">               -2.44</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            303.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 471000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-48-56\n",
      "  done: false\n",
      "  episode_len_mean: 304.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.439999999999992\n",
      "  episode_reward_mean: -3.0410999999999797\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1322\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.06501975478545605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.039090249935786\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.043384856702173895\n",
      "          policy_loss: 0.036822271015908986\n",
      "          total_loss: 0.03643569598595301\n",
      "          vf_explained_var: 0.7075374126434326\n",
      "          vf_loss: 0.007183455666139101\n",
      "    num_agent_steps_sampled: 471000\n",
      "    num_agent_steps_trained: 471000\n",
      "    num_steps_sampled: 471000\n",
      "    num_steps_trained: 471000\n",
      "  iterations_since_restore: 471\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.65507246376811\n",
      "    ram_util_percent: 47.213043478260865\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04012246197021529\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.513197678039454\n",
      "    mean_inference_ms: 1.7523357684180767\n",
      "    mean_raw_obs_processing_ms: 1.8000305375010273\n",
      "  time_since_restore: 13305.998113155365\n",
      "  time_this_iter_s: 48.96242809295654\n",
      "  time_total_s: 13305.998113155365\n",
      "  timers:\n",
      "    learn_throughput: 1176.543\n",
      "    learn_time_ms: 849.947\n",
      "    load_throughput: 41731.628\n",
      "    load_time_ms: 23.963\n",
      "    sample_throughput: 28.052\n",
      "    sample_time_ms: 35648.674\n",
      "    update_time_ms: 4.865\n",
      "  timestamp: 1634856536\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 471000\n",
      "  training_iteration: 471\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   471</td><td style=\"text-align: right;\">           13306</td><td style=\"text-align: right;\">471000</td><td style=\"text-align: right;\"> -3.0411</td><td style=\"text-align: right;\">               -2.44</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            304.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 472000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-49-29\n",
      "  done: false\n",
      "  episode_len_mean: 301.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.439999999999992\n",
      "  episode_reward_mean: -3.0185999999999797\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1326\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09752963217818403\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.841186934709549\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0075147580930414095\n",
      "          policy_loss: -0.018629476345247694\n",
      "          total_loss: -0.018223907550175986\n",
      "          vf_explained_var: 0.6575758457183838\n",
      "          vf_loss: 0.008084527801515327\n",
      "    num_agent_steps_sampled: 472000\n",
      "    num_agent_steps_trained: 472000\n",
      "    num_steps_sampled: 472000\n",
      "    num_steps_trained: 472000\n",
      "  iterations_since_restore: 472\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.96458333333334\n",
      "    ram_util_percent: 47.1625\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040123829048880656\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.526379246467485\n",
      "    mean_inference_ms: 1.75237980817849\n",
      "    mean_raw_obs_processing_ms: 1.801921658452439\n",
      "  time_since_restore: 13339.045835494995\n",
      "  time_this_iter_s: 33.04772233963013\n",
      "  time_total_s: 13339.045835494995\n",
      "  timers:\n",
      "    learn_throughput: 1178.505\n",
      "    learn_time_ms: 848.533\n",
      "    load_throughput: 41670.308\n",
      "    load_time_ms: 23.998\n",
      "    sample_throughput: 29.277\n",
      "    sample_time_ms: 34156.991\n",
      "    update_time_ms: 4.574\n",
      "  timestamp: 1634856569\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 472000\n",
      "  training_iteration: 472\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   472</td><td style=\"text-align: right;\">           13339</td><td style=\"text-align: right;\">472000</td><td style=\"text-align: right;\"> -3.0186</td><td style=\"text-align: right;\">               -2.44</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            301.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 473000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-50-02\n",
      "  done: false\n",
      "  episode_len_mean: 300.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.439999999999992\n",
      "  episode_reward_mean: -3.009899999999979\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1329\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09752963217818403\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9423380335172017\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011421880960405102\n",
      "          policy_loss: 0.034606580519013934\n",
      "          total_loss: 0.033546061482694414\n",
      "          vf_explained_var: 0.6750606298446655\n",
      "          vf_loss: 0.007248884671005524\n",
      "    num_agent_steps_sampled: 473000\n",
      "    num_agent_steps_trained: 473000\n",
      "    num_steps_sampled: 473000\n",
      "    num_steps_trained: 473000\n",
      "  iterations_since_restore: 473\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.85416666666667\n",
      "    ram_util_percent: 47.133333333333326\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04012490779429346\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.536346928737483\n",
      "    mean_inference_ms: 1.7524121494107703\n",
      "    mean_raw_obs_processing_ms: 1.8033699267500851\n",
      "  time_since_restore: 13372.601306915283\n",
      "  time_this_iter_s: 33.555471420288086\n",
      "  time_total_s: 13372.601306915283\n",
      "  timers:\n",
      "    learn_throughput: 1162.302\n",
      "    learn_time_ms: 860.361\n",
      "    load_throughput: 41037.306\n",
      "    load_time_ms: 24.368\n",
      "    sample_throughput: 29.438\n",
      "    sample_time_ms: 33969.397\n",
      "    update_time_ms: 5.371\n",
      "  timestamp: 1634856602\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 473000\n",
      "  training_iteration: 473\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   473</td><td style=\"text-align: right;\">         13372.6</td><td style=\"text-align: right;\">473000</td><td style=\"text-align: right;\"> -3.0099</td><td style=\"text-align: right;\">               -2.44</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            300.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 474000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-50-37\n",
      "  done: false\n",
      "  episode_len_mean: 298.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.439999999999992\n",
      "  episode_reward_mean: -2.98889999999998\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1333\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09752963217818403\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7938600076569451\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010791373588624253\n",
      "          policy_loss: -0.021786310896277427\n",
      "          total_loss: -0.017982562548584408\n",
      "          vf_explained_var: 0.5424261093139648\n",
      "          vf_loss: 0.010689872037619352\n",
      "    num_agent_steps_sampled: 474000\n",
      "    num_agent_steps_trained: 474000\n",
      "    num_steps_sampled: 474000\n",
      "    num_steps_trained: 474000\n",
      "  iterations_since_restore: 474\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.698\n",
      "    ram_util_percent: 47.15000000000002\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040126316281533875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.549803037143633\n",
      "    mean_inference_ms: 1.7524542792998568\n",
      "    mean_raw_obs_processing_ms: 1.8038029783761105\n",
      "  time_since_restore: 13407.64859509468\n",
      "  time_this_iter_s: 35.04728817939758\n",
      "  time_total_s: 13407.64859509468\n",
      "  timers:\n",
      "    learn_throughput: 1162.045\n",
      "    learn_time_ms: 860.552\n",
      "    load_throughput: 43339.705\n",
      "    load_time_ms: 23.074\n",
      "    sample_throughput: 29.134\n",
      "    sample_time_ms: 34324.45\n",
      "    update_time_ms: 5.352\n",
      "  timestamp: 1634856637\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 474000\n",
      "  training_iteration: 474\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   474</td><td style=\"text-align: right;\">         13407.6</td><td style=\"text-align: right;\">474000</td><td style=\"text-align: right;\"> -2.9889</td><td style=\"text-align: right;\">               -2.44</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            298.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 475000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-51-11\n",
      "  done: false\n",
      "  episode_len_mean: 298.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.439999999999992\n",
      "  episode_reward_mean: -2.98319999999998\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1336\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09752963217818403\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5672854161924786\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008725321354629835\n",
      "          policy_loss: -0.005631541005439228\n",
      "          total_loss: -0.0024880739135874643\n",
      "          vf_explained_var: 0.5254338979721069\n",
      "          vf_loss: 0.007965344651084807\n",
      "    num_agent_steps_sampled: 475000\n",
      "    num_agent_steps_trained: 475000\n",
      "    num_steps_sampled: 475000\n",
      "    num_steps_trained: 475000\n",
      "  iterations_since_restore: 475\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.18125000000002\n",
      "    ram_util_percent: 47.335416666666674\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04012739356122411\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.560077322031997\n",
      "    mean_inference_ms: 1.7524866769283935\n",
      "    mean_raw_obs_processing_ms: 1.8041050950360267\n",
      "  time_since_restore: 13441.633598804474\n",
      "  time_this_iter_s: 33.98500370979309\n",
      "  time_total_s: 13441.633598804474\n",
      "  timers:\n",
      "    learn_throughput: 1165.436\n",
      "    learn_time_ms: 858.048\n",
      "    load_throughput: 44580.607\n",
      "    load_time_ms: 22.431\n",
      "    sample_throughput: 29.096\n",
      "    sample_time_ms: 34369.262\n",
      "    update_time_ms: 5.215\n",
      "  timestamp: 1634856671\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 475000\n",
      "  training_iteration: 475\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   475</td><td style=\"text-align: right;\">         13441.6</td><td style=\"text-align: right;\">475000</td><td style=\"text-align: right;\"> -2.9832</td><td style=\"text-align: right;\">               -2.44</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            298.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 476000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-51-44\n",
      "  done: false\n",
      "  episode_len_mean: 297.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.439999999999992\n",
      "  episode_reward_mean: -2.97339999999998\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1339\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09752963217818403\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7225643191072676\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014720081283933853\n",
      "          policy_loss: -0.1487306756277879\n",
      "          total_loss: -0.1423749460114373\n",
      "          vf_explained_var: 0.39942067861557007\n",
      "          vf_loss: 0.01214572699326608\n",
      "    num_agent_steps_sampled: 476000\n",
      "    num_agent_steps_trained: 476000\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "  iterations_since_restore: 476\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.12391304347825\n",
      "    ram_util_percent: 47.30434782608695\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040128479555685305\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.570509397485242\n",
      "    mean_inference_ms: 1.7525197636819843\n",
      "    mean_raw_obs_processing_ms: 1.8044132151700851\n",
      "  time_since_restore: 13474.016454935074\n",
      "  time_this_iter_s: 32.382856130599976\n",
      "  time_total_s: 13474.016454935074\n",
      "  timers:\n",
      "    learn_throughput: 1165.717\n",
      "    learn_time_ms: 857.841\n",
      "    load_throughput: 46478.506\n",
      "    load_time_ms: 21.515\n",
      "    sample_throughput: 29.157\n",
      "    sample_time_ms: 34296.772\n",
      "    update_time_ms: 5.241\n",
      "  timestamp: 1634856704\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 476\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   476</td><td style=\"text-align: right;\">           13474</td><td style=\"text-align: right;\">476000</td><td style=\"text-align: right;\"> -2.9734</td><td style=\"text-align: right;\">               -2.44</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            297.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 477000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-52-19\n",
      "  done: false\n",
      "  episode_len_mean: 295.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.439999999999992\n",
      "  episode_reward_mean: -2.9511999999999805\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1343\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09752963217818403\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4541748176018397\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005054678426167432\n",
      "          policy_loss: -0.034677093972762425\n",
      "          total_loss: -0.027394323299328487\n",
      "          vf_explained_var: 0.3973146080970764\n",
      "          vf_loss: 0.011331538949161769\n",
      "    num_agent_steps_sampled: 477000\n",
      "    num_agent_steps_trained: 477000\n",
      "    num_steps_sampled: 477000\n",
      "    num_steps_trained: 477000\n",
      "  iterations_since_restore: 477\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.17843137254901\n",
      "    ram_util_percent: 47.28431372549019\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04012988157629926\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.584715204517266\n",
      "    mean_inference_ms: 1.7525607723465044\n",
      "    mean_raw_obs_processing_ms: 1.8049082064874225\n",
      "  time_since_restore: 13509.181143045425\n",
      "  time_this_iter_s: 35.16468811035156\n",
      "  time_total_s: 13509.181143045425\n",
      "  timers:\n",
      "    learn_throughput: 1166.343\n",
      "    learn_time_ms: 857.38\n",
      "    load_throughput: 48830.198\n",
      "    load_time_ms: 20.479\n",
      "    sample_throughput: 29.059\n",
      "    sample_time_ms: 34412.78\n",
      "    update_time_ms: 4.778\n",
      "  timestamp: 1634856739\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 477000\n",
      "  training_iteration: 477\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   477</td><td style=\"text-align: right;\">         13509.2</td><td style=\"text-align: right;\">477000</td><td style=\"text-align: right;\"> -2.9512</td><td style=\"text-align: right;\">               -2.44</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            295.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 478000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-52-54\n",
      "  done: false\n",
      "  episode_len_mean: 292.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.439999999999992\n",
      "  episode_reward_mean: -2.9256999999999813\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1347\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09752963217818403\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.49697572224669984\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00836815733988598\n",
      "          policy_loss: 0.012218754324648116\n",
      "          total_loss: 0.019482154150803885\n",
      "          vf_explained_var: 0.42362070083618164\n",
      "          vf_loss: 0.011417011057751046\n",
      "    num_agent_steps_sampled: 478000\n",
      "    num_agent_steps_trained: 478000\n",
      "    num_steps_sampled: 478000\n",
      "    num_steps_trained: 478000\n",
      "  iterations_since_restore: 478\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.88979591836735\n",
      "    ram_util_percent: 47.344897959183676\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04013151973495846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.599362610216485\n",
      "    mean_inference_ms: 1.7526030614922314\n",
      "    mean_raw_obs_processing_ms: 1.8054113903624138\n",
      "  time_since_restore: 13543.875061035156\n",
      "  time_this_iter_s: 34.693917989730835\n",
      "  time_total_s: 13543.875061035156\n",
      "  timers:\n",
      "    learn_throughput: 1168.617\n",
      "    learn_time_ms: 855.712\n",
      "    load_throughput: 51692.951\n",
      "    load_time_ms: 19.345\n",
      "    sample_throughput: 28.987\n",
      "    sample_time_ms: 34498.314\n",
      "    update_time_ms: 5.154\n",
      "  timestamp: 1634856774\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 478000\n",
      "  training_iteration: 478\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   478</td><td style=\"text-align: right;\">         13543.9</td><td style=\"text-align: right;\">478000</td><td style=\"text-align: right;\"> -2.9257</td><td style=\"text-align: right;\">               -2.44</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            292.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 479000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-53-47\n",
      "  done: false\n",
      "  episode_len_mean: 289.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -2.8973999999999815\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1351\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09752963217818403\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6459424952665965\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008772049288100605\n",
      "          policy_loss: 0.023172561327616373\n",
      "          total_loss: 0.02844389950235685\n",
      "          vf_explained_var: 0.44378793239593506\n",
      "          vf_loss: 0.010875225734586517\n",
      "    num_agent_steps_sampled: 479000\n",
      "    num_agent_steps_trained: 479000\n",
      "    num_steps_sampled: 479000\n",
      "    num_steps_trained: 479000\n",
      "  iterations_since_restore: 479\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.07631578947367\n",
      "    ram_util_percent: 47.222368421052636\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040133928297863636\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.614463138326073\n",
      "    mean_inference_ms: 1.7526432777248306\n",
      "    mean_raw_obs_processing_ms: 1.8073997208062307\n",
      "  time_since_restore: 13596.838949680328\n",
      "  time_this_iter_s: 52.96388864517212\n",
      "  time_total_s: 13596.838949680328\n",
      "  timers:\n",
      "    learn_throughput: 1170.325\n",
      "    learn_time_ms: 854.464\n",
      "    load_throughput: 53826.403\n",
      "    load_time_ms: 18.578\n",
      "    sample_throughput: 27.339\n",
      "    sample_time_ms: 36578.192\n",
      "    update_time_ms: 4.635\n",
      "  timestamp: 1634856827\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 479000\n",
      "  training_iteration: 479\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   479</td><td style=\"text-align: right;\">         13596.8</td><td style=\"text-align: right;\">479000</td><td style=\"text-align: right;\"> -2.8974</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            289.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 480000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-54-22\n",
      "  done: false\n",
      "  episode_len_mean: 288.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -2.8799999999999826\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1355\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09752963217818403\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6422377202245925\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007847468416715249\n",
      "          policy_loss: 0.005858300543493695\n",
      "          total_loss: 0.012699977142943276\n",
      "          vf_explained_var: 0.4086560904979706\n",
      "          vf_loss: 0.012498693075031042\n",
      "    num_agent_steps_sampled: 480000\n",
      "    num_agent_steps_trained: 480000\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "  iterations_since_restore: 480\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.11176470588235\n",
      "    ram_util_percent: 47.08039215686275\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04013625341505361\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.629741957242622\n",
      "    mean_inference_ms: 1.7526805174686604\n",
      "    mean_raw_obs_processing_ms: 1.8094652454258533\n",
      "  time_since_restore: 13632.673486232758\n",
      "  time_this_iter_s: 35.8345365524292\n",
      "  time_total_s: 13632.673486232758\n",
      "  timers:\n",
      "    learn_throughput: 1170.07\n",
      "    learn_time_ms: 854.65\n",
      "    load_throughput: 52355.045\n",
      "    load_time_ms: 19.1\n",
      "    sample_throughput: 27.262\n",
      "    sample_time_ms: 36681.648\n",
      "    update_time_ms: 3.953\n",
      "  timestamp: 1634856862\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 480\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   480</td><td style=\"text-align: right;\">         13632.7</td><td style=\"text-align: right;\">480000</td><td style=\"text-align: right;\">   -2.88</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">               288</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 481000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-54-59\n",
      "  done: false\n",
      "  episode_len_mean: 285.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -2.8581999999999823\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1358\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09752963217818403\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6909684816996257\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014355215859639496\n",
      "          policy_loss: -0.10983141544792387\n",
      "          total_loss: -0.10311220337947209\n",
      "          vf_explained_var: 0.40127304196357727\n",
      "          vf_loss: 0.012228838964882824\n",
      "    num_agent_steps_sampled: 481000\n",
      "    num_agent_steps_trained: 481000\n",
      "    num_steps_sampled: 481000\n",
      "    num_steps_trained: 481000\n",
      "  iterations_since_restore: 481\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.13461538461539\n",
      "    ram_util_percent: 47.11730769230769\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04013796715528147\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.64162024498479\n",
      "    mean_inference_ms: 1.7527071616906085\n",
      "    mean_raw_obs_processing_ms: 1.810652194870413\n",
      "  time_since_restore: 13669.423623800278\n",
      "  time_this_iter_s: 36.75013756752014\n",
      "  time_total_s: 13669.423623800278\n",
      "  timers:\n",
      "    learn_throughput: 1169.776\n",
      "    learn_time_ms: 854.865\n",
      "    load_throughput: 52898.539\n",
      "    load_time_ms: 18.904\n",
      "    sample_throughput: 28.2\n",
      "    sample_time_ms: 35460.697\n",
      "    update_time_ms: 3.848\n",
      "  timestamp: 1634856899\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 481000\n",
      "  training_iteration: 481\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   481</td><td style=\"text-align: right;\">         13669.4</td><td style=\"text-align: right;\">481000</td><td style=\"text-align: right;\"> -2.8582</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            285.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 482000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-55-36\n",
      "  done: false\n",
      "  episode_len_mean: 284.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -2.847099999999983\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1362\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09752963217818403\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.604940465092659\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023152161060429825\n",
      "          policy_loss: -0.017967298006018\n",
      "          total_loss: -0.011019663761059444\n",
      "          vf_explained_var: 0.426461398601532\n",
      "          vf_loss: 0.010739016657074293\n",
      "    num_agent_steps_sampled: 482000\n",
      "    num_agent_steps_trained: 482000\n",
      "    num_steps_sampled: 482000\n",
      "    num_steps_trained: 482000\n",
      "  iterations_since_restore: 482\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.12075471698114\n",
      "    ram_util_percent: 47.424528301886795\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040140309885128046\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.657597072368855\n",
      "    mean_inference_ms: 1.7527490876414071\n",
      "    mean_raw_obs_processing_ms: 1.8112704774796011\n",
      "  time_since_restore: 13706.48271536827\n",
      "  time_this_iter_s: 37.059091567993164\n",
      "  time_total_s: 13706.48271536827\n",
      "  timers:\n",
      "    learn_throughput: 1169.033\n",
      "    learn_time_ms: 855.408\n",
      "    load_throughput: 50179.924\n",
      "    load_time_ms: 19.928\n",
      "    sample_throughput: 27.887\n",
      "    sample_time_ms: 35859.596\n",
      "    update_time_ms: 4.567\n",
      "  timestamp: 1634856936\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 482000\n",
      "  training_iteration: 482\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   482</td><td style=\"text-align: right;\">         13706.5</td><td style=\"text-align: right;\">482000</td><td style=\"text-align: right;\"> -2.8471</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            284.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 483000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-56-10\n",
      "  done: false\n",
      "  episode_len_mean: 283.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -2.833499999999984\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1366\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1462944482672761\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5293343805604511\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007653589512885987\n",
      "          policy_loss: 0.021754684588975375\n",
      "          total_loss: 0.029176701770888436\n",
      "          vf_explained_var: 0.37863603234291077\n",
      "          vf_loss: 0.011595683053342833\n",
      "    num_agent_steps_sampled: 483000\n",
      "    num_agent_steps_trained: 483000\n",
      "    num_steps_sampled: 483000\n",
      "    num_steps_trained: 483000\n",
      "  iterations_since_restore: 483\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.91666666666667\n",
      "    ram_util_percent: 47.09583333333333\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040142619593164716\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.67359366957554\n",
      "    mean_inference_ms: 1.75279150429142\n",
      "    mean_raw_obs_processing_ms: 1.8118980057929477\n",
      "  time_since_restore: 13739.785595655441\n",
      "  time_this_iter_s: 33.30288028717041\n",
      "  time_total_s: 13739.785595655441\n",
      "  timers:\n",
      "    learn_throughput: 1187.416\n",
      "    learn_time_ms: 842.165\n",
      "    load_throughput: 49750.188\n",
      "    load_time_ms: 20.1\n",
      "    sample_throughput: 27.895\n",
      "    sample_time_ms: 35848.443\n",
      "    update_time_ms: 3.836\n",
      "  timestamp: 1634856970\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 483000\n",
      "  training_iteration: 483\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   483</td><td style=\"text-align: right;\">         13739.8</td><td style=\"text-align: right;\">483000</td><td style=\"text-align: right;\"> -2.8335</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            283.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 484000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-56-44\n",
      "  done: false\n",
      "  episode_len_mean: 282.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -2.826099999999983\n",
      "  episode_reward_min: -4.4499999999999496\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1369\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1462944482672761\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5498123447100322\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00572264485359503\n",
      "          policy_loss: -0.10850717566079564\n",
      "          total_loss: -0.10191965450843175\n",
      "          vf_explained_var: 0.43948906660079956\n",
      "          vf_loss: 0.011248453365017971\n",
      "    num_agent_steps_sampled: 484000\n",
      "    num_agent_steps_trained: 484000\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 484000\n",
      "  iterations_since_restore: 484\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.8\n",
      "    ram_util_percent: 47.126\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04014436964865573\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.685650946372302\n",
      "    mean_inference_ms: 1.7528222766472359\n",
      "    mean_raw_obs_processing_ms: 1.8123903079003643\n",
      "  time_since_restore: 13774.635757446289\n",
      "  time_this_iter_s: 34.85016179084778\n",
      "  time_total_s: 13774.635757446289\n",
      "  timers:\n",
      "    learn_throughput: 1186.634\n",
      "    learn_time_ms: 842.72\n",
      "    load_throughput: 48511.721\n",
      "    load_time_ms: 20.614\n",
      "    sample_throughput: 27.911\n",
      "    sample_time_ms: 35827.844\n",
      "    update_time_ms: 3.669\n",
      "  timestamp: 1634857004\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 484\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   484</td><td style=\"text-align: right;\">         13774.6</td><td style=\"text-align: right;\">484000</td><td style=\"text-align: right;\"> -2.8261</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">               -4.45</td><td style=\"text-align: right;\">            282.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 485000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-57-18\n",
      "  done: false\n",
      "  episode_len_mean: 280.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -2.8058999999999834\n",
      "  episode_reward_min: -3.7399999999999642\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1373\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1462944482672761\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5856406122446061\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012942933148203531\n",
      "          policy_loss: 0.016022122320201664\n",
      "          total_loss: 0.021258702418870398\n",
      "          vf_explained_var: 0.4905703365802765\n",
      "          vf_loss: 0.009199506002995702\n",
      "    num_agent_steps_sampled: 485000\n",
      "    num_agent_steps_trained: 485000\n",
      "    num_steps_sampled: 485000\n",
      "    num_steps_trained: 485000\n",
      "  iterations_since_restore: 485\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.21702127659574\n",
      "    ram_util_percent: 47.138297872340424\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040146682100072356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.702083984509272\n",
      "    mean_inference_ms: 1.7528643738216885\n",
      "    mean_raw_obs_processing_ms: 1.8130686011547663\n",
      "  time_since_restore: 13808.13361954689\n",
      "  time_this_iter_s: 33.497862100601196\n",
      "  time_total_s: 13808.13361954689\n",
      "  timers:\n",
      "    learn_throughput: 1189.921\n",
      "    learn_time_ms: 840.392\n",
      "    load_throughput: 49221.988\n",
      "    load_time_ms: 20.316\n",
      "    sample_throughput: 27.947\n",
      "    sample_time_ms: 35781.905\n",
      "    update_time_ms: 3.659\n",
      "  timestamp: 1634857038\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 485000\n",
      "  training_iteration: 485\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   485</td><td style=\"text-align: right;\">         13808.1</td><td style=\"text-align: right;\">485000</td><td style=\"text-align: right;\"> -2.8059</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">               -3.74</td><td style=\"text-align: right;\">            280.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 486000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-57-42\n",
      "  done: false\n",
      "  episode_len_mean: 282.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.339999999999994\n",
      "  episode_reward_mean: -2.8259999999999836\n",
      "  episode_reward_min: -4.199999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1376\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.1462944482672761\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1915424194600848\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.05031980375667224\n",
      "          policy_loss: -0.001131073468261295\n",
      "          total_loss: 0.0016084965732362534\n",
      "          vf_explained_var: 0.18295754492282867\n",
      "          vf_loss: 0.007293487232851071\n",
      "    num_agent_steps_sampled: 486000\n",
      "    num_agent_steps_trained: 486000\n",
      "    num_steps_sampled: 486000\n",
      "    num_steps_trained: 486000\n",
      "  iterations_since_restore: 486\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.72941176470589\n",
      "    ram_util_percent: 47.1970588235294\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04014832614863581\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.713914562697173\n",
      "    mean_inference_ms: 1.7528937391829829\n",
      "    mean_raw_obs_processing_ms: 1.8136061897134834\n",
      "  time_since_restore: 13831.868817329407\n",
      "  time_this_iter_s: 23.73519778251648\n",
      "  time_total_s: 13831.868817329407\n",
      "  timers:\n",
      "    learn_throughput: 1192.95\n",
      "    learn_time_ms: 838.258\n",
      "    load_throughput: 46995.266\n",
      "    load_time_ms: 21.279\n",
      "    sample_throughput: 28.638\n",
      "    sample_time_ms: 34918.038\n",
      "    update_time_ms: 3.705\n",
      "  timestamp: 1634857062\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 486000\n",
      "  training_iteration: 486\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   486</td><td style=\"text-align: right;\">         13831.9</td><td style=\"text-align: right;\">486000</td><td style=\"text-align: right;\">  -2.826</td><td style=\"text-align: right;\">               -2.34</td><td style=\"text-align: right;\">                -4.2</td><td style=\"text-align: right;\">             282.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 487000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-58-33\n",
      "  done: false\n",
      "  episode_len_mean: 281.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.289999999999995\n",
      "  episode_reward_mean: -2.8149999999999835\n",
      "  episode_reward_min: -4.199999999999955\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1380\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21944167240091414\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6324451363748974\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007642684901623155\n",
      "          policy_loss: 0.007905528901351823\n",
      "          total_loss: 0.01227438992096318\n",
      "          vf_explained_var: 0.6358770728111267\n",
      "          vf_loss: 0.009016190386480756\n",
      "    num_agent_steps_sampled: 487000\n",
      "    num_agent_steps_trained: 487000\n",
      "    num_steps_sampled: 487000\n",
      "    num_steps_trained: 487000\n",
      "  iterations_since_restore: 487\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.38767123287671\n",
      "    ram_util_percent: 47.09863013698631\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04015042868278738\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.729957182542442\n",
      "    mean_inference_ms: 1.7529304939510444\n",
      "    mean_raw_obs_processing_ms: 1.815682534168818\n",
      "  time_since_restore: 13882.77692604065\n",
      "  time_this_iter_s: 50.908108711242676\n",
      "  time_total_s: 13882.77692604065\n",
      "  timers:\n",
      "    learn_throughput: 1193.812\n",
      "    learn_time_ms: 837.653\n",
      "    load_throughput: 47113.459\n",
      "    load_time_ms: 21.225\n",
      "    sample_throughput: 27.403\n",
      "    sample_time_ms: 36492.951\n",
      "    update_time_ms: 3.83\n",
      "  timestamp: 1634857113\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 487000\n",
      "  training_iteration: 487\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   487</td><td style=\"text-align: right;\">         13882.8</td><td style=\"text-align: right;\">487000</td><td style=\"text-align: right;\">  -2.815</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">                -4.2</td><td style=\"text-align: right;\">             281.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 488000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-59-08\n",
      "  done: false\n",
      "  episode_len_mean: 280.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.289999999999995\n",
      "  episode_reward_mean: -2.8059999999999845\n",
      "  episode_reward_min: -4.199999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1383\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21944167240091414\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6775808248254988\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011300215271690128\n",
      "          policy_loss: 0.011234187955657641\n",
      "          total_loss: 0.014794287582238516\n",
      "          vf_explained_var: 0.6108174920082092\n",
      "          vf_loss: 0.007856171957812168\n",
      "    num_agent_steps_sampled: 488000\n",
      "    num_agent_steps_trained: 488000\n",
      "    num_steps_sampled: 488000\n",
      "    num_steps_trained: 488000\n",
      "  iterations_since_restore: 488\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.056\n",
      "    ram_util_percent: 46.86600000000001\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04015189050386061\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.742146319972004\n",
      "    mean_inference_ms: 1.7529568532404753\n",
      "    mean_raw_obs_processing_ms: 1.8172682931881763\n",
      "  time_since_restore: 13918.082922697067\n",
      "  time_this_iter_s: 35.30599665641785\n",
      "  time_total_s: 13918.082922697067\n",
      "  timers:\n",
      "    learn_throughput: 1188.538\n",
      "    learn_time_ms: 841.37\n",
      "    load_throughput: 44790.985\n",
      "    load_time_ms: 22.326\n",
      "    sample_throughput: 27.36\n",
      "    sample_time_ms: 36550.054\n",
      "    update_time_ms: 3.349\n",
      "  timestamp: 1634857148\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 488000\n",
      "  training_iteration: 488\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   488</td><td style=\"text-align: right;\">         13918.1</td><td style=\"text-align: right;\">488000</td><td style=\"text-align: right;\">  -2.806</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">                -4.2</td><td style=\"text-align: right;\">             280.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 489000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_22-59-45\n",
      "  done: false\n",
      "  episode_len_mean: 278.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.289999999999995\n",
      "  episode_reward_mean: -2.789299999999985\n",
      "  episode_reward_min: -4.199999999999955\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1387\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21944167240091414\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4274478766653273\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009155577224603192\n",
      "          policy_loss: 0.046115663399298984\n",
      "          total_loss: 0.05236618493994077\n",
      "          vf_explained_var: 0.501542329788208\n",
      "          vf_loss: 0.008515886641624901\n",
      "    num_agent_steps_sampled: 489000\n",
      "    num_agent_steps_trained: 489000\n",
      "    num_steps_sampled: 489000\n",
      "    num_steps_trained: 489000\n",
      "  iterations_since_restore: 489\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.83962264150944\n",
      "    ram_util_percent: 46.91698113207547\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040153587759352016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.75869771887744\n",
      "    mean_inference_ms: 1.7529913255270975\n",
      "    mean_raw_obs_processing_ms: 1.819385362480567\n",
      "  time_since_restore: 13955.286968946457\n",
      "  time_this_iter_s: 37.20404624938965\n",
      "  time_total_s: 13955.286968946457\n",
      "  timers:\n",
      "    learn_throughput: 1186.8\n",
      "    learn_time_ms: 842.602\n",
      "    load_throughput: 43256.703\n",
      "    load_time_ms: 23.118\n",
      "    sample_throughput: 28.595\n",
      "    sample_time_ms: 34971.636\n",
      "    update_time_ms: 3.523\n",
      "  timestamp: 1634857185\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 489000\n",
      "  training_iteration: 489\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   489</td><td style=\"text-align: right;\">         13955.3</td><td style=\"text-align: right;\">489000</td><td style=\"text-align: right;\"> -2.7893</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">                -4.2</td><td style=\"text-align: right;\">            278.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 490000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-00-22\n",
      "  done: false\n",
      "  episode_len_mean: 277.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.289999999999995\n",
      "  episode_reward_mean: -2.773199999999985\n",
      "  episode_reward_min: -4.199999999999955\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1391\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21944167240091414\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6038019822703468\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007701151812080238\n",
      "          policy_loss: 0.021938034809297985\n",
      "          total_loss: 0.027500049852662616\n",
      "          vf_explained_var: 0.4876991808414459\n",
      "          vf_loss: 0.009910079133179452\n",
      "    num_agent_steps_sampled: 490000\n",
      "    num_agent_steps_trained: 490000\n",
      "    num_steps_sampled: 490000\n",
      "    num_steps_trained: 490000\n",
      "  iterations_since_restore: 490\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.7423076923077\n",
      "    ram_util_percent: 47.00192307692308\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040155217012087775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.77540678388683\n",
      "    mean_inference_ms: 1.7530218926474186\n",
      "    mean_raw_obs_processing_ms: 1.8200699560009885\n",
      "  time_since_restore: 13991.596329450607\n",
      "  time_this_iter_s: 36.30936050415039\n",
      "  time_total_s: 13991.596329450607\n",
      "  timers:\n",
      "    learn_throughput: 1129.521\n",
      "    learn_time_ms: 885.331\n",
      "    load_throughput: 43230.264\n",
      "    load_time_ms: 23.132\n",
      "    sample_throughput: 28.591\n",
      "    sample_time_ms: 34976.304\n",
      "    update_time_ms: 3.596\n",
      "  timestamp: 1634857222\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 490000\n",
      "  training_iteration: 490\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   490</td><td style=\"text-align: right;\">         13991.6</td><td style=\"text-align: right;\">490000</td><td style=\"text-align: right;\"> -2.7732</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">                -4.2</td><td style=\"text-align: right;\">            277.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 491000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-00-59\n",
      "  done: false\n",
      "  episode_len_mean: 276.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.289999999999995\n",
      "  episode_reward_mean: -2.762999999999985\n",
      "  episode_reward_min: -4.199999999999955\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1395\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.21944167240091414\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5586050907770793\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004568163129861662\n",
      "          policy_loss: 0.019791207172804408\n",
      "          total_loss: 0.02702044054037995\n",
      "          vf_explained_var: 0.3240925073623657\n",
      "          vf_loss: 0.011812841654237774\n",
      "    num_agent_steps_sampled: 491000\n",
      "    num_agent_steps_trained: 491000\n",
      "    num_steps_sampled: 491000\n",
      "    num_steps_trained: 491000\n",
      "  iterations_since_restore: 491\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.96666666666668\n",
      "    ram_util_percent: 47.09814814814815\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04015717287088012\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.79231124723625\n",
      "    mean_inference_ms: 1.7530528241983483\n",
      "    mean_raw_obs_processing_ms: 1.820760725091415\n",
      "  time_since_restore: 14029.245946884155\n",
      "  time_this_iter_s: 37.649617433547974\n",
      "  time_total_s: 14029.245946884155\n",
      "  timers:\n",
      "    learn_throughput: 1129.795\n",
      "    learn_time_ms: 885.116\n",
      "    load_throughput: 45112.995\n",
      "    load_time_ms: 22.167\n",
      "    sample_throughput: 28.516\n",
      "    sample_time_ms: 35067.528\n",
      "    update_time_ms: 3.51\n",
      "  timestamp: 1634857259\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 491000\n",
      "  training_iteration: 491\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   491</td><td style=\"text-align: right;\">         14029.2</td><td style=\"text-align: right;\">491000</td><td style=\"text-align: right;\">  -2.763</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">                -4.2</td><td style=\"text-align: right;\">             276.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 492000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-01-34\n",
      "  done: false\n",
      "  episode_len_mean: 275.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.289999999999995\n",
      "  episode_reward_mean: -2.7571999999999846\n",
      "  episode_reward_min: -4.199999999999955\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1399\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10972083620045707\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6071177353461583\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00745189107630853\n",
      "          policy_loss: -0.05298366240329212\n",
      "          total_loss: -0.04839641793320577\n",
      "          vf_explained_var: 0.4673498868942261\n",
      "          vf_loss: 0.009840792671052947\n",
      "    num_agent_steps_sampled: 492000\n",
      "    num_agent_steps_trained: 492000\n",
      "    num_steps_sampled: 492000\n",
      "    num_steps_trained: 492000\n",
      "  iterations_since_restore: 492\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.54599999999999\n",
      "    ram_util_percent: 47.11600000000001\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040159105572383155\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.809253837946976\n",
      "    mean_inference_ms: 1.7530836788627897\n",
      "    mean_raw_obs_processing_ms: 1.8214908847154414\n",
      "  time_since_restore: 14064.274783849716\n",
      "  time_this_iter_s: 35.02883696556091\n",
      "  time_total_s: 14064.274783849716\n",
      "  timers:\n",
      "    learn_throughput: 1134.026\n",
      "    learn_time_ms: 881.814\n",
      "    load_throughput: 47566.17\n",
      "    load_time_ms: 21.023\n",
      "    sample_throughput: 28.678\n",
      "    sample_time_ms: 34869.655\n",
      "    update_time_ms: 2.81\n",
      "  timestamp: 1634857294\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 492000\n",
      "  training_iteration: 492\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   492</td><td style=\"text-align: right;\">         14064.3</td><td style=\"text-align: right;\">492000</td><td style=\"text-align: right;\"> -2.7572</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">                -4.2</td><td style=\"text-align: right;\">            275.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 493000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-02-10\n",
      "  done: false\n",
      "  episode_len_mean: 274.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.289999999999995\n",
      "  episode_reward_mean: -2.747299999999985\n",
      "  episode_reward_min: -4.199999999999955\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1403\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10972083620045707\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.49502877824836305\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004630142446339312\n",
      "          policy_loss: -0.02031034462981754\n",
      "          total_loss: -0.013736575593551\n",
      "          vf_explained_var: 0.31556302309036255\n",
      "          vf_loss: 0.011016033227658935\n",
      "    num_agent_steps_sampled: 493000\n",
      "    num_agent_steps_trained: 493000\n",
      "    num_steps_sampled: 493000\n",
      "    num_steps_trained: 493000\n",
      "  iterations_since_restore: 493\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.92884615384614\n",
      "    ram_util_percent: 47.10576923076923\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04016094195455912\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.826361958450033\n",
      "    mean_inference_ms: 1.7531121077891343\n",
      "    mean_raw_obs_processing_ms: 1.8222238319939192\n",
      "  time_since_restore: 14100.401428699493\n",
      "  time_this_iter_s: 36.12664484977722\n",
      "  time_total_s: 14100.401428699493\n",
      "  timers:\n",
      "    learn_throughput: 1133.712\n",
      "    learn_time_ms: 882.058\n",
      "    load_throughput: 47009.329\n",
      "    load_time_ms: 21.272\n",
      "    sample_throughput: 28.449\n",
      "    sample_time_ms: 35151.021\n",
      "    update_time_ms: 3.308\n",
      "  timestamp: 1634857330\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 493000\n",
      "  training_iteration: 493\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   493</td><td style=\"text-align: right;\">         14100.4</td><td style=\"text-align: right;\">493000</td><td style=\"text-align: right;\"> -2.7473</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">                -4.2</td><td style=\"text-align: right;\">            274.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 494000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-02-48\n",
      "  done: false\n",
      "  episode_len_mean: 274.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.289999999999995\n",
      "  episode_reward_mean: -2.740399999999986\n",
      "  episode_reward_min: -4.199999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1406\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.054860418100228535\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4990555375814438\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005005079129668862\n",
      "          policy_loss: -0.07411663557092349\n",
      "          total_loss: -0.0670279539293713\n",
      "          vf_explained_var: 0.3047462999820709\n",
      "          vf_loss: 0.011804652358922694\n",
      "    num_agent_steps_sampled: 494000\n",
      "    num_agent_steps_trained: 494000\n",
      "    num_steps_sampled: 494000\n",
      "    num_steps_trained: 494000\n",
      "  iterations_since_restore: 494\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.93773584905662\n",
      "    ram_util_percent: 47.116981132075466\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04016231871414531\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.83929934552646\n",
      "    mean_inference_ms: 1.7531327227368394\n",
      "    mean_raw_obs_processing_ms: 1.822791799754307\n",
      "  time_since_restore: 14137.806370973587\n",
      "  time_this_iter_s: 37.40494227409363\n",
      "  time_total_s: 14137.806370973587\n",
      "  timers:\n",
      "    learn_throughput: 1133.63\n",
      "    learn_time_ms: 882.122\n",
      "    load_throughput: 48363.207\n",
      "    load_time_ms: 20.677\n",
      "    sample_throughput: 28.243\n",
      "    sample_time_ms: 35406.749\n",
      "    update_time_ms: 3.48\n",
      "  timestamp: 1634857368\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 494000\n",
      "  training_iteration: 494\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   494</td><td style=\"text-align: right;\">         14137.8</td><td style=\"text-align: right;\">494000</td><td style=\"text-align: right;\"> -2.7404</td><td style=\"text-align: right;\">               -2.29</td><td style=\"text-align: right;\">                -4.2</td><td style=\"text-align: right;\">            274.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 495000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-03-44\n",
      "  done: false\n",
      "  episode_len_mean: 272.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.721299999999985\n",
      "  episode_reward_min: -4.199999999999955\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1411\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.054860418100228535\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4465164926317003\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004024310572562149\n",
      "          policy_loss: -0.020306114148762492\n",
      "          total_loss: -0.014120872774057918\n",
      "          vf_explained_var: 0.5290691256523132\n",
      "          vf_loss: 0.010429631525443659\n",
      "    num_agent_steps_sampled: 495000\n",
      "    num_agent_steps_trained: 495000\n",
      "    num_steps_sampled: 495000\n",
      "    num_steps_trained: 495000\n",
      "  iterations_since_restore: 495\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.77654320987654\n",
      "    ram_util_percent: 47.00617283950617\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040164529128519914\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.861375980404947\n",
      "    mean_inference_ms: 1.7531650864439607\n",
      "    mean_raw_obs_processing_ms: 1.8254643063570564\n",
      "  time_since_restore: 14194.25006365776\n",
      "  time_this_iter_s: 56.443692684173584\n",
      "  time_total_s: 14194.25006365776\n",
      "  timers:\n",
      "    learn_throughput: 1130.658\n",
      "    learn_time_ms: 884.441\n",
      "    load_throughput: 48424.852\n",
      "    load_time_ms: 20.651\n",
      "    sample_throughput: 26.526\n",
      "    sample_time_ms: 37698.924\n",
      "    update_time_ms: 3.503\n",
      "  timestamp: 1634857424\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 495000\n",
      "  training_iteration: 495\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   495</td><td style=\"text-align: right;\">         14194.3</td><td style=\"text-align: right;\">495000</td><td style=\"text-align: right;\"> -2.7213</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">                -4.2</td><td style=\"text-align: right;\">            272.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 496000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-04-20\n",
      "  done: false\n",
      "  episode_len_mean: 272.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.7212999999999865\n",
      "  episode_reward_min: -4.199999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1414\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.027430209050114267\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.625793566637569\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020191960832465602\n",
      "          policy_loss: -0.06452685064739651\n",
      "          total_loss: -0.06044889423582289\n",
      "          vf_explained_var: -0.08266779780387878\n",
      "          vf_loss: 0.009782023047510949\n",
      "    num_agent_steps_sampled: 496000\n",
      "    num_agent_steps_trained: 496000\n",
      "    num_steps_sampled: 496000\n",
      "    num_steps_trained: 496000\n",
      "  iterations_since_restore: 496\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.948\n",
      "    ram_util_percent: 47.093999999999994\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04016579822534602\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.87458960866354\n",
      "    mean_inference_ms: 1.7531836588357035\n",
      "    mean_raw_obs_processing_ms: 1.827105413175215\n",
      "  time_since_restore: 14229.647721767426\n",
      "  time_this_iter_s: 35.39765810966492\n",
      "  time_total_s: 14229.647721767426\n",
      "  timers:\n",
      "    learn_throughput: 1129.391\n",
      "    learn_time_ms: 885.433\n",
      "    load_throughput: 50188.21\n",
      "    load_time_ms: 19.925\n",
      "    sample_throughput: 25.73\n",
      "    sample_time_ms: 38865.059\n",
      "    update_time_ms: 3.431\n",
      "  timestamp: 1634857460\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 496000\n",
      "  training_iteration: 496\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   496</td><td style=\"text-align: right;\">         14229.6</td><td style=\"text-align: right;\">496000</td><td style=\"text-align: right;\"> -2.7213</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">                -4.2</td><td style=\"text-align: right;\">            272.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 497000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-04-58\n",
      "  done: false\n",
      "  episode_len_mean: 270.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.7077999999999856\n",
      "  episode_reward_min: -4.199999999999955\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1418\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.04114531357517139\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5636664681964451\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.042328828384842684\n",
      "          policy_loss: -0.017109900464614233\n",
      "          total_loss: -0.010756391369634204\n",
      "          vf_explained_var: 0.5207743048667908\n",
      "          vf_loss: 0.01024853995639003\n",
      "    num_agent_steps_sampled: 497000\n",
      "    num_agent_steps_trained: 497000\n",
      "    num_steps_sampled: 497000\n",
      "    num_steps_trained: 497000\n",
      "  iterations_since_restore: 497\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.13272727272728\n",
      "    ram_util_percent: 47.11272727272728\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040167592227257645\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.892586180478528\n",
      "    mean_inference_ms: 1.7532085187942328\n",
      "    mean_raw_obs_processing_ms: 1.8292632182871769\n",
      "  time_since_restore: 14267.88807630539\n",
      "  time_this_iter_s: 38.24035453796387\n",
      "  time_total_s: 14267.88807630539\n",
      "  timers:\n",
      "    learn_throughput: 1128.032\n",
      "    learn_time_ms: 886.5\n",
      "    load_throughput: 47821.116\n",
      "    load_time_ms: 20.911\n",
      "    sample_throughput: 26.598\n",
      "    sample_time_ms: 37596.166\n",
      "    update_time_ms: 3.342\n",
      "  timestamp: 1634857498\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 497000\n",
      "  training_iteration: 497\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   497</td><td style=\"text-align: right;\">         14267.9</td><td style=\"text-align: right;\">497000</td><td style=\"text-align: right;\"> -2.7078</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">                -4.2</td><td style=\"text-align: right;\">            270.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 498000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-05-25\n",
      "  done: false\n",
      "  episode_len_mean: 271.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.7168999999999857\n",
      "  episode_reward_min: -4.199999999999955\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1421\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.0617179703627571\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.240935620996687\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.04730445703171024\n",
      "          policy_loss: -0.14671271153622203\n",
      "          total_loss: -0.14454963621166017\n",
      "          vf_explained_var: 0.3317853808403015\n",
      "          vf_loss: 0.011652901722118258\n",
      "    num_agent_steps_sampled: 498000\n",
      "    num_agent_steps_trained: 498000\n",
      "    num_steps_sampled: 498000\n",
      "    num_steps_trained: 498000\n",
      "  iterations_since_restore: 498\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.7421052631579\n",
      "    ram_util_percent: 47.105263157894726\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04016899427702374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.905719195911814\n",
      "    mean_inference_ms: 1.7532284371062916\n",
      "    mean_raw_obs_processing_ms: 1.8301601887662704\n",
      "  time_since_restore: 14294.95789718628\n",
      "  time_this_iter_s: 27.069820880889893\n",
      "  time_total_s: 14294.95789718628\n",
      "  timers:\n",
      "    learn_throughput: 1130.311\n",
      "    learn_time_ms: 884.712\n",
      "    load_throughput: 50043.776\n",
      "    load_time_ms: 19.983\n",
      "    sample_throughput: 27.192\n",
      "    sample_time_ms: 36775.1\n",
      "    update_time_ms: 3.439\n",
      "  timestamp: 1634857525\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 498000\n",
      "  training_iteration: 498\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   498</td><td style=\"text-align: right;\">           14295</td><td style=\"text-align: right;\">498000</td><td style=\"text-align: right;\"> -2.7169</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">                -4.2</td><td style=\"text-align: right;\">            271.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 499000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-05-54\n",
      "  done: false\n",
      "  episode_len_mean: 273.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.7362999999999857\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1424\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09257695554413563\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5213571333222919\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03166826464160583\n",
      "          policy_loss: -0.09905071987046135\n",
      "          total_loss: -0.07895028707053926\n",
      "          vf_explained_var: -0.11926410347223282\n",
      "          vf_loss: 0.022382253718872865\n",
      "    num_agent_steps_sampled: 499000\n",
      "    num_agent_steps_trained: 499000\n",
      "    num_steps_sampled: 499000\n",
      "    num_steps_trained: 499000\n",
      "  iterations_since_restore: 499\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.09047619047618\n",
      "    ram_util_percent: 47.15238095238095\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040170358007038436\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.91859228519074\n",
      "    mean_inference_ms: 1.7532463501095614\n",
      "    mean_raw_obs_processing_ms: 1.830700879555159\n",
      "  time_since_restore: 14324.064098119736\n",
      "  time_this_iter_s: 29.10620093345642\n",
      "  time_total_s: 14324.064098119736\n",
      "  timers:\n",
      "    learn_throughput: 1131.526\n",
      "    learn_time_ms: 883.762\n",
      "    load_throughput: 50446.201\n",
      "    load_time_ms: 19.823\n",
      "    sample_throughput: 27.803\n",
      "    sample_time_ms: 35966.734\n",
      "    update_time_ms: 3.303\n",
      "  timestamp: 1634857554\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 499000\n",
      "  training_iteration: 499\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   499</td><td style=\"text-align: right;\">         14324.1</td><td style=\"text-align: right;\">499000</td><td style=\"text-align: right;\"> -2.7363</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">            273.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 500000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-06-27\n",
      "  done: false\n",
      "  episode_len_mean: 273.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.734099999999985\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1428\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.13886543331620343\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9618497312068939\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.027945956241017377\n",
      "          policy_loss: 0.03749265070590708\n",
      "          total_loss: 0.04063266314980057\n",
      "          vf_explained_var: 0.22141647338867188\n",
      "          vf_loss: 0.008877780777402223\n",
      "    num_agent_steps_sampled: 500000\n",
      "    num_agent_steps_trained: 500000\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "  iterations_since_restore: 500\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.60638297872342\n",
      "    ram_util_percent: 47.176595744680846\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04017211775445835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.93578683009975\n",
      "    mean_inference_ms: 1.7532672732357348\n",
      "    mean_raw_obs_processing_ms: 1.831404968901608\n",
      "  time_since_restore: 14357.275047302246\n",
      "  time_this_iter_s: 33.210949182510376\n",
      "  time_total_s: 14357.275047302246\n",
      "  timers:\n",
      "    learn_throughput: 1193.662\n",
      "    learn_time_ms: 837.758\n",
      "    load_throughput: 53282.722\n",
      "    load_time_ms: 18.768\n",
      "    sample_throughput: 28.009\n",
      "    sample_time_ms: 35703.214\n",
      "    update_time_ms: 4.043\n",
      "  timestamp: 1634857587\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 500\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   500</td><td style=\"text-align: right;\">         14357.3</td><td style=\"text-align: right;\">500000</td><td style=\"text-align: right;\"> -2.7341</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">            273.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 501000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-07-00\n",
      "  done: false\n",
      "  episode_len_mean: 273.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.7385999999999844\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1431\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2082981499743051\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.987537956237793\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013319992101869557\n",
      "          policy_loss: -0.023529274927245245\n",
      "          total_loss: -0.020871039728323618\n",
      "          vf_explained_var: 0.0771874189376831\n",
      "          vf_loss: 0.009759085669389201\n",
      "    num_agent_steps_sampled: 501000\n",
      "    num_agent_steps_trained: 501000\n",
      "    num_steps_sampled: 501000\n",
      "    num_steps_trained: 501000\n",
      "  iterations_since_restore: 501\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.77659574468085\n",
      "    ram_util_percent: 47.14255319148937\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04017339241923493\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.94844451181034\n",
      "    mean_inference_ms: 1.753281461512746\n",
      "    mean_raw_obs_processing_ms: 1.8319507043496486\n",
      "  time_since_restore: 14389.734661340714\n",
      "  time_this_iter_s: 32.45961403846741\n",
      "  time_total_s: 14389.734661340714\n",
      "  timers:\n",
      "    learn_throughput: 1200.425\n",
      "    learn_time_ms: 833.038\n",
      "    load_throughput: 53073.913\n",
      "    load_time_ms: 18.842\n",
      "    sample_throughput: 28.419\n",
      "    sample_time_ms: 35188.234\n",
      "    update_time_ms: 4.602\n",
      "  timestamp: 1634857620\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 501000\n",
      "  training_iteration: 501\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   501</td><td style=\"text-align: right;\">         14389.7</td><td style=\"text-align: right;\">501000</td><td style=\"text-align: right;\"> -2.7386</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">            273.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 502000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-07-33\n",
      "  done: false\n",
      "  episode_len_mean: 273.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.7334999999999847\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1435\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2082981499743051\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6690354294247097\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006421491285599343\n",
      "          policy_loss: 0.044956319779157636\n",
      "          total_loss: 0.04659163032968839\n",
      "          vf_explained_var: 0.41713473200798035\n",
      "          vf_loss: 0.006988078003956212\n",
      "    num_agent_steps_sampled: 502000\n",
      "    num_agent_steps_trained: 502000\n",
      "    num_steps_sampled: 502000\n",
      "    num_steps_trained: 502000\n",
      "  iterations_since_restore: 502\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.61086956521739\n",
      "    ram_util_percent: 47.05652173913043\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04017501763487566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.965232947870863\n",
      "    mean_inference_ms: 1.7532981638527674\n",
      "    mean_raw_obs_processing_ms: 1.8326630371663628\n",
      "  time_since_restore: 14422.543936491013\n",
      "  time_this_iter_s: 32.80927515029907\n",
      "  time_total_s: 14422.543936491013\n",
      "  timers:\n",
      "    learn_throughput: 1194.651\n",
      "    learn_time_ms: 837.065\n",
      "    load_throughput: 52515.385\n",
      "    load_time_ms: 19.042\n",
      "    sample_throughput: 28.602\n",
      "    sample_time_ms: 34961.988\n",
      "    update_time_ms: 4.611\n",
      "  timestamp: 1634857653\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 502000\n",
      "  training_iteration: 502\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   502</td><td style=\"text-align: right;\">         14422.5</td><td style=\"text-align: right;\">502000</td><td style=\"text-align: right;\"> -2.7335</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">            273.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 503000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-08-02\n",
      "  done: false\n",
      "  episode_len_mean: 273.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.738299999999985\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1438\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2082981499743051\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2447051286697388\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012896288125019436\n",
      "          policy_loss: 0.005206187731689877\n",
      "          total_loss: 0.0015212610363960267\n",
      "          vf_explained_var: 0.46092361211776733\n",
      "          vf_loss: 0.006075853265226922\n",
      "    num_agent_steps_sampled: 503000\n",
      "    num_agent_steps_trained: 503000\n",
      "    num_steps_sampled: 503000\n",
      "    num_steps_trained: 503000\n",
      "  iterations_since_restore: 503\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.68372093023255\n",
      "    ram_util_percent: 47.12558139534883\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04017621268236243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.977484393459296\n",
      "    mean_inference_ms: 1.7533089043473609\n",
      "    mean_raw_obs_processing_ms: 1.8332173373232192\n",
      "  time_since_restore: 14452.061613082886\n",
      "  time_this_iter_s: 29.51767659187317\n",
      "  time_total_s: 14452.061613082886\n",
      "  timers:\n",
      "    learn_throughput: 1155.547\n",
      "    learn_time_ms: 865.391\n",
      "    load_throughput: 52146.881\n",
      "    load_time_ms: 19.177\n",
      "    sample_throughput: 29.177\n",
      "    sample_time_ms: 34273.253\n",
      "    update_time_ms: 4.041\n",
      "  timestamp: 1634857682\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 503000\n",
      "  training_iteration: 503\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   503</td><td style=\"text-align: right;\">         14452.1</td><td style=\"text-align: right;\">503000</td><td style=\"text-align: right;\"> -2.7383</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">            273.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 504000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-08-53\n",
      "  done: false\n",
      "  episode_len_mean: 275.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.751999999999985\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1441\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2082981499743051\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.430640086862776\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.04802550650893635\n",
      "          policy_loss: 0.0021254846619235147\n",
      "          total_loss: 0.0020953266157044304\n",
      "          vf_explained_var: 0.7587254643440247\n",
      "          vf_loss: 0.004272616664982505\n",
      "    num_agent_steps_sampled: 504000\n",
      "    num_agent_steps_trained: 504000\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "  iterations_since_restore: 504\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.2236111111111\n",
      "    ram_util_percent: 47.22222222222222\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040177423131784476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.989665428494828\n",
      "    mean_inference_ms: 1.753317854771932\n",
      "    mean_raw_obs_processing_ms: 1.834782874143776\n",
      "  time_since_restore: 14502.852573394775\n",
      "  time_this_iter_s: 50.79096031188965\n",
      "  time_total_s: 14502.852573394775\n",
      "  timers:\n",
      "    learn_throughput: 1156.171\n",
      "    learn_time_ms: 864.924\n",
      "    load_throughput: 49372.401\n",
      "    load_time_ms: 20.254\n",
      "    sample_throughput: 28.081\n",
      "    sample_time_ms: 35611.353\n",
      "    update_time_ms: 3.885\n",
      "  timestamp: 1634857733\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 504\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   504</td><td style=\"text-align: right;\">         14502.9</td><td style=\"text-align: right;\">504000</td><td style=\"text-align: right;\">  -2.752</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">             275.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 505000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-09-27\n",
      "  done: false\n",
      "  episode_len_mean: 274.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.747699999999985\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1445\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3124472249614577\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6618993765778012\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006408153777785626\n",
      "          policy_loss: 7.1696937084197995e-06\n",
      "          total_loss: 0.009589574734369914\n",
      "          vf_explained_var: 0.24694472551345825\n",
      "          vf_loss: 0.014199186478637986\n",
      "    num_agent_steps_sampled: 505000\n",
      "    num_agent_steps_trained: 505000\n",
      "    num_steps_sampled: 505000\n",
      "    num_steps_trained: 505000\n",
      "  iterations_since_restore: 505\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.84897959183672\n",
      "    ram_util_percent: 47.204081632653065\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04017892333529374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.00588709844843\n",
      "    mean_inference_ms: 1.7533285998956187\n",
      "    mean_raw_obs_processing_ms: 1.836850297728895\n",
      "  time_since_restore: 14536.951870441437\n",
      "  time_this_iter_s: 34.09929704666138\n",
      "  time_total_s: 14536.951870441437\n",
      "  timers:\n",
      "    learn_throughput: 1156.077\n",
      "    learn_time_ms: 864.995\n",
      "    load_throughput: 47257.9\n",
      "    load_time_ms: 21.16\n",
      "    sample_throughput: 29.963\n",
      "    sample_time_ms: 33375.014\n",
      "    update_time_ms: 4.679\n",
      "  timestamp: 1634857767\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 505000\n",
      "  training_iteration: 505\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   505</td><td style=\"text-align: right;\">           14537</td><td style=\"text-align: right;\">505000</td><td style=\"text-align: right;\"> -2.7477</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">            274.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 506000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-09-59\n",
      "  done: false\n",
      "  episode_len_mean: 276.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.766899999999985\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1448\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3124472249614577\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.172153123219808\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014462620768565495\n",
      "          policy_loss: -0.013992080589135487\n",
      "          total_loss: -0.013010065588686202\n",
      "          vf_explained_var: 0.4750766456127167\n",
      "          vf_loss: 0.008184740309500033\n",
      "    num_agent_steps_sampled: 506000\n",
      "    num_agent_steps_trained: 506000\n",
      "    num_steps_sampled: 506000\n",
      "    num_steps_trained: 506000\n",
      "  iterations_since_restore: 506\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.32444444444445\n",
      "    ram_util_percent: 47.02666666666667\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04017971525234737\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.017868624678563\n",
      "    mean_inference_ms: 1.7533362136685005\n",
      "    mean_raw_obs_processing_ms: 1.8380243211893483\n",
      "  time_since_restore: 14568.308963060379\n",
      "  time_this_iter_s: 31.35709261894226\n",
      "  time_total_s: 14568.308963060379\n",
      "  timers:\n",
      "    learn_throughput: 1157.301\n",
      "    learn_time_ms: 864.079\n",
      "    load_throughput: 48572.395\n",
      "    load_time_ms: 20.588\n",
      "    sample_throughput: 30.328\n",
      "    sample_time_ms: 32972.577\n",
      "    update_time_ms: 4.69\n",
      "  timestamp: 1634857799\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 506000\n",
      "  training_iteration: 506\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   506</td><td style=\"text-align: right;\">         14568.3</td><td style=\"text-align: right;\">506000</td><td style=\"text-align: right;\"> -2.7669</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">            276.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 507000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-10-30\n",
      "  done: false\n",
      "  episode_len_mean: 278.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.7799999999999843\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1452\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3124472249614577\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1656923479504055\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01010942759916694\n",
      "          policy_loss: -0.00040735362304581535\n",
      "          total_loss: 0.002511421259906557\n",
      "          vf_explained_var: 0.4940005838871002\n",
      "          vf_loss: 0.011417035479098558\n",
      "    num_agent_steps_sampled: 507000\n",
      "    num_agent_steps_trained: 507000\n",
      "    num_steps_sampled: 507000\n",
      "    num_steps_trained: 507000\n",
      "  iterations_since_restore: 507\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.5090909090909\n",
      "    ram_util_percent: 46.943181818181806\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04018011929001366\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.033447356927862\n",
      "    mean_inference_ms: 1.7533473401943944\n",
      "    mean_raw_obs_processing_ms: 1.8386219108300605\n",
      "  time_since_restore: 14599.180495262146\n",
      "  time_this_iter_s: 30.871532201766968\n",
      "  time_total_s: 14599.180495262146\n",
      "  timers:\n",
      "    learn_throughput: 1156.577\n",
      "    learn_time_ms: 864.62\n",
      "    load_throughput: 48415.126\n",
      "    load_time_ms: 20.655\n",
      "    sample_throughput: 31.022\n",
      "    sample_time_ms: 32235.18\n",
      "    update_time_ms: 4.628\n",
      "  timestamp: 1634857830\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 507000\n",
      "  training_iteration: 507\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   507</td><td style=\"text-align: right;\">         14599.2</td><td style=\"text-align: right;\">507000</td><td style=\"text-align: right;\">   -2.78</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">               278</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 508000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-11-07\n",
      "  done: false\n",
      "  episode_len_mean: 278.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.7799999999999843\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1456\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3124472249614577\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7912040233612061\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0035290157091409924\n",
      "          policy_loss: -0.009953533940845066\n",
      "          total_loss: -0.006403201094104184\n",
      "          vf_explained_var: 0.5167177319526672\n",
      "          vf_loss: 0.010359741374850273\n",
      "    num_agent_steps_sampled: 508000\n",
      "    num_agent_steps_trained: 508000\n",
      "    num_steps_sampled: 508000\n",
      "    num_steps_trained: 508000\n",
      "  iterations_since_restore: 508\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.98679245283017\n",
      "    ram_util_percent: 46.994339622641505\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04018048279699683\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.049036718246924\n",
      "    mean_inference_ms: 1.7533586202297728\n",
      "    mean_raw_obs_processing_ms: 1.8392244618248197\n",
      "  time_since_restore: 14636.305634975433\n",
      "  time_this_iter_s: 37.12513971328735\n",
      "  time_total_s: 14636.305634975433\n",
      "  timers:\n",
      "    learn_throughput: 1154.971\n",
      "    learn_time_ms: 865.822\n",
      "    load_throughput: 46316.524\n",
      "    load_time_ms: 21.591\n",
      "    sample_throughput: 30.086\n",
      "    sample_time_ms: 33238.193\n",
      "    update_time_ms: 4.851\n",
      "  timestamp: 1634857867\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 508000\n",
      "  training_iteration: 508\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   508</td><td style=\"text-align: right;\">         14636.3</td><td style=\"text-align: right;\">508000</td><td style=\"text-align: right;\">   -2.78</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">               278</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 509000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-11-43\n",
      "  done: false\n",
      "  episode_len_mean: 277.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.7763999999999838\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1459\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15622361248072886\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6888232833809322\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00846089830981341\n",
      "          policy_loss: -0.060130093081129925\n",
      "          total_loss: -0.0563814918200175\n",
      "          vf_explained_var: 0.6219021081924438\n",
      "          vf_loss: 0.009315040816242496\n",
      "    num_agent_steps_sampled: 509000\n",
      "    num_agent_steps_trained: 509000\n",
      "    num_steps_sampled: 509000\n",
      "    num_steps_trained: 509000\n",
      "  iterations_since_restore: 509\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.90588235294118\n",
      "    ram_util_percent: 47.0392156862745\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04018068086337454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.06056445132197\n",
      "    mean_inference_ms: 1.7533652064603773\n",
      "    mean_raw_obs_processing_ms: 1.839688461734201\n",
      "  time_since_restore: 14672.160733938217\n",
      "  time_this_iter_s: 35.85509896278381\n",
      "  time_total_s: 14672.160733938217\n",
      "  timers:\n",
      "    learn_throughput: 1153.034\n",
      "    learn_time_ms: 867.277\n",
      "    load_throughput: 45729.188\n",
      "    load_time_ms: 21.868\n",
      "    sample_throughput: 29.488\n",
      "    sample_time_ms: 33911.582\n",
      "    update_time_ms: 4.698\n",
      "  timestamp: 1634857903\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 509000\n",
      "  training_iteration: 509\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   509</td><td style=\"text-align: right;\">         14672.2</td><td style=\"text-align: right;\">509000</td><td style=\"text-align: right;\"> -2.7764</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">            277.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 510000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-12-20\n",
      "  done: false\n",
      "  episode_len_mean: 277.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.7762999999999844\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1463\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15622361248072886\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7364351507690218\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007134562406944016\n",
      "          policy_loss: 0.023061188931266467\n",
      "          total_loss: 0.028032465361886555\n",
      "          vf_explained_var: 0.5104951858520508\n",
      "          vf_loss: 0.01122104318605529\n",
      "    num_agent_steps_sampled: 510000\n",
      "    num_agent_steps_trained: 510000\n",
      "    num_steps_sampled: 510000\n",
      "    num_steps_trained: 510000\n",
      "  iterations_since_restore: 510\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.8943396226415\n",
      "    ram_util_percent: 47.288679245283014\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040180993900004285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.076011836962362\n",
      "    mean_inference_ms: 1.7533723509377221\n",
      "    mean_raw_obs_processing_ms: 1.840300402559486\n",
      "  time_since_restore: 14709.344034433365\n",
      "  time_this_iter_s: 37.183300495147705\n",
      "  time_total_s: 14709.344034433365\n",
      "  timers:\n",
      "    learn_throughput: 1138.623\n",
      "    learn_time_ms: 878.253\n",
      "    load_throughput: 41781.138\n",
      "    load_time_ms: 23.934\n",
      "    sample_throughput: 29.158\n",
      "    sample_time_ms: 34295.645\n",
      "    update_time_ms: 4.328\n",
      "  timestamp: 1634857940\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 510000\n",
      "  training_iteration: 510\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   510</td><td style=\"text-align: right;\">         14709.3</td><td style=\"text-align: right;\">510000</td><td style=\"text-align: right;\"> -2.7763</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">            277.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 511000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-12-55\n",
      "  done: false\n",
      "  episode_len_mean: 278.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.7869999999999844\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1466\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15622361248072886\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0940993713008034\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.038057610246557454\n",
      "          policy_loss: -0.11392093383603626\n",
      "          total_loss: -0.10968768778774474\n",
      "          vf_explained_var: 0.6635743379592896\n",
      "          vf_loss: 0.00922874306432075\n",
      "    num_agent_steps_sampled: 511000\n",
      "    num_agent_steps_trained: 511000\n",
      "    num_steps_sampled: 511000\n",
      "    num_steps_trained: 511000\n",
      "  iterations_since_restore: 511\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.478\n",
      "    ram_util_percent: 47.336000000000006\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04018166635939238\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.087751974785988\n",
      "    mean_inference_ms: 1.7533812533686617\n",
      "    mean_raw_obs_processing_ms: 1.8407369840314924\n",
      "  time_since_restore: 14744.4036257267\n",
      "  time_this_iter_s: 35.05959129333496\n",
      "  time_total_s: 14744.4036257267\n",
      "  timers:\n",
      "    learn_throughput: 1132.687\n",
      "    learn_time_ms: 882.856\n",
      "    load_throughput: 40337.526\n",
      "    load_time_ms: 24.791\n",
      "    sample_throughput: 28.944\n",
      "    sample_time_ms: 34549.53\n",
      "    update_time_ms: 3.763\n",
      "  timestamp: 1634857975\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 511000\n",
      "  training_iteration: 511\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   511</td><td style=\"text-align: right;\">         14744.4</td><td style=\"text-align: right;\">511000</td><td style=\"text-align: right;\">  -2.787</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">             278.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 512000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-13-50\n",
      "  done: false\n",
      "  episode_len_mean: 277.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.777799999999985\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1470\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6772908535268571\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005658383271149554\n",
      "          policy_loss: -0.03816055585112837\n",
      "          total_loss: -0.03310243975785043\n",
      "          vf_explained_var: 0.5584376454353333\n",
      "          vf_loss: 0.010505062714219093\n",
      "    num_agent_steps_sampled: 512000\n",
      "    num_agent_steps_trained: 512000\n",
      "    num_steps_sampled: 512000\n",
      "    num_steps_trained: 512000\n",
      "  iterations_since_restore: 512\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.32435897435897\n",
      "    ram_util_percent: 47.28974358974358\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040182598106586156\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.103396222840583\n",
      "    mean_inference_ms: 1.753398002964887\n",
      "    mean_raw_obs_processing_ms: 1.8427224081109717\n",
      "  time_since_restore: 14799.295204639435\n",
      "  time_this_iter_s: 54.891578912734985\n",
      "  time_total_s: 14799.295204639435\n",
      "  timers:\n",
      "    learn_throughput: 1130.017\n",
      "    learn_time_ms: 884.942\n",
      "    load_throughput: 40376.357\n",
      "    load_time_ms: 24.767\n",
      "    sample_throughput: 27.207\n",
      "    sample_time_ms: 36755.777\n",
      "    update_time_ms: 3.671\n",
      "  timestamp: 1634858030\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 512000\n",
      "  training_iteration: 512\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   512</td><td style=\"text-align: right;\">         14799.3</td><td style=\"text-align: right;\">512000</td><td style=\"text-align: right;\"> -2.7778</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">            277.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 513000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-14-24\n",
      "  done: false\n",
      "  episode_len_mean: 277.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.7778999999999843\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1474\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8476710067854987\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01003436193020022\n",
      "          policy_loss: -0.005915932854016622\n",
      "          total_loss: -0.001006193541818195\n",
      "          vf_explained_var: 0.532079815864563\n",
      "          vf_loss: 0.011035044491291047\n",
      "    num_agent_steps_sampled: 513000\n",
      "    num_agent_steps_trained: 513000\n",
      "    num_steps_sampled: 513000\n",
      "    num_steps_trained: 513000\n",
      "  iterations_since_restore: 513\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.36874999999999\n",
      "    ram_util_percent: 47.210416666666674\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0401835849675571\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.119199983256593\n",
      "    mean_inference_ms: 1.753414635895818\n",
      "    mean_raw_obs_processing_ms: 1.8447102423861008\n",
      "  time_since_restore: 14833.026959180832\n",
      "  time_this_iter_s: 33.731754541397095\n",
      "  time_total_s: 14833.026959180832\n",
      "  timers:\n",
      "    learn_throughput: 1171.717\n",
      "    learn_time_ms: 853.449\n",
      "    load_throughput: 40741.69\n",
      "    load_time_ms: 24.545\n",
      "    sample_throughput: 26.876\n",
      "    sample_time_ms: 37208.403\n",
      "    update_time_ms: 4.161\n",
      "  timestamp: 1634858064\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 513000\n",
      "  training_iteration: 513\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   513</td><td style=\"text-align: right;\">           14833</td><td style=\"text-align: right;\">513000</td><td style=\"text-align: right;\"> -2.7779</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">            277.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 514000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-14-59\n",
      "  done: false\n",
      "  episode_len_mean: 274.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.743899999999986\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1478\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8734263936678569\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006962780174244168\n",
      "          policy_loss: -0.0012861428161462147\n",
      "          total_loss: 0.001195356912083096\n",
      "          vf_explained_var: 0.547210156917572\n",
      "          vf_loss: 0.009584138850267562\n",
      "    num_agent_steps_sampled: 514000\n",
      "    num_agent_steps_trained: 514000\n",
      "    num_steps_sampled: 514000\n",
      "    num_steps_trained: 514000\n",
      "  iterations_since_restore: 514\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.24807692307692\n",
      "    ram_util_percent: 47.13653846153847\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04018459586740879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.135563020404696\n",
      "    mean_inference_ms: 1.7534353189119951\n",
      "    mean_raw_obs_processing_ms: 1.8460423515706341\n",
      "  time_since_restore: 14868.93741607666\n",
      "  time_this_iter_s: 35.91045689582825\n",
      "  time_total_s: 14868.93741607666\n",
      "  timers:\n",
      "    learn_throughput: 1162.357\n",
      "    learn_time_ms: 860.321\n",
      "    load_throughput: 40524.011\n",
      "    load_time_ms: 24.677\n",
      "    sample_throughput: 28.001\n",
      "    sample_time_ms: 35713.51\n",
      "    update_time_ms: 4.153\n",
      "  timestamp: 1634858099\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 514000\n",
      "  training_iteration: 514\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   514</td><td style=\"text-align: right;\">         14868.9</td><td style=\"text-align: right;\">514000</td><td style=\"text-align: right;\"> -2.7439</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">            274.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 515000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-15-37\n",
      "  done: false\n",
      "  episode_len_mean: 275.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.7503999999999853\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1481\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.23433541872109329\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6946769376595815\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004971676985621186\n",
      "          policy_loss: -0.10925626763039165\n",
      "          total_loss: -0.10320887996090783\n",
      "          vf_explained_var: 0.456130713224411\n",
      "          vf_loss: 0.01182911769590444\n",
      "    num_agent_steps_sampled: 515000\n",
      "    num_agent_steps_trained: 515000\n",
      "    num_steps_sampled: 515000\n",
      "    num_steps_trained: 515000\n",
      "  iterations_since_restore: 515\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.50566037735848\n",
      "    ram_util_percent: 47.07735849056603\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04018538537445723\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.148040382590704\n",
      "    mean_inference_ms: 1.7534516829257316\n",
      "    mean_raw_obs_processing_ms: 1.846507556829726\n",
      "  time_since_restore: 14906.5226957798\n",
      "  time_this_iter_s: 37.58527970314026\n",
      "  time_total_s: 14906.5226957798\n",
      "  timers:\n",
      "    learn_throughput: 1158.865\n",
      "    learn_time_ms: 862.913\n",
      "    load_throughput: 40061.128\n",
      "    load_time_ms: 24.962\n",
      "    sample_throughput: 27.732\n",
      "    sample_time_ms: 36059.797\n",
      "    update_time_ms: 3.666\n",
      "  timestamp: 1634858137\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 515000\n",
      "  training_iteration: 515\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   515</td><td style=\"text-align: right;\">         14906.5</td><td style=\"text-align: right;\">515000</td><td style=\"text-align: right;\"> -2.7504</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">            275.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 516000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-16-14\n",
      "  done: false\n",
      "  episode_len_mean: 274.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.7448999999999852\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1485\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11716770936054664\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6228111180994246\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006109880913447914\n",
      "          policy_loss: 0.002636559804280599\n",
      "          total_loss: 0.008267021675904592\n",
      "          vf_explained_var: 0.4115751385688782\n",
      "          vf_loss: 0.011142695488201248\n",
      "    num_agent_steps_sampled: 516000\n",
      "    num_agent_steps_trained: 516000\n",
      "    num_steps_sampled: 516000\n",
      "    num_steps_trained: 516000\n",
      "  iterations_since_restore: 516\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.59811320754719\n",
      "    ram_util_percent: 47.04716981132075\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040186548753458\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.164550089622036\n",
      "    mean_inference_ms: 1.7534746962782812\n",
      "    mean_raw_obs_processing_ms: 1.8471573582952272\n",
      "  time_since_restore: 14943.618588209152\n",
      "  time_this_iter_s: 37.09589242935181\n",
      "  time_total_s: 14943.618588209152\n",
      "  timers:\n",
      "    learn_throughput: 1155.597\n",
      "    learn_time_ms: 865.354\n",
      "    load_throughput: 38495.746\n",
      "    load_time_ms: 25.977\n",
      "    sample_throughput: 27.3\n",
      "    sample_time_ms: 36630.218\n",
      "    update_time_ms: 3.671\n",
      "  timestamp: 1634858174\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 516000\n",
      "  training_iteration: 516\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   516</td><td style=\"text-align: right;\">         14943.6</td><td style=\"text-align: right;\">516000</td><td style=\"text-align: right;\"> -2.7449</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">            274.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 517000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-16-50\n",
      "  done: false\n",
      "  episode_len_mean: 274.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.7479999999999847\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1489\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11716770936054664\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6614859388934241\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005369788246058777\n",
      "          policy_loss: 0.02038173195388582\n",
      "          total_loss: 0.026164884782499738\n",
      "          vf_explained_var: 0.43124091625213623\n",
      "          vf_loss: 0.011768845830940539\n",
      "    num_agent_steps_sampled: 517000\n",
      "    num_agent_steps_trained: 517000\n",
      "    num_steps_sampled: 517000\n",
      "    num_steps_trained: 517000\n",
      "  iterations_since_restore: 517\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.70769230769231\n",
      "    ram_util_percent: 47.20576923076922\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04018798797959465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.180943408173317\n",
      "    mean_inference_ms: 1.753500453079409\n",
      "    mean_raw_obs_processing_ms: 1.84781440861917\n",
      "  time_since_restore: 14979.76835346222\n",
      "  time_this_iter_s: 36.14976525306702\n",
      "  time_total_s: 14979.76835346222\n",
      "  timers:\n",
      "    learn_throughput: 1144.446\n",
      "    learn_time_ms: 873.785\n",
      "    load_throughput: 38357.321\n",
      "    load_time_ms: 26.071\n",
      "    sample_throughput: 26.918\n",
      "    sample_time_ms: 37149.538\n",
      "    update_time_ms: 3.653\n",
      "  timestamp: 1634858210\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 517000\n",
      "  training_iteration: 517\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   517</td><td style=\"text-align: right;\">         14979.8</td><td style=\"text-align: right;\">517000</td><td style=\"text-align: right;\">  -2.748</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">             274.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 518000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-17-26\n",
      "  done: false\n",
      "  episode_len_mean: 276.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.7604999999999844\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1493\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.11716770936054664\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.088359420829349\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.05130250024370724\n",
      "          policy_loss: -0.032376889636119205\n",
      "          total_loss: -0.027891238033771516\n",
      "          vf_explained_var: 0.5192078351974487\n",
      "          vf_loss: 0.009358248770392189\n",
      "    num_agent_steps_sampled: 518000\n",
      "    num_agent_steps_trained: 518000\n",
      "    num_steps_sampled: 518000\n",
      "    num_steps_trained: 518000\n",
      "  iterations_since_restore: 518\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.516\n",
      "    ram_util_percent: 47.29200000000001\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04018937432641666\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.197136444402307\n",
      "    mean_inference_ms: 1.7535269934780553\n",
      "    mean_raw_obs_processing_ms: 1.8484761305988264\n",
      "  time_since_restore: 15014.909374952316\n",
      "  time_this_iter_s: 35.141021490097046\n",
      "  time_total_s: 15014.909374952316\n",
      "  timers:\n",
      "    learn_throughput: 1143.737\n",
      "    learn_time_ms: 874.327\n",
      "    load_throughput: 38351.744\n",
      "    load_time_ms: 26.074\n",
      "    sample_throughput: 27.063\n",
      "    sample_time_ms: 36950.921\n",
      "    update_time_ms: 3.494\n",
      "  timestamp: 1634858246\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 518000\n",
      "  training_iteration: 518\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   518</td><td style=\"text-align: right;\">         15014.9</td><td style=\"text-align: right;\">518000</td><td style=\"text-align: right;\"> -2.7605</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">            276.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 519000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-18-02\n",
      "  done: false\n",
      "  episode_len_mean: 276.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.7610999999999852\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1496\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.17575156404082\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.632233691877789\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005204542822727139\n",
      "          policy_loss: -0.1020135785970423\n",
      "          total_loss: -0.09549595225188467\n",
      "          vf_explained_var: 0.3758816719055176\n",
      "          vf_loss: 0.011925259004864427\n",
      "    num_agent_steps_sampled: 519000\n",
      "    num_agent_steps_trained: 519000\n",
      "    num_steps_sampled: 519000\n",
      "    num_steps_trained: 519000\n",
      "  iterations_since_restore: 519\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.53653846153847\n",
      "    ram_util_percent: 47.24230769230769\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040190379201525454\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.209326361425\n",
      "    mean_inference_ms: 1.753548306317503\n",
      "    mean_raw_obs_processing_ms: 1.8489602497148645\n",
      "  time_since_restore: 15051.57756781578\n",
      "  time_this_iter_s: 36.668192863464355\n",
      "  time_total_s: 15051.57756781578\n",
      "  timers:\n",
      "    learn_throughput: 1145.01\n",
      "    learn_time_ms: 873.355\n",
      "    load_throughput: 38422.255\n",
      "    load_time_ms: 26.027\n",
      "    sample_throughput: 27.003\n",
      "    sample_time_ms: 37033.168\n",
      "    update_time_ms: 3.492\n",
      "  timestamp: 1634858282\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 519000\n",
      "  training_iteration: 519\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   519</td><td style=\"text-align: right;\">         15051.6</td><td style=\"text-align: right;\">519000</td><td style=\"text-align: right;\"> -2.7611</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">            276.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 520000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-18-54\n",
      "  done: false\n",
      "  episode_len_mean: 275.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.758899999999985\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1500\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.17575156404082\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6367360101805792\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004903416560193148\n",
      "          policy_loss: -0.003511941846874025\n",
      "          total_loss: 0.00245940958460172\n",
      "          vf_explained_var: 0.41845595836639404\n",
      "          vf_loss: 0.011476930768953428\n",
      "    num_agent_steps_sampled: 520000\n",
      "    num_agent_steps_trained: 520000\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "  iterations_since_restore: 520\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.08666666666667\n",
      "    ram_util_percent: 47.22933333333333\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040191857898237704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.225602302390158\n",
      "    mean_inference_ms: 1.753578663290946\n",
      "    mean_raw_obs_processing_ms: 1.8508409337673613\n",
      "  time_since_restore: 15103.544687986374\n",
      "  time_this_iter_s: 51.96712017059326\n",
      "  time_total_s: 15103.544687986374\n",
      "  timers:\n",
      "    learn_throughput: 1155.898\n",
      "    learn_time_ms: 865.129\n",
      "    load_throughput: 39701.381\n",
      "    load_time_ms: 25.188\n",
      "    sample_throughput: 25.96\n",
      "    sample_time_ms: 38521.185\n",
      "    update_time_ms: 3.156\n",
      "  timestamp: 1634858334\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 520\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   520</td><td style=\"text-align: right;\">         15103.5</td><td style=\"text-align: right;\">520000</td><td style=\"text-align: right;\"> -2.7589</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">            275.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 521000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-19-28\n",
      "  done: false\n",
      "  episode_len_mean: 276.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.7619999999999845\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1503\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08787578202041\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8958372539944119\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010586114582975368\n",
      "          policy_loss: -0.18117373858888944\n",
      "          total_loss: -0.17848145680295097\n",
      "          vf_explained_var: 0.49958181381225586\n",
      "          vf_loss: 0.010720395514120658\n",
      "    num_agent_steps_sampled: 521000\n",
      "    num_agent_steps_trained: 521000\n",
      "    num_steps_sampled: 521000\n",
      "    num_steps_trained: 521000\n",
      "  iterations_since_restore: 521\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.73958333333331\n",
      "    ram_util_percent: 47.083333333333336\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04019303303146622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.237775474608856\n",
      "    mean_inference_ms: 1.753606337874093\n",
      "    mean_raw_obs_processing_ms: 1.8522300539264989\n",
      "  time_since_restore: 15137.721223115921\n",
      "  time_this_iter_s: 34.17653512954712\n",
      "  time_total_s: 15137.721223115921\n",
      "  timers:\n",
      "    learn_throughput: 1152.402\n",
      "    learn_time_ms: 867.752\n",
      "    load_throughput: 39746.076\n",
      "    load_time_ms: 25.16\n",
      "    sample_throughput: 26.021\n",
      "    sample_time_ms: 38431.068\n",
      "    update_time_ms: 3.527\n",
      "  timestamp: 1634858368\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 521000\n",
      "  training_iteration: 521\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   521</td><td style=\"text-align: right;\">         15137.7</td><td style=\"text-align: right;\">521000</td><td style=\"text-align: right;\">  -2.762</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">             276.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 522000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-20-03\n",
      "  done: false\n",
      "  episode_len_mean: 278.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.149999999999998\n",
      "  episode_reward_mean: -2.782399999999984\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1507\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08787578202041\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7725296715895335\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01157385186715747\n",
      "          policy_loss: 0.025999985966417526\n",
      "          total_loss: 0.02797273016638226\n",
      "          vf_explained_var: 0.5909097194671631\n",
      "          vf_loss: 0.008680981859086185\n",
      "    num_agent_steps_sampled: 522000\n",
      "    num_agent_steps_trained: 522000\n",
      "    num_steps_sampled: 522000\n",
      "    num_steps_trained: 522000\n",
      "  iterations_since_restore: 522\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.60612244897959\n",
      "    ram_util_percent: 46.989795918367335\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04019460035519048\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.253443289916135\n",
      "    mean_inference_ms: 1.7536443369009158\n",
      "    mean_raw_obs_processing_ms: 1.8537742142086384\n",
      "  time_since_restore: 15172.053596019745\n",
      "  time_this_iter_s: 34.33237290382385\n",
      "  time_total_s: 15172.053596019745\n",
      "  timers:\n",
      "    learn_throughput: 1155.095\n",
      "    learn_time_ms: 865.73\n",
      "    load_throughput: 38183.458\n",
      "    load_time_ms: 26.189\n",
      "    sample_throughput: 27.491\n",
      "    sample_time_ms: 36376.154\n",
      "    update_time_ms: 3.526\n",
      "  timestamp: 1634858403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 522000\n",
      "  training_iteration: 522\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   522</td><td style=\"text-align: right;\">         15172.1</td><td style=\"text-align: right;\">522000</td><td style=\"text-align: right;\"> -2.7824</td><td style=\"text-align: right;\">               -2.15</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">            278.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 523000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-20-38\n",
      "  done: false\n",
      "  episode_len_mean: 279.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.299999999999995\n",
      "  episode_reward_mean: -2.7972999999999844\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1511\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08787578202041\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8829357034630245\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016135534453972776\n",
      "          policy_loss: -0.012264960259199143\n",
      "          total_loss: -0.007960578261150254\n",
      "          vf_explained_var: 0.47752419114112854\n",
      "          vf_loss: 0.011715815888924732\n",
      "    num_agent_steps_sampled: 523000\n",
      "    num_agent_steps_trained: 523000\n",
      "    num_steps_sampled: 523000\n",
      "    num_steps_trained: 523000\n",
      "  iterations_since_restore: 523\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.66862745098038\n",
      "    ram_util_percent: 47.0\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040196382225473906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.268975233580157\n",
      "    mean_inference_ms: 1.7536865962518207\n",
      "    mean_raw_obs_processing_ms: 1.8542548170434388\n",
      "  time_since_restore: 15207.577223777771\n",
      "  time_this_iter_s: 35.52362775802612\n",
      "  time_total_s: 15207.577223777771\n",
      "  timers:\n",
      "    learn_throughput: 1150.274\n",
      "    learn_time_ms: 869.358\n",
      "    load_throughput: 37941.673\n",
      "    load_time_ms: 26.356\n",
      "    sample_throughput: 27.358\n",
      "    sample_time_ms: 36551.898\n",
      "    update_time_ms: 3.053\n",
      "  timestamp: 1634858438\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 523000\n",
      "  training_iteration: 523\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   523</td><td style=\"text-align: right;\">         15207.6</td><td style=\"text-align: right;\">523000</td><td style=\"text-align: right;\"> -2.7973</td><td style=\"text-align: right;\">                -2.3</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">            279.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 524000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-21-12\n",
      "  done: false\n",
      "  episode_len_mean: 280.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.299999999999995\n",
      "  episode_reward_mean: -2.8026999999999846\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1514\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.08787578202041\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9758203791247474\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.03432603155561965\n",
      "          policy_loss: -0.0717001839644379\n",
      "          total_loss: -0.07006839364767074\n",
      "          vf_explained_var: 0.5829194188117981\n",
      "          vf_loss: 0.00837356524651922\n",
      "    num_agent_steps_sampled: 524000\n",
      "    num_agent_steps_trained: 524000\n",
      "    num_steps_sampled: 524000\n",
      "    num_steps_trained: 524000\n",
      "  iterations_since_restore: 524\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.41666666666667\n",
      "    ram_util_percent: 47.18125\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040197697834895206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.280465418840745\n",
      "    mean_inference_ms: 1.753719685840883\n",
      "    mean_raw_obs_processing_ms: 1.854620620084601\n",
      "  time_since_restore: 15241.113145589828\n",
      "  time_this_iter_s: 33.535921812057495\n",
      "  time_total_s: 15241.113145589828\n",
      "  timers:\n",
      "    learn_throughput: 1156.015\n",
      "    learn_time_ms: 865.041\n",
      "    load_throughput: 39826.69\n",
      "    load_time_ms: 25.109\n",
      "    sample_throughput: 27.533\n",
      "    sample_time_ms: 36319.939\n",
      "    update_time_ms: 3.068\n",
      "  timestamp: 1634858472\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 524000\n",
      "  training_iteration: 524\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   524</td><td style=\"text-align: right;\">         15241.1</td><td style=\"text-align: right;\">524000</td><td style=\"text-align: right;\"> -2.8027</td><td style=\"text-align: right;\">                -2.3</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">            280.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 525000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-21-46\n",
      "  done: false\n",
      "  episode_len_mean: 281.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.299999999999995\n",
      "  episode_reward_mean: -2.812199999999984\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1517\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.13181367303061498\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7245148933596082\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0084175545171512\n",
      "          policy_loss: -0.1681638992495007\n",
      "          total_loss: -0.16219142907195622\n",
      "          vf_explained_var: 0.4731777310371399\n",
      "          vf_loss: 0.012108071272571881\n",
      "    num_agent_steps_sampled: 525000\n",
      "    num_agent_steps_trained: 525000\n",
      "    num_steps_sampled: 525000\n",
      "    num_steps_trained: 525000\n",
      "  iterations_since_restore: 525\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.3938775510204\n",
      "    ram_util_percent: 47.23265306122449\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040198976718194335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.291683549915206\n",
      "    mean_inference_ms: 1.7537536142451597\n",
      "    mean_raw_obs_processing_ms: 1.854988407897753\n",
      "  time_since_restore: 15275.58930516243\n",
      "  time_this_iter_s: 34.47615957260132\n",
      "  time_total_s: 15275.58930516243\n",
      "  timers:\n",
      "    learn_throughput: 1157.21\n",
      "    learn_time_ms: 864.148\n",
      "    load_throughput: 41696.157\n",
      "    load_time_ms: 23.983\n",
      "    sample_throughput: 27.769\n",
      "    sample_time_ms: 36011.222\n",
      "    update_time_ms: 2.933\n",
      "  timestamp: 1634858506\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 525000\n",
      "  training_iteration: 525\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   525</td><td style=\"text-align: right;\">         15275.6</td><td style=\"text-align: right;\">525000</td><td style=\"text-align: right;\"> -2.8122</td><td style=\"text-align: right;\">                -2.3</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">            281.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 526000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-22-22\n",
      "  done: false\n",
      "  episode_len_mean: 280.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.299999999999995\n",
      "  episode_reward_mean: -2.8096999999999834\n",
      "  episode_reward_min: -5.649999999999924\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1521\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.13181367303061498\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6397123826874627\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006298111053176707\n",
      "          policy_loss: -0.008874957511822382\n",
      "          total_loss: -0.005911606053511302\n",
      "          vf_explained_var: 0.5720086097717285\n",
      "          vf_loss: 0.008530295663513243\n",
      "    num_agent_steps_sampled: 526000\n",
      "    num_agent_steps_trained: 526000\n",
      "    num_steps_sampled: 526000\n",
      "    num_steps_trained: 526000\n",
      "  iterations_since_restore: 526\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.39607843137254\n",
      "    ram_util_percent: 47.288235294117655\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0402006672537725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.307268611803202\n",
      "    mean_inference_ms: 1.7537996915709289\n",
      "    mean_raw_obs_processing_ms: 1.8554573567219745\n",
      "  time_since_restore: 15311.376846551895\n",
      "  time_this_iter_s: 35.78754138946533\n",
      "  time_total_s: 15311.376846551895\n",
      "  timers:\n",
      "    learn_throughput: 1159.942\n",
      "    learn_time_ms: 862.112\n",
      "    load_throughput: 43412.33\n",
      "    load_time_ms: 23.035\n",
      "    sample_throughput: 27.868\n",
      "    sample_time_ms: 35883.195\n",
      "    update_time_ms: 3.003\n",
      "  timestamp: 1634858542\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 526000\n",
      "  training_iteration: 526\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   526</td><td style=\"text-align: right;\">         15311.4</td><td style=\"text-align: right;\">526000</td><td style=\"text-align: right;\"> -2.8097</td><td style=\"text-align: right;\">                -2.3</td><td style=\"text-align: right;\">               -5.65</td><td style=\"text-align: right;\">            280.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 527000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-22-56\n",
      "  done: false\n",
      "  episode_len_mean: 278.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.299999999999995\n",
      "  episode_reward_mean: -2.788599999999984\n",
      "  episode_reward_min: -4.229999999999954\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1525\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.13181367303061498\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8736242334047953\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01296757342701075\n",
      "          policy_loss: 0.011549840950303607\n",
      "          total_loss: 0.01696177042192883\n",
      "          vf_explained_var: 0.4486713409423828\n",
      "          vf_loss: 0.012438867561933067\n",
      "    num_agent_steps_sampled: 527000\n",
      "    num_agent_steps_trained: 527000\n",
      "    num_steps_sampled: 527000\n",
      "    num_steps_trained: 527000\n",
      "  iterations_since_restore: 527\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.70833333333333\n",
      "    ram_util_percent: 47.37291666666666\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040202643988936006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.32301611505812\n",
      "    mean_inference_ms: 1.7538464644668026\n",
      "    mean_raw_obs_processing_ms: 1.8559987257692638\n",
      "  time_since_restore: 15344.797712087631\n",
      "  time_this_iter_s: 33.420865535736084\n",
      "  time_total_s: 15344.797712087631\n",
      "  timers:\n",
      "    learn_throughput: 1172.909\n",
      "    learn_time_ms: 852.581\n",
      "    load_throughput: 43102.63\n",
      "    load_time_ms: 23.2\n",
      "    sample_throughput: 28.074\n",
      "    sample_time_ms: 35619.876\n",
      "    update_time_ms: 2.946\n",
      "  timestamp: 1634858576\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 527000\n",
      "  training_iteration: 527\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   527</td><td style=\"text-align: right;\">         15344.8</td><td style=\"text-align: right;\">527000</td><td style=\"text-align: right;\"> -2.7886</td><td style=\"text-align: right;\">                -2.3</td><td style=\"text-align: right;\">               -4.23</td><td style=\"text-align: right;\">            278.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 528000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-23-31\n",
      "  done: false\n",
      "  episode_len_mean: 278.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.299999999999995\n",
      "  episode_reward_mean: -2.7877999999999847\n",
      "  episode_reward_min: -4.229999999999954\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1529\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.13181367303061498\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7247985184192658\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0057627181774790965\n",
      "          policy_loss: 0.021342643019225862\n",
      "          total_loss: 0.025704831961128445\n",
      "          vf_explained_var: 0.5127815008163452\n",
      "          vf_loss: 0.010850569212602244\n",
      "    num_agent_steps_sampled: 528000\n",
      "    num_agent_steps_trained: 528000\n",
      "    num_steps_sampled: 528000\n",
      "    num_steps_trained: 528000\n",
      "  iterations_since_restore: 528\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.63137254901962\n",
      "    ram_util_percent: 47.333333333333336\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04020462087261787\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.338911404170975\n",
      "    mean_inference_ms: 1.7538941281715414\n",
      "    mean_raw_obs_processing_ms: 1.8565466299360311\n",
      "  time_since_restore: 15380.584784507751\n",
      "  time_this_iter_s: 35.78707242012024\n",
      "  time_total_s: 15380.584784507751\n",
      "  timers:\n",
      "    learn_throughput: 1172.041\n",
      "    learn_time_ms: 853.212\n",
      "    load_throughput: 42957.613\n",
      "    load_time_ms: 23.279\n",
      "    sample_throughput: 28.024\n",
      "    sample_time_ms: 35683.098\n",
      "    update_time_ms: 3.617\n",
      "  timestamp: 1634858611\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 528000\n",
      "  training_iteration: 528\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   528</td><td style=\"text-align: right;\">         15380.6</td><td style=\"text-align: right;\">528000</td><td style=\"text-align: right;\"> -2.7878</td><td style=\"text-align: right;\">                -2.3</td><td style=\"text-align: right;\">               -4.23</td><td style=\"text-align: right;\">            278.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 529000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-24-19\n",
      "  done: false\n",
      "  episode_len_mean: 278.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.785699999999984\n",
      "  episode_reward_min: -4.229999999999954\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1532\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.13181367303061498\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.428917889462577\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.027995499812700542\n",
      "          policy_loss: 0.0534080032673147\n",
      "          total_loss: 0.04808088400297695\n",
      "          vf_explained_var: 0.7692311406135559\n",
      "          vf_loss: 0.005271867638738412\n",
      "    num_agent_steps_sampled: 529000\n",
      "    num_agent_steps_trained: 529000\n",
      "    num_steps_sampled: 529000\n",
      "    num_steps_trained: 529000\n",
      "  iterations_since_restore: 529\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.38840579710146\n",
      "    ram_util_percent: 47.1913043478261\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0402066404788957\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.35069767168209\n",
      "    mean_inference_ms: 1.7539306691778211\n",
      "    mean_raw_obs_processing_ms: 1.8579095581097773\n",
      "  time_since_restore: 15428.54716181755\n",
      "  time_this_iter_s: 47.962377309799194\n",
      "  time_total_s: 15428.54716181755\n",
      "  timers:\n",
      "    learn_throughput: 1176.308\n",
      "    learn_time_ms: 850.117\n",
      "    load_throughput: 44919.64\n",
      "    load_time_ms: 22.262\n",
      "    sample_throughput: 27.162\n",
      "    sample_time_ms: 36816.552\n",
      "    update_time_ms: 3.654\n",
      "  timestamp: 1634858659\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 529000\n",
      "  training_iteration: 529\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   529</td><td style=\"text-align: right;\">         15428.5</td><td style=\"text-align: right;\">529000</td><td style=\"text-align: right;\"> -2.7857</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -4.23</td><td style=\"text-align: right;\">            278.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 530000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-24-57\n",
      "  done: false\n",
      "  episode_len_mean: 277.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.7783999999999853\n",
      "  episode_reward_min: -4.229999999999954\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1536\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19772050954592252\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.684778071112103\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00695691713533365\n",
      "          policy_loss: -0.001236222187678019\n",
      "          total_loss: 0.00544845602578587\n",
      "          vf_explained_var: 0.43504130840301514\n",
      "          vf_loss: 0.012156932490567367\n",
      "    num_agent_steps_sampled: 530000\n",
      "    num_agent_steps_trained: 530000\n",
      "    num_steps_sampled: 530000\n",
      "    num_steps_trained: 530000\n",
      "  iterations_since_restore: 530\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.8566037735849\n",
      "    ram_util_percent: 47.17547169811321\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04020945729080281\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.366826234986185\n",
      "    mean_inference_ms: 1.7539818582624167\n",
      "    mean_raw_obs_processing_ms: 1.8597211604898307\n",
      "  time_since_restore: 15466.179366827011\n",
      "  time_this_iter_s: 37.63220500946045\n",
      "  time_total_s: 15466.179366827011\n",
      "  timers:\n",
      "    learn_throughput: 1173.057\n",
      "    learn_time_ms: 852.474\n",
      "    load_throughput: 46224.033\n",
      "    load_time_ms: 21.634\n",
      "    sample_throughput: 28.264\n",
      "    sample_time_ms: 35381.247\n",
      "    update_time_ms: 3.587\n",
      "  timestamp: 1634858697\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 530000\n",
      "  training_iteration: 530\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   530</td><td style=\"text-align: right;\">         15466.2</td><td style=\"text-align: right;\">530000</td><td style=\"text-align: right;\"> -2.7784</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -4.23</td><td style=\"text-align: right;\">            277.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 531000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-25-27\n",
      "  done: false\n",
      "  episode_len_mean: 278.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.7883999999999842\n",
      "  episode_reward_min: -4.649999999999945\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1538\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19772050954592252\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.560646798213323\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.09134241560631122\n",
      "          policy_loss: -0.09037613231274817\n",
      "          total_loss: -0.08106098601387607\n",
      "          vf_explained_var: 0.8246917724609375\n",
      "          vf_loss: 0.006861344340076256\n",
      "    num_agent_steps_sampled: 531000\n",
      "    num_agent_steps_trained: 531000\n",
      "    num_steps_sampled: 531000\n",
      "    num_steps_trained: 531000\n",
      "  iterations_since_restore: 531\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.14186046511628\n",
      "    ram_util_percent: 47.24418604651163\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04021088102546042\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.374902651874176\n",
      "    mean_inference_ms: 1.7540087569610676\n",
      "    mean_raw_obs_processing_ms: 1.8606116906844195\n",
      "  time_since_restore: 15495.76293373108\n",
      "  time_this_iter_s: 29.583566904067993\n",
      "  time_total_s: 15495.76293373108\n",
      "  timers:\n",
      "    learn_throughput: 1171.033\n",
      "    learn_time_ms: 853.947\n",
      "    load_throughput: 46291.885\n",
      "    load_time_ms: 21.602\n",
      "    sample_throughput: 28.636\n",
      "    sample_time_ms: 34921.045\n",
      "    update_time_ms: 3.226\n",
      "  timestamp: 1634858727\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 531000\n",
      "  training_iteration: 531\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   531</td><td style=\"text-align: right;\">         15495.8</td><td style=\"text-align: right;\">531000</td><td style=\"text-align: right;\"> -2.7884</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -4.65</td><td style=\"text-align: right;\">            278.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 532000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-25-59\n",
      "  done: false\n",
      "  episode_len_mean: 278.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.784699999999984\n",
      "  episode_reward_min: -4.649999999999945\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1542\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.29658076431888375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9550244477060106\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02115438113337665\n",
      "          policy_loss: -0.06500604243742095\n",
      "          total_loss: -0.06051959755520026\n",
      "          vf_explained_var: 0.7530812621116638\n",
      "          vf_loss: 0.007762706980833577\n",
      "    num_agent_steps_sampled: 532000\n",
      "    num_agent_steps_trained: 532000\n",
      "    num_steps_sampled: 532000\n",
      "    num_steps_trained: 532000\n",
      "  iterations_since_restore: 532\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.5311111111111\n",
      "    ram_util_percent: 47.264444444444436\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04021369757174095\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.39070956462172\n",
      "    mean_inference_ms: 1.7540660460386794\n",
      "    mean_raw_obs_processing_ms: 1.8610832221512128\n",
      "  time_since_restore: 15527.692334890366\n",
      "  time_this_iter_s: 31.9294011592865\n",
      "  time_total_s: 15527.692334890366\n",
      "  timers:\n",
      "    learn_throughput: 1167.46\n",
      "    learn_time_ms: 856.56\n",
      "    load_throughput: 46249.824\n",
      "    load_time_ms: 21.622\n",
      "    sample_throughput: 28.837\n",
      "    sample_time_ms: 34677.857\n",
      "    update_time_ms: 3.222\n",
      "  timestamp: 1634858759\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 532000\n",
      "  training_iteration: 532\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   532</td><td style=\"text-align: right;\">         15527.7</td><td style=\"text-align: right;\">532000</td><td style=\"text-align: right;\"> -2.7847</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -4.65</td><td style=\"text-align: right;\">            278.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 533000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-26-23\n",
      "  done: false\n",
      "  episode_len_mean: 282.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.8240999999999827\n",
      "  episode_reward_min: -5.219999999999933\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1544\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4448711464783256\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9085237913661532\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.024660101506524906\n",
      "          policy_loss: 0.05289585217833519\n",
      "          total_loss: 0.047922846116125586\n",
      "          vf_explained_var: 0.45990535616874695\n",
      "          vf_loss: 0.0031416617534382064\n",
      "    num_agent_steps_sampled: 533000\n",
      "    num_agent_steps_trained: 533000\n",
      "    num_steps_sampled: 533000\n",
      "    num_steps_trained: 533000\n",
      "  iterations_since_restore: 533\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.34857142857142\n",
      "    ram_util_percent: 47.25428571428571\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040215142010855846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.398287628011058\n",
      "    mean_inference_ms: 1.7540967355843207\n",
      "    mean_raw_obs_processing_ms: 1.8613045368752177\n",
      "  time_since_restore: 15552.072897911072\n",
      "  time_this_iter_s: 24.380563020706177\n",
      "  time_total_s: 15552.072897911072\n",
      "  timers:\n",
      "    learn_throughput: 1167.786\n",
      "    learn_time_ms: 856.321\n",
      "    load_throughput: 47761.651\n",
      "    load_time_ms: 20.937\n",
      "    sample_throughput: 29.794\n",
      "    sample_time_ms: 33564.279\n",
      "    update_time_ms: 3.528\n",
      "  timestamp: 1634858783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 533000\n",
      "  training_iteration: 533\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   533</td><td style=\"text-align: right;\">         15552.1</td><td style=\"text-align: right;\">533000</td><td style=\"text-align: right;\"> -2.8241</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -5.22</td><td style=\"text-align: right;\">            282.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 534000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-26-56\n",
      "  done: false\n",
      "  episode_len_mean: 281.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.815099999999984\n",
      "  episode_reward_min: -5.219999999999933\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1548\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6673067197174883\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8944945606920455\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005415481968285086\n",
      "          policy_loss: -0.07037271613048182\n",
      "          total_loss: -0.06561588305566046\n",
      "          vf_explained_var: 0.5445055961608887\n",
      "          vf_loss: 0.010087992100872927\n",
      "    num_agent_steps_sampled: 534000\n",
      "    num_agent_steps_trained: 534000\n",
      "    num_steps_sampled: 534000\n",
      "    num_steps_trained: 534000\n",
      "  iterations_since_restore: 534\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.6125\n",
      "    ram_util_percent: 47.35625000000001\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04021808869831469\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.413620419881028\n",
      "    mean_inference_ms: 1.7541623928254255\n",
      "    mean_raw_obs_processing_ms: 1.8617253077975136\n",
      "  time_since_restore: 15585.460911989212\n",
      "  time_this_iter_s: 33.38801407814026\n",
      "  time_total_s: 15585.460911989212\n",
      "  timers:\n",
      "    learn_throughput: 1166.885\n",
      "    learn_time_ms: 856.983\n",
      "    load_throughput: 47410.944\n",
      "    load_time_ms: 21.092\n",
      "    sample_throughput: 29.808\n",
      "    sample_time_ms: 33548.451\n",
      "    update_time_ms: 3.622\n",
      "  timestamp: 1634858816\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 534000\n",
      "  training_iteration: 534\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   534</td><td style=\"text-align: right;\">         15585.5</td><td style=\"text-align: right;\">534000</td><td style=\"text-align: right;\"> -2.8151</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -5.22</td><td style=\"text-align: right;\">            281.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 535000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-27-32\n",
      "  done: false\n",
      "  episode_len_mean: 280.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.8062999999999843\n",
      "  episode_reward_min: -5.219999999999933\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1552\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6673067197174883\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6154280728764004\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0017261347116972717\n",
      "          policy_loss: -0.011831928458478715\n",
      "          total_loss: -0.005200755844513575\n",
      "          vf_explained_var: 0.3984777331352234\n",
      "          vf_loss: 0.011633593796028031\n",
      "    num_agent_steps_sampled: 535000\n",
      "    num_agent_steps_trained: 535000\n",
      "    num_steps_sampled: 535000\n",
      "    num_steps_trained: 535000\n",
      "  iterations_since_restore: 535\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.986\n",
      "    ram_util_percent: 47.374\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040221027550393876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.42915916295193\n",
      "    mean_inference_ms: 1.7542277366187224\n",
      "    mean_raw_obs_processing_ms: 1.8621831399355506\n",
      "  time_since_restore: 15620.961079120636\n",
      "  time_this_iter_s: 35.50016713142395\n",
      "  time_total_s: 15620.961079120636\n",
      "  timers:\n",
      "    learn_throughput: 1143.584\n",
      "    learn_time_ms: 874.444\n",
      "    load_throughput: 45473.118\n",
      "    load_time_ms: 21.991\n",
      "    sample_throughput: 29.733\n",
      "    sample_time_ms: 33632.779\n",
      "    update_time_ms: 3.422\n",
      "  timestamp: 1634858852\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 535000\n",
      "  training_iteration: 535\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   535</td><td style=\"text-align: right;\">           15621</td><td style=\"text-align: right;\">535000</td><td style=\"text-align: right;\"> -2.8063</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -5.22</td><td style=\"text-align: right;\">            280.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 536000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-28-03\n",
      "  done: false\n",
      "  episode_len_mean: 281.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.8166999999999844\n",
      "  episode_reward_min: -5.219999999999933\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1555\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.33365335985874417\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6833131922615899\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014191004185207825\n",
      "          policy_loss: 0.03858140764964951\n",
      "          total_loss: 0.044719645712110734\n",
      "          vf_explained_var: 0.5205349922180176\n",
      "          vf_loss: 0.00823649807393344\n",
      "    num_agent_steps_sampled: 536000\n",
      "    num_agent_steps_trained: 536000\n",
      "    num_steps_sampled: 536000\n",
      "    num_steps_trained: 536000\n",
      "  iterations_since_restore: 536\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.53555555555553\n",
      "    ram_util_percent: 47.36\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04022326393492173\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.440441873759678\n",
      "    mean_inference_ms: 1.7542789208540435\n",
      "    mean_raw_obs_processing_ms: 1.862530567042917\n",
      "  time_since_restore: 15652.45059633255\n",
      "  time_this_iter_s: 31.489517211914062\n",
      "  time_total_s: 15652.45059633255\n",
      "  timers:\n",
      "    learn_throughput: 1140.512\n",
      "    learn_time_ms: 876.8\n",
      "    load_throughput: 43438.677\n",
      "    load_time_ms: 23.021\n",
      "    sample_throughput: 30.121\n",
      "    sample_time_ms: 33199.751\n",
      "    update_time_ms: 3.36\n",
      "  timestamp: 1634858883\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 536000\n",
      "  training_iteration: 536\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   536</td><td style=\"text-align: right;\">         15652.5</td><td style=\"text-align: right;\">536000</td><td style=\"text-align: right;\"> -2.8167</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -5.22</td><td style=\"text-align: right;\">            281.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 537000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-28-38\n",
      "  done: false\n",
      "  episode_len_mean: 283.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.830799999999984\n",
      "  episode_reward_min: -5.219999999999933\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1558\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.33365335985874417\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9462871220376756\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009912813227136831\n",
      "          policy_loss: 0.03171752393245697\n",
      "          total_loss: 0.034423879202869205\n",
      "          vf_explained_var: 0.45893394947052\n",
      "          vf_loss: 0.008861781228592411\n",
      "    num_agent_steps_sampled: 537000\n",
      "    num_agent_steps_trained: 537000\n",
      "    num_steps_sampled: 537000\n",
      "    num_steps_trained: 537000\n",
      "  iterations_since_restore: 537\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.84897959183674\n",
      "    ram_util_percent: 47.28571428571428\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04022560004857914\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.451753356071723\n",
      "    mean_inference_ms: 1.7543322906139478\n",
      "    mean_raw_obs_processing_ms: 1.8628517414809407\n",
      "  time_since_restore: 15686.694724559784\n",
      "  time_this_iter_s: 34.24412822723389\n",
      "  time_total_s: 15686.694724559784\n",
      "  timers:\n",
      "    learn_throughput: 1138.455\n",
      "    learn_time_ms: 878.383\n",
      "    load_throughput: 44049.718\n",
      "    load_time_ms: 22.702\n",
      "    sample_throughput: 30.048\n",
      "    sample_time_ms: 33279.968\n",
      "    update_time_ms: 4.175\n",
      "  timestamp: 1634858918\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 537000\n",
      "  training_iteration: 537\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   537</td><td style=\"text-align: right;\">         15686.7</td><td style=\"text-align: right;\">537000</td><td style=\"text-align: right;\"> -2.8308</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -5.22</td><td style=\"text-align: right;\">            283.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 538000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-29-32\n",
      "  done: false\n",
      "  episode_len_mean: 282.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.828899999999984\n",
      "  episode_reward_min: -5.219999999999933\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1562\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.33365335985874417\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8177714354462093\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006717143542026547\n",
      "          policy_loss: 0.0208448626101017\n",
      "          total_loss: 0.026199301415019565\n",
      "          vf_explained_var: 0.417985200881958\n",
      "          vf_loss: 0.011290956686975228\n",
      "    num_agent_steps_sampled: 538000\n",
      "    num_agent_steps_trained: 538000\n",
      "    num_steps_sampled: 538000\n",
      "    num_steps_trained: 538000\n",
      "  iterations_since_restore: 538\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.98205128205127\n",
      "    ram_util_percent: 47.21923076923076\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040228664926997906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.466745734178684\n",
      "    mean_inference_ms: 1.7544021942080088\n",
      "    mean_raw_obs_processing_ms: 1.8645651616630108\n",
      "  time_since_restore: 15740.98963356018\n",
      "  time_this_iter_s: 54.29490900039673\n",
      "  time_total_s: 15740.98963356018\n",
      "  timers:\n",
      "    learn_throughput: 1138.641\n",
      "    learn_time_ms: 878.24\n",
      "    load_throughput: 44580.133\n",
      "    load_time_ms: 22.432\n",
      "    sample_throughput: 28.464\n",
      "    sample_time_ms: 35131.84\n",
      "    update_time_ms: 3.432\n",
      "  timestamp: 1634858972\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 538000\n",
      "  training_iteration: 538\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   538</td><td style=\"text-align: right;\">           15741</td><td style=\"text-align: right;\">538000</td><td style=\"text-align: right;\"> -2.8289</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -5.22</td><td style=\"text-align: right;\">            282.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 539000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-29-58\n",
      "  done: false\n",
      "  episode_len_mean: 284.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.8428999999999838\n",
      "  episode_reward_min: -5.219999999999933\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1564\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.33365335985874417\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4699549674987793\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02824286259086727\n",
      "          policy_loss: -0.10913688888152441\n",
      "          total_loss: -0.09959208799733056\n",
      "          vf_explained_var: 0.4940943121910095\n",
      "          vf_loss: 0.01482102370613979\n",
      "    num_agent_steps_sampled: 539000\n",
      "    num_agent_steps_trained: 539000\n",
      "    num_steps_sampled: 539000\n",
      "    num_steps_trained: 539000\n",
      "  iterations_since_restore: 539\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.4054054054054\n",
      "    ram_util_percent: 47.21621621621622\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040230048162861014\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.47392907764423\n",
      "    mean_inference_ms: 1.754436457569512\n",
      "    mean_raw_obs_processing_ms: 1.8654064210537349\n",
      "  time_since_restore: 15766.823061466217\n",
      "  time_this_iter_s: 25.833427906036377\n",
      "  time_total_s: 15766.823061466217\n",
      "  timers:\n",
      "    learn_throughput: 1134.089\n",
      "    learn_time_ms: 881.765\n",
      "    load_throughput: 42348.869\n",
      "    load_time_ms: 23.613\n",
      "    sample_throughput: 30.382\n",
      "    sample_time_ms: 32914.308\n",
      "    update_time_ms: 3.434\n",
      "  timestamp: 1634858998\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 539000\n",
      "  training_iteration: 539\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   539</td><td style=\"text-align: right;\">         15766.8</td><td style=\"text-align: right;\">539000</td><td style=\"text-align: right;\"> -2.8429</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -5.22</td><td style=\"text-align: right;\">            284.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 540000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-30-23\n",
      "  done: false\n",
      "  episode_len_mean: 288.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.8825999999999823\n",
      "  episode_reward_min: -5.219999999999933\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1567\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5004800397881163\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5042992883258395\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01415713043942855\n",
      "          policy_loss: 0.01968182838625378\n",
      "          total_loss: 0.018569143116474153\n",
      "          vf_explained_var: -0.048288021236658096\n",
      "          vf_loss: 0.0068449485503758\n",
      "    num_agent_steps_sampled: 540000\n",
      "    num_agent_steps_trained: 540000\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "  iterations_since_restore: 540\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.65277777777777\n",
      "    ram_util_percent: 47.238888888888894\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04023196755296371\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.484154156282397\n",
      "    mean_inference_ms: 1.7544867994629536\n",
      "    mean_raw_obs_processing_ms: 1.866313478842932\n",
      "  time_since_restore: 15792.215316534042\n",
      "  time_this_iter_s: 25.392255067825317\n",
      "  time_total_s: 15792.215316534042\n",
      "  timers:\n",
      "    learn_throughput: 1131.588\n",
      "    learn_time_ms: 883.714\n",
      "    load_throughput: 43231.824\n",
      "    load_time_ms: 23.131\n",
      "    sample_throughput: 31.556\n",
      "    sample_time_ms: 31689.212\n",
      "    update_time_ms: 3.446\n",
      "  timestamp: 1634859023\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 540\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   540</td><td style=\"text-align: right;\">         15792.2</td><td style=\"text-align: right;\">540000</td><td style=\"text-align: right;\"> -2.8826</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -5.22</td><td style=\"text-align: right;\">            288.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 541000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-30-54\n",
      "  done: false\n",
      "  episode_len_mean: 289.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.8974999999999818\n",
      "  episode_reward_min: -5.219999999999933\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1570\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5004800397881163\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2549593839380475\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01218555512483748\n",
      "          policy_loss: -0.12891447792450586\n",
      "          total_loss: -0.12469801993833649\n",
      "          vf_explained_var: 0.3665786683559418\n",
      "          vf_loss: 0.01066741987855898\n",
      "    num_agent_steps_sampled: 541000\n",
      "    num_agent_steps_trained: 541000\n",
      "    num_steps_sampled: 541000\n",
      "    num_steps_trained: 541000\n",
      "  iterations_since_restore: 541\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.77272727272727\n",
      "    ram_util_percent: 47.26590909090908\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040233948973338866\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.49415498746033\n",
      "    mean_inference_ms: 1.754537132468972\n",
      "    mean_raw_obs_processing_ms: 1.8665099386312647\n",
      "  time_since_restore: 15822.922709465027\n",
      "  time_this_iter_s: 30.707392930984497\n",
      "  time_total_s: 15822.922709465027\n",
      "  timers:\n",
      "    learn_throughput: 1135.905\n",
      "    learn_time_ms: 880.355\n",
      "    load_throughput: 43755.753\n",
      "    load_time_ms: 22.854\n",
      "    sample_throughput: 31.442\n",
      "    sample_time_ms: 31804.732\n",
      "    update_time_ms: 3.852\n",
      "  timestamp: 1634859054\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 541000\n",
      "  training_iteration: 541\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   541</td><td style=\"text-align: right;\">         15822.9</td><td style=\"text-align: right;\">541000</td><td style=\"text-align: right;\"> -2.8975</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -5.22</td><td style=\"text-align: right;\">            289.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 542000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-31-24\n",
      "  done: false\n",
      "  episode_len_mean: 292.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.9516999999999816\n",
      "  episode_reward_min: -6.989999999999951\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1573\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5004800397881163\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4860978543758392\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.027933239973840625\n",
      "          policy_loss: -0.031089649515019524\n",
      "          total_loss: 0.10066661373712123\n",
      "          vf_explained_var: 0.27611881494522095\n",
      "          vf_loss: 0.13263721331540082\n",
      "    num_agent_steps_sampled: 542000\n",
      "    num_agent_steps_trained: 542000\n",
      "    num_steps_sampled: 542000\n",
      "    num_steps_trained: 542000\n",
      "  iterations_since_restore: 542\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.3904761904762\n",
      "    ram_util_percent: 47.30238095238095\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04023598169636518\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.503906892070308\n",
      "    mean_inference_ms: 1.7545907225141786\n",
      "    mean_raw_obs_processing_ms: 1.8667101927555154\n",
      "  time_since_restore: 15852.928904056549\n",
      "  time_this_iter_s: 30.006194591522217\n",
      "  time_total_s: 15852.928904056549\n",
      "  timers:\n",
      "    learn_throughput: 1141.4\n",
      "    learn_time_ms: 876.117\n",
      "    load_throughput: 46032.372\n",
      "    load_time_ms: 21.724\n",
      "    sample_throughput: 31.628\n",
      "    sample_time_ms: 31617.984\n",
      "    update_time_ms: 3.844\n",
      "  timestamp: 1634859084\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 542000\n",
      "  training_iteration: 542\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   542</td><td style=\"text-align: right;\">         15852.9</td><td style=\"text-align: right;\">542000</td><td style=\"text-align: right;\"> -2.9517</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -6.99</td><td style=\"text-align: right;\">            292.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 543000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-31-55\n",
      "  done: false\n",
      "  episode_len_mean: 293.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.961399999999981\n",
      "  episode_reward_min: -6.989999999999951\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1577\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7507200596821744\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6462839669651456\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00893772916307106\n",
      "          policy_loss: 0.038903765794303685\n",
      "          total_loss: 0.06042686464885871\n",
      "          vf_explained_var: 0.30151379108428955\n",
      "          vf_loss: 0.02127620574707786\n",
      "    num_agent_steps_sampled: 543000\n",
      "    num_agent_steps_trained: 543000\n",
      "    num_steps_sampled: 543000\n",
      "    num_steps_trained: 543000\n",
      "  iterations_since_restore: 543\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.76888888888888\n",
      "    ram_util_percent: 47.39111111111111\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04023866984952295\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.5166521140052\n",
      "    mean_inference_ms: 1.7546609806962126\n",
      "    mean_raw_obs_processing_ms: 1.866954334032548\n",
      "  time_since_restore: 15883.825499296188\n",
      "  time_this_iter_s: 30.896595239639282\n",
      "  time_total_s: 15883.825499296188\n",
      "  timers:\n",
      "    learn_throughput: 1141.471\n",
      "    learn_time_ms: 876.063\n",
      "    load_throughput: 44357.039\n",
      "    load_time_ms: 22.544\n",
      "    sample_throughput: 30.989\n",
      "    sample_time_ms: 32269.052\n",
      "    update_time_ms: 3.518\n",
      "  timestamp: 1634859115\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 543000\n",
      "  training_iteration: 543\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   543</td><td style=\"text-align: right;\">         15883.8</td><td style=\"text-align: right;\">543000</td><td style=\"text-align: right;\"> -2.9614</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -6.99</td><td style=\"text-align: right;\">             293.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 544000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-32-24\n",
      "  done: false\n",
      "  episode_len_mean: 295.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.982499999999981\n",
      "  episode_reward_min: -6.989999999999951\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1579\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7507200596821744\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.663170936372545\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013461483847778845\n",
      "          policy_loss: -0.10115492687457138\n",
      "          total_loss: -0.09730842386682828\n",
      "          vf_explained_var: 0.6289772391319275\n",
      "          vf_loss: 0.010372403178674479\n",
      "    num_agent_steps_sampled: 544000\n",
      "    num_agent_steps_trained: 544000\n",
      "    num_steps_sampled: 544000\n",
      "    num_steps_trained: 544000\n",
      "  iterations_since_restore: 544\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.3\n",
      "    ram_util_percent: 47.37560975609756\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04024002536499996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.522797566283806\n",
      "    mean_inference_ms: 1.7546968626928274\n",
      "    mean_raw_obs_processing_ms: 1.867062240800026\n",
      "  time_since_restore: 15912.970670700073\n",
      "  time_this_iter_s: 29.145171403884888\n",
      "  time_total_s: 15912.970670700073\n",
      "  timers:\n",
      "    learn_throughput: 1142.463\n",
      "    learn_time_ms: 875.302\n",
      "    load_throughput: 42609.495\n",
      "    load_time_ms: 23.469\n",
      "    sample_throughput: 31.402\n",
      "    sample_time_ms: 31844.612\n",
      "    update_time_ms: 3.512\n",
      "  timestamp: 1634859144\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 544000\n",
      "  training_iteration: 544\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   544</td><td style=\"text-align: right;\">           15913</td><td style=\"text-align: right;\">544000</td><td style=\"text-align: right;\"> -2.9825</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -6.99</td><td style=\"text-align: right;\">            295.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 545000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-32-54\n",
      "  done: false\n",
      "  episode_len_mean: 295.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.2199999999999966\n",
      "  episode_reward_mean: -2.981099999999981\n",
      "  episode_reward_min: -6.989999999999951\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1582\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7507200596821744\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9151265439059999\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007812778475964363\n",
      "          policy_loss: -0.0768105393482579\n",
      "          total_loss: -0.05939227690299352\n",
      "          vf_explained_var: 0.257935494184494\n",
      "          vf_loss: 0.020704319804079004\n",
      "    num_agent_steps_sampled: 545000\n",
      "    num_agent_steps_trained: 545000\n",
      "    num_steps_sampled: 545000\n",
      "    num_steps_trained: 545000\n",
      "  iterations_since_restore: 545\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.71666666666667\n",
      "    ram_util_percent: 47.35952380952381\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04024206511926416\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.5316226279371\n",
      "    mean_inference_ms: 1.7547513511217914\n",
      "    mean_raw_obs_processing_ms: 1.8672117823955694\n",
      "  time_since_restore: 15942.320639371872\n",
      "  time_this_iter_s: 29.349968671798706\n",
      "  time_total_s: 15942.320639371872\n",
      "  timers:\n",
      "    learn_throughput: 1166.161\n",
      "    learn_time_ms: 857.514\n",
      "    load_throughput: 42538.062\n",
      "    load_time_ms: 23.508\n",
      "    sample_throughput: 32.003\n",
      "    sample_time_ms: 31246.859\n",
      "    update_time_ms: 3.973\n",
      "  timestamp: 1634859174\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 545000\n",
      "  training_iteration: 545\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   545</td><td style=\"text-align: right;\">         15942.3</td><td style=\"text-align: right;\">545000</td><td style=\"text-align: right;\"> -2.9811</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">               -6.99</td><td style=\"text-align: right;\">            295.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 546000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-33-21\n",
      "  done: false\n",
      "  episode_len_mean: 298.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.599999999999999\n",
      "  episode_reward_mean: -2.99629999999998\n",
      "  episode_reward_min: -6.989999999999951\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1585\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7507200596821744\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4383900066216786\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0124131984002811\n",
      "          policy_loss: -0.08423556192881532\n",
      "          total_loss: -0.047861592678560154\n",
      "          vf_explained_var: 0.5942037105560303\n",
      "          vf_loss: 0.041439028694811794\n",
      "    num_agent_steps_sampled: 546000\n",
      "    num_agent_steps_trained: 546000\n",
      "    num_steps_sampled: 546000\n",
      "    num_steps_trained: 546000\n",
      "  iterations_since_restore: 546\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.66153846153848\n",
      "    ram_util_percent: 47.34358974358974\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04024417194891047\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.540049183970368\n",
      "    mean_inference_ms: 1.7548079021517418\n",
      "    mean_raw_obs_processing_ms: 1.8673334139556679\n",
      "  time_since_restore: 15969.597787857056\n",
      "  time_this_iter_s: 27.277148485183716\n",
      "  time_total_s: 15969.597787857056\n",
      "  timers:\n",
      "    learn_throughput: 1166.851\n",
      "    learn_time_ms: 857.008\n",
      "    load_throughput: 44286.926\n",
      "    load_time_ms: 22.58\n",
      "    sample_throughput: 32.439\n",
      "    sample_time_ms: 30826.875\n",
      "    update_time_ms: 3.957\n",
      "  timestamp: 1634859201\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 546000\n",
      "  training_iteration: 546\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   546</td><td style=\"text-align: right;\">         15969.6</td><td style=\"text-align: right;\">546000</td><td style=\"text-align: right;\"> -2.9963</td><td style=\"text-align: right;\">                -1.6</td><td style=\"text-align: right;\">               -6.99</td><td style=\"text-align: right;\">            298.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 547000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-33-50\n",
      "  done: false\n",
      "  episode_len_mean: 302.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.599999999999999\n",
      "  episode_reward_mean: -3.0334999999999797\n",
      "  episode_reward_min: -6.989999999999951\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1588\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7507200596821744\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.513462746805615\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010120088841495326\n",
      "          policy_loss: 0.09376635367257727\n",
      "          total_loss: 0.0958228278077311\n",
      "          vf_explained_var: 0.7081937789916992\n",
      "          vf_loss: 0.009593747670037879\n",
      "    num_agent_steps_sampled: 547000\n",
      "    num_agent_steps_trained: 547000\n",
      "    num_steps_sampled: 547000\n",
      "    num_steps_trained: 547000\n",
      "  iterations_since_restore: 547\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.45\n",
      "    ram_util_percent: 47.354761904761894\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04024620709861159\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.54805653875366\n",
      "    mean_inference_ms: 1.7548641458355498\n",
      "    mean_raw_obs_processing_ms: 1.8674561504882694\n",
      "  time_since_restore: 15998.723678350449\n",
      "  time_this_iter_s: 29.125890493392944\n",
      "  time_total_s: 15998.723678350449\n",
      "  timers:\n",
      "    learn_throughput: 1165.266\n",
      "    learn_time_ms: 858.173\n",
      "    load_throughput: 46359.066\n",
      "    load_time_ms: 21.571\n",
      "    sample_throughput: 32.987\n",
      "    sample_time_ms: 30315.209\n",
      "    update_time_ms: 3.213\n",
      "  timestamp: 1634859230\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 547000\n",
      "  training_iteration: 547\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   547</td><td style=\"text-align: right;\">         15998.7</td><td style=\"text-align: right;\">547000</td><td style=\"text-align: right;\"> -3.0335</td><td style=\"text-align: right;\">                -1.6</td><td style=\"text-align: right;\">               -6.99</td><td style=\"text-align: right;\">            302.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 548000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-34-38\n",
      "  done: false\n",
      "  episode_len_mean: 304.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.599999999999999\n",
      "  episode_reward_mean: -3.0534999999999797\n",
      "  episode_reward_min: -6.989999999999951\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1591\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7507200596821744\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3619101173347896\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010154122209900151\n",
      "          policy_loss: 0.011690600050820244\n",
      "          total_loss: 0.020275824848148556\n",
      "          vf_explained_var: 0.23529425263404846\n",
      "          vf_loss: 0.014581422660396332\n",
      "    num_agent_steps_sampled: 548000\n",
      "    num_agent_steps_trained: 548000\n",
      "    num_steps_sampled: 548000\n",
      "    num_steps_trained: 548000\n",
      "  iterations_since_restore: 548\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.83088235294117\n",
      "    ram_util_percent: 47.47941176470589\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040248282381656904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.55593794213339\n",
      "    mean_inference_ms: 1.754925140551534\n",
      "    mean_raw_obs_processing_ms: 1.8684804817230414\n",
      "  time_since_restore: 16046.602020978928\n",
      "  time_this_iter_s: 47.878342628479004\n",
      "  time_total_s: 16046.602020978928\n",
      "  timers:\n",
      "    learn_throughput: 1164.345\n",
      "    learn_time_ms: 858.852\n",
      "    load_throughput: 45852.116\n",
      "    load_time_ms: 21.809\n",
      "    sample_throughput: 33.701\n",
      "    sample_time_ms: 29672.59\n",
      "    update_time_ms: 3.265\n",
      "  timestamp: 1634859278\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 548000\n",
      "  training_iteration: 548\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   548</td><td style=\"text-align: right;\">         16046.6</td><td style=\"text-align: right;\">548000</td><td style=\"text-align: right;\"> -3.0535</td><td style=\"text-align: right;\">                -1.6</td><td style=\"text-align: right;\">               -6.99</td><td style=\"text-align: right;\">            304.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 549000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-35-05\n",
      "  done: false\n",
      "  episode_len_mean: 307.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.599999999999999\n",
      "  episode_reward_mean: -3.05839999999998\n",
      "  episode_reward_min: -6.989999999999951\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1594\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7507200596821744\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8735261638959249\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012270253152601789\n",
      "          policy_loss: -0.060635235119197106\n",
      "          total_loss: -0.022225452918145393\n",
      "          vf_explained_var: 0.5708224177360535\n",
      "          vf_loss: 0.04793352171416498\n",
      "    num_agent_steps_sampled: 549000\n",
      "    num_agent_steps_trained: 549000\n",
      "    num_steps_sampled: 549000\n",
      "    num_steps_trained: 549000\n",
      "  iterations_since_restore: 549\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.65897435897436\n",
      "    ram_util_percent: 47.59487179487179\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04025054326553735\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.563453656961165\n",
      "    mean_inference_ms: 1.7549902793676153\n",
      "    mean_raw_obs_processing_ms: 1.8694765697419795\n",
      "  time_since_restore: 16073.667478561401\n",
      "  time_this_iter_s: 27.065457582473755\n",
      "  time_total_s: 16073.667478561401\n",
      "  timers:\n",
      "    learn_throughput: 1162.195\n",
      "    learn_time_ms: 860.441\n",
      "    load_throughput: 48477.407\n",
      "    load_time_ms: 20.628\n",
      "    sample_throughput: 33.564\n",
      "    sample_time_ms: 29794.083\n",
      "    update_time_ms: 3.238\n",
      "  timestamp: 1634859305\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 549000\n",
      "  training_iteration: 549\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   549</td><td style=\"text-align: right;\">         16073.7</td><td style=\"text-align: right;\">549000</td><td style=\"text-align: right;\"> -3.0584</td><td style=\"text-align: right;\">                -1.6</td><td style=\"text-align: right;\">               -6.99</td><td style=\"text-align: right;\">             307.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 550000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-35-37\n",
      "  done: false\n",
      "  episode_len_mean: 308.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.0399999999999963\n",
      "  episode_reward_mean: -3.041699999999979\n",
      "  episode_reward_min: -6.989999999999951\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1597\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7507200596821744\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9004690647125244\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016463852621530547\n",
      "          policy_loss: -0.023141060686773723\n",
      "          total_loss: 0.03849399354722765\n",
      "          vf_explained_var: 0.346019983291626\n",
      "          vf_loss: 0.05827999864187505\n",
      "    num_agent_steps_sampled: 550000\n",
      "    num_agent_steps_trained: 550000\n",
      "    num_steps_sampled: 550000\n",
      "    num_steps_trained: 550000\n",
      "  iterations_since_restore: 550\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.94000000000001\n",
      "    ram_util_percent: 47.577777777777776\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04025276160282475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.570678442011168\n",
      "    mean_inference_ms: 1.7550572569284753\n",
      "    mean_raw_obs_processing_ms: 1.8701674623313371\n",
      "  time_since_restore: 16105.624422073364\n",
      "  time_this_iter_s: 31.95694351196289\n",
      "  time_total_s: 16105.624422073364\n",
      "  timers:\n",
      "    learn_throughput: 1162.381\n",
      "    learn_time_ms: 860.303\n",
      "    load_throughput: 47740.667\n",
      "    load_time_ms: 20.947\n",
      "    sample_throughput: 32.842\n",
      "    sample_time_ms: 30448.944\n",
      "    update_time_ms: 4.287\n",
      "  timestamp: 1634859337\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 550000\n",
      "  training_iteration: 550\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   550</td><td style=\"text-align: right;\">         16105.6</td><td style=\"text-align: right;\">550000</td><td style=\"text-align: right;\"> -3.0417</td><td style=\"text-align: right;\">               -1.04</td><td style=\"text-align: right;\">               -6.99</td><td style=\"text-align: right;\">            308.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 551000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-36-08\n",
      "  done: false\n",
      "  episode_len_mean: 309.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.0399999999999963\n",
      "  episode_reward_mean: -3.0468999999999795\n",
      "  episode_reward_min: -6.989999999999951\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1600\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7507200596821744\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6709163361125522\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007804205610817484\n",
      "          policy_loss: -0.10824929045306311\n",
      "          total_loss: -0.08290777073966132\n",
      "          vf_explained_var: 0.4428415894508362\n",
      "          vf_loss: 0.02619190605150329\n",
      "    num_agent_steps_sampled: 551000\n",
      "    num_agent_steps_trained: 551000\n",
      "    num_steps_sampled: 551000\n",
      "    num_steps_trained: 551000\n",
      "  iterations_since_restore: 551\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.01555555555555\n",
      "    ram_util_percent: 47.59777777777777\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04025493420061976\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.577791647621808\n",
      "    mean_inference_ms: 1.7551230881181559\n",
      "    mean_raw_obs_processing_ms: 1.8702083550232722\n",
      "  time_since_restore: 16137.115765810013\n",
      "  time_this_iter_s: 31.49134373664856\n",
      "  time_total_s: 16137.115765810013\n",
      "  timers:\n",
      "    learn_throughput: 1161.649\n",
      "    learn_time_ms: 860.845\n",
      "    load_throughput: 47777.701\n",
      "    load_time_ms: 20.93\n",
      "    sample_throughput: 32.758\n",
      "    sample_time_ms: 30527.282\n",
      "    update_time_ms: 3.867\n",
      "  timestamp: 1634859368\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 551000\n",
      "  training_iteration: 551\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   551</td><td style=\"text-align: right;\">         16137.1</td><td style=\"text-align: right;\">551000</td><td style=\"text-align: right;\"> -3.0469</td><td style=\"text-align: right;\">               -1.04</td><td style=\"text-align: right;\">               -6.99</td><td style=\"text-align: right;\">            309.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 552000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-36-34\n",
      "  done: false\n",
      "  episode_len_mean: 312.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.0399999999999963\n",
      "  episode_reward_mean: -3.0875999999999784\n",
      "  episode_reward_min: -6.989999999999951\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1602\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7507200596821744\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8862085037761265\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019239310594780137\n",
      "          policy_loss: -0.09797456603911188\n",
      "          total_loss: 0.0031062051653862\n",
      "          vf_explained_var: 0.6928490400314331\n",
      "          vf_loss: 0.10549951808320152\n",
      "    num_agent_steps_sampled: 552000\n",
      "    num_agent_steps_trained: 552000\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "  iterations_since_restore: 552\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.35555555555555\n",
      "    ram_util_percent: 47.7388888888889\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040256655066503395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.58219564940274\n",
      "    mean_inference_ms: 1.7551667187286537\n",
      "    mean_raw_obs_processing_ms: 1.870237447584496\n",
      "  time_since_restore: 16162.528915643692\n",
      "  time_this_iter_s: 25.4131498336792\n",
      "  time_total_s: 16162.528915643692\n",
      "  timers:\n",
      "    learn_throughput: 1159.245\n",
      "    learn_time_ms: 862.631\n",
      "    load_throughput: 47805.745\n",
      "    load_time_ms: 20.918\n",
      "    sample_throughput: 33.261\n",
      "    sample_time_ms: 30065.515\n",
      "    update_time_ms: 4.679\n",
      "  timestamp: 1634859394\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 552\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   552</td><td style=\"text-align: right;\">         16162.5</td><td style=\"text-align: right;\">552000</td><td style=\"text-align: right;\"> -3.0876</td><td style=\"text-align: right;\">               -1.04</td><td style=\"text-align: right;\">               -6.99</td><td style=\"text-align: right;\">            312.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 553000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-37-00\n",
      "  done: false\n",
      "  episode_len_mean: 315.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.0399999999999963\n",
      "  episode_reward_mean: -3.146899999999978\n",
      "  episode_reward_min: -9.529999999999939\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1605\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7507200596821744\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9572295930650498\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023203547312046702\n",
      "          policy_loss: -0.10621733110811975\n",
      "          total_loss: 0.1144340183171961\n",
      "          vf_explained_var: 0.6911612749099731\n",
      "          vf_loss: 0.22280428053604232\n",
      "    num_agent_steps_sampled: 553000\n",
      "    num_agent_steps_trained: 553000\n",
      "    num_steps_sampled: 553000\n",
      "    num_steps_trained: 553000\n",
      "  iterations_since_restore: 553\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.35263157894737\n",
      "    ram_util_percent: 47.681578947368415\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04025925112660023\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.58846255916599\n",
      "    mean_inference_ms: 1.7552324195764104\n",
      "    mean_raw_obs_processing_ms: 1.8702541605921377\n",
      "  time_since_restore: 16188.783347845078\n",
      "  time_this_iter_s: 26.254432201385498\n",
      "  time_total_s: 16188.783347845078\n",
      "  timers:\n",
      "    learn_throughput: 1160.037\n",
      "    learn_time_ms: 862.042\n",
      "    load_throughput: 50734.214\n",
      "    load_time_ms: 19.711\n",
      "    sample_throughput: 33.78\n",
      "    sample_time_ms: 29603.118\n",
      "    update_time_ms: 4.758\n",
      "  timestamp: 1634859420\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 553000\n",
      "  training_iteration: 553\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   553</td><td style=\"text-align: right;\">         16188.8</td><td style=\"text-align: right;\">553000</td><td style=\"text-align: right;\"> -3.1469</td><td style=\"text-align: right;\">               -1.04</td><td style=\"text-align: right;\">               -9.53</td><td style=\"text-align: right;\">             315.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 554000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-37-28\n",
      "  done: false\n",
      "  episode_len_mean: 318.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.0399999999999963\n",
      "  episode_reward_mean: -3.194099999999977\n",
      "  episode_reward_min: -9.529999999999939\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1608\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4953568597634634\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013608468279316494\n",
      "          policy_loss: -0.047956320726209214\n",
      "          total_loss: 0.023835057102971607\n",
      "          vf_explained_var: 0.7366610169410706\n",
      "          vf_loss: 0.07142072301875386\n",
      "    num_agent_steps_sampled: 554000\n",
      "    num_agent_steps_trained: 554000\n",
      "    num_steps_sampled: 554000\n",
      "    num_steps_trained: 554000\n",
      "  iterations_since_restore: 554\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.56666666666666\n",
      "    ram_util_percent: 47.6974358974359\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04026185393447502\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.5944311065629\n",
      "    mean_inference_ms: 1.7552975518808622\n",
      "    mean_raw_obs_processing_ms: 1.8702449143865818\n",
      "  time_since_restore: 16216.205083370209\n",
      "  time_this_iter_s: 27.421735525131226\n",
      "  time_total_s: 16216.205083370209\n",
      "  timers:\n",
      "    learn_throughput: 1161.27\n",
      "    learn_time_ms: 861.126\n",
      "    load_throughput: 49291.055\n",
      "    load_time_ms: 20.288\n",
      "    sample_throughput: 33.977\n",
      "    sample_time_ms: 29431.278\n",
      "    update_time_ms: 4.788\n",
      "  timestamp: 1634859448\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 554000\n",
      "  training_iteration: 554\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   554</td><td style=\"text-align: right;\">         16216.2</td><td style=\"text-align: right;\">554000</td><td style=\"text-align: right;\"> -3.1941</td><td style=\"text-align: right;\">               -1.04</td><td style=\"text-align: right;\">               -9.53</td><td style=\"text-align: right;\">            318.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 555000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-37-54\n",
      "  done: false\n",
      "  episode_len_mean: 320.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.309999999999976\n",
      "  episode_reward_mean: -3.169499999999977\n",
      "  episode_reward_min: -9.529999999999939\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1610\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5511732319990794\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012199775280642743\n",
      "          policy_loss: -0.13527706944280202\n",
      "          total_loss: -0.05993957105610106\n",
      "          vf_explained_var: 0.7926420569419861\n",
      "          vf_loss: 0.07711130550338162\n",
      "    num_agent_steps_sampled: 555000\n",
      "    num_agent_steps_trained: 555000\n",
      "    num_steps_sampled: 555000\n",
      "    num_steps_trained: 555000\n",
      "  iterations_since_restore: 555\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.25000000000001\n",
      "    ram_util_percent: 47.681578947368415\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04026359612168691\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.59816521417666\n",
      "    mean_inference_ms: 1.7553400757950783\n",
      "    mean_raw_obs_processing_ms: 1.8702200240386886\n",
      "  time_since_restore: 16242.692865133286\n",
      "  time_this_iter_s: 26.487781763076782\n",
      "  time_total_s: 16242.692865133286\n",
      "  timers:\n",
      "    learn_throughput: 1161.072\n",
      "    learn_time_ms: 861.273\n",
      "    load_throughput: 49388.447\n",
      "    load_time_ms: 20.248\n",
      "    sample_throughput: 34.311\n",
      "    sample_time_ms: 29145.269\n",
      "    update_time_ms: 4.335\n",
      "  timestamp: 1634859474\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 555000\n",
      "  training_iteration: 555\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   555</td><td style=\"text-align: right;\">         16242.7</td><td style=\"text-align: right;\">555000</td><td style=\"text-align: right;\"> -3.1695</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">               -9.53</td><td style=\"text-align: right;\">            320.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 556000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-38-20\n",
      "  done: false\n",
      "  episode_len_mean: 324.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.309999999999976\n",
      "  episode_reward_mean: -3.2492999999999754\n",
      "  episode_reward_min: -9.529999999999939\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1613\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.887889019648234\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015113109797614656\n",
      "          policy_loss: 0.05473861574298806\n",
      "          total_loss: 0.14175312783983018\n",
      "          vf_explained_var: 0.7054204344749451\n",
      "          vf_loss: 0.08887483032627239\n",
      "    num_agent_steps_sampled: 556000\n",
      "    num_agent_steps_trained: 556000\n",
      "    num_steps_sampled: 556000\n",
      "    num_steps_trained: 556000\n",
      "  iterations_since_restore: 556\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.7189189189189\n",
      "    ram_util_percent: 47.764864864864855\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04026623658630027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.603428650850198\n",
      "    mean_inference_ms: 1.7554050020223773\n",
      "    mean_raw_obs_processing_ms: 1.8701595149627515\n",
      "  time_since_restore: 16268.497497320175\n",
      "  time_this_iter_s: 25.80463218688965\n",
      "  time_total_s: 16268.497497320175\n",
      "  timers:\n",
      "    learn_throughput: 1153.853\n",
      "    learn_time_ms: 866.662\n",
      "    load_throughput: 49579.409\n",
      "    load_time_ms: 20.17\n",
      "    sample_throughput: 34.492\n",
      "    sample_time_ms: 28992.641\n",
      "    update_time_ms: 4.481\n",
      "  timestamp: 1634859500\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 556000\n",
      "  training_iteration: 556\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   556</td><td style=\"text-align: right;\">         16268.5</td><td style=\"text-align: right;\">556000</td><td style=\"text-align: right;\"> -3.2493</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">               -9.53</td><td style=\"text-align: right;\">            324.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 557000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-38-49\n",
      "  done: false\n",
      "  episode_len_mean: 326.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.309999999999976\n",
      "  episode_reward_mean: -3.296599999999975\n",
      "  episode_reward_min: -9.529999999999939\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1615\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.006280071205563\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012782124266685907\n",
      "          policy_loss: 0.021048004925251006\n",
      "          total_loss: 0.10014936584565375\n",
      "          vf_explained_var: 0.7351067662239075\n",
      "          vf_loss: 0.08477046363469627\n",
      "    num_agent_steps_sampled: 557000\n",
      "    num_agent_steps_trained: 557000\n",
      "    num_steps_sampled: 557000\n",
      "    num_steps_trained: 557000\n",
      "  iterations_since_restore: 557\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 88.490243902439\n",
      "    ram_util_percent: 47.94878048780488\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04026810028431738\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.6067566867711\n",
      "    mean_inference_ms: 1.7554506458968115\n",
      "    mean_raw_obs_processing_ms: 1.8701120953751147\n",
      "  time_since_restore: 16297.023052215576\n",
      "  time_this_iter_s: 28.525554895401\n",
      "  time_total_s: 16297.023052215576\n",
      "  timers:\n",
      "    learn_throughput: 1155.528\n",
      "    learn_time_ms: 865.405\n",
      "    load_throughput: 49422.375\n",
      "    load_time_ms: 20.234\n",
      "    sample_throughput: 34.561\n",
      "    sample_time_ms: 28933.949\n",
      "    update_time_ms: 4.52\n",
      "  timestamp: 1634859529\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 557000\n",
      "  training_iteration: 557\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   557</td><td style=\"text-align: right;\">           16297</td><td style=\"text-align: right;\">557000</td><td style=\"text-align: right;\"> -3.2966</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">               -9.53</td><td style=\"text-align: right;\">            326.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 558000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-39-18\n",
      "  done: false\n",
      "  episode_len_mean: 327.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.309999999999976\n",
      "  episode_reward_mean: -3.3689999999999753\n",
      "  episode_reward_min: -11.179999999999948\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1618\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3154184407658047\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014008043427129262\n",
      "          policy_loss: -0.0289624460041523\n",
      "          total_loss: 0.15860875133011076\n",
      "          vf_explained_var: 0.7335397601127625\n",
      "          vf_loss: 0.1849512021160788\n",
      "    num_agent_steps_sampled: 558000\n",
      "    num_agent_steps_trained: 558000\n",
      "    num_steps_sampled: 558000\n",
      "    num_steps_trained: 558000\n",
      "  iterations_since_restore: 558\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.83170731707318\n",
      "    ram_util_percent: 47.74634146341463\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04027084869415664\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.61147298490537\n",
      "    mean_inference_ms: 1.755518006151926\n",
      "    mean_raw_obs_processing_ms: 1.8700313244620137\n",
      "  time_since_restore: 16326.185204744339\n",
      "  time_this_iter_s: 29.162152528762817\n",
      "  time_total_s: 16326.185204744339\n",
      "  timers:\n",
      "    learn_throughput: 1157.54\n",
      "    learn_time_ms: 863.901\n",
      "    load_throughput: 52072.3\n",
      "    load_time_ms: 19.204\n",
      "    sample_throughput: 36.948\n",
      "    sample_time_ms: 27064.929\n",
      "    update_time_ms: 4.433\n",
      "  timestamp: 1634859558\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 558000\n",
      "  training_iteration: 558\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   558</td><td style=\"text-align: right;\">         16326.2</td><td style=\"text-align: right;\">558000</td><td style=\"text-align: right;\">  -3.369</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -11.18</td><td style=\"text-align: right;\">            327.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 559000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-40-01\n",
      "  done: false\n",
      "  episode_len_mean: 330.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.309999999999976\n",
      "  episode_reward_mean: -3.4341999999999753\n",
      "  episode_reward_min: -11.179999999999948\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1621\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9624329182836744\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013368069980450798\n",
      "          policy_loss: 0.009956189658906725\n",
      "          total_loss: 0.17539866003725263\n",
      "          vf_explained_var: 0.7938399314880371\n",
      "          vf_loss: 0.17001327959199747\n",
      "    num_agent_steps_sampled: 559000\n",
      "    num_agent_steps_trained: 559000\n",
      "    num_steps_sampled: 559000\n",
      "    num_steps_trained: 559000\n",
      "  iterations_since_restore: 559\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.27096774193548\n",
      "    ram_util_percent: 47.65806451612904\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04027368080445208\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.6158327480865\n",
      "    mean_inference_ms: 1.7555839017610442\n",
      "    mean_raw_obs_processing_ms: 1.8708257475431056\n",
      "  time_since_restore: 16369.848764896393\n",
      "  time_this_iter_s: 43.66356015205383\n",
      "  time_total_s: 16369.848764896393\n",
      "  timers:\n",
      "    learn_throughput: 1156.594\n",
      "    learn_time_ms: 864.608\n",
      "    load_throughput: 49655.421\n",
      "    load_time_ms: 20.139\n",
      "    sample_throughput: 34.814\n",
      "    sample_time_ms: 28724.231\n",
      "    update_time_ms: 4.644\n",
      "  timestamp: 1634859601\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 559000\n",
      "  training_iteration: 559\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   559</td><td style=\"text-align: right;\">         16369.8</td><td style=\"text-align: right;\">559000</td><td style=\"text-align: right;\"> -3.4342</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -11.18</td><td style=\"text-align: right;\">            330.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 560000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-40-24\n",
      "  done: false\n",
      "  episode_len_mean: 334.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.309999999999976\n",
      "  episode_reward_mean: -3.5191999999999735\n",
      "  episode_reward_min: -11.179999999999948\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1623\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.87421272860633\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01526945592284474\n",
      "          policy_loss: -0.0776471205883556\n",
      "          total_loss: 0.05235169115993712\n",
      "          vf_explained_var: 0.8353196978569031\n",
      "          vf_loss: 0.13154631232221922\n",
      "    num_agent_steps_sampled: 560000\n",
      "    num_agent_steps_trained: 560000\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "  iterations_since_restore: 560\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.4\n",
      "    ram_util_percent: 47.628125\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04027540151044955\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.618326198705535\n",
      "    mean_inference_ms: 1.755627025605914\n",
      "    mean_raw_obs_processing_ms: 1.8713560555383553\n",
      "  time_since_restore: 16392.116575956345\n",
      "  time_this_iter_s: 22.267811059951782\n",
      "  time_total_s: 16392.116575956345\n",
      "  timers:\n",
      "    learn_throughput: 1163.136\n",
      "    learn_time_ms: 859.745\n",
      "    load_throughput: 50610.186\n",
      "    load_time_ms: 19.759\n",
      "    sample_throughput: 36.02\n",
      "    sample_time_ms: 27761.998\n",
      "    update_time_ms: 3.638\n",
      "  timestamp: 1634859624\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 560\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   560</td><td style=\"text-align: right;\">         16392.1</td><td style=\"text-align: right;\">560000</td><td style=\"text-align: right;\"> -3.5192</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -11.18</td><td style=\"text-align: right;\">            334.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 561000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-40-44\n",
      "  done: false\n",
      "  episode_len_mean: 338.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.309999999999976\n",
      "  episode_reward_mean: -3.5560999999999723\n",
      "  episode_reward_min: -11.179999999999948\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1625\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8349717418352762\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013174688686214846\n",
      "          policy_loss: -0.04596393174595303\n",
      "          total_loss: 0.08280051781071557\n",
      "          vf_explained_var: 0.7797275185585022\n",
      "          vf_loss: 0.13227841462939977\n",
      "    num_agent_steps_sampled: 561000\n",
      "    num_agent_steps_trained: 561000\n",
      "    num_steps_sampled: 561000\n",
      "    num_steps_trained: 561000\n",
      "  iterations_since_restore: 561\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.38666666666667\n",
      "    ram_util_percent: 47.559999999999995\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04027716704973197\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.62057603534098\n",
      "    mean_inference_ms: 1.7556695774212054\n",
      "    mean_raw_obs_processing_ms: 1.8718274276771436\n",
      "  time_since_restore: 16412.84954881668\n",
      "  time_this_iter_s: 20.732972860336304\n",
      "  time_total_s: 16412.84954881668\n",
      "  timers:\n",
      "    learn_throughput: 1166.681\n",
      "    learn_time_ms: 857.132\n",
      "    load_throughput: 49169.536\n",
      "    load_time_ms: 20.338\n",
      "    sample_throughput: 37.47\n",
      "    sample_time_ms: 26688.184\n",
      "    update_time_ms: 3.692\n",
      "  timestamp: 1634859644\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 561000\n",
      "  training_iteration: 561\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   561</td><td style=\"text-align: right;\">         16412.8</td><td style=\"text-align: right;\">561000</td><td style=\"text-align: right;\"> -3.5561</td><td style=\"text-align: right;\">               -0.31</td><td style=\"text-align: right;\">              -11.18</td><td style=\"text-align: right;\">            338.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 562000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-41-09\n",
      "  done: false\n",
      "  episode_len_mean: 342.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.09999999999996788\n",
      "  episode_reward_mean: -3.553599999999972\n",
      "  episode_reward_min: -11.179999999999948\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1628\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9698124064339533\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011816230076917596\n",
      "          policy_loss: -0.08069136713941892\n",
      "          total_loss: 0.0901381069380376\n",
      "          vf_explained_var: 0.6410759687423706\n",
      "          vf_loss: 0.17722158146401246\n",
      "    num_agent_steps_sampled: 562000\n",
      "    num_agent_steps_trained: 562000\n",
      "    num_steps_sampled: 562000\n",
      "    num_steps_trained: 562000\n",
      "  iterations_since_restore: 562\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.83714285714285\n",
      "    ram_util_percent: 47.54571428571428\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0402798041194348\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.623314744476097\n",
      "    mean_inference_ms: 1.7557333578860037\n",
      "    mean_raw_obs_processing_ms: 1.8725386824728045\n",
      "  time_since_restore: 16437.334396600723\n",
      "  time_this_iter_s: 24.48484778404236\n",
      "  time_total_s: 16437.334396600723\n",
      "  timers:\n",
      "    learn_throughput: 1169.242\n",
      "    learn_time_ms: 855.255\n",
      "    load_throughput: 49242.734\n",
      "    load_time_ms: 20.308\n",
      "    sample_throughput: 37.598\n",
      "    sample_time_ms: 26597.175\n",
      "    update_time_ms: 3.695\n",
      "  timestamp: 1634859669\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 562000\n",
      "  training_iteration: 562\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   562</td><td style=\"text-align: right;\">         16437.3</td><td style=\"text-align: right;\">562000</td><td style=\"text-align: right;\"> -3.5536</td><td style=\"text-align: right;\">                -0.1</td><td style=\"text-align: right;\">              -11.18</td><td style=\"text-align: right;\">            342.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 563000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-41-31\n",
      "  done: false\n",
      "  episode_len_mean: 346.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.4200000000000816\n",
      "  episode_reward_mean: -3.4955999999999716\n",
      "  episode_reward_min: -11.179999999999948\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1630\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1353041251500446\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016149841985671286\n",
      "          policy_loss: -0.11085587566097578\n",
      "          total_loss: 0.01317141064339214\n",
      "          vf_explained_var: 0.8525574803352356\n",
      "          vf_loss: 0.12719431097308795\n",
      "    num_agent_steps_sampled: 563000\n",
      "    num_agent_steps_trained: 563000\n",
      "    num_steps_sampled: 563000\n",
      "    num_steps_trained: 563000\n",
      "  iterations_since_restore: 563\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.740625\n",
      "    ram_util_percent: 47.375\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04028136907532892\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.624876694416972\n",
      "    mean_inference_ms: 1.755776498398718\n",
      "    mean_raw_obs_processing_ms: 1.8726710422620818\n",
      "  time_since_restore: 16459.562289714813\n",
      "  time_this_iter_s: 22.227893114089966\n",
      "  time_total_s: 16459.562289714813\n",
      "  timers:\n",
      "    learn_throughput: 1170.496\n",
      "    learn_time_ms: 854.338\n",
      "    load_throughput: 46914.315\n",
      "    load_time_ms: 21.315\n",
      "    sample_throughput: 38.177\n",
      "    sample_time_ms: 26194.068\n",
      "    update_time_ms: 3.982\n",
      "  timestamp: 1634859691\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 563000\n",
      "  training_iteration: 563\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   563</td><td style=\"text-align: right;\">         16459.6</td><td style=\"text-align: right;\">563000</td><td style=\"text-align: right;\"> -3.4956</td><td style=\"text-align: right;\">                2.42</td><td style=\"text-align: right;\">              -11.18</td><td style=\"text-align: right;\">            346.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 564000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-41-54\n",
      "  done: false\n",
      "  episode_len_mean: 348.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.4200000000000816\n",
      "  episode_reward_mean: -3.5973999999999706\n",
      "  episode_reward_min: -12.409999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1632\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.131290758980645\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013266491485181402\n",
      "          policy_loss: -0.09510460876756244\n",
      "          total_loss: 0.03487378723091549\n",
      "          vf_explained_var: 0.6713628172874451\n",
      "          vf_loss: 0.13635217249393464\n",
      "    num_agent_steps_sampled: 564000\n",
      "    num_agent_steps_trained: 564000\n",
      "    num_steps_sampled: 564000\n",
      "    num_steps_trained: 564000\n",
      "  iterations_since_restore: 564\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.61250000000001\n",
      "    ram_util_percent: 47.375\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04028275184105259\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.62618143725506\n",
      "    mean_inference_ms: 1.7558192788974951\n",
      "    mean_raw_obs_processing_ms: 1.8724618954290542\n",
      "  time_since_restore: 16481.96593093872\n",
      "  time_this_iter_s: 22.40364122390747\n",
      "  time_total_s: 16481.96593093872\n",
      "  timers:\n",
      "    learn_throughput: 1171.772\n",
      "    learn_time_ms: 853.408\n",
      "    load_throughput: 50295.516\n",
      "    load_time_ms: 19.882\n",
      "    sample_throughput: 38.919\n",
      "    sample_time_ms: 25694.638\n",
      "    update_time_ms: 3.857\n",
      "  timestamp: 1634859714\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 564000\n",
      "  training_iteration: 564\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   564</td><td style=\"text-align: right;\">           16482</td><td style=\"text-align: right;\">564000</td><td style=\"text-align: right;\"> -3.5974</td><td style=\"text-align: right;\">                2.42</td><td style=\"text-align: right;\">              -12.41</td><td style=\"text-align: right;\">            348.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 565000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-42-20\n",
      "  done: false\n",
      "  episode_len_mean: 352.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.4200000000000816\n",
      "  episode_reward_mean: -3.6061999999999705\n",
      "  episode_reward_min: -12.409999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1635\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9728207005394829\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013357377353701436\n",
      "          policy_loss: -0.11610116387406985\n",
      "          total_loss: -0.05433337630497085\n",
      "          vf_explained_var: 0.9473462700843811\n",
      "          vf_loss: 0.06645451444718573\n",
      "    num_agent_steps_sampled: 565000\n",
      "    num_agent_steps_trained: 565000\n",
      "    num_steps_sampled: 565000\n",
      "    num_steps_trained: 565000\n",
      "  iterations_since_restore: 565\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.6972972972973\n",
      "    ram_util_percent: 47.38108108108109\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04028478882784737\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.62750553379315\n",
      "    mean_inference_ms: 1.755882787462374\n",
      "    mean_raw_obs_processing_ms: 1.8721532121345783\n",
      "  time_since_restore: 16508.33264183998\n",
      "  time_this_iter_s: 26.366710901260376\n",
      "  time_total_s: 16508.33264183998\n",
      "  timers:\n",
      "    learn_throughput: 1174.711\n",
      "    learn_time_ms: 851.273\n",
      "    load_throughput: 52772.481\n",
      "    load_time_ms: 18.949\n",
      "    sample_throughput: 38.932\n",
      "    sample_time_ms: 25685.609\n",
      "    update_time_ms: 3.966\n",
      "  timestamp: 1634859740\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 565000\n",
      "  training_iteration: 565\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   565</td><td style=\"text-align: right;\">         16508.3</td><td style=\"text-align: right;\">565000</td><td style=\"text-align: right;\"> -3.6062</td><td style=\"text-align: right;\">                2.42</td><td style=\"text-align: right;\">              -12.41</td><td style=\"text-align: right;\">            352.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 566000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-42-43\n",
      "  done: false\n",
      "  episode_len_mean: 356.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.4200000000000816\n",
      "  episode_reward_mean: -3.573299999999971\n",
      "  episode_reward_min: -12.409999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1637\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.965590046511756\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012406644708590915\n",
      "          policy_loss: -0.006911846498648326\n",
      "          total_loss: 0.07225843361682362\n",
      "          vf_explained_var: 0.8965105414390564\n",
      "          vf_loss: 0.0848553016781807\n",
      "    num_agent_steps_sampled: 566000\n",
      "    num_agent_steps_trained: 566000\n",
      "    num_steps_sampled: 566000\n",
      "    num_steps_trained: 566000\n",
      "  iterations_since_restore: 566\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.50882352941177\n",
      "    ram_util_percent: 47.53235294117647\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04028617660692679\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.628184217629528\n",
      "    mean_inference_ms: 1.7559243894377383\n",
      "    mean_raw_obs_processing_ms: 1.8719208414975685\n",
      "  time_since_restore: 16531.61232161522\n",
      "  time_this_iter_s: 23.279679775238037\n",
      "  time_total_s: 16531.61232161522\n",
      "  timers:\n",
      "    learn_throughput: 1127.926\n",
      "    learn_time_ms: 886.583\n",
      "    load_throughput: 52666.391\n",
      "    load_time_ms: 18.987\n",
      "    sample_throughput: 39.375\n",
      "    sample_time_ms: 25397.133\n",
      "    update_time_ms: 4.616\n",
      "  timestamp: 1634859763\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 566000\n",
      "  training_iteration: 566\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   566</td><td style=\"text-align: right;\">         16531.6</td><td style=\"text-align: right;\">566000</td><td style=\"text-align: right;\"> -3.5733</td><td style=\"text-align: right;\">                2.42</td><td style=\"text-align: right;\">              -12.41</td><td style=\"text-align: right;\">            356.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 567000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-43-08\n",
      "  done: false\n",
      "  episode_len_mean: 357.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.4200000000000816\n",
      "  episode_reward_mean: -3.51639999999997\n",
      "  episode_reward_min: -12.409999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1639\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.948891912566291\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010814454661701294\n",
      "          policy_loss: -0.16645747961269486\n",
      "          total_loss: -0.10275764142473538\n",
      "          vf_explained_var: 0.8864635825157166\n",
      "          vf_loss: 0.07101081602482331\n",
      "    num_agent_steps_sampled: 567000\n",
      "    num_agent_steps_trained: 567000\n",
      "    num_steps_sampled: 567000\n",
      "    num_steps_trained: 567000\n",
      "  iterations_since_restore: 567\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.85714285714286\n",
      "    ram_util_percent: 47.54857142857143\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040287559128635915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.62863631109301\n",
      "    mean_inference_ms: 1.7559646326688563\n",
      "    mean_raw_obs_processing_ms: 1.8716923870755726\n",
      "  time_since_restore: 16556.1261780262\n",
      "  time_this_iter_s: 24.513856410980225\n",
      "  time_total_s: 16556.1261780262\n",
      "  timers:\n",
      "    learn_throughput: 1126.633\n",
      "    learn_time_ms: 887.6\n",
      "    load_throughput: 50773.889\n",
      "    load_time_ms: 19.695\n",
      "    sample_throughput: 40.009\n",
      "    sample_time_ms: 24994.404\n",
      "    update_time_ms: 4.696\n",
      "  timestamp: 1634859788\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 567000\n",
      "  training_iteration: 567\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   567</td><td style=\"text-align: right;\">         16556.1</td><td style=\"text-align: right;\">567000</td><td style=\"text-align: right;\"> -3.5164</td><td style=\"text-align: right;\">                2.42</td><td style=\"text-align: right;\">              -12.41</td><td style=\"text-align: right;\">            357.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 568000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-43-32\n",
      "  done: false\n",
      "  episode_len_mean: 360.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.4200000000000816\n",
      "  episode_reward_mean: -3.53279999999997\n",
      "  episode_reward_min: -12.409999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1641\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8794397777981229\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015539301429891333\n",
      "          policy_loss: -0.09869468907515208\n",
      "          total_loss: -0.015540089789364072\n",
      "          vf_explained_var: 0.8477363586425781\n",
      "          vf_loss: 0.08445049832678503\n",
      "    num_agent_steps_sampled: 568000\n",
      "    num_agent_steps_trained: 568000\n",
      "    num_steps_sampled: 568000\n",
      "    num_steps_trained: 568000\n",
      "  iterations_since_restore: 568\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 87.12352941176471\n",
      "    ram_util_percent: 47.65588235294118\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04028900939353885\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.62889371113065\n",
      "    mean_inference_ms: 1.7560063726988406\n",
      "    mean_raw_obs_processing_ms: 1.8714367918885175\n",
      "  time_since_restore: 16580.512429714203\n",
      "  time_this_iter_s: 24.38625168800354\n",
      "  time_total_s: 16580.512429714203\n",
      "  timers:\n",
      "    learn_throughput: 1129.283\n",
      "    learn_time_ms: 885.518\n",
      "    load_throughput: 49804.123\n",
      "    load_time_ms: 20.079\n",
      "    sample_throughput: 40.786\n",
      "    sample_time_ms: 24518.15\n",
      "    update_time_ms: 4.898\n",
      "  timestamp: 1634859812\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 568000\n",
      "  training_iteration: 568\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   568</td><td style=\"text-align: right;\">         16580.5</td><td style=\"text-align: right;\">568000</td><td style=\"text-align: right;\"> -3.5328</td><td style=\"text-align: right;\">                2.42</td><td style=\"text-align: right;\">              -12.41</td><td style=\"text-align: right;\">            360.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 569000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-44-01\n",
      "  done: false\n",
      "  episode_len_mean: 358.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.4200000000000816\n",
      "  episode_reward_mean: -3.4761999999999706\n",
      "  episode_reward_min: -12.409999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1644\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.40601523982154\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012367712990936102\n",
      "          policy_loss: -0.06236137329704232\n",
      "          total_loss: 0.05689941292835606\n",
      "          vf_explained_var: 0.920965313911438\n",
      "          vf_loss: 0.11939390634910928\n",
      "    num_agent_steps_sampled: 569000\n",
      "    num_agent_steps_trained: 569000\n",
      "    num_steps_sampled: 569000\n",
      "    num_steps_trained: 569000\n",
      "  iterations_since_restore: 569\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.73414634146341\n",
      "    ram_util_percent: 47.58292682926829\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04029115638311296\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.629437675826498\n",
      "    mean_inference_ms: 1.7560673235109718\n",
      "    mean_raw_obs_processing_ms: 1.871031349161698\n",
      "  time_since_restore: 16608.796347379684\n",
      "  time_this_iter_s: 28.283917665481567\n",
      "  time_total_s: 16608.796347379684\n",
      "  timers:\n",
      "    learn_throughput: 1134.279\n",
      "    learn_time_ms: 881.617\n",
      "    load_throughput: 52250.105\n",
      "    load_time_ms: 19.139\n",
      "    sample_throughput: 43.507\n",
      "    sample_time_ms: 22984.967\n",
      "    update_time_ms: 4.935\n",
      "  timestamp: 1634859841\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 569000\n",
      "  training_iteration: 569\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   569</td><td style=\"text-align: right;\">         16608.8</td><td style=\"text-align: right;\">569000</td><td style=\"text-align: right;\"> -3.4762</td><td style=\"text-align: right;\">                2.42</td><td style=\"text-align: right;\">              -12.41</td><td style=\"text-align: right;\">            358.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 570000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-44-26\n",
      "  done: false\n",
      "  episode_len_mean: 363.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.4200000000000816\n",
      "  episode_reward_mean: -3.4597999999999702\n",
      "  episode_reward_min: -12.409999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1647\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9209901849428812\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014939529396114122\n",
      "          policy_loss: 0.007129642739892006\n",
      "          total_loss: 0.21113756553580362\n",
      "          vf_explained_var: 0.9171961545944214\n",
      "          vf_loss: 0.2063947202430831\n",
      "    num_agent_steps_sampled: 570000\n",
      "    num_agent_steps_trained: 570000\n",
      "    num_steps_sampled: 570000\n",
      "    num_steps_trained: 570000\n",
      "  iterations_since_restore: 570\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.14166666666668\n",
      "    ram_util_percent: 47.730555555555554\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04029331880200915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.629612851589055\n",
      "    mean_inference_ms: 1.756127335796526\n",
      "    mean_raw_obs_processing_ms: 1.8706639379479726\n",
      "  time_since_restore: 16634.45877289772\n",
      "  time_this_iter_s: 25.66242551803589\n",
      "  time_total_s: 16634.45877289772\n",
      "  timers:\n",
      "    learn_throughput: 1131.546\n",
      "    learn_time_ms: 883.747\n",
      "    load_throughput: 51645.658\n",
      "    load_time_ms: 19.363\n",
      "    sample_throughput: 42.879\n",
      "    sample_time_ms: 23321.214\n",
      "    update_time_ms: 5.668\n",
      "  timestamp: 1634859866\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 570000\n",
      "  training_iteration: 570\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   570</td><td style=\"text-align: right;\">         16634.5</td><td style=\"text-align: right;\">570000</td><td style=\"text-align: right;\"> -3.4598</td><td style=\"text-align: right;\">                2.42</td><td style=\"text-align: right;\">              -12.41</td><td style=\"text-align: right;\">            363.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 571000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-44-53\n",
      "  done: false\n",
      "  episode_len_mean: 365.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.4200000000000816\n",
      "  episode_reward_mean: -3.46359999999997\n",
      "  episode_reward_min: -12.409999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1649\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.973683269818624\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01240626912580096\n",
      "          policy_loss: -0.1541039394835631\n",
      "          total_loss: -0.0653945172826449\n",
      "          vf_explained_var: 0.921789824962616\n",
      "          vf_loss: 0.09447580441418621\n",
      "    num_agent_steps_sampled: 571000\n",
      "    num_agent_steps_trained: 571000\n",
      "    num_steps_sampled: 571000\n",
      "    num_steps_trained: 571000\n",
      "  iterations_since_restore: 571\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.8078947368421\n",
      "    ram_util_percent: 47.77368421052631\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04029475110718243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.629547108042587\n",
      "    mean_inference_ms: 1.7561677925201713\n",
      "    mean_raw_obs_processing_ms: 1.8703913577564242\n",
      "  time_since_restore: 16661.03869533539\n",
      "  time_this_iter_s: 26.579922437667847\n",
      "  time_total_s: 16661.03869533539\n",
      "  timers:\n",
      "    learn_throughput: 1115.181\n",
      "    learn_time_ms: 896.715\n",
      "    load_throughput: 53398.789\n",
      "    load_time_ms: 18.727\n",
      "    sample_throughput: 41.854\n",
      "    sample_time_ms: 23892.766\n",
      "    update_time_ms: 6.111\n",
      "  timestamp: 1634859893\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 571000\n",
      "  training_iteration: 571\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   571</td><td style=\"text-align: right;\">           16661</td><td style=\"text-align: right;\">571000</td><td style=\"text-align: right;\"> -3.4636</td><td style=\"text-align: right;\">                2.42</td><td style=\"text-align: right;\">              -12.41</td><td style=\"text-align: right;\">            365.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 572000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-45-36\n",
      "  done: false\n",
      "  episode_len_mean: 369.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.4200000000000816\n",
      "  episode_reward_mean: -3.5015999999999696\n",
      "  episode_reward_min: -12.409999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1652\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9602665225664775\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016158677558706338\n",
      "          policy_loss: -0.10555156231340435\n",
      "          total_loss: 0.0567410534247756\n",
      "          vf_explained_var: 0.8650498390197754\n",
      "          vf_loss: 0.16369931374986965\n",
      "    num_agent_steps_sampled: 572000\n",
      "    num_agent_steps_trained: 572000\n",
      "    num_steps_sampled: 572000\n",
      "    num_steps_trained: 572000\n",
      "  iterations_since_restore: 572\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.21290322580644\n",
      "    ram_util_percent: 47.619354838709675\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04029691937693045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.629116358386177\n",
      "    mean_inference_ms: 1.7562318328171571\n",
      "    mean_raw_obs_processing_ms: 1.8708861294380492\n",
      "  time_since_restore: 16704.503866434097\n",
      "  time_this_iter_s: 43.465171098709106\n",
      "  time_total_s: 16704.503866434097\n",
      "  timers:\n",
      "    learn_throughput: 1110.205\n",
      "    learn_time_ms: 900.735\n",
      "    load_throughput: 50884.272\n",
      "    load_time_ms: 19.652\n",
      "    sample_throughput: 38.781\n",
      "    sample_time_ms: 25785.954\n",
      "    update_time_ms: 5.959\n",
      "  timestamp: 1634859936\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 572000\n",
      "  training_iteration: 572\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   572</td><td style=\"text-align: right;\">         16704.5</td><td style=\"text-align: right;\">572000</td><td style=\"text-align: right;\"> -3.5016</td><td style=\"text-align: right;\">                2.42</td><td style=\"text-align: right;\">              -12.41</td><td style=\"text-align: right;\">            369.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 573000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-46-00\n",
      "  done: false\n",
      "  episode_len_mean: 371.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.4200000000000816\n",
      "  episode_reward_mean: -3.6031999999999687\n",
      "  episode_reward_min: -12.409999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1654\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.058903691503737\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012385860426667743\n",
      "          policy_loss: -0.06715631203518974\n",
      "          total_loss: 0.01387577603260676\n",
      "          vf_explained_var: 0.8215344548225403\n",
      "          vf_loss: 0.08767365552484989\n",
      "    num_agent_steps_sampled: 573000\n",
      "    num_agent_steps_trained: 573000\n",
      "    num_steps_sampled: 573000\n",
      "    num_steps_trained: 573000\n",
      "  iterations_since_restore: 573\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.69428571428573\n",
      "    ram_util_percent: 47.35428571428571\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04029834108066676\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.628564290970626\n",
      "    mean_inference_ms: 1.7562732546659918\n",
      "    mean_raw_obs_processing_ms: 1.8712177972513484\n",
      "  time_since_restore: 16728.52534532547\n",
      "  time_this_iter_s: 24.02147889137268\n",
      "  time_total_s: 16728.52534532547\n",
      "  timers:\n",
      "    learn_throughput: 1108.735\n",
      "    learn_time_ms: 901.929\n",
      "    load_throughput: 53712.117\n",
      "    load_time_ms: 18.618\n",
      "    sample_throughput: 38.513\n",
      "    sample_time_ms: 25965.135\n",
      "    update_time_ms: 6.007\n",
      "  timestamp: 1634859960\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 573000\n",
      "  training_iteration: 573\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   573</td><td style=\"text-align: right;\">         16728.5</td><td style=\"text-align: right;\">573000</td><td style=\"text-align: right;\"> -3.6032</td><td style=\"text-align: right;\">                2.42</td><td style=\"text-align: right;\">              -12.41</td><td style=\"text-align: right;\">            371.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 574000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-46-40\n",
      "  done: false\n",
      "  episode_len_mean: 372.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.65\n",
      "  episode_reward_mean: -3.4587999999999695\n",
      "  episode_reward_min: -12.409999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1657\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0799255741967095\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01837839768270779\n",
      "          policy_loss: -0.0960179107884566\n",
      "          total_loss: 0.34868683223095204\n",
      "          vf_explained_var: 0.7647917866706848\n",
      "          vf_loss: 0.44480845352841747\n",
      "    num_agent_steps_sampled: 574000\n",
      "    num_agent_steps_trained: 574000\n",
      "    num_steps_sampled: 574000\n",
      "    num_steps_trained: 574000\n",
      "  iterations_since_restore: 574\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.52678571428571\n",
      "    ram_util_percent: 47.30178571428571\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040300430800756404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.62722156971178\n",
      "    mean_inference_ms: 1.7563339976849142\n",
      "    mean_raw_obs_processing_ms: 1.8725757687591291\n",
      "  time_since_restore: 16767.885855674744\n",
      "  time_this_iter_s: 39.36051034927368\n",
      "  time_total_s: 16767.885855674744\n",
      "  timers:\n",
      "    learn_throughput: 1111.758\n",
      "    learn_time_ms: 899.477\n",
      "    load_throughput: 51536.953\n",
      "    load_time_ms: 19.404\n",
      "    sample_throughput: 36.15\n",
      "    sample_time_ms: 27662.278\n",
      "    update_time_ms: 6.205\n",
      "  timestamp: 1634860000\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 574000\n",
      "  training_iteration: 574\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   574</td><td style=\"text-align: right;\">         16767.9</td><td style=\"text-align: right;\">574000</td><td style=\"text-align: right;\"> -3.4588</td><td style=\"text-align: right;\">                9.65</td><td style=\"text-align: right;\">              -12.41</td><td style=\"text-align: right;\">            372.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 575000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-47-37\n",
      "  done: false\n",
      "  episode_len_mean: 372.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.670000000000002\n",
      "  episode_reward_mean: -3.3458999999999683\n",
      "  episode_reward_min: -12.409999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1660\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9905449403656854\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016643890716867886\n",
      "          policy_loss: -0.11704133989082442\n",
      "          total_loss: 0.3843447715457943\n",
      "          vf_explained_var: 0.7928651571273804\n",
      "          vf_loss: 0.5025492089490096\n",
      "    num_agent_steps_sampled: 575000\n",
      "    num_agent_steps_trained: 575000\n",
      "    num_steps_sampled: 575000\n",
      "    num_steps_trained: 575000\n",
      "  iterations_since_restore: 575\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.60853658536585\n",
      "    ram_util_percent: 47.42317073170731\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040302435105205\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.625377188480126\n",
      "    mean_inference_ms: 1.756394312468907\n",
      "    mean_raw_obs_processing_ms: 1.8748752139260327\n",
      "  time_since_restore: 16825.189126491547\n",
      "  time_this_iter_s: 57.30327081680298\n",
      "  time_total_s: 16825.189126491547\n",
      "  timers:\n",
      "    learn_throughput: 1110.216\n",
      "    learn_time_ms: 900.725\n",
      "    load_throughput: 49191.334\n",
      "    load_time_ms: 20.329\n",
      "    sample_throughput: 32.517\n",
      "    sample_time_ms: 30753.354\n",
      "    update_time_ms: 6.516\n",
      "  timestamp: 1634860057\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 575000\n",
      "  training_iteration: 575\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   575</td><td style=\"text-align: right;\">         16825.2</td><td style=\"text-align: right;\">575000</td><td style=\"text-align: right;\"> -3.3459</td><td style=\"text-align: right;\">                9.67</td><td style=\"text-align: right;\">              -12.41</td><td style=\"text-align: right;\">            372.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 576000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-48-00\n",
      "  done: false\n",
      "  episode_len_mean: 377.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.670000000000002\n",
      "  episode_reward_mean: -3.347799999999968\n",
      "  episode_reward_min: -12.409999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1662\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9281410257021585\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018871148904435527\n",
      "          policy_loss: -0.021392945448557535\n",
      "          total_loss: 0.23471744805574418\n",
      "          vf_explained_var: 0.8410070538520813\n",
      "          vf_loss: 0.2541413828316662\n",
      "    num_agent_steps_sampled: 576000\n",
      "    num_agent_steps_trained: 576000\n",
      "    num_steps_sampled: 576000\n",
      "    num_steps_trained: 576000\n",
      "  iterations_since_restore: 576\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.7125\n",
      "    ram_util_percent: 47.375\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04030373971568482\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.623851264112815\n",
      "    mean_inference_ms: 1.7564339843142391\n",
      "    mean_raw_obs_processing_ms: 1.876157066317734\n",
      "  time_since_restore: 16847.621282100677\n",
      "  time_this_iter_s: 22.43215560913086\n",
      "  time_total_s: 16847.621282100677\n",
      "  timers:\n",
      "    learn_throughput: 1164.088\n",
      "    learn_time_ms: 859.042\n",
      "    load_throughput: 49178.933\n",
      "    load_time_ms: 20.334\n",
      "    sample_throughput: 32.562\n",
      "    sample_time_ms: 30711.003\n",
      "    update_time_ms: 5.831\n",
      "  timestamp: 1634860080\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 576000\n",
      "  training_iteration: 576\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   576</td><td style=\"text-align: right;\">         16847.6</td><td style=\"text-align: right;\">576000</td><td style=\"text-align: right;\"> -3.3478</td><td style=\"text-align: right;\">                9.67</td><td style=\"text-align: right;\">              -12.41</td><td style=\"text-align: right;\">            377.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 577000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-48-28\n",
      "  done: false\n",
      "  episode_len_mean: 377.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.670000000000002\n",
      "  episode_reward_mean: -3.4302999999999684\n",
      "  episode_reward_min: -12.409999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1665\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.1260800895232614\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8973881522814433\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.023985590158510896\n",
      "          policy_loss: -0.03145864920483695\n",
      "          total_loss: 0.38087465597523584\n",
      "          vf_explained_var: 0.8320890665054321\n",
      "          vf_loss: 0.40429749745461674\n",
      "    num_agent_steps_sampled: 577000\n",
      "    num_agent_steps_trained: 577000\n",
      "    num_steps_sampled: 577000\n",
      "    num_steps_trained: 577000\n",
      "  iterations_since_restore: 577\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.665\n",
      "    ram_util_percent: 47.152499999999996\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04030565739454712\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.621742244534133\n",
      "    mean_inference_ms: 1.7564906233961972\n",
      "    mean_raw_obs_processing_ms: 1.8781159157426308\n",
      "  time_since_restore: 16875.907257080078\n",
      "  time_this_iter_s: 28.285974979400635\n",
      "  time_total_s: 16875.907257080078\n",
      "  timers:\n",
      "    learn_throughput: 1164.047\n",
      "    learn_time_ms: 859.072\n",
      "    load_throughput: 50861.503\n",
      "    load_time_ms: 19.661\n",
      "    sample_throughput: 32.166\n",
      "    sample_time_ms: 31088.875\n",
      "    update_time_ms: 5.718\n",
      "  timestamp: 1634860108\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 577000\n",
      "  training_iteration: 577\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   577</td><td style=\"text-align: right;\">         16875.9</td><td style=\"text-align: right;\">577000</td><td style=\"text-align: right;\"> -3.4303</td><td style=\"text-align: right;\">                9.67</td><td style=\"text-align: right;\">              -12.41</td><td style=\"text-align: right;\">            377.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 578000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-48-58\n",
      "  done: false\n",
      "  episode_len_mean: 377.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.670000000000002\n",
      "  episode_reward_mean: -3.3766999999999694\n",
      "  episode_reward_min: -12.409999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1668\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8736491468217638\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014402963603148874\n",
      "          policy_loss: -0.056122853027449716\n",
      "          total_loss: 0.32117916515303985\n",
      "          vf_explained_var: 0.7873299717903137\n",
      "          vf_loss: 0.3717101706398858\n",
      "    num_agent_steps_sampled: 578000\n",
      "    num_agent_steps_trained: 578000\n",
      "    num_steps_sampled: 578000\n",
      "    num_steps_trained: 578000\n",
      "  iterations_since_restore: 578\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.83809523809524\n",
      "    ram_util_percent: 47.09285714285714\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04030750319526801\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.6197745540057\n",
      "    mean_inference_ms: 1.7565440566713675\n",
      "    mean_raw_obs_processing_ms: 1.880079661232957\n",
      "  time_since_restore: 16905.619226694107\n",
      "  time_this_iter_s: 29.71196961402893\n",
      "  time_total_s: 16905.619226694107\n",
      "  timers:\n",
      "    learn_throughput: 1165.312\n",
      "    learn_time_ms: 858.139\n",
      "    load_throughput: 49580.933\n",
      "    load_time_ms: 20.169\n",
      "    sample_throughput: 31.623\n",
      "    sample_time_ms: 31622.423\n",
      "    update_time_ms: 5.48\n",
      "  timestamp: 1634860138\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 578000\n",
      "  training_iteration: 578\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   578</td><td style=\"text-align: right;\">         16905.6</td><td style=\"text-align: right;\">578000</td><td style=\"text-align: right;\"> -3.3767</td><td style=\"text-align: right;\">                9.67</td><td style=\"text-align: right;\">              -12.41</td><td style=\"text-align: right;\">            377.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 579000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-49-25\n",
      "  done: false\n",
      "  episode_len_mean: 378.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.670000000000002\n",
      "  episode_reward_mean: -3.3982999999999697\n",
      "  episode_reward_min: -12.409999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1671\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7228122923109266\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007627710984382071\n",
      "          policy_loss: -0.06890480029914114\n",
      "          total_loss: 0.16389551088213922\n",
      "          vf_explained_var: 0.6470365524291992\n",
      "          vf_loss: 0.23714431774699027\n",
      "    num_agent_steps_sampled: 579000\n",
      "    num_agent_steps_trained: 579000\n",
      "    num_steps_sampled: 579000\n",
      "    num_steps_trained: 579000\n",
      "  iterations_since_restore: 579\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.02564102564102\n",
      "    ram_util_percent: 47.04871794871794\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040309324683991316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.617628846954826\n",
      "    mean_inference_ms: 1.7565951455582276\n",
      "    mean_raw_obs_processing_ms: 1.8820468117542657\n",
      "  time_since_restore: 16932.733124494553\n",
      "  time_this_iter_s: 27.113897800445557\n",
      "  time_total_s: 16932.733124494553\n",
      "  timers:\n",
      "    learn_throughput: 1166.26\n",
      "    learn_time_ms: 857.442\n",
      "    load_throughput: 49624.284\n",
      "    load_time_ms: 20.151\n",
      "    sample_throughput: 31.74\n",
      "    sample_time_ms: 31506.325\n",
      "    update_time_ms: 5.222\n",
      "  timestamp: 1634860165\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 579000\n",
      "  training_iteration: 579\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   579</td><td style=\"text-align: right;\">         16932.7</td><td style=\"text-align: right;\">579000</td><td style=\"text-align: right;\"> -3.3983</td><td style=\"text-align: right;\">                9.67</td><td style=\"text-align: right;\">              -12.41</td><td style=\"text-align: right;\">            378.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 580000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-49-52\n",
      "  done: false\n",
      "  episode_len_mean: 378.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.670000000000002\n",
      "  episode_reward_mean: -3.3643999999999696\n",
      "  episode_reward_min: -12.409999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1673\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.05752368900511\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007382282249748486\n",
      "          policy_loss: -0.067207932472229\n",
      "          total_loss: 0.01872993525531557\n",
      "          vf_explained_var: 0.7131083011627197\n",
      "          vf_loss: 0.09404354362438122\n",
      "    num_agent_steps_sampled: 580000\n",
      "    num_agent_steps_trained: 580000\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "  iterations_since_restore: 580\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.74615384615385\n",
      "    ram_util_percent: 46.992307692307705\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04031050784918553\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.616130923875172\n",
      "    mean_inference_ms: 1.756628100723747\n",
      "    mean_raw_obs_processing_ms: 1.8833391311105572\n",
      "  time_since_restore: 16959.96925854683\n",
      "  time_this_iter_s: 27.23613405227661\n",
      "  time_total_s: 16959.96925854683\n",
      "  timers:\n",
      "    learn_throughput: 1167.751\n",
      "    learn_time_ms: 856.347\n",
      "    load_throughput: 49086.96\n",
      "    load_time_ms: 20.372\n",
      "    sample_throughput: 31.581\n",
      "    sample_time_ms: 31664.916\n",
      "    update_time_ms: 4.424\n",
      "  timestamp: 1634860192\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 580\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   580</td><td style=\"text-align: right;\">           16960</td><td style=\"text-align: right;\">580000</td><td style=\"text-align: right;\"> -3.3644</td><td style=\"text-align: right;\">                9.67</td><td style=\"text-align: right;\">              -12.41</td><td style=\"text-align: right;\">            378.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 581000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-50-17\n",
      "  done: false\n",
      "  episode_len_mean: 383.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.670000000000002\n",
      "  episode_reward_mean: -3.471399999999968\n",
      "  episode_reward_min: -12.409999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1676\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.027350068092346\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010588596024645862\n",
      "          policy_loss: -0.056942169864972435\n",
      "          total_loss: 0.14337007225387627\n",
      "          vf_explained_var: 0.687515914440155\n",
      "          vf_loss: 0.20270033306959603\n",
      "    num_agent_steps_sampled: 581000\n",
      "    num_agent_steps_trained: 581000\n",
      "    num_steps_sampled: 581000\n",
      "    num_steps_trained: 581000\n",
      "  iterations_since_restore: 581\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.54857142857142\n",
      "    ram_util_percent: 47.05428571428571\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04031230335718943\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.61355608839425\n",
      "    mean_inference_ms: 1.756678361870104\n",
      "    mean_raw_obs_processing_ms: 1.8852781496925928\n",
      "  time_since_restore: 16984.629977464676\n",
      "  time_this_iter_s: 24.66071891784668\n",
      "  time_total_s: 16984.629977464676\n",
      "  timers:\n",
      "    learn_throughput: 1184.948\n",
      "    learn_time_ms: 843.919\n",
      "    load_throughput: 50100.564\n",
      "    load_time_ms: 19.96\n",
      "    sample_throughput: 31.76\n",
      "    sample_time_ms: 31486.423\n",
      "    update_time_ms: 3.97\n",
      "  timestamp: 1634860217\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 581000\n",
      "  training_iteration: 581\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   581</td><td style=\"text-align: right;\">         16984.6</td><td style=\"text-align: right;\">581000</td><td style=\"text-align: right;\"> -3.4714</td><td style=\"text-align: right;\">                9.67</td><td style=\"text-align: right;\">              -12.41</td><td style=\"text-align: right;\">            383.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 582000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-50-44\n",
      "  done: false\n",
      "  episode_len_mean: 383.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.670000000000002\n",
      "  episode_reward_mean: -3.4047999999999683\n",
      "  episode_reward_min: -12.409999999999926\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1678\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.888028085231781\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009577300146264456\n",
      "          policy_loss: -0.09759626388549805\n",
      "          total_loss: 0.04425827090938886\n",
      "          vf_explained_var: 0.7779462933540344\n",
      "          vf_loss: 0.14455760152389605\n",
      "    num_agent_steps_sampled: 582000\n",
      "    num_agent_steps_trained: 582000\n",
      "    num_steps_sampled: 582000\n",
      "    num_steps_trained: 582000\n",
      "  iterations_since_restore: 582\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.64102564102564\n",
      "    ram_util_percent: 47.14615384615385\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04031348733550941\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.611787591666907\n",
      "    mean_inference_ms: 1.7567118513096176\n",
      "    mean_raw_obs_processing_ms: 1.8865427687927836\n",
      "  time_since_restore: 17011.462106227875\n",
      "  time_this_iter_s: 26.832128763198853\n",
      "  time_total_s: 17011.462106227875\n",
      "  timers:\n",
      "    learn_throughput: 1186.275\n",
      "    learn_time_ms: 842.975\n",
      "    load_throughput: 51222.447\n",
      "    load_time_ms: 19.523\n",
      "    sample_throughput: 33.529\n",
      "    sample_time_ms: 29825.154\n",
      "    update_time_ms: 3.388\n",
      "  timestamp: 1634860244\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 582000\n",
      "  training_iteration: 582\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   582</td><td style=\"text-align: right;\">         17011.5</td><td style=\"text-align: right;\">582000</td><td style=\"text-align: right;\"> -3.4048</td><td style=\"text-align: right;\">                9.67</td><td style=\"text-align: right;\">              -12.41</td><td style=\"text-align: right;\">            383.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 583000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-51-12\n",
      "  done: false\n",
      "  episode_len_mean: 386.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.670000000000002\n",
      "  episode_reward_mean: -3.5147999999999673\n",
      "  episode_reward_min: -12.409999999999926\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1681\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8140712353918287\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013066908744927335\n",
      "          policy_loss: 0.012988115598758062\n",
      "          total_loss: 0.2854510011772315\n",
      "          vf_explained_var: 0.8450190424919128\n",
      "          vf_loss: 0.26853202448950875\n",
      "    num_agent_steps_sampled: 583000\n",
      "    num_agent_steps_trained: 583000\n",
      "    num_steps_sampled: 583000\n",
      "    num_steps_trained: 583000\n",
      "  iterations_since_restore: 583\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.69500000000001\n",
      "    ram_util_percent: 47.2775\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040315220481372184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.609056632513884\n",
      "    mean_inference_ms: 1.7567608298280775\n",
      "    mean_raw_obs_processing_ms: 1.8884600179141229\n",
      "  time_since_restore: 17039.517703294754\n",
      "  time_this_iter_s: 28.055597066879272\n",
      "  time_total_s: 17039.517703294754\n",
      "  timers:\n",
      "    learn_throughput: 1183.792\n",
      "    learn_time_ms: 844.743\n",
      "    load_throughput: 51103.246\n",
      "    load_time_ms: 19.568\n",
      "    sample_throughput: 33.083\n",
      "    sample_time_ms: 30227.167\n",
      "    update_time_ms: 2.963\n",
      "  timestamp: 1634860272\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 583000\n",
      "  training_iteration: 583\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   583</td><td style=\"text-align: right;\">         17039.5</td><td style=\"text-align: right;\">583000</td><td style=\"text-align: right;\"> -3.5148</td><td style=\"text-align: right;\">                9.67</td><td style=\"text-align: right;\">              -12.41</td><td style=\"text-align: right;\">            386.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 584000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-51-56\n",
      "  done: false\n",
      "  episode_len_mean: 383.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.670000000000002\n",
      "  episode_reward_mean: -3.485899999999969\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1684\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.838239942656623\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013727973308052753\n",
      "          policy_loss: -0.06479488826460308\n",
      "          total_loss: 0.3818844722997811\n",
      "          vf_explained_var: 0.848931610584259\n",
      "          vf_loss: 0.4418735542231136\n",
      "    num_agent_steps_sampled: 584000\n",
      "    num_agent_steps_trained: 584000\n",
      "    num_steps_sampled: 584000\n",
      "    num_steps_trained: 584000\n",
      "  iterations_since_restore: 584\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.10000000000004\n",
      "    ram_util_percent: 47.33650793650794\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04031687673005071\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.606304816217143\n",
      "    mean_inference_ms: 1.7568072654520999\n",
      "    mean_raw_obs_processing_ms: 1.8912402567273445\n",
      "  time_since_restore: 17083.87201499939\n",
      "  time_this_iter_s: 44.35431170463562\n",
      "  time_total_s: 17083.87201499939\n",
      "  timers:\n",
      "    learn_throughput: 1157.016\n",
      "    learn_time_ms: 864.292\n",
      "    load_throughput: 51052.737\n",
      "    load_time_ms: 19.588\n",
      "    sample_throughput: 32.566\n",
      "    sample_time_ms: 30707.259\n",
      "    update_time_ms: 2.748\n",
      "  timestamp: 1634860316\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 584000\n",
      "  training_iteration: 584\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   584</td><td style=\"text-align: right;\">         17083.9</td><td style=\"text-align: right;\">584000</td><td style=\"text-align: right;\"> -3.4859</td><td style=\"text-align: right;\">                9.67</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">            383.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 585000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-52-23\n",
      "  done: false\n",
      "  episode_len_mean: 385.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.670000000000002\n",
      "  episode_reward_mean: -3.408699999999969\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1687\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6555459645059374\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009298319569760514\n",
      "          policy_loss: -0.21215047761797906\n",
      "          total_loss: -0.06616126572092375\n",
      "          vf_explained_var: 0.9059346318244934\n",
      "          vf_loss: 0.1468386895954609\n",
      "    num_agent_steps_sampled: 585000\n",
      "    num_agent_steps_trained: 585000\n",
      "    num_steps_sampled: 585000\n",
      "    num_steps_trained: 585000\n",
      "  iterations_since_restore: 585\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.67948717948718\n",
      "    ram_util_percent: 47.466666666666676\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040318458208771286\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.603489595233476\n",
      "    mean_inference_ms: 1.7568509886300747\n",
      "    mean_raw_obs_processing_ms: 1.8940221883277315\n",
      "  time_since_restore: 17111.012755155563\n",
      "  time_this_iter_s: 27.140740156173706\n",
      "  time_total_s: 17111.012755155563\n",
      "  timers:\n",
      "    learn_throughput: 1163.7\n",
      "    learn_time_ms: 859.328\n",
      "    load_throughput: 52434.24\n",
      "    load_time_ms: 19.072\n",
      "    sample_throughput: 36.105\n",
      "    sample_time_ms: 27696.993\n",
      "    update_time_ms: 2.354\n",
      "  timestamp: 1634860343\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 585000\n",
      "  training_iteration: 585\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   585</td><td style=\"text-align: right;\">           17111</td><td style=\"text-align: right;\">585000</td><td style=\"text-align: right;\"> -3.4087</td><td style=\"text-align: right;\">                9.67</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">            385.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 586000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-52-51\n",
      "  done: false\n",
      "  episode_len_mean: 386.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.670000000000002\n",
      "  episode_reward_mean: -3.416799999999968\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1689\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.82020200226042\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007841552328590002\n",
      "          policy_loss: -0.03608209060298072\n",
      "          total_loss: 0.03490118324342701\n",
      "          vf_explained_var: 0.8846591711044312\n",
      "          vf_loss: 0.07593996957358387\n",
      "    num_agent_steps_sampled: 586000\n",
      "    num_agent_steps_trained: 586000\n",
      "    num_steps_sampled: 586000\n",
      "    num_steps_trained: 586000\n",
      "  iterations_since_restore: 586\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.93076923076923\n",
      "    ram_util_percent: 47.541025641025634\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04031946863781278\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.601534152205048\n",
      "    mean_inference_ms: 1.7568778782909078\n",
      "    mean_raw_obs_processing_ms: 1.8955577713827183\n",
      "  time_since_restore: 17138.425740242004\n",
      "  time_this_iter_s: 27.41298508644104\n",
      "  time_total_s: 17138.425740242004\n",
      "  timers:\n",
      "    learn_throughput: 1164.595\n",
      "    learn_time_ms: 858.667\n",
      "    load_throughput: 52399.653\n",
      "    load_time_ms: 19.084\n",
      "    sample_throughput: 35.466\n",
      "    sample_time_ms: 28195.685\n",
      "    update_time_ms: 2.305\n",
      "  timestamp: 1634860371\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 586000\n",
      "  training_iteration: 586\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   586</td><td style=\"text-align: right;\">         17138.4</td><td style=\"text-align: right;\">586000</td><td style=\"text-align: right;\"> -3.4168</td><td style=\"text-align: right;\">                9.67</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">            386.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 587000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-53-17\n",
      "  done: false\n",
      "  episode_len_mean: 385.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.670000000000002\n",
      "  episode_reward_mean: -3.313999999999968\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1692\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6778541101349724\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010822602210725308\n",
      "          policy_loss: -0.06334996215171285\n",
      "          total_loss: 0.033148831894828215\n",
      "          vf_explained_var: 0.8844096064567566\n",
      "          vf_loss: 0.09499665964394807\n",
      "    num_agent_steps_sampled: 587000\n",
      "    num_agent_steps_trained: 587000\n",
      "    num_steps_sampled: 587000\n",
      "    num_steps_trained: 587000\n",
      "  iterations_since_restore: 587\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.2421052631579\n",
      "    ram_util_percent: 47.515789473684215\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040320915847423346\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.59848154141614\n",
      "    mean_inference_ms: 1.7569124639132476\n",
      "    mean_raw_obs_processing_ms: 1.8973850128528067\n",
      "  time_since_restore: 17165.03371667862\n",
      "  time_this_iter_s: 26.60797643661499\n",
      "  time_total_s: 17165.03371667862\n",
      "  timers:\n",
      "    learn_throughput: 1168.389\n",
      "    learn_time_ms: 855.879\n",
      "    load_throughput: 51434.633\n",
      "    load_time_ms: 19.442\n",
      "    sample_throughput: 35.676\n",
      "    sample_time_ms: 28030.431\n",
      "    update_time_ms: 2.218\n",
      "  timestamp: 1634860397\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 587000\n",
      "  training_iteration: 587\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   587</td><td style=\"text-align: right;\">           17165</td><td style=\"text-align: right;\">587000</td><td style=\"text-align: right;\">  -3.314</td><td style=\"text-align: right;\">                9.67</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">            385.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 588000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-54-37\n",
      "  done: false\n",
      "  episode_len_mean: 383.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.670000000000002\n",
      "  episode_reward_mean: -3.048199999999967\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1696\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.642313876416948\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006504098428556166\n",
      "          policy_loss: 0.03508731596585777\n",
      "          total_loss: 0.11054300557201108\n",
      "          vf_explained_var: 0.9056372046470642\n",
      "          vf_loss: 0.08089262530621555\n",
      "    num_agent_steps_sampled: 588000\n",
      "    num_agent_steps_trained: 588000\n",
      "    num_steps_sampled: 588000\n",
      "    num_steps_trained: 588000\n",
      "  iterations_since_restore: 588\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.70884955752211\n",
      "    ram_util_percent: 47.33274336283185\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04032254875207601\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.593857476433428\n",
      "    mean_inference_ms: 1.7569506570701656\n",
      "    mean_raw_obs_processing_ms: 1.9038178379385016\n",
      "  time_since_restore: 17244.543803453445\n",
      "  time_this_iter_s: 79.51008677482605\n",
      "  time_total_s: 17244.543803453445\n",
      "  timers:\n",
      "    learn_throughput: 1163.44\n",
      "    learn_time_ms: 859.52\n",
      "    load_throughput: 53949.71\n",
      "    load_time_ms: 18.536\n",
      "    sample_throughput: 30.296\n",
      "    sample_time_ms: 33007.506\n",
      "    update_time_ms: 2.213\n",
      "  timestamp: 1634860477\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 588000\n",
      "  training_iteration: 588\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   588</td><td style=\"text-align: right;\">         17244.5</td><td style=\"text-align: right;\">588000</td><td style=\"text-align: right;\"> -3.0482</td><td style=\"text-align: right;\">                9.67</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">            383.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 589000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-55-04\n",
      "  done: false\n",
      "  episode_len_mean: 384.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.670000000000002\n",
      "  episode_reward_mean: -3.105699999999967\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1698\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8806994888517592\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012597262414588743\n",
      "          policy_loss: -0.03693060808711582\n",
      "          total_loss: 0.11961686097913318\n",
      "          vf_explained_var: 0.8393374681472778\n",
      "          vf_loss: 0.15407617112828625\n",
      "    num_agent_steps_sampled: 589000\n",
      "    num_agent_steps_trained: 589000\n",
      "    num_steps_sampled: 589000\n",
      "    num_steps_trained: 589000\n",
      "  iterations_since_restore: 589\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.14358974358974\n",
      "    ram_util_percent: 47.469230769230776\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040323371200811954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.591376877023514\n",
      "    mean_inference_ms: 1.7569699252153028\n",
      "    mean_raw_obs_processing_ms: 1.9070314345629842\n",
      "  time_since_restore: 17271.39974784851\n",
      "  time_this_iter_s: 26.855944395065308\n",
      "  time_total_s: 17271.39974784851\n",
      "  timers:\n",
      "    learn_throughput: 1160.017\n",
      "    learn_time_ms: 862.056\n",
      "    load_throughput: 53854.117\n",
      "    load_time_ms: 18.569\n",
      "    sample_throughput: 30.322\n",
      "    sample_time_ms: 32979.088\n",
      "    update_time_ms: 2.278\n",
      "  timestamp: 1634860504\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 589000\n",
      "  training_iteration: 589\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   589</td><td style=\"text-align: right;\">         17271.4</td><td style=\"text-align: right;\">589000</td><td style=\"text-align: right;\"> -3.1057</td><td style=\"text-align: right;\">                9.67</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">            384.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 590000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-55-25\n",
      "  done: false\n",
      "  episode_len_mean: 387.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.670000000000002\n",
      "  episode_reward_mean: -3.052399999999967\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1701\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7018996781773037\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006889194976957233\n",
      "          policy_loss: -0.07015501877499951\n",
      "          total_loss: 0.013856160009486808\n",
      "          vf_explained_var: 0.8859594464302063\n",
      "          vf_loss: 0.0893934979957218\n",
      "    num_agent_steps_sampled: 590000\n",
      "    num_agent_steps_trained: 590000\n",
      "    num_steps_sampled: 590000\n",
      "    num_steps_trained: 590000\n",
      "  iterations_since_restore: 590\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.71333333333334\n",
      "    ram_util_percent: 47.49666666666667\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0403244470506511\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.587280947812197\n",
      "    mean_inference_ms: 1.7569979004914813\n",
      "    mean_raw_obs_processing_ms: 1.9118370776817883\n",
      "  time_since_restore: 17292.847553491592\n",
      "  time_this_iter_s: 21.447805643081665\n",
      "  time_total_s: 17292.847553491592\n",
      "  timers:\n",
      "    learn_throughput: 1157.666\n",
      "    learn_time_ms: 863.807\n",
      "    load_throughput: 53627.303\n",
      "    load_time_ms: 18.647\n",
      "    sample_throughput: 30.865\n",
      "    sample_time_ms: 32398.921\n",
      "    update_time_ms: 2.243\n",
      "  timestamp: 1634860525\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 590000\n",
      "  training_iteration: 590\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   590</td><td style=\"text-align: right;\">         17292.8</td><td style=\"text-align: right;\">590000</td><td style=\"text-align: right;\"> -3.0524</td><td style=\"text-align: right;\">                9.67</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">            387.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 591000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-56-29\n",
      "  done: false\n",
      "  episode_len_mean: 384.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.83\n",
      "  episode_reward_mean: -2.8415999999999677\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1704\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7582982301712036\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009972786801335884\n",
      "          policy_loss: -0.058675410225987434\n",
      "          total_loss: 0.275490844849911\n",
      "          vf_explained_var: 0.7944864630699158\n",
      "          vf_loss: 0.33490399834182527\n",
      "    num_agent_steps_sampled: 591000\n",
      "    num_agent_steps_trained: 591000\n",
      "    num_steps_sampled: 591000\n",
      "    num_steps_trained: 591000\n",
      "  iterations_since_restore: 591\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.84835164835165\n",
      "    ram_util_percent: 47.362637362637365\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04032530667874168\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.583195116153558\n",
      "    mean_inference_ms: 1.7570231715232212\n",
      "    mean_raw_obs_processing_ms: 1.9186166328281218\n",
      "  time_since_restore: 17356.287400007248\n",
      "  time_this_iter_s: 63.43984651565552\n",
      "  time_total_s: 17356.287400007248\n",
      "  timers:\n",
      "    learn_throughput: 1156.167\n",
      "    learn_time_ms: 864.927\n",
      "    load_throughput: 53862.07\n",
      "    load_time_ms: 18.566\n",
      "    sample_throughput: 27.567\n",
      "    sample_time_ms: 36274.751\n",
      "    update_time_ms: 2.643\n",
      "  timestamp: 1634860589\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 591000\n",
      "  training_iteration: 591\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   591</td><td style=\"text-align: right;\">         17356.3</td><td style=\"text-align: right;\">591000</td><td style=\"text-align: right;\"> -2.8416</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">            384.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 592000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-57-31\n",
      "  done: false\n",
      "  episode_len_mean: 383.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.83\n",
      "  episode_reward_mean: -2.657999999999969\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1707\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6576776994599236\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00826235905748912\n",
      "          policy_loss: 0.02699641612254911\n",
      "          total_loss: 0.13291273098438977\n",
      "          vf_explained_var: 0.8639044165611267\n",
      "          vf_loss: 0.10853697351283498\n",
      "    num_agent_steps_sampled: 592000\n",
      "    num_agent_steps_trained: 592000\n",
      "    num_steps_sampled: 592000\n",
      "    num_steps_trained: 592000\n",
      "  iterations_since_restore: 592\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.88539325842697\n",
      "    ram_util_percent: 47.52022471910112\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04032616729263783\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.578979321270822\n",
      "    mean_inference_ms: 1.7570471392176878\n",
      "    mean_raw_obs_processing_ms: 1.927318871876989\n",
      "  time_since_restore: 17418.55947113037\n",
      "  time_this_iter_s: 62.27207112312317\n",
      "  time_total_s: 17418.55947113037\n",
      "  timers:\n",
      "    learn_throughput: 1163.439\n",
      "    learn_time_ms: 859.521\n",
      "    load_throughput: 55513.623\n",
      "    load_time_ms: 18.014\n",
      "    sample_throughput: 25.11\n",
      "    sample_time_ms: 39824.858\n",
      "    update_time_ms: 2.565\n",
      "  timestamp: 1634860651\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 592000\n",
      "  training_iteration: 592\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   592</td><td style=\"text-align: right;\">         17418.6</td><td style=\"text-align: right;\">592000</td><td style=\"text-align: right;\">  -2.658</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">            383.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 593000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-58-46\n",
      "  done: false\n",
      "  episode_len_mean: 376.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.83\n",
      "  episode_reward_mean: -2.548899999999969\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1711\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.878729768594106\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018173188031921604\n",
      "          policy_loss: -0.08214920692973667\n",
      "          total_loss: 0.34011484020286137\n",
      "          vf_explained_var: 0.7831286787986755\n",
      "          vf_loss: 0.41035464868570365\n",
      "    num_agent_steps_sampled: 593000\n",
      "    num_agent_steps_trained: 593000\n",
      "    num_steps_sampled: 593000\n",
      "    num_steps_trained: 593000\n",
      "  iterations_since_restore: 593\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.69814814814815\n",
      "    ram_util_percent: 47.4861111111111\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04032721003744319\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.573527471281476\n",
      "    mean_inference_ms: 1.7570764336538096\n",
      "    mean_raw_obs_processing_ms: 1.9421245502090638\n",
      "  time_since_restore: 17494.041199684143\n",
      "  time_this_iter_s: 75.48172855377197\n",
      "  time_total_s: 17494.041199684143\n",
      "  timers:\n",
      "    learn_throughput: 1171.04\n",
      "    learn_time_ms: 853.942\n",
      "    load_throughput: 54838.543\n",
      "    load_time_ms: 18.235\n",
      "    sample_throughput: 22.435\n",
      "    sample_time_ms: 44572.867\n",
      "    update_time_ms: 2.577\n",
      "  timestamp: 1634860726\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 593000\n",
      "  training_iteration: 593\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   593</td><td style=\"text-align: right;\">           17494</td><td style=\"text-align: right;\">593000</td><td style=\"text-align: right;\"> -2.5489</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">            376.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 594000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-59-11\n",
      "  done: false\n",
      "  episode_len_mean: 376.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.83\n",
      "  episode_reward_mean: -2.495899999999969\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1713\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.907365526093377\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012881259155823936\n",
      "          policy_loss: -0.05761626466280884\n",
      "          total_loss: 0.1782267494748036\n",
      "          vf_explained_var: 0.8082355260848999\n",
      "          vf_loss: 0.2331586761607064\n",
      "    num_agent_steps_sampled: 594000\n",
      "    num_agent_steps_trained: 594000\n",
      "    num_steps_sampled: 594000\n",
      "    num_steps_trained: 594000\n",
      "  iterations_since_restore: 594\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.46\n",
      "    ram_util_percent: 47.45142857142858\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04032766981146402\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.57077745877695\n",
      "    mean_inference_ms: 1.7570901286750424\n",
      "    mean_raw_obs_processing_ms: 1.9495170092755572\n",
      "  time_since_restore: 17518.917749881744\n",
      "  time_this_iter_s: 24.87655019760132\n",
      "  time_total_s: 17518.917749881744\n",
      "  timers:\n",
      "    learn_throughput: 1197.149\n",
      "    learn_time_ms: 835.318\n",
      "    load_throughput: 57867.395\n",
      "    load_time_ms: 17.281\n",
      "    sample_throughput: 23.45\n",
      "    sample_time_ms: 42644.576\n",
      "    update_time_ms: 2.661\n",
      "  timestamp: 1634860751\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 594000\n",
      "  training_iteration: 594\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   594</td><td style=\"text-align: right;\">         17518.9</td><td style=\"text-align: right;\">594000</td><td style=\"text-align: right;\"> -2.4959</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">            376.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 595000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-21_23-59-35\n",
      "  done: false\n",
      "  episode_len_mean: 378.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.83\n",
      "  episode_reward_mean: -2.43169999999997\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1716\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6263914969232347\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011837244831106552\n",
      "          policy_loss: -0.043834859629472096\n",
      "          total_loss: 0.170338209428721\n",
      "          vf_explained_var: 0.8297490477561951\n",
      "          vf_loss: 0.2104424587968323\n",
      "    num_agent_steps_sampled: 595000\n",
      "    num_agent_steps_trained: 595000\n",
      "    num_steps_sampled: 595000\n",
      "    num_steps_trained: 595000\n",
      "  iterations_since_restore: 595\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.42647058823528\n",
      "    ram_util_percent: 47.300000000000004\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040328197147391595\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.56638120077404\n",
      "    mean_inference_ms: 1.757105635918392\n",
      "    mean_raw_obs_processing_ms: 1.9606223025068834\n",
      "  time_since_restore: 17542.439650774002\n",
      "  time_this_iter_s: 23.52190089225769\n",
      "  time_total_s: 17542.439650774002\n",
      "  timers:\n",
      "    learn_throughput: 1190.088\n",
      "    learn_time_ms: 840.274\n",
      "    load_throughput: 59417.064\n",
      "    load_time_ms: 16.83\n",
      "    sample_throughput: 23.653\n",
      "    sample_time_ms: 42277.447\n",
      "    update_time_ms: 3.053\n",
      "  timestamp: 1634860775\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 595000\n",
      "  training_iteration: 595\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   595</td><td style=\"text-align: right;\">         17542.4</td><td style=\"text-align: right;\">595000</td><td style=\"text-align: right;\"> -2.4317</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">            378.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 596000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_00-01-05\n",
      "  done: false\n",
      "  episode_len_mean: 370.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.83\n",
      "  episode_reward_mean: -1.8957999999999697\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1720\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6871310353279114\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013603514012834545\n",
      "          policy_loss: -0.10062694748242697\n",
      "          total_loss: 0.25633223172691133\n",
      "          vf_explained_var: 0.8502057194709778\n",
      "          vf_loss: 0.3508525199360318\n",
      "    num_agent_steps_sampled: 596000\n",
      "    num_agent_steps_trained: 596000\n",
      "    num_steps_sampled: 596000\n",
      "    num_steps_trained: 596000\n",
      "  iterations_since_restore: 596\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.45736434108527\n",
      "    ram_util_percent: 47.33643410852713\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04032880473302216\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.560070893469348\n",
      "    mean_inference_ms: 1.7571250439022057\n",
      "    mean_raw_obs_processing_ms: 1.9794554325949947\n",
      "  time_since_restore: 17632.802837133408\n",
      "  time_this_iter_s: 90.36318635940552\n",
      "  time_total_s: 17632.802837133408\n",
      "  timers:\n",
      "    learn_throughput: 1187.628\n",
      "    learn_time_ms: 842.015\n",
      "    load_throughput: 60009.328\n",
      "    load_time_ms: 16.664\n",
      "    sample_throughput: 20.589\n",
      "    sample_time_ms: 48570.46\n",
      "    update_time_ms: 3.556\n",
      "  timestamp: 1634860865\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 596000\n",
      "  training_iteration: 596\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   596</td><td style=\"text-align: right;\">         17632.8</td><td style=\"text-align: right;\">596000</td><td style=\"text-align: right;\"> -1.8958</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">            370.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 597000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_00-02-32\n",
      "  done: false\n",
      "  episode_len_mean: 361.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.83\n",
      "  episode_reward_mean: -1.5929999999999718\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 1725\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6330071793662178\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012394970423439055\n",
      "          policy_loss: -0.058536325270930924\n",
      "          total_loss: 0.12773828899694814\n",
      "          vf_explained_var: 0.8274555802345276\n",
      "          vf_loss: 0.18166809228973257\n",
      "    num_agent_steps_sampled: 597000\n",
      "    num_agent_steps_trained: 597000\n",
      "    num_steps_sampled: 597000\n",
      "    num_steps_trained: 597000\n",
      "  iterations_since_restore: 597\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.8709677419355\n",
      "    ram_util_percent: 47.53629032258064\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040329464175818834\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.55301452159712\n",
      "    mean_inference_ms: 1.7571520751162673\n",
      "    mean_raw_obs_processing_ms: 2.007138111281252\n",
      "  time_since_restore: 17719.89254951477\n",
      "  time_this_iter_s: 87.08971238136292\n",
      "  time_total_s: 17719.89254951477\n",
      "  timers:\n",
      "    learn_throughput: 1187.17\n",
      "    learn_time_ms: 842.34\n",
      "    load_throughput: 61539.22\n",
      "    load_time_ms: 16.25\n",
      "    sample_throughput: 18.309\n",
      "    sample_time_ms: 54618.769\n",
      "    update_time_ms: 3.562\n",
      "  timestamp: 1634860952\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 597000\n",
      "  training_iteration: 597\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   597</td><td style=\"text-align: right;\">         17719.9</td><td style=\"text-align: right;\">597000</td><td style=\"text-align: right;\">  -1.593</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">            361.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 598000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_00-03-30\n",
      "  done: false\n",
      "  episode_len_mean: 357.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.83\n",
      "  episode_reward_mean: -1.4657999999999711\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1728\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8078252620167203\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009217179587327233\n",
      "          policy_loss: -0.14701388875643412\n",
      "          total_loss: -0.0346491997440656\n",
      "          vf_explained_var: 0.9039649963378906\n",
      "          vf_loss: 0.11487401566571659\n",
      "    num_agent_steps_sampled: 598000\n",
      "    num_agent_steps_trained: 598000\n",
      "    num_steps_sampled: 598000\n",
      "    num_steps_trained: 598000\n",
      "  iterations_since_restore: 598\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 74.57682926829268\n",
      "    ram_util_percent: 47.571951219512194\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04032983670588633\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.549269824083098\n",
      "    mean_inference_ms: 1.757168246439137\n",
      "    mean_raw_obs_processing_ms: 2.0252439865809992\n",
      "  time_since_restore: 17777.176377534866\n",
      "  time_this_iter_s: 57.283828020095825\n",
      "  time_total_s: 17777.176377534866\n",
      "  timers:\n",
      "    learn_throughput: 1187.45\n",
      "    learn_time_ms: 842.141\n",
      "    load_throughput: 60678.585\n",
      "    load_time_ms: 16.48\n",
      "    sample_throughput: 19.086\n",
      "    sample_time_ms: 52395.288\n",
      "    update_time_ms: 3.714\n",
      "  timestamp: 1634861010\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 598000\n",
      "  training_iteration: 598\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   598</td><td style=\"text-align: right;\">         17777.2</td><td style=\"text-align: right;\">598000</td><td style=\"text-align: right;\"> -1.4658</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">             357.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 599000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_00-05-55\n",
      "  done: false\n",
      "  episode_len_mean: 336.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.83\n",
      "  episode_reward_mean: -0.8516999999999739\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1735\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6192177401648626\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012387708682837416\n",
      "          policy_loss: -0.06987894264360268\n",
      "          total_loss: 0.36095827755828697\n",
      "          vf_explained_var: 0.9093260765075684\n",
      "          vf_loss: 0.4261050649194254\n",
      "    num_agent_steps_sampled: 599000\n",
      "    num_agent_steps_trained: 599000\n",
      "    num_steps_sampled: 599000\n",
      "    num_steps_trained: 599000\n",
      "  iterations_since_restore: 599\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.15291262135922\n",
      "    ram_util_percent: 47.53543689320388\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04033080117725139\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.54161406312608\n",
      "    mean_inference_ms: 1.7572014518725052\n",
      "    mean_raw_obs_processing_ms: 2.081499914921332\n",
      "  time_since_restore: 17922.10163283348\n",
      "  time_this_iter_s: 144.9252552986145\n",
      "  time_total_s: 17922.10163283348\n",
      "  timers:\n",
      "    learn_throughput: 1185.297\n",
      "    learn_time_ms: 843.67\n",
      "    load_throughput: 57002.232\n",
      "    load_time_ms: 17.543\n",
      "    sample_throughput: 15.576\n",
      "    sample_time_ms: 64199.71\n",
      "    update_time_ms: 3.65\n",
      "  timestamp: 1634861155\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 599000\n",
      "  training_iteration: 599\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   599</td><td style=\"text-align: right;\">         17922.1</td><td style=\"text-align: right;\">599000</td><td style=\"text-align: right;\"> -0.8517</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">            336.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 600000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_00-06-56\n",
      "  done: false\n",
      "  episode_len_mean: 328.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.83\n",
      "  episode_reward_mean: -0.6982999999999739\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1739\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.708025089899699\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011994530628277131\n",
      "          policy_loss: -0.057850226304597324\n",
      "          total_loss: 0.31973068041519986\n",
      "          vf_explained_var: 0.9148386716842651\n",
      "          vf_loss: 0.3744009560983007\n",
      "    num_agent_steps_sampled: 600000\n",
      "    num_agent_steps_trained: 600000\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "  iterations_since_restore: 600\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.85113636363636\n",
      "    ram_util_percent: 47.54886363636364\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040331220478548875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.537698542950714\n",
      "    mean_inference_ms: 1.7572190614706704\n",
      "    mean_raw_obs_processing_ms: 2.1161513103566283\n",
      "  time_since_restore: 17983.42913389206\n",
      "  time_this_iter_s: 61.32750105857849\n",
      "  time_total_s: 17983.42913389206\n",
      "  timers:\n",
      "    learn_throughput: 1187.697\n",
      "    learn_time_ms: 841.965\n",
      "    load_throughput: 56413.269\n",
      "    load_time_ms: 17.726\n",
      "    sample_throughput: 14.665\n",
      "    sample_time_ms: 68189.058\n",
      "    update_time_ms: 3.825\n",
      "  timestamp: 1634861216\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 600\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   600</td><td style=\"text-align: right;\">         17983.4</td><td style=\"text-align: right;\">600000</td><td style=\"text-align: right;\"> -0.6983</td><td style=\"text-align: right;\">                9.83</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">            328.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 601000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_00-08-38\n",
      "  done: false\n",
      "  episode_len_mean: 318.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.850000000000001\n",
      "  episode_reward_mean: -0.31459999999997557\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1745\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7969722045792473\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009562457686714895\n",
      "          policy_loss: -0.1216080923875173\n",
      "          total_loss: 0.37419660737117133\n",
      "          vf_explained_var: 0.869198203086853\n",
      "          vf_loss: 0.49762228346533244\n",
      "    num_agent_steps_sampled: 601000\n",
      "    num_agent_steps_trained: 601000\n",
      "    num_steps_sampled: 601000\n",
      "    num_steps_trained: 601000\n",
      "  iterations_since_restore: 601\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.83287671232878\n",
      "    ram_util_percent: 47.39041095890411\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040331611990055964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.53224937297871\n",
      "    mean_inference_ms: 1.7572391343111804\n",
      "    mean_raw_obs_processing_ms: 2.175798259600864\n",
      "  time_since_restore: 18085.882581710815\n",
      "  time_this_iter_s: 102.4534478187561\n",
      "  time_total_s: 18085.882581710815\n",
      "  timers:\n",
      "    learn_throughput: 1151.089\n",
      "    learn_time_ms: 868.742\n",
      "    load_throughput: 53445.807\n",
      "    load_time_ms: 18.711\n",
      "    sample_throughput: 13.877\n",
      "    sample_time_ms: 72063.892\n",
      "    update_time_ms: 3.488\n",
      "  timestamp: 1634861318\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 601000\n",
      "  training_iteration: 601\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   601</td><td style=\"text-align: right;\">         18085.9</td><td style=\"text-align: right;\">601000</td><td style=\"text-align: right;\"> -0.3146</td><td style=\"text-align: right;\">                9.85</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">             318.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 602000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_00-09-09\n",
      "  done: false\n",
      "  episode_len_mean: 316.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.850000000000001\n",
      "  episode_reward_mean: -0.24829999999997596\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1748\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7689760128657024\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012266543850957035\n",
      "          policy_loss: -0.1130177553743124\n",
      "          total_loss: 0.10455727295743095\n",
      "          vf_explained_var: 0.9231011867523193\n",
      "          vf_loss: 0.21454512029886247\n",
      "    num_agent_steps_sampled: 602000\n",
      "    num_agent_steps_trained: 602000\n",
      "    num_steps_sampled: 602000\n",
      "    num_steps_trained: 602000\n",
      "  iterations_since_restore: 602\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.64090909090908\n",
      "    ram_util_percent: 47.51363636363636\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040331762951800604\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.529766969737647\n",
      "    mean_inference_ms: 1.7572469760800145\n",
      "    mean_raw_obs_processing_ms: 2.2056183643726754\n",
      "  time_since_restore: 18116.328630447388\n",
      "  time_this_iter_s: 30.446048736572266\n",
      "  time_total_s: 18116.328630447388\n",
      "  timers:\n",
      "    learn_throughput: 1146.729\n",
      "    learn_time_ms: 872.045\n",
      "    load_throughput: 53229.91\n",
      "    load_time_ms: 18.786\n",
      "    sample_throughput: 14.519\n",
      "    sample_time_ms: 68877.475\n",
      "    update_time_ms: 3.548\n",
      "  timestamp: 1634861349\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 602000\n",
      "  training_iteration: 602\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   602</td><td style=\"text-align: right;\">         18116.3</td><td style=\"text-align: right;\">602000</td><td style=\"text-align: right;\"> -0.2483</td><td style=\"text-align: right;\">                9.85</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">            316.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 603000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_00-09-37\n",
      "  done: false\n",
      "  episode_len_mean: 316.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.850000000000001\n",
      "  episode_reward_mean: -0.27339999999997544\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1750\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.749110844400194\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00972357545661587\n",
      "          policy_loss: -0.05655689239501953\n",
      "          total_loss: 0.09684262379176087\n",
      "          vf_explained_var: 0.8566310405731201\n",
      "          vf_loss: 0.1544663308809201\n",
      "    num_agent_steps_sampled: 603000\n",
      "    num_agent_steps_trained: 603000\n",
      "    num_steps_sampled: 603000\n",
      "    num_steps_trained: 603000\n",
      "  iterations_since_restore: 603\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 84.52250000000001\n",
      "    ram_util_percent: 47.2975\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0403319407265739\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.52819325038742\n",
      "    mean_inference_ms: 1.7572510289487775\n",
      "    mean_raw_obs_processing_ms: 2.225163097495399\n",
      "  time_since_restore: 18144.284149885178\n",
      "  time_this_iter_s: 27.955519437789917\n",
      "  time_total_s: 18144.284149885178\n",
      "  timers:\n",
      "    learn_throughput: 1143.312\n",
      "    learn_time_ms: 874.652\n",
      "    load_throughput: 53710.26\n",
      "    load_time_ms: 18.618\n",
      "    sample_throughput: 15.595\n",
      "    sample_time_ms: 64122.351\n",
      "    update_time_ms: 3.558\n",
      "  timestamp: 1634861377\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 603000\n",
      "  training_iteration: 603\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   603</td><td style=\"text-align: right;\">         18144.3</td><td style=\"text-align: right;\">603000</td><td style=\"text-align: right;\"> -0.2734</td><td style=\"text-align: right;\">                9.85</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">            316.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 604000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_00-12-40\n",
      "  done: false\n",
      "  episode_len_mean: 297.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.850000000000001\n",
      "  episode_reward_mean: 0.4104000000000228\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1759\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.700043307410346\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011652154781375811\n",
      "          policy_loss: -0.06222221470541424\n",
      "          total_loss: 0.42668062473336854\n",
      "          vf_explained_var: 0.873824417591095\n",
      "          vf_loss: 0.4862213822702567\n",
      "    num_agent_steps_sampled: 604000\n",
      "    num_agent_steps_trained: 604000\n",
      "    num_steps_sampled: 604000\n",
      "    num_steps_trained: 604000\n",
      "  iterations_since_restore: 604\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.65653846153845\n",
      "    ram_util_percent: 47.31653846153846\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0403328192343609\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.5225082386563\n",
      "    mean_inference_ms: 1.757260910673198\n",
      "    mean_raw_obs_processing_ms: 2.332133192196144\n",
      "  time_since_restore: 18327.087418556213\n",
      "  time_this_iter_s: 182.80326867103577\n",
      "  time_total_s: 18327.087418556213\n",
      "  timers:\n",
      "    learn_throughput: 1144.257\n",
      "    learn_time_ms: 873.929\n",
      "    load_throughput: 53495.569\n",
      "    load_time_ms: 18.693\n",
      "    sample_throughput: 12.513\n",
      "    sample_time_ms: 79915.577\n",
      "    update_time_ms: 3.654\n",
      "  timestamp: 1634861560\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 604000\n",
      "  training_iteration: 604\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   604</td><td style=\"text-align: right;\">         18327.1</td><td style=\"text-align: right;\">604000</td><td style=\"text-align: right;\">  0.4104</td><td style=\"text-align: right;\">                9.85</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">            297.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 605000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_00-15-10\n",
      "  done: false\n",
      "  episode_len_mean: 274.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.870000000000001\n",
      "  episode_reward_mean: 1.2533000000000214\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1766\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5388513061735365\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012719715070556938\n",
      "          policy_loss: -0.08358174926704831\n",
      "          total_loss: 0.38766068075266147\n",
      "          vf_explained_var: 0.8715565800666809\n",
      "          vf_loss: 0.46514581232849095\n",
      "    num_agent_steps_sampled: 605000\n",
      "    num_agent_steps_trained: 605000\n",
      "    num_steps_sampled: 605000\n",
      "    num_steps_trained: 605000\n",
      "  iterations_since_restore: 605\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.62777777777777\n",
      "    ram_util_percent: 47.460185185185175\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04033354808281958\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.51872656289642\n",
      "    mean_inference_ms: 1.7572681951906965\n",
      "    mean_raw_obs_processing_ms: 2.426316664568711\n",
      "  time_since_restore: 18477.790955781937\n",
      "  time_this_iter_s: 150.70353722572327\n",
      "  time_total_s: 18477.790955781937\n",
      "  timers:\n",
      "    learn_throughput: 1143.449\n",
      "    learn_time_ms: 874.547\n",
      "    load_throughput: 50732.557\n",
      "    load_time_ms: 19.711\n",
      "    sample_throughput: 10.795\n",
      "    sample_time_ms: 92632.631\n",
      "    update_time_ms: 3.345\n",
      "  timestamp: 1634861710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 605000\n",
      "  training_iteration: 605\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   605</td><td style=\"text-align: right;\">         18477.8</td><td style=\"text-align: right;\">605000</td><td style=\"text-align: right;\">  1.2533</td><td style=\"text-align: right;\">                9.87</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">            274.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 606000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_00-17-52\n",
      "  done: false\n",
      "  episode_len_mean: 252.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.870000000000001\n",
      "  episode_reward_mean: 2.1841000000000204\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1775\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4745124366548326\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008393230433716268\n",
      "          policy_loss: -0.03472464291585816\n",
      "          total_loss: 0.16961167264315818\n",
      "          vf_explained_var: 0.9413489699363708\n",
      "          vf_loss: 0.20490426479114426\n",
      "    num_agent_steps_sampled: 606000\n",
      "    num_agent_steps_trained: 606000\n",
      "    num_steps_sampled: 606000\n",
      "    num_steps_trained: 606000\n",
      "  iterations_since_restore: 606\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.01434782608696\n",
      "    ram_util_percent: 47.477826086956526\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04033453407648999\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.514007745335718\n",
      "    mean_inference_ms: 1.7572836822088727\n",
      "    mean_raw_obs_processing_ms: 2.567427972297695\n",
      "  time_since_restore: 18639.433066129684\n",
      "  time_this_iter_s: 161.6421103477478\n",
      "  time_total_s: 18639.433066129684\n",
      "  timers:\n",
      "    learn_throughput: 1144.433\n",
      "    learn_time_ms: 873.795\n",
      "    load_throughput: 49969.43\n",
      "    load_time_ms: 20.012\n",
      "    sample_throughput: 10.024\n",
      "    sample_time_ms: 99761.564\n",
      "    update_time_ms: 2.821\n",
      "  timestamp: 1634861872\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 606000\n",
      "  training_iteration: 606\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   606</td><td style=\"text-align: right;\">         18639.4</td><td style=\"text-align: right;\">606000</td><td style=\"text-align: right;\">  2.1841</td><td style=\"text-align: right;\">                9.87</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">            252.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 607000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_00-18-57\n",
      "  done: false\n",
      "  episode_len_mean: 245.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.870000000000001\n",
      "  episode_reward_mean: 2.4480000000000195\n",
      "  episode_reward_min: -14.790000000000054\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1779\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.693574419286516\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010615992864847603\n",
      "          policy_loss: -0.07200325586729579\n",
      "          total_loss: 0.10165073018934992\n",
      "          vf_explained_var: 0.8613227605819702\n",
      "          vf_loss: 0.1726580451346106\n",
      "    num_agent_steps_sampled: 607000\n",
      "    num_agent_steps_trained: 607000\n",
      "    num_steps_sampled: 607000\n",
      "    num_steps_trained: 607000\n",
      "  iterations_since_restore: 607\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.85217391304349\n",
      "    ram_util_percent: 47.536956521739114\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04033493135403287\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.51221267458808\n",
      "    mean_inference_ms: 1.7572889312965156\n",
      "    mean_raw_obs_processing_ms: 2.6325714972189873\n",
      "  time_since_restore: 18704.026458740234\n",
      "  time_this_iter_s: 64.59339261054993\n",
      "  time_total_s: 18704.026458740234\n",
      "  timers:\n",
      "    learn_throughput: 1145.613\n",
      "    learn_time_ms: 872.895\n",
      "    load_throughput: 50038.403\n",
      "    load_time_ms: 19.985\n",
      "    sample_throughput: 10.255\n",
      "    sample_time_ms: 97512.423\n",
      "    update_time_ms: 3.22\n",
      "  timestamp: 1634861937\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 607000\n",
      "  training_iteration: 607\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   607</td><td style=\"text-align: right;\">           18704</td><td style=\"text-align: right;\">607000</td><td style=\"text-align: right;\">   2.448</td><td style=\"text-align: right;\">                9.87</td><td style=\"text-align: right;\">              -14.79</td><td style=\"text-align: right;\">             245.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 608000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_00-21-21\n",
      "  done: false\n",
      "  episode_len_mean: 226.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.870000000000001\n",
      "  episode_reward_mean: 3.2714000000000185\n",
      "  episode_reward_min: -10.869999999999957\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1787\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6515039934052362\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010014383860565997\n",
      "          policy_loss: -0.05847271250353919\n",
      "          total_loss: 0.2080266492234336\n",
      "          vf_explained_var: 0.9613874554634094\n",
      "          vf_loss: 0.2660989056030909\n",
      "    num_agent_steps_sampled: 608000\n",
      "    num_agent_steps_trained: 608000\n",
      "    num_steps_sampled: 608000\n",
      "    num_steps_trained: 608000\n",
      "  iterations_since_restore: 608\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.72233009708738\n",
      "    ram_util_percent: 47.52135922330096\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04033568996199631\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.509010063176802\n",
      "    mean_inference_ms: 1.7572977421027405\n",
      "    mean_raw_obs_processing_ms: 2.776139655570631\n",
      "  time_since_restore: 18848.026255846024\n",
      "  time_this_iter_s: 143.99979710578918\n",
      "  time_total_s: 18848.026255846024\n",
      "  timers:\n",
      "    learn_throughput: 1145.992\n",
      "    learn_time_ms: 872.607\n",
      "    load_throughput: 48198.978\n",
      "    load_time_ms: 20.747\n",
      "    sample_throughput: 9.418\n",
      "    sample_time_ms: 106184.089\n",
      "    update_time_ms: 3.256\n",
      "  timestamp: 1634862081\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 608000\n",
      "  training_iteration: 608\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   608</td><td style=\"text-align: right;\">           18848</td><td style=\"text-align: right;\">608000</td><td style=\"text-align: right;\">  3.2714</td><td style=\"text-align: right;\">                9.87</td><td style=\"text-align: right;\">              -10.87</td><td style=\"text-align: right;\">            226.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 609000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_00-22-42\n",
      "  done: false\n",
      "  episode_len_mean: 222.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.870000000000001\n",
      "  episode_reward_mean: 3.4096000000000184\n",
      "  episode_reward_min: -10.869999999999957\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1791\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5847404374016656\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008964979305238588\n",
      "          policy_loss: 0.007583998930123117\n",
      "          total_loss: 0.3461668787731065\n",
      "          vf_explained_var: 0.9301505088806152\n",
      "          vf_loss: 0.33928735703229906\n",
      "    num_agent_steps_sampled: 609000\n",
      "    num_agent_steps_trained: 609000\n",
      "    num_steps_sampled: 609000\n",
      "    num_steps_trained: 609000\n",
      "  iterations_since_restore: 609\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.33620689655173\n",
      "    ram_util_percent: 47.50775862068966\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04033596813599274\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.507581848188156\n",
      "    mean_inference_ms: 1.757305154686464\n",
      "    mean_raw_obs_processing_ms: 2.851089010690356\n",
      "  time_since_restore: 18929.20450925827\n",
      "  time_this_iter_s: 81.1782534122467\n",
      "  time_total_s: 18929.20450925827\n",
      "  timers:\n",
      "    learn_throughput: 1149.258\n",
      "    learn_time_ms: 870.127\n",
      "    load_throughput: 50898.844\n",
      "    load_time_ms: 19.647\n",
      "    sample_throughput: 10.019\n",
      "    sample_time_ms: 99812.156\n",
      "    update_time_ms: 4.087\n",
      "  timestamp: 1634862162\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 609000\n",
      "  training_iteration: 609\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   609</td><td style=\"text-align: right;\">         18929.2</td><td style=\"text-align: right;\">609000</td><td style=\"text-align: right;\">  3.4096</td><td style=\"text-align: right;\">                9.87</td><td style=\"text-align: right;\">              -10.87</td><td style=\"text-align: right;\">            222.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 610000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_00-24-46\n",
      "  done: false\n",
      "  episode_len_mean: 213.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.870000000000001\n",
      "  episode_reward_mean: 3.6672000000000167\n",
      "  episode_reward_min: -10.869999999999957\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1798\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5616029249297247\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010068189954770348\n",
      "          policy_loss: -0.032351880644758545\n",
      "          total_loss: 0.2653903265380197\n",
      "          vf_explained_var: 0.9226712584495544\n",
      "          vf_loss: 0.2963518560760551\n",
      "    num_agent_steps_sampled: 610000\n",
      "    num_agent_steps_trained: 610000\n",
      "    num_steps_sampled: 610000\n",
      "    num_steps_trained: 610000\n",
      "  iterations_since_restore: 610\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.86193181818182\n",
      "    ram_util_percent: 47.48636363636364\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04033650743933673\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.50669543890751\n",
      "    mean_inference_ms: 1.7573235729592995\n",
      "    mean_raw_obs_processing_ms: 2.9863642195558238\n",
      "  time_since_restore: 19052.88365674019\n",
      "  time_this_iter_s: 123.67914748191833\n",
      "  time_total_s: 19052.88365674019\n",
      "  timers:\n",
      "    learn_throughput: 1147.05\n",
      "    learn_time_ms: 871.802\n",
      "    load_throughput: 50403.826\n",
      "    load_time_ms: 19.84\n",
      "    sample_throughput: 9.43\n",
      "    sample_time_ms: 106045.548\n",
      "    update_time_ms: 4.04\n",
      "  timestamp: 1634862286\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 610000\n",
      "  training_iteration: 610\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   610</td><td style=\"text-align: right;\">         19052.9</td><td style=\"text-align: right;\">610000</td><td style=\"text-align: right;\">  3.6672</td><td style=\"text-align: right;\">                9.87</td><td style=\"text-align: right;\">              -10.87</td><td style=\"text-align: right;\">            213.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 611000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_00-30-40\n",
      "  done: false\n",
      "  episode_len_mean: 164.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.870000000000001\n",
      "  episode_reward_mean: 5.159600000000015\n",
      "  episode_reward_min: -9.459999999999944\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 1815\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3177789171536765\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010629057561068992\n",
      "          policy_loss: -0.025745101935333674\n",
      "          total_loss: 0.6383928507566452\n",
      "          vf_explained_var: 0.9124143123626709\n",
      "          vf_loss: 0.6593619843324026\n",
      "    num_agent_steps_sampled: 611000\n",
      "    num_agent_steps_trained: 611000\n",
      "    num_steps_sampled: 611000\n",
      "    num_steps_trained: 611000\n",
      "  iterations_since_restore: 611\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.50375494071146\n",
      "    ram_util_percent: 47.49683794466403\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040337102849856234\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.510287168359188\n",
      "    mean_inference_ms: 1.7573625416059537\n",
      "    mean_raw_obs_processing_ms: 3.379851717916756\n",
      "  time_since_restore: 19407.263379335403\n",
      "  time_this_iter_s: 354.37972259521484\n",
      "  time_total_s: 19407.263379335403\n",
      "  timers:\n",
      "    learn_throughput: 1184.779\n",
      "    learn_time_ms: 844.04\n",
      "    load_throughput: 50326.233\n",
      "    load_time_ms: 19.87\n",
      "    sample_throughput: 7.618\n",
      "    sample_time_ms: 131265.842\n",
      "    update_time_ms: 3.941\n",
      "  timestamp: 1634862640\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 611000\n",
      "  training_iteration: 611\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   611</td><td style=\"text-align: right;\">         19407.3</td><td style=\"text-align: right;\">611000</td><td style=\"text-align: right;\">  5.1596</td><td style=\"text-align: right;\">                9.87</td><td style=\"text-align: right;\">               -9.46</td><td style=\"text-align: right;\">            164.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 612000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_00-36-01\n",
      "  done: false\n",
      "  episode_len_mean: 135.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.870000000000001\n",
      "  episode_reward_mean: 6.049200000000011\n",
      "  episode_reward_min: -3.899999999999965\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1830\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2691343506177266\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009565055221492575\n",
      "          policy_loss: -0.15284000519249175\n",
      "          total_loss: 0.4983591707216369\n",
      "          vf_explained_var: 0.9326359033584595\n",
      "          vf_loss: 0.6477339926693174\n",
      "    num_agent_steps_sampled: 612000\n",
      "    num_agent_steps_trained: 612000\n",
      "    num_steps_sampled: 612000\n",
      "    num_steps_trained: 612000\n",
      "  iterations_since_restore: 612\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.70807860262009\n",
      "    ram_util_percent: 47.48558951965066\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04033778548309361\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.517492276620743\n",
      "    mean_inference_ms: 1.757403419862454\n",
      "    mean_raw_obs_processing_ms: 3.755347430439483\n",
      "  time_since_restore: 19728.122713327408\n",
      "  time_this_iter_s: 320.8593339920044\n",
      "  time_total_s: 19728.122713327408\n",
      "  timers:\n",
      "    learn_throughput: 1184.767\n",
      "    learn_time_ms: 844.048\n",
      "    load_throughput: 50300.341\n",
      "    load_time_ms: 19.881\n",
      "    sample_throughput: 6.238\n",
      "    sample_time_ms: 160307.457\n",
      "    update_time_ms: 3.957\n",
      "  timestamp: 1634862961\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 612000\n",
      "  training_iteration: 612\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   612</td><td style=\"text-align: right;\">         19728.1</td><td style=\"text-align: right;\">612000</td><td style=\"text-align: right;\">  6.0492</td><td style=\"text-align: right;\">                9.87</td><td style=\"text-align: right;\">                -3.9</td><td style=\"text-align: right;\">            135.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 613000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_00-38-28\n",
      "  done: false\n",
      "  episode_len_mean: 137.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.870000000000001\n",
      "  episode_reward_mean: 6.093700000000012\n",
      "  episode_reward_min: -3.899999999999965\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1838\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5242449733946057\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008786871040677054\n",
      "          policy_loss: -0.01813535276386473\n",
      "          total_loss: 0.32201309899489083\n",
      "          vf_explained_var: 0.9007323384284973\n",
      "          vf_loss: 0.34054881350861654\n",
      "    num_agent_steps_sampled: 613000\n",
      "    num_agent_steps_trained: 613000\n",
      "    num_steps_sampled: 613000\n",
      "    num_steps_trained: 613000\n",
      "  iterations_since_restore: 613\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.60666666666667\n",
      "    ram_util_percent: 47.611428571428576\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040338090486095754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.521996609208244\n",
      "    mean_inference_ms: 1.7574350724871264\n",
      "    mean_raw_obs_processing_ms: 3.9494605112080112\n",
      "  time_since_restore: 19875.316495656967\n",
      "  time_this_iter_s: 147.19378232955933\n",
      "  time_total_s: 19875.316495656967\n",
      "  timers:\n",
      "    learn_throughput: 1175.959\n",
      "    learn_time_ms: 850.37\n",
      "    load_throughput: 48131.002\n",
      "    load_time_ms: 20.777\n",
      "    sample_throughput: 5.806\n",
      "    sample_time_ms: 172223.988\n",
      "    update_time_ms: 4.021\n",
      "  timestamp: 1634863108\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 613000\n",
      "  training_iteration: 613\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   613</td><td style=\"text-align: right;\">         19875.3</td><td style=\"text-align: right;\">613000</td><td style=\"text-align: right;\">  6.0937</td><td style=\"text-align: right;\">                9.87</td><td style=\"text-align: right;\">                -3.9</td><td style=\"text-align: right;\">            137.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 614000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_00-42-12\n",
      "  done: false\n",
      "  episode_len_mean: 113.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.870000000000001\n",
      "  episode_reward_mean: 6.836000000000012\n",
      "  episode_reward_min: -3.899999999999965\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1849\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4362152285046048\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010327762628220298\n",
      "          policy_loss: -0.12154533490538597\n",
      "          total_loss: 0.5136065772010221\n",
      "          vf_explained_var: 0.9099442958831787\n",
      "          vf_loss: 0.6320692333910201\n",
      "    num_agent_steps_sampled: 614000\n",
      "    num_agent_steps_trained: 614000\n",
      "    num_steps_sampled: 614000\n",
      "    num_steps_trained: 614000\n",
      "  iterations_since_restore: 614\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.06206896551724\n",
      "    ram_util_percent: 47.615047021943575\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04033850524469194\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.529756978034232\n",
      "    mean_inference_ms: 1.7574774985351218\n",
      "    mean_raw_obs_processing_ms: 4.233466991286171\n",
      "  time_since_restore: 20099.223295927048\n",
      "  time_this_iter_s: 223.90680027008057\n",
      "  time_total_s: 20099.223295927048\n",
      "  timers:\n",
      "    learn_throughput: 1145.289\n",
      "    learn_time_ms: 873.142\n",
      "    load_throughput: 46006.015\n",
      "    load_time_ms: 21.736\n",
      "    sample_throughput: 5.672\n",
      "    sample_time_ms: 176310.086\n",
      "    update_time_ms: 4.444\n",
      "  timestamp: 1634863332\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 614000\n",
      "  training_iteration: 614\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   614</td><td style=\"text-align: right;\">         20099.2</td><td style=\"text-align: right;\">614000</td><td style=\"text-align: right;\">   6.836</td><td style=\"text-align: right;\">                9.87</td><td style=\"text-align: right;\">                -3.9</td><td style=\"text-align: right;\">            113.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 615000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_00-44-08\n",
      "  done: false\n",
      "  episode_len_mean: 111.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.870000000000001\n",
      "  episode_reward_mean: 6.9863000000000115\n",
      "  episode_reward_min: -3.899999999999965\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1856\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3509185983075036\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006151170335345392\n",
      "          policy_loss: 0.00865279449046486\n",
      "          total_loss: 0.11727501182920402\n",
      "          vf_explained_var: 0.9024975895881653\n",
      "          vf_loss: 0.11174133927561343\n",
      "    num_agent_steps_sampled: 615000\n",
      "    num_agent_steps_trained: 615000\n",
      "    num_steps_sampled: 615000\n",
      "    num_steps_trained: 615000\n",
      "  iterations_since_restore: 615\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.76445783132532\n",
      "    ram_util_percent: 47.53433734939758\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040338561658878634\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.53383018113276\n",
      "    mean_inference_ms: 1.75750562193363\n",
      "    mean_raw_obs_processing_ms: 4.408840526187226\n",
      "  time_since_restore: 20215.54616045952\n",
      "  time_this_iter_s: 116.3228645324707\n",
      "  time_total_s: 20215.54616045952\n",
      "  timers:\n",
      "    learn_throughput: 1151.609\n",
      "    learn_time_ms: 868.35\n",
      "    load_throughput: 48150.728\n",
      "    load_time_ms: 20.768\n",
      "    sample_throughput: 5.784\n",
      "    sample_time_ms: 172877.834\n",
      "    update_time_ms: 4.497\n",
      "  timestamp: 1634863448\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 615000\n",
      "  training_iteration: 615\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   615</td><td style=\"text-align: right;\">         20215.5</td><td style=\"text-align: right;\">615000</td><td style=\"text-align: right;\">  6.9863</td><td style=\"text-align: right;\">                9.87</td><td style=\"text-align: right;\">                -3.9</td><td style=\"text-align: right;\">            111.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 616000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_00-46-21\n",
      "  done: false\n",
      "  episode_len_mean: 113.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.9\n",
      "  episode_reward_mean: 6.978400000000011\n",
      "  episode_reward_min: -3.899999999999965\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1864\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5819291651248932\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008633010656661591\n",
      "          policy_loss: 0.04210949486328496\n",
      "          total_loss: 0.3107933384883735\n",
      "          vf_explained_var: 0.8777355551719666\n",
      "          vf_loss: 0.26992094384299387\n",
      "    num_agent_steps_sampled: 616000\n",
      "    num_agent_steps_trained: 616000\n",
      "    num_steps_sampled: 616000\n",
      "    num_steps_trained: 616000\n",
      "  iterations_since_restore: 616\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.76772486772487\n",
      "    ram_util_percent: 47.65502645502646\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04033887811039727\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.539880837276563\n",
      "    mean_inference_ms: 1.75753882593693\n",
      "    mean_raw_obs_processing_ms: 4.607936564401387\n",
      "  time_since_restore: 20347.91504240036\n",
      "  time_this_iter_s: 132.36888194084167\n",
      "  time_total_s: 20347.91504240036\n",
      "  timers:\n",
      "    learn_throughput: 1132.229\n",
      "    learn_time_ms: 883.214\n",
      "    load_throughput: 46688.143\n",
      "    load_time_ms: 21.419\n",
      "    sample_throughput: 5.885\n",
      "    sample_time_ms: 169935.046\n",
      "    update_time_ms: 4.5\n",
      "  timestamp: 1634863581\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 616000\n",
      "  training_iteration: 616\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   616</td><td style=\"text-align: right;\">         20347.9</td><td style=\"text-align: right;\">616000</td><td style=\"text-align: right;\">  6.9784</td><td style=\"text-align: right;\">                 9.9</td><td style=\"text-align: right;\">                -3.9</td><td style=\"text-align: right;\">            113.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 617000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_00-53-40\n",
      "  done: false\n",
      "  episode_len_mean: 94.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.9\n",
      "  episode_reward_mean: 7.43390000000001\n",
      "  episode_reward_min: -3.899999999999965\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 1885\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3392702394061617\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012154671448067856\n",
      "          policy_loss: -0.09562496989965438\n",
      "          total_loss: 0.45768886531392733\n",
      "          vf_explained_var: 0.9168016314506531\n",
      "          vf_loss: 0.5461758391724693\n",
      "    num_agent_steps_sampled: 617000\n",
      "    num_agent_steps_trained: 617000\n",
      "    num_steps_sampled: 617000\n",
      "    num_steps_trained: 617000\n",
      "  iterations_since_restore: 617\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.6524720893142\n",
      "    ram_util_percent: 47.47703349282296\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040339598699695306\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.557213566678126\n",
      "    mean_inference_ms: 1.7576158141597995\n",
      "    mean_raw_obs_processing_ms: 5.19394164946055\n",
      "  time_since_restore: 20787.3810441494\n",
      "  time_this_iter_s: 439.4660017490387\n",
      "  time_total_s: 20787.3810441494\n",
      "  timers:\n",
      "    learn_throughput: 1130.382\n",
      "    learn_time_ms: 884.657\n",
      "    load_throughput: 46706.079\n",
      "    load_time_ms: 21.41\n",
      "    sample_throughput: 4.821\n",
      "    sample_time_ms: 207420.968\n",
      "    update_time_ms: 4.271\n",
      "  timestamp: 1634864020\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 617000\n",
      "  training_iteration: 617\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   617</td><td style=\"text-align: right;\">         20787.4</td><td style=\"text-align: right;\">617000</td><td style=\"text-align: right;\">  7.4339</td><td style=\"text-align: right;\">                 9.9</td><td style=\"text-align: right;\">                -3.9</td><td style=\"text-align: right;\">              94.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 618000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_01-01-55\n",
      "  done: false\n",
      "  episode_len_mean: 74.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.9\n",
      "  episode_reward_mean: 8.033600000000009\n",
      "  episode_reward_min: -1.9099999999999644\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 1909\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2796900272369385\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007038090403101529\n",
      "          policy_loss: -0.1141086774567763\n",
      "          total_loss: 0.3502018527024322\n",
      "          vf_explained_var: 0.9359392523765564\n",
      "          vf_loss: 0.46521925131479896\n",
      "    num_agent_steps_sampled: 618000\n",
      "    num_agent_steps_trained: 618000\n",
      "    num_steps_sampled: 618000\n",
      "    num_steps_trained: 618000\n",
      "  iterations_since_restore: 618\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.68215297450426\n",
      "    ram_util_percent: 47.589376770538244\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04034051508743227\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.573349051827776\n",
      "    mean_inference_ms: 1.757688085277271\n",
      "    mean_raw_obs_processing_ms: 5.898791001025583\n",
      "  time_since_restore: 21281.86212015152\n",
      "  time_this_iter_s: 494.481076002121\n",
      "  time_total_s: 21281.86212015152\n",
      "  timers:\n",
      "    learn_throughput: 1132.308\n",
      "    learn_time_ms: 883.152\n",
      "    load_throughput: 48915.448\n",
      "    load_time_ms: 20.443\n",
      "    sample_throughput: 4.124\n",
      "    sample_time_ms: 242471.334\n",
      "    update_time_ms: 4.3\n",
      "  timestamp: 1634864515\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 618000\n",
      "  training_iteration: 618\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   618</td><td style=\"text-align: right;\">         21281.9</td><td style=\"text-align: right;\">618000</td><td style=\"text-align: right;\">  8.0336</td><td style=\"text-align: right;\">                 9.9</td><td style=\"text-align: right;\">               -1.91</td><td style=\"text-align: right;\">             74.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 619000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_01-08-22\n",
      "  done: false\n",
      "  episode_len_mean: 73.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.9\n",
      "  episode_reward_mean: 8.06450000000001\n",
      "  episode_reward_min: -1.9099999999999644\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1929\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2026768114831712\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007891654050174158\n",
      "          policy_loss: -0.11984435518582662\n",
      "          total_loss: 0.18829616771803961\n",
      "          vf_explained_var: 0.9656645655632019\n",
      "          vf_loss: 0.30683733804358376\n",
      "    num_agent_steps_sampled: 619000\n",
      "    num_agent_steps_trained: 619000\n",
      "    num_steps_sampled: 619000\n",
      "    num_steps_trained: 619000\n",
      "  iterations_since_restore: 619\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.24303797468353\n",
      "    ram_util_percent: 47.63128390596745\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04034093888969711\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.58627438371376\n",
      "    mean_inference_ms: 1.7577382694536545\n",
      "    mean_raw_obs_processing_ms: 6.468405277255861\n",
      "  time_since_restore: 21669.44802570343\n",
      "  time_this_iter_s: 387.5859055519104\n",
      "  time_total_s: 21669.44802570343\n",
      "  timers:\n",
      "    learn_throughput: 1133.449\n",
      "    learn_time_ms: 882.263\n",
      "    load_throughput: 46931.428\n",
      "    load_time_ms: 21.308\n",
      "    sample_throughput: 3.661\n",
      "    sample_time_ms: 273112.955\n",
      "    update_time_ms: 3.517\n",
      "  timestamp: 1634864902\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 619000\n",
      "  training_iteration: 619\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   619</td><td style=\"text-align: right;\">         21669.4</td><td style=\"text-align: right;\">619000</td><td style=\"text-align: right;\">  8.0645</td><td style=\"text-align: right;\">                 9.9</td><td style=\"text-align: right;\">               -1.91</td><td style=\"text-align: right;\">             73.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 620000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_01-10-08\n",
      "  done: false\n",
      "  episode_len_mean: 73.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.9\n",
      "  episode_reward_mean: 8.043900000000008\n",
      "  episode_reward_min: -1.9099999999999644\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1935\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6799057642618815\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01138592692747251\n",
      "          policy_loss: -0.08968873963587815\n",
      "          total_loss: 0.18422626629471778\n",
      "          vf_explained_var: 0.9227951169013977\n",
      "          vf_loss: 0.2714818651477496\n",
      "    num_agent_steps_sampled: 620000\n",
      "    num_agent_steps_trained: 620000\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "  iterations_since_restore: 620\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.24133333333334\n",
      "    ram_util_percent: 47.716666666666676\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04034105817017431\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.590063790226075\n",
      "    mean_inference_ms: 1.7577494515556282\n",
      "    mean_raw_obs_processing_ms: 6.62828424418014\n",
      "  time_since_restore: 21774.496997833252\n",
      "  time_this_iter_s: 105.04897212982178\n",
      "  time_total_s: 21774.496997833252\n",
      "  timers:\n",
      "    learn_throughput: 1137.237\n",
      "    learn_time_ms: 879.324\n",
      "    load_throughput: 45641.409\n",
      "    load_time_ms: 21.91\n",
      "    sample_throughput: 3.687\n",
      "    sample_time_ms: 271252.202\n",
      "    update_time_ms: 3.379\n",
      "  timestamp: 1634865008\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 620\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   620</td><td style=\"text-align: right;\">         21774.5</td><td style=\"text-align: right;\">620000</td><td style=\"text-align: right;\">  8.0439</td><td style=\"text-align: right;\">                 9.9</td><td style=\"text-align: right;\">               -1.91</td><td style=\"text-align: right;\">             73.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 621000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_01-13-29\n",
      "  done: false\n",
      "  episode_len_mean: 73.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.9\n",
      "  episode_reward_mean: 8.019500000000011\n",
      "  episode_reward_min: -2.199999999999989\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1946\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.328982338640425\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00826768868888238\n",
      "          policy_loss: 0.030382520291540357\n",
      "          total_loss: 0.2874292659262816\n",
      "          vf_explained_var: 0.9307875633239746\n",
      "          vf_loss: 0.2563714522454474\n",
      "    num_agent_steps_sampled: 621000\n",
      "    num_agent_steps_trained: 621000\n",
      "    num_steps_sampled: 621000\n",
      "    num_steps_trained: 621000\n",
      "  iterations_since_restore: 621\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.06597222222223\n",
      "    ram_util_percent: 47.81388888888889\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04034130642422839\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.59784738863349\n",
      "    mean_inference_ms: 1.757774232181613\n",
      "    mean_raw_obs_processing_ms: 6.921669036078385\n",
      "  time_since_restore: 21976.026221752167\n",
      "  time_this_iter_s: 201.5292239189148\n",
      "  time_total_s: 21976.026221752167\n",
      "  timers:\n",
      "    learn_throughput: 1133.097\n",
      "    learn_time_ms: 882.537\n",
      "    load_throughput: 45980.546\n",
      "    load_time_ms: 21.748\n",
      "    sample_throughput: 3.907\n",
      "    sample_time_ms: 255963.948\n",
      "    update_time_ms: 3.501\n",
      "  timestamp: 1634865209\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 621000\n",
      "  training_iteration: 621\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   621</td><td style=\"text-align: right;\">           21976</td><td style=\"text-align: right;\">621000</td><td style=\"text-align: right;\">  8.0195</td><td style=\"text-align: right;\">                 9.9</td><td style=\"text-align: right;\">                -2.2</td><td style=\"text-align: right;\">             73.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 622000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_01-17-36\n",
      "  done: false\n",
      "  episode_len_mean: 64.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.9\n",
      "  episode_reward_mean: 8.183300000000008\n",
      "  episode_reward_min: -2.199999999999989\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1959\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.397108915117052\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007132829326498009\n",
      "          policy_loss: 0.05226166215207842\n",
      "          total_loss: 0.24469319780667623\n",
      "          vf_explained_var: 0.9021660685539246\n",
      "          vf_loss: 0.1943544179201126\n",
      "    num_agent_steps_sampled: 622000\n",
      "    num_agent_steps_trained: 622000\n",
      "    num_steps_sampled: 622000\n",
      "    num_steps_trained: 622000\n",
      "  iterations_since_restore: 622\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.75426136363636\n",
      "    ram_util_percent: 47.81107954545455\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04034142325630857\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.60848137329619\n",
      "    mean_inference_ms: 1.757800595509896\n",
      "    mean_raw_obs_processing_ms: 7.281687434536946\n",
      "  time_since_restore: 22222.942388534546\n",
      "  time_this_iter_s: 246.91616678237915\n",
      "  time_total_s: 22222.942388534546\n",
      "  timers:\n",
      "    learn_throughput: 1132.301\n",
      "    learn_time_ms: 883.158\n",
      "    load_throughput: 44048.654\n",
      "    load_time_ms: 22.702\n",
      "    sample_throughput: 4.023\n",
      "    sample_time_ms: 248567.648\n",
      "    update_time_ms: 3.414\n",
      "  timestamp: 1634865456\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 622000\n",
      "  training_iteration: 622\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   622</td><td style=\"text-align: right;\">         22222.9</td><td style=\"text-align: right;\">622000</td><td style=\"text-align: right;\">  8.1833</td><td style=\"text-align: right;\">                 9.9</td><td style=\"text-align: right;\">                -2.2</td><td style=\"text-align: right;\">             64.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 623000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_01-22-18\n",
      "  done: false\n",
      "  episode_len_mean: 65.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.88\n",
      "  episode_reward_mean: 8.084400000000006\n",
      "  episode_reward_min: -2.2699999999999854\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 1973\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2007998612191941\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008961715561102047\n",
      "          policy_loss: -0.008728638208574719\n",
      "          total_loss: 0.40374419076575174\n",
      "          vf_explained_var: 0.8441107869148254\n",
      "          vf_loss: 0.4093434178166919\n",
      "    num_agent_steps_sampled: 623000\n",
      "    num_agent_steps_trained: 623000\n",
      "    num_steps_sampled: 623000\n",
      "    num_steps_trained: 623000\n",
      "  iterations_since_restore: 623\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.99429280397023\n",
      "    ram_util_percent: 47.74640198511167\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04034129017812413\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.617827673938546\n",
      "    mean_inference_ms: 1.757827660683938\n",
      "    mean_raw_obs_processing_ms: 7.645851459361358\n",
      "  time_since_restore: 22505.18395614624\n",
      "  time_this_iter_s: 282.24156761169434\n",
      "  time_total_s: 22505.18395614624\n",
      "  timers:\n",
      "    learn_throughput: 1137.558\n",
      "    learn_time_ms: 879.076\n",
      "    load_throughput: 45871.172\n",
      "    load_time_ms: 21.8\n",
      "    sample_throughput: 3.816\n",
      "    sample_time_ms: 262077.423\n",
      "    update_time_ms: 3.367\n",
      "  timestamp: 1634865738\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 623000\n",
      "  training_iteration: 623\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   623</td><td style=\"text-align: right;\">         22505.2</td><td style=\"text-align: right;\">623000</td><td style=\"text-align: right;\">  8.0844</td><td style=\"text-align: right;\">                9.88</td><td style=\"text-align: right;\">               -2.27</td><td style=\"text-align: right;\">             65.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 624000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_01-26-10\n",
      "  done: false\n",
      "  episode_len_mean: 70.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.88\n",
      "  episode_reward_mean: 7.897300000000009\n",
      "  episode_reward_min: -7.539999999999937\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1985\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6290888574388291\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015197655950262975\n",
      "          policy_loss: 0.04019417237076495\n",
      "          total_loss: 0.3950015595803658\n",
      "          vf_explained_var: 0.9085731506347656\n",
      "          vf_loss: 0.34542760211560464\n",
      "    num_agent_steps_sampled: 624000\n",
      "    num_agent_steps_trained: 624000\n",
      "    num_steps_sampled: 624000\n",
      "    num_steps_trained: 624000\n",
      "  iterations_since_restore: 624\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.00090909090909\n",
      "    ram_util_percent: 47.92454545454545\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04034121761420728\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.626095546392094\n",
      "    mean_inference_ms: 1.757850768875723\n",
      "    mean_raw_obs_processing_ms: 7.967027034061655\n",
      "  time_since_restore: 22736.426053524017\n",
      "  time_this_iter_s: 231.2420973777771\n",
      "  time_total_s: 22736.426053524017\n",
      "  timers:\n",
      "    learn_throughput: 1166.783\n",
      "    learn_time_ms: 857.058\n",
      "    load_throughput: 47382.718\n",
      "    load_time_ms: 21.105\n",
      "    sample_throughput: 3.805\n",
      "    sample_time_ms: 262834.029\n",
      "    update_time_ms: 3.171\n",
      "  timestamp: 1634865970\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 624000\n",
      "  training_iteration: 624\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   624</td><td style=\"text-align: right;\">         22736.4</td><td style=\"text-align: right;\">624000</td><td style=\"text-align: right;\">  7.8973</td><td style=\"text-align: right;\">                9.88</td><td style=\"text-align: right;\">               -7.54</td><td style=\"text-align: right;\">              70.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 625000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_01-32-43\n",
      "  done: false\n",
      "  episode_len_mean: 72.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.88\n",
      "  episode_reward_mean: 7.869200000000009\n",
      "  episode_reward_min: -7.539999999999937\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2005\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2996943540043302\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008062008563856398\n",
      "          policy_loss: 0.06468590224782626\n",
      "          total_loss: 0.6279495692915387\n",
      "          vf_explained_var: 0.9481797814369202\n",
      "          vf_loss: 0.5626429029636912\n",
      "    num_agent_steps_sampled: 625000\n",
      "    num_agent_steps_trained: 625000\n",
      "    num_steps_sampled: 625000\n",
      "    num_steps_trained: 625000\n",
      "  iterations_since_restore: 625\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.50926916221034\n",
      "    ram_util_percent: 47.89090909090908\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04034111247075675\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.64102875897662\n",
      "    mean_inference_ms: 1.7578899840844004\n",
      "    mean_raw_obs_processing_ms: 8.466179898606958\n",
      "  time_since_restore: 23129.544893741608\n",
      "  time_this_iter_s: 393.11884021759033\n",
      "  time_total_s: 23129.544893741608\n",
      "  timers:\n",
      "    learn_throughput: 1160.091\n",
      "    learn_time_ms: 862.001\n",
      "    load_throughput: 47737.189\n",
      "    load_time_ms: 20.948\n",
      "    sample_throughput: 3.442\n",
      "    sample_time_ms: 290508.727\n",
      "    update_time_ms: 3.299\n",
      "  timestamp: 1634866363\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 625000\n",
      "  training_iteration: 625\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   625</td><td style=\"text-align: right;\">         23129.5</td><td style=\"text-align: right;\">625000</td><td style=\"text-align: right;\">  7.8692</td><td style=\"text-align: right;\">                9.88</td><td style=\"text-align: right;\">               -7.54</td><td style=\"text-align: right;\">             72.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 626000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_01-37-36\n",
      "  done: false\n",
      "  episode_len_mean: 75.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.88\n",
      "  episode_reward_mean: 7.820900000000008\n",
      "  episode_reward_min: -7.539999999999937\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 2019\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3726669192314147\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008875355373897678\n",
      "          policy_loss: 0.11083278213110236\n",
      "          total_loss: 0.34513319954276084\n",
      "          vf_explained_var: 0.9451344609260559\n",
      "          vf_loss: 0.23303554471996096\n",
      "    num_agent_steps_sampled: 626000\n",
      "    num_agent_steps_trained: 626000\n",
      "    num_steps_sampled: 626000\n",
      "    num_steps_trained: 626000\n",
      "  iterations_since_restore: 626\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.95334928229666\n",
      "    ram_util_percent: 47.91866028708134\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040341132260911926\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.65221602380646\n",
      "    mean_inference_ms: 1.757917446489877\n",
      "    mean_raw_obs_processing_ms: 8.815137259597831\n",
      "  time_since_restore: 23422.937423944473\n",
      "  time_this_iter_s: 293.3925302028656\n",
      "  time_total_s: 23422.937423944473\n",
      "  timers:\n",
      "    learn_throughput: 1181.445\n",
      "    learn_time_ms: 846.421\n",
      "    load_throughput: 48303.333\n",
      "    load_time_ms: 20.703\n",
      "    sample_throughput: 3.261\n",
      "    sample_time_ms: 306626.118\n",
      "    update_time_ms: 3.422\n",
      "  timestamp: 1634866656\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 626000\n",
      "  training_iteration: 626\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   626</td><td style=\"text-align: right;\">         23422.9</td><td style=\"text-align: right;\">626000</td><td style=\"text-align: right;\">  7.8209</td><td style=\"text-align: right;\">                9.88</td><td style=\"text-align: right;\">               -7.54</td><td style=\"text-align: right;\">             75.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 627000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_01-46-44\n",
      "  done: false\n",
      "  episode_len_mean: 57.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.88\n",
      "  episode_reward_mean: 8.407400000000006\n",
      "  episode_reward_min: -7.539999999999937\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 2047\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2487392518255445\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007295473351145344\n",
      "          policy_loss: -0.2579962133533425\n",
      "          total_loss: 0.17941801005767452\n",
      "          vf_explained_var: 0.9547567367553711\n",
      "          vf_loss: 0.43757868309815723\n",
      "    num_agent_steps_sampled: 627000\n",
      "    num_agent_steps_trained: 627000\n",
      "    num_steps_sampled: 627000\n",
      "    num_steps_trained: 627000\n",
      "  iterations_since_restore: 627\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.67535121328226\n",
      "    ram_util_percent: 47.93690932311622\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040341302475293356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.672814723636083\n",
      "    mean_inference_ms: 1.7579733943749312\n",
      "    mean_raw_obs_processing_ms: 9.641436586995912\n",
      "  time_since_restore: 23971.202910900116\n",
      "  time_this_iter_s: 548.2654869556427\n",
      "  time_total_s: 23971.202910900116\n",
      "  timers:\n",
      "    learn_throughput: 1182.411\n",
      "    learn_time_ms: 845.729\n",
      "    load_throughput: 47259.018\n",
      "    load_time_ms: 21.16\n",
      "    sample_throughput: 3.15\n",
      "    sample_time_ms: 317506.053\n",
      "    update_time_ms: 3.698\n",
      "  timestamp: 1634867204\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 627000\n",
      "  training_iteration: 627\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   627</td><td style=\"text-align: right;\">         23971.2</td><td style=\"text-align: right;\">627000</td><td style=\"text-align: right;\">  8.4074</td><td style=\"text-align: right;\">                9.88</td><td style=\"text-align: right;\">               -7.54</td><td style=\"text-align: right;\">             57.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 628000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_01-53-30\n",
      "  done: false\n",
      "  episode_len_mean: 56.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.870000000000001\n",
      "  episode_reward_mean: 8.506700000000006\n",
      "  episode_reward_min: -7.539999999999937\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 2068\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5424119287066989\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007263647771051228\n",
      "          policy_loss: 0.1355286568403244\n",
      "          total_loss: 0.35600035029153027\n",
      "          vf_explained_var: 0.9663116335868835\n",
      "          vf_loss: 0.22362663592729304\n",
      "    num_agent_steps_sampled: 628000\n",
      "    num_agent_steps_trained: 628000\n",
      "    num_steps_sampled: 628000\n",
      "    num_steps_trained: 628000\n",
      "  iterations_since_restore: 628\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.40794473229705\n",
      "    ram_util_percent: 48.038514680483594\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04034132536285517\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.68323520139659\n",
      "    mean_inference_ms: 1.7580009519950759\n",
      "    mean_raw_obs_processing_ms: 10.244002556541377\n",
      "  time_since_restore: 24376.9394698143\n",
      "  time_this_iter_s: 405.73655891418457\n",
      "  time_total_s: 24376.9394698143\n",
      "  timers:\n",
      "    learn_throughput: 1181.461\n",
      "    learn_time_ms: 846.409\n",
      "    load_throughput: 46883.166\n",
      "    load_time_ms: 21.33\n",
      "    sample_throughput: 3.24\n",
      "    sample_time_ms: 308631.089\n",
      "    update_time_ms: 3.573\n",
      "  timestamp: 1634867610\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 628000\n",
      "  training_iteration: 628\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   628</td><td style=\"text-align: right;\">         24376.9</td><td style=\"text-align: right;\">628000</td><td style=\"text-align: right;\">  8.5067</td><td style=\"text-align: right;\">                9.87</td><td style=\"text-align: right;\">               -7.54</td><td style=\"text-align: right;\">             56.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 629000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_01-58-27\n",
      "  done: false\n",
      "  episode_len_mean: 47.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.870000000000001\n",
      "  episode_reward_mean: 8.809500000000003\n",
      "  episode_reward_min: -0.22999999999996085\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 2083\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4067807820108202\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009624400646534663\n",
      "          policy_loss: -0.04412259426381853\n",
      "          total_loss: 0.3286592723594772\n",
      "          vf_explained_var: 0.9524074792861938\n",
      "          vf_loss: 0.3705929042564498\n",
      "    num_agent_steps_sampled: 629000\n",
      "    num_agent_steps_trained: 629000\n",
      "    num_steps_sampled: 629000\n",
      "    num_steps_trained: 629000\n",
      "  iterations_since_restore: 629\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.84444444444445\n",
      "    ram_util_percent: 48.04278959810874\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040341454646521176\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.691339146388476\n",
      "    mean_inference_ms: 1.7580243185197586\n",
      "    mean_raw_obs_processing_ms: 10.670038709245649\n",
      "  time_since_restore: 24673.569114923477\n",
      "  time_this_iter_s: 296.62964510917664\n",
      "  time_total_s: 24673.569114923477\n",
      "  timers:\n",
      "    learn_throughput: 1178.334\n",
      "    learn_time_ms: 848.656\n",
      "    load_throughput: 46582.674\n",
      "    load_time_ms: 21.467\n",
      "    sample_throughput: 3.339\n",
      "    sample_time_ms: 299533.055\n",
      "    update_time_ms: 3.518\n",
      "  timestamp: 1634867907\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 629000\n",
      "  training_iteration: 629\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   629</td><td style=\"text-align: right;\">         24673.6</td><td style=\"text-align: right;\">629000</td><td style=\"text-align: right;\">  8.8095</td><td style=\"text-align: right;\">                9.87</td><td style=\"text-align: right;\">               -0.23</td><td style=\"text-align: right;\">             47.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 630000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_02-08-14\n",
      "  done: false\n",
      "  episode_len_mean: 41.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.89\n",
      "  episode_reward_mean: 8.995100000000006\n",
      "  episode_reward_min: -0.22999999999996085\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 2113\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.332741958565182\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007139835428435445\n",
      "          policy_loss: 0.04047258579068714\n",
      "          total_loss: 0.4303689956665039\n",
      "          vf_explained_var: 0.9593889117240906\n",
      "          vf_loss: 0.3911637917160988\n",
      "    num_agent_steps_sampled: 630000\n",
      "    num_agent_steps_trained: 630000\n",
      "    num_steps_sampled: 630000\n",
      "    num_steps_trained: 630000\n",
      "  iterations_since_restore: 630\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.78579952267303\n",
      "    ram_util_percent: 48.080190930787595\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0403416598994327\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.703675747728212\n",
      "    mean_inference_ms: 1.7580769813857144\n",
      "    mean_raw_obs_processing_ms: 11.563123109477015\n",
      "  time_since_restore: 25260.832865953445\n",
      "  time_this_iter_s: 587.2637510299683\n",
      "  time_total_s: 25260.832865953445\n",
      "  timers:\n",
      "    learn_throughput: 1176.593\n",
      "    learn_time_ms: 849.912\n",
      "    load_throughput: 48202.191\n",
      "    load_time_ms: 20.746\n",
      "    sample_throughput: 2.876\n",
      "    sample_time_ms: 347753.966\n",
      "    update_time_ms: 3.527\n",
      "  timestamp: 1634868494\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 630000\n",
      "  training_iteration: 630\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   630</td><td style=\"text-align: right;\">         25260.8</td><td style=\"text-align: right;\">630000</td><td style=\"text-align: right;\">  8.9951</td><td style=\"text-align: right;\">                9.89</td><td style=\"text-align: right;\">               -0.23</td><td style=\"text-align: right;\">             41.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 631000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_02-16-19\n",
      "  done: false\n",
      "  episode_len_mean: 44.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.89\n",
      "  episode_reward_mean: 8.924900000000006\n",
      "  episode_reward_min: -0.22999999999996085\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2138\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1771917833222283\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0069554188440556905\n",
      "          policy_loss: 0.009556945537527402\n",
      "          total_loss: 0.2831556381450759\n",
      "          vf_explained_var: 0.9606853723526001\n",
      "          vf_loss: 0.2736220748888122\n",
      "    num_agent_steps_sampled: 631000\n",
      "    num_agent_steps_trained: 631000\n",
      "    num_steps_sampled: 631000\n",
      "    num_steps_trained: 631000\n",
      "  iterations_since_restore: 631\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.90650289017341\n",
      "    ram_util_percent: 48.13251445086705\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04034202212109304\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.711067697472668\n",
      "    mean_inference_ms: 1.7581180902080238\n",
      "    mean_raw_obs_processing_ms: 12.246970959987905\n",
      "  time_since_restore: 25745.42275595665\n",
      "  time_this_iter_s: 484.58989000320435\n",
      "  time_total_s: 25745.42275595665\n",
      "  timers:\n",
      "    learn_throughput: 1186.167\n",
      "    learn_time_ms: 843.051\n",
      "    load_throughput: 48222.474\n",
      "    load_time_ms: 20.737\n",
      "    sample_throughput: 2.659\n",
      "    sample_time_ms: 376067.153\n",
      "    update_time_ms: 3.451\n",
      "  timestamp: 1634868979\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 631000\n",
      "  training_iteration: 631\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   631</td><td style=\"text-align: right;\">         25745.4</td><td style=\"text-align: right;\">631000</td><td style=\"text-align: right;\">  8.9249</td><td style=\"text-align: right;\">                9.89</td><td style=\"text-align: right;\">               -0.23</td><td style=\"text-align: right;\">             44.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 632000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_02-21-28\n",
      "  done: false\n",
      "  episode_len_mean: 47.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.89\n",
      "  episode_reward_mean: 8.800200000000006\n",
      "  episode_reward_min: -2.0899999999999492\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 2154\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4485694845517476\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006741258337095256\n",
      "          policy_loss: 0.032092429531945126\n",
      "          total_loss: 0.4736613094806671\n",
      "          vf_explained_var: 0.9401871562004089\n",
      "          vf_loss: 0.4446677769223849\n",
      "    num_agent_steps_sampled: 632000\n",
      "    num_agent_steps_trained: 632000\n",
      "    num_steps_sampled: 632000\n",
      "    num_steps_trained: 632000\n",
      "  iterations_since_restore: 632\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.6076923076923\n",
      "    ram_util_percent: 48.20904977375566\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04034236940428346\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.71700862252565\n",
      "    mean_inference_ms: 1.7581450357894273\n",
      "    mean_raw_obs_processing_ms: 12.679162323886437\n",
      "  time_since_restore: 26055.09119772911\n",
      "  time_this_iter_s: 309.66844177246094\n",
      "  time_total_s: 26055.09119772911\n",
      "  timers:\n",
      "    learn_throughput: 1146.101\n",
      "    learn_time_ms: 872.523\n",
      "    load_throughput: 47507.068\n",
      "    load_time_ms: 21.049\n",
      "    sample_throughput: 2.616\n",
      "    sample_time_ms: 382312.84\n",
      "    update_time_ms: 3.535\n",
      "  timestamp: 1634869288\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 632000\n",
      "  training_iteration: 632\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   632</td><td style=\"text-align: right;\">         26055.1</td><td style=\"text-align: right;\">632000</td><td style=\"text-align: right;\">  8.8002</td><td style=\"text-align: right;\">                9.89</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">             47.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 633000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_02-27-25\n",
      "  done: false\n",
      "  episode_len_mean: 48.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.89\n",
      "  episode_reward_mean: 8.775700000000006\n",
      "  episode_reward_min: -2.0899999999999492\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 2172\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5371317969428169\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011219626516634563\n",
      "          policy_loss: -0.1568142768409517\n",
      "          total_loss: 0.24676498158110513\n",
      "          vf_explained_var: 0.8443039059638977\n",
      "          vf_loss: 0.39999927083651227\n",
      "    num_agent_steps_sampled: 633000\n",
      "    num_agent_steps_trained: 633000\n",
      "    num_steps_sampled: 633000\n",
      "    num_steps_trained: 633000\n",
      "  iterations_since_restore: 633\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.56581532416503\n",
      "    ram_util_percent: 48.286247544204315\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04034303883513285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.723347988968797\n",
      "    mean_inference_ms: 1.758184064533917\n",
      "    mean_raw_obs_processing_ms: 13.179778422603617\n",
      "  time_since_restore: 26411.803684473038\n",
      "  time_this_iter_s: 356.712486743927\n",
      "  time_total_s: 26411.803684473038\n",
      "  timers:\n",
      "    learn_throughput: 1148.148\n",
      "    learn_time_ms: 870.968\n",
      "    load_throughput: 47360.354\n",
      "    load_time_ms: 21.115\n",
      "    sample_throughput: 2.566\n",
      "    sample_time_ms: 389761.363\n",
      "    update_time_ms: 3.58\n",
      "  timestamp: 1634869645\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 633000\n",
      "  training_iteration: 633\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   633</td><td style=\"text-align: right;\">         26411.8</td><td style=\"text-align: right;\">633000</td><td style=\"text-align: right;\">  8.7757</td><td style=\"text-align: right;\">                9.89</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">             48.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 634000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_02-32-45\n",
      "  done: false\n",
      "  episode_len_mean: 46.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.89\n",
      "  episode_reward_mean: 8.914400000000008\n",
      "  episode_reward_min: -2.0899999999999492\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 2188\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.455291990439097\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009950690611177078\n",
      "          policy_loss: -0.07328153875552945\n",
      "          total_loss: 0.430066628050473\n",
      "          vf_explained_var: 0.9204579591751099\n",
      "          vf_loss: 0.5010931899150213\n",
      "    num_agent_steps_sampled: 634000\n",
      "    num_agent_steps_trained: 634000\n",
      "    num_steps_sampled: 634000\n",
      "    num_steps_trained: 634000\n",
      "  iterations_since_restore: 634\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.66901098901099\n",
      "    ram_util_percent: 48.31406593406593\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04034342679921635\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.729683189066186\n",
      "    mean_inference_ms: 1.758217203678098\n",
      "    mean_raw_obs_processing_ms: 13.599624884910188\n",
      "  time_since_restore: 26731.102872610092\n",
      "  time_this_iter_s: 319.29918813705444\n",
      "  time_total_s: 26731.102872610092\n",
      "  timers:\n",
      "    learn_throughput: 1148.655\n",
      "    learn_time_ms: 870.583\n",
      "    load_throughput: 47028.199\n",
      "    load_time_ms: 21.264\n",
      "    sample_throughput: 2.509\n",
      "    sample_time_ms: 398567.418\n",
      "    update_time_ms: 3.174\n",
      "  timestamp: 1634869965\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 634000\n",
      "  training_iteration: 634\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   634</td><td style=\"text-align: right;\">         26731.1</td><td style=\"text-align: right;\">634000</td><td style=\"text-align: right;\">  8.9144</td><td style=\"text-align: right;\">                9.89</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">             46.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 635000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_02-40-54\n",
      "  done: false\n",
      "  episode_len_mean: 49.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.88\n",
      "  episode_reward_mean: 8.823300000000007\n",
      "  episode_reward_min: -2.0899999999999492\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2213\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.132205193572574\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007030227300779796\n",
      "          policy_loss: 0.0008696401077840063\n",
      "          total_loss: 0.355818246346381\n",
      "          vf_explained_var: 0.9589000940322876\n",
      "          vf_loss: 0.35439575877454543\n",
      "    num_agent_steps_sampled: 635000\n",
      "    num_agent_steps_trained: 635000\n",
      "    num_steps_sampled: 635000\n",
      "    num_steps_trained: 635000\n",
      "  iterations_since_restore: 635\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.89154727793697\n",
      "    ram_util_percent: 48.30229226361031\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040344016456070414\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.74087230633505\n",
      "    mean_inference_ms: 1.758270255478712\n",
      "    mean_raw_obs_processing_ms: 14.280764193408077\n",
      "  time_since_restore: 27220.14314007759\n",
      "  time_this_iter_s: 489.0402674674988\n",
      "  time_total_s: 27220.14314007759\n",
      "  timers:\n",
      "    learn_throughput: 1149.904\n",
      "    learn_time_ms: 869.638\n",
      "    load_throughput: 45971.827\n",
      "    load_time_ms: 21.752\n",
      "    sample_throughput: 2.45\n",
      "    sample_time_ms: 408160.241\n",
      "    update_time_ms: 2.892\n",
      "  timestamp: 1634870454\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 635000\n",
      "  training_iteration: 635\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   635</td><td style=\"text-align: right;\">         27220.1</td><td style=\"text-align: right;\">635000</td><td style=\"text-align: right;\">  8.8233</td><td style=\"text-align: right;\">                9.88</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">             49.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 636000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_02-50-17\n",
      "  done: false\n",
      "  episode_len_mean: 44.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.88\n",
      "  episode_reward_mean: 8.958300000000005\n",
      "  episode_reward_min: -2.0899999999999492\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 2241\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1314803812238905\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0054460223246711025\n",
      "          policy_loss: -0.11688912784059842\n",
      "          total_loss: 0.3057043980807066\n",
      "          vf_explained_var: 0.9589693546295166\n",
      "          vf_loss: 0.4247093414266904\n",
      "    num_agent_steps_sampled: 636000\n",
      "    num_agent_steps_trained: 636000\n",
      "    num_steps_sampled: 636000\n",
      "    num_steps_trained: 636000\n",
      "  iterations_since_restore: 636\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.89813664596274\n",
      "    ram_util_percent: 48.46310559006211\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04034450830326228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.752662695244485\n",
      "    mean_inference_ms: 1.7583390647506782\n",
      "    mean_raw_obs_processing_ms: 15.063661721802754\n",
      "  time_since_restore: 27783.880249500275\n",
      "  time_this_iter_s: 563.7371094226837\n",
      "  time_total_s: 27783.880249500275\n",
      "  timers:\n",
      "    learn_throughput: 1147.337\n",
      "    learn_time_ms: 871.583\n",
      "    load_throughput: 44750.651\n",
      "    load_time_ms: 22.346\n",
      "    sample_throughput: 2.298\n",
      "    sample_time_ms: 435192.389\n",
      "    update_time_ms: 3.12\n",
      "  timestamp: 1634871017\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 636000\n",
      "  training_iteration: 636\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   636</td><td style=\"text-align: right;\">         27783.9</td><td style=\"text-align: right;\">636000</td><td style=\"text-align: right;\">  8.9583</td><td style=\"text-align: right;\">                9.88</td><td style=\"text-align: right;\">               -2.09</td><td style=\"text-align: right;\">             44.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 637000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_02-56-59\n",
      "  done: false\n",
      "  episode_len_mean: 47.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.88\n",
      "  episode_reward_mean: 8.990200000000007\n",
      "  episode_reward_min: -0.6599999999999868\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 2262\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2733125938309564\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008103870100549986\n",
      "          policy_loss: -0.03323927033278677\n",
      "          total_loss: 0.3010784011955063\n",
      "          vf_explained_var: 0.959050178527832\n",
      "          vf_loss: 0.3333623907632298\n",
      "    num_agent_steps_sampled: 637000\n",
      "    num_agent_steps_trained: 637000\n",
      "    num_steps_sampled: 637000\n",
      "    num_steps_trained: 637000\n",
      "  iterations_since_restore: 637\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.60977312390926\n",
      "    ram_util_percent: 48.49930191972077\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040344717638881186\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.760287258936952\n",
      "    mean_inference_ms: 1.7583951077948365\n",
      "    mean_raw_obs_processing_ms: 15.650524285147753\n",
      "  time_since_restore: 28185.57372903824\n",
      "  time_this_iter_s: 401.69347953796387\n",
      "  time_total_s: 28185.57372903824\n",
      "  timers:\n",
      "    learn_throughput: 1146.931\n",
      "    learn_time_ms: 871.892\n",
      "    load_throughput: 44757.48\n",
      "    load_time_ms: 22.343\n",
      "    sample_throughput: 2.378\n",
      "    sample_time_ms: 420535.217\n",
      "    update_time_ms: 2.847\n",
      "  timestamp: 1634871419\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 637000\n",
      "  training_iteration: 637\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   637</td><td style=\"text-align: right;\">         28185.6</td><td style=\"text-align: right;\">637000</td><td style=\"text-align: right;\">  8.9902</td><td style=\"text-align: right;\">                9.88</td><td style=\"text-align: right;\">               -0.66</td><td style=\"text-align: right;\">             47.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 638000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_03-03-04\n",
      "  done: false\n",
      "  episode_len_mean: 47.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.88\n",
      "  episode_reward_mean: 8.926900000000005\n",
      "  episode_reward_min: -2.129999999999974\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 2281\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.406743840376536\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007785010763654802\n",
      "          policy_loss: -0.022783192329936558\n",
      "          total_loss: 0.258462587164508\n",
      "          vf_explained_var: 0.8911638259887695\n",
      "          vf_loss: 0.2821633965190914\n",
      "    num_agent_steps_sampled: 638000\n",
      "    num_agent_steps_trained: 638000\n",
      "    num_steps_sampled: 638000\n",
      "    num_steps_trained: 638000\n",
      "  iterations_since_restore: 638\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.53096153846154\n",
      "    ram_util_percent: 48.503076923076925\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04034483330413519\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.765445989083556\n",
      "    mean_inference_ms: 1.7584374386575232\n",
      "    mean_raw_obs_processing_ms: 16.180599601177153\n",
      "  time_since_restore: 28550.102642774582\n",
      "  time_this_iter_s: 364.5289137363434\n",
      "  time_total_s: 28550.102642774582\n",
      "  timers:\n",
      "    learn_throughput: 1147.769\n",
      "    learn_time_ms: 871.256\n",
      "    load_throughput: 43371.749\n",
      "    load_time_ms: 23.056\n",
      "    sample_throughput: 2.401\n",
      "    sample_time_ms: 416414.393\n",
      "    update_time_ms: 2.829\n",
      "  timestamp: 1634871784\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 638000\n",
      "  training_iteration: 638\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   638</td><td style=\"text-align: right;\">         28550.1</td><td style=\"text-align: right;\">638000</td><td style=\"text-align: right;\">  8.9269</td><td style=\"text-align: right;\">                9.88</td><td style=\"text-align: right;\">               -2.13</td><td style=\"text-align: right;\">             47.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 639000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_03-11-45\n",
      "  done: false\n",
      "  episode_len_mean: 41.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.88\n",
      "  episode_reward_mean: 9.075500000000003\n",
      "  episode_reward_min: -2.129999999999974\n",
      "  episodes_this_iter: 27\n",
      "  episodes_total: 2308\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3397853281762866\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009260838349718832\n",
      "          policy_loss: 0.030177855491638185\n",
      "          total_loss: 0.37174542653891773\n",
      "          vf_explained_var: 0.9565302729606628\n",
      "          vf_loss: 0.3393227549062835\n",
      "    num_agent_steps_sampled: 639000\n",
      "    num_agent_steps_trained: 639000\n",
      "    num_steps_sampled: 639000\n",
      "    num_steps_trained: 639000\n",
      "  iterations_since_restore: 639\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.86182795698925\n",
      "    ram_util_percent: 48.45309139784946\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04034516485271682\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.77097773078356\n",
      "    mean_inference_ms: 1.7584990591683287\n",
      "    mean_raw_obs_processing_ms: 16.934070815525246\n",
      "  time_since_restore: 29071.44045162201\n",
      "  time_this_iter_s: 521.3378088474274\n",
      "  time_total_s: 29071.44045162201\n",
      "  timers:\n",
      "    learn_throughput: 1148.57\n",
      "    learn_time_ms: 870.648\n",
      "    load_throughput: 45349.955\n",
      "    load_time_ms: 22.051\n",
      "    sample_throughput: 2.278\n",
      "    sample_time_ms: 438886.552\n",
      "    update_time_ms: 3.146\n",
      "  timestamp: 1634872305\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 639000\n",
      "  training_iteration: 639\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   639</td><td style=\"text-align: right;\">         29071.4</td><td style=\"text-align: right;\">639000</td><td style=\"text-align: right;\">  9.0755</td><td style=\"text-align: right;\">                9.88</td><td style=\"text-align: right;\">               -2.13</td><td style=\"text-align: right;\">             41.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 640000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_03-23-24\n",
      "  done: false\n",
      "  episode_len_mean: 37.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.88\n",
      "  episode_reward_mean: 9.138900000000003\n",
      "  episode_reward_min: -2.129999999999974\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 2343\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1438952154583402\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006139850219054008\n",
      "          policy_loss: 0.030946958975659477\n",
      "          total_loss: 0.3759477436542511\n",
      "          vf_explained_var: 0.9626505374908447\n",
      "          vf_loss: 0.34606879336966406\n",
      "    num_agent_steps_sampled: 640000\n",
      "    num_agent_steps_trained: 640000\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "  iterations_since_restore: 640\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.93951855566701\n",
      "    ram_util_percent: 48.61554663991976\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040346044396067926\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.778571137377885\n",
      "    mean_inference_ms: 1.7585685566063527\n",
      "    mean_raw_obs_processing_ms: 17.946775520895457\n",
      "  time_since_restore: 29770.050002336502\n",
      "  time_this_iter_s: 698.6095507144928\n",
      "  time_total_s: 29770.050002336502\n",
      "  timers:\n",
      "    learn_throughput: 1149.977\n",
      "    learn_time_ms: 869.582\n",
      "    load_throughput: 44527.884\n",
      "    load_time_ms: 22.458\n",
      "    sample_throughput: 2.222\n",
      "    sample_time_ms: 450021.148\n",
      "    update_time_ms: 3.962\n",
      "  timestamp: 1634873004\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 640\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   640</td><td style=\"text-align: right;\">         29770.1</td><td style=\"text-align: right;\">640000</td><td style=\"text-align: right;\">  9.1389</td><td style=\"text-align: right;\">                9.88</td><td style=\"text-align: right;\">               -2.13</td><td style=\"text-align: right;\">             37.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 641000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_03-35-02\n",
      "  done: false\n",
      "  episode_len_mean: 35.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.89\n",
      "  episode_reward_mean: 9.200400000000004\n",
      "  episode_reward_min: -2.129999999999974\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 2378\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1392703652381897\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005959825533184216\n",
      "          policy_loss: -0.05556810936994023\n",
      "          total_loss: 0.17774049693511593\n",
      "          vf_explained_var: 0.9781795740127563\n",
      "          vf_loss: 0.23463444776005216\n",
      "    num_agent_steps_sampled: 641000\n",
      "    num_agent_steps_trained: 641000\n",
      "    num_steps_sampled: 641000\n",
      "    num_steps_trained: 641000\n",
      "  iterations_since_restore: 641\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.4531124497992\n",
      "    ram_util_percent: 48.72148594377511\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04034811246710026\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.786854906984196\n",
      "    mean_inference_ms: 1.75862731300653\n",
      "    mean_raw_obs_processing_ms: 19.00780070093611\n",
      "  time_since_restore: 30468.10044693947\n",
      "  time_this_iter_s: 698.0504446029663\n",
      "  time_total_s: 30468.10044693947\n",
      "  timers:\n",
      "    learn_throughput: 1142.903\n",
      "    learn_time_ms: 874.965\n",
      "    load_throughput: 44026.46\n",
      "    load_time_ms: 22.714\n",
      "    sample_throughput: 2.122\n",
      "    sample_time_ms: 471361.349\n",
      "    update_time_ms: 3.934\n",
      "  timestamp: 1634873702\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 641000\n",
      "  training_iteration: 641\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   641</td><td style=\"text-align: right;\">         30468.1</td><td style=\"text-align: right;\">641000</td><td style=\"text-align: right;\">  9.2004</td><td style=\"text-align: right;\">                9.89</td><td style=\"text-align: right;\">               -2.13</td><td style=\"text-align: right;\">             35.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 642000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_03-48-38\n",
      "  done: false\n",
      "  episode_len_mean: 27.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.9\n",
      "  episode_reward_mean: 9.4817\n",
      "  episode_reward_min: 2.9300000000000406\n",
      "  episodes_this_iter: 41\n",
      "  episodes_total: 2419\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.6891201342848923\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0860453989770678\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00422571706716469\n",
      "          policy_loss: 0.03053279436296887\n",
      "          total_loss: 0.2156438780327638\n",
      "          vf_explained_var: 0.9830191135406494\n",
      "          vf_loss: 0.1888337971435653\n",
      "    num_agent_steps_sampled: 642000\n",
      "    num_agent_steps_trained: 642000\n",
      "    num_steps_sampled: 642000\n",
      "    num_steps_trained: 642000\n",
      "  iterations_since_restore: 642\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.4674678111588\n",
      "    ram_util_percent: 48.755708154506436\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04035059677519939\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.796150670942712\n",
      "    mean_inference_ms: 1.7586850113762915\n",
      "    mean_raw_obs_processing_ms: 20.234860838660726\n",
      "  time_since_restore: 31284.70104789734\n",
      "  time_this_iter_s: 816.6006009578705\n",
      "  time_total_s: 31284.70104789734\n",
      "  timers:\n",
      "    learn_throughput: 1184.368\n",
      "    learn_time_ms: 844.332\n",
      "    load_throughput: 43961.672\n",
      "    load_time_ms: 22.747\n",
      "    sample_throughput: 1.915\n",
      "    sample_time_ms: 522084.597\n",
      "    update_time_ms: 4.655\n",
      "  timestamp: 1634874518\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 642000\n",
      "  training_iteration: 642\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   642</td><td style=\"text-align: right;\">         31284.7</td><td style=\"text-align: right;\">642000</td><td style=\"text-align: right;\">  9.4817</td><td style=\"text-align: right;\">                 9.9</td><td style=\"text-align: right;\">                2.93</td><td style=\"text-align: right;\">             27.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 643000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_04-01-12\n",
      "  done: false\n",
      "  episode_len_mean: 26.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.9\n",
      "  episode_reward_mean: 9.499200000000002\n",
      "  episode_reward_min: 4.390000000000011\n",
      "  episodes_this_iter: 38\n",
      "  episodes_total: 2457\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8445600671424461\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1881545371479458\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00953362854984879\n",
      "          policy_loss: -0.00642642734779252\n",
      "          total_loss: 0.22895587506807513\n",
      "          vf_explained_var: 0.9809329509735107\n",
      "          vf_loss: 0.23921212769216962\n",
      "    num_agent_steps_sampled: 643000\n",
      "    num_agent_steps_trained: 643000\n",
      "    num_steps_sampled: 643000\n",
      "    num_steps_trained: 643000\n",
      "  iterations_since_restore: 643\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.1893953488372\n",
      "    ram_util_percent: 48.815069767441855\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040352138297036336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.803727057778715\n",
      "    mean_inference_ms: 1.7587439631979778\n",
      "    mean_raw_obs_processing_ms: 21.342912391873178\n",
      "  time_since_restore: 32038.187655687332\n",
      "  time_this_iter_s: 753.4866077899933\n",
      "  time_total_s: 32038.187655687332\n",
      "  timers:\n",
      "    learn_throughput: 1189.609\n",
      "    learn_time_ms: 840.612\n",
      "    load_throughput: 43073.593\n",
      "    load_time_ms: 23.216\n",
      "    sample_throughput: 1.78\n",
      "    sample_time_ms: 561765.298\n",
      "    update_time_ms: 4.626\n",
      "  timestamp: 1634875272\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 643000\n",
      "  training_iteration: 643\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   643</td><td style=\"text-align: right;\">         32038.2</td><td style=\"text-align: right;\">643000</td><td style=\"text-align: right;\">  9.4992</td><td style=\"text-align: right;\">                 9.9</td><td style=\"text-align: right;\">                4.39</td><td style=\"text-align: right;\">             26.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 644000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_04-12-32\n",
      "  done: false\n",
      "  episode_len_mean: 25.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.9\n",
      "  episode_reward_mean: 9.547100000000002\n",
      "  episode_reward_min: 4.390000000000011\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 2491\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8445600671424461\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.110697857538859\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00732211839182663\n",
      "          policy_loss: -0.13012778013944626\n",
      "          total_loss: 0.08588521778583527\n",
      "          vf_explained_var: 0.9822896122932434\n",
      "          vf_loss: 0.22093600614203346\n",
      "    num_agent_steps_sampled: 644000\n",
      "    num_agent_steps_trained: 644000\n",
      "    num_steps_sampled: 644000\n",
      "    num_steps_trained: 644000\n",
      "  iterations_since_restore: 644\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.58681771369723\n",
      "    ram_util_percent: 48.825952626158596\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04035270117427289\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.809826864328514\n",
      "    mean_inference_ms: 1.7588035712579153\n",
      "    mean_raw_obs_processing_ms: 22.29367560253453\n",
      "  time_since_restore: 32718.15452647209\n",
      "  time_this_iter_s: 679.9668707847595\n",
      "  time_total_s: 32718.15452647209\n",
      "  timers:\n",
      "    learn_throughput: 1186.495\n",
      "    learn_time_ms: 842.819\n",
      "    load_throughput: 42100.073\n",
      "    load_time_ms: 23.753\n",
      "    sample_throughput: 1.673\n",
      "    sample_time_ms: 597829.485\n",
      "    update_time_ms: 4.616\n",
      "  timestamp: 1634875952\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 644000\n",
      "  training_iteration: 644\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   644</td><td style=\"text-align: right;\">         32718.2</td><td style=\"text-align: right;\">644000</td><td style=\"text-align: right;\">  9.5471</td><td style=\"text-align: right;\">                 9.9</td><td style=\"text-align: right;\">                4.39</td><td style=\"text-align: right;\">             25.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 645000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_04-17-57\n",
      "  done: false\n",
      "  episode_len_mean: 31.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.88\n",
      "  episode_reward_mean: 9.422700000000004\n",
      "  episode_reward_min: 2.1900000000000794\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 2508\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8445600671424461\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9914806167284648\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013670982332408401\n",
      "          policy_loss: 0.0608132016327646\n",
      "          total_loss: 0.25819269286261665\n",
      "          vf_explained_var: 0.9584059715270996\n",
      "          vf_loss: 0.19574832618236543\n",
      "    num_agent_steps_sampled: 645000\n",
      "    num_agent_steps_trained: 645000\n",
      "    num_steps_sampled: 645000\n",
      "    num_steps_trained: 645000\n",
      "  iterations_since_restore: 645\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.44913793103449\n",
      "    ram_util_percent: 48.99655172413792\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040353046753983775\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.81574936042746\n",
      "    mean_inference_ms: 1.7588520770063634\n",
      "    mean_raw_obs_processing_ms: 22.71276889993903\n",
      "  time_since_restore: 33043.55956840515\n",
      "  time_this_iter_s: 325.4050419330597\n",
      "  time_total_s: 33043.55956840515\n",
      "  timers:\n",
      "    learn_throughput: 1188.346\n",
      "    learn_time_ms: 841.506\n",
      "    load_throughput: 42753.736\n",
      "    load_time_ms: 23.39\n",
      "    sample_throughput: 1.72\n",
      "    sample_time_ms: 581467.728\n",
      "    update_time_ms: 4.593\n",
      "  timestamp: 1634876277\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 645000\n",
      "  training_iteration: 645\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   645</td><td style=\"text-align: right;\">         33043.6</td><td style=\"text-align: right;\">645000</td><td style=\"text-align: right;\">  9.4227</td><td style=\"text-align: right;\">                9.88</td><td style=\"text-align: right;\">                2.19</td><td style=\"text-align: right;\">             31.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 646000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_04-31-49\n",
      "  done: false\n",
      "  episode_len_mean: 31.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.89\n",
      "  episode_reward_mean: 9.471200000000003\n",
      "  episode_reward_min: 2.1900000000000794\n",
      "  episodes_this_iter: 42\n",
      "  episodes_total: 2550\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8445600671424461\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0504484626981947\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007407324714228138\n",
      "          policy_loss: -0.03591293882992533\n",
      "          total_loss: 0.1688719785047902\n",
      "          vf_explained_var: 0.9795944094657898\n",
      "          vf_loss: 0.20903347043527498\n",
      "    num_agent_steps_sampled: 646000\n",
      "    num_agent_steps_trained: 646000\n",
      "    num_steps_sampled: 646000\n",
      "    num_steps_trained: 646000\n",
      "  iterations_since_restore: 646\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.21120471777591\n",
      "    ram_util_percent: 49.104549283909016\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04035403165817189\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.83169990425433\n",
      "    mean_inference_ms: 1.7590184611096316\n",
      "    mean_raw_obs_processing_ms: 23.914328040239543\n",
      "  time_since_restore: 33875.09678339958\n",
      "  time_this_iter_s: 831.5372149944305\n",
      "  time_total_s: 33875.09678339958\n",
      "  timers:\n",
      "    learn_throughput: 1194.75\n",
      "    learn_time_ms: 836.995\n",
      "    load_throughput: 44480.52\n",
      "    load_time_ms: 22.482\n",
      "    sample_throughput: 1.644\n",
      "    sample_time_ms: 608253.469\n",
      "    update_time_ms: 4.269\n",
      "  timestamp: 1634877109\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 646000\n",
      "  training_iteration: 646\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   646</td><td style=\"text-align: right;\">         33875.1</td><td style=\"text-align: right;\">646000</td><td style=\"text-align: right;\">  9.4712</td><td style=\"text-align: right;\">                9.89</td><td style=\"text-align: right;\">                2.19</td><td style=\"text-align: right;\">             31.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 647000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_04-42-39\n",
      "  done: false\n",
      "  episode_len_mean: 33.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.89\n",
      "  episode_reward_mean: 9.420200000000005\n",
      "  episode_reward_min: 2.1900000000000794\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 2583\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8445600671424461\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2273651281992595\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013770955944040361\n",
      "          policy_loss: 0.05088536325428221\n",
      "          total_loss: 0.17977653137511676\n",
      "          vf_explained_var: 0.8972699642181396\n",
      "          vf_loss: 0.12953442194395595\n",
      "    num_agent_steps_sampled: 647000\n",
      "    num_agent_steps_trained: 647000\n",
      "    num_steps_sampled: 647000\n",
      "    num_steps_trained: 647000\n",
      "  iterations_since_restore: 647\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.03894282632146\n",
      "    ram_util_percent: 49.18878101402373\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04035850165801404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.84585921708598\n",
      "    mean_inference_ms: 1.759156747161826\n",
      "    mean_raw_obs_processing_ms: 24.81236212190321\n",
      "  time_since_restore: 34525.00638794899\n",
      "  time_this_iter_s: 649.909604549408\n",
      "  time_total_s: 34525.00638794899\n",
      "  timers:\n",
      "    learn_throughput: 1191.012\n",
      "    learn_time_ms: 839.622\n",
      "    load_throughput: 43474.787\n",
      "    load_time_ms: 23.002\n",
      "    sample_throughput: 1.58\n",
      "    sample_time_ms: 633072.017\n",
      "    update_time_ms: 4.24\n",
      "  timestamp: 1634877759\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 647000\n",
      "  training_iteration: 647\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   647</td><td style=\"text-align: right;\">           34525</td><td style=\"text-align: right;\">647000</td><td style=\"text-align: right;\">  9.4202</td><td style=\"text-align: right;\">                9.89</td><td style=\"text-align: right;\">                2.19</td><td style=\"text-align: right;\">             33.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 648000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_04-53-04\n",
      "  done: false\n",
      "  episode_len_mean: 28.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.89\n",
      "  episode_reward_mean: 9.541500000000003\n",
      "  episode_reward_min: -0.10999999999996321\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 2615\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8445600671424461\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1860440095265707\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014050241382622196\n",
      "          policy_loss: 0.05074766779111491\n",
      "          total_loss: 0.29236667801936467\n",
      "          vf_explained_var: 0.9755855798721313\n",
      "          vf_loss: 0.24161317265695995\n",
      "    num_agent_steps_sampled: 648000\n",
      "    num_agent_steps_trained: 648000\n",
      "    num_steps_sampled: 648000\n",
      "    num_steps_trained: 648000\n",
      "  iterations_since_restore: 648\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.51210762331839\n",
      "    ram_util_percent: 49.263228699551576\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04036263352006688\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.857153552274827\n",
      "    mean_inference_ms: 1.7592600511179068\n",
      "    mean_raw_obs_processing_ms: 25.714670268567023\n",
      "  time_since_restore: 35149.78610634804\n",
      "  time_this_iter_s: 624.7797183990479\n",
      "  time_total_s: 35149.78610634804\n",
      "  timers:\n",
      "    learn_throughput: 1188.059\n",
      "    learn_time_ms: 841.709\n",
      "    load_throughput: 45309.783\n",
      "    load_time_ms: 22.07\n",
      "    sample_throughput: 1.517\n",
      "    sample_time_ms: 659095.933\n",
      "    update_time_ms: 4.241\n",
      "  timestamp: 1634878384\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 648000\n",
      "  training_iteration: 648\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   648</td><td style=\"text-align: right;\">         35149.8</td><td style=\"text-align: right;\">648000</td><td style=\"text-align: right;\">  9.5415</td><td style=\"text-align: right;\">                9.89</td><td style=\"text-align: right;\">               -0.11</td><td style=\"text-align: right;\">             28.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 649000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_05-02-31\n",
      "  done: false\n",
      "  episode_len_mean: 31.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 9.496200000000004\n",
      "  episode_reward_min: -0.10999999999996321\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 2644\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8445600671424461\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.278764463795556\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009586500075925061\n",
      "          policy_loss: 0.0460032203545173\n",
      "          total_loss: 0.27883441783487795\n",
      "          vf_explained_var: 0.8316453099250793\n",
      "          vf_loss: 0.23752246871590615\n",
      "    num_agent_steps_sampled: 649000\n",
      "    num_agent_steps_trained: 649000\n",
      "    num_steps_sampled: 649000\n",
      "    num_steps_trained: 649000\n",
      "  iterations_since_restore: 649\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.27317676143387\n",
      "    ram_util_percent: 49.37095179233621\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04036650073138171\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.86584381318203\n",
      "    mean_inference_ms: 1.7593346165114412\n",
      "    mean_raw_obs_processing_ms: 26.458664287201135\n",
      "  time_since_restore: 35717.09316563606\n",
      "  time_this_iter_s: 567.3070592880249\n",
      "  time_total_s: 35717.09316563606\n",
      "  timers:\n",
      "    learn_throughput: 1185.751\n",
      "    learn_time_ms: 843.347\n",
      "    load_throughput: 43363.453\n",
      "    load_time_ms: 23.061\n",
      "    sample_throughput: 1.507\n",
      "    sample_time_ms: 663690.474\n",
      "    update_time_ms: 3.932\n",
      "  timestamp: 1634878951\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 649000\n",
      "  training_iteration: 649\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   649</td><td style=\"text-align: right;\">         35717.1</td><td style=\"text-align: right;\">649000</td><td style=\"text-align: right;\">  9.4962</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -0.11</td><td style=\"text-align: right;\">             31.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 650000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_05-10-55\n",
      "  done: false\n",
      "  episode_len_mean: 32.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 9.516000000000004\n",
      "  episode_reward_min: -0.10999999999996321\n",
      "  episodes_this_iter: 25\n",
      "  episodes_total: 2669\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8445600671424461\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8760995341671838\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007965059220867054\n",
      "          policy_loss: 0.05004816858304872\n",
      "          total_loss: 0.1610892424152957\n",
      "          vf_explained_var: 0.7483657598495483\n",
      "          vf_loss: 0.11307509826082322\n",
      "    num_agent_steps_sampled: 650000\n",
      "    num_agent_steps_trained: 650000\n",
      "    num_steps_sampled: 650000\n",
      "    num_steps_trained: 650000\n",
      "  iterations_since_restore: 650\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.81944444444444\n",
      "    ram_util_percent: 49.42986111111111\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04036782875690839\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.871625312481328\n",
      "    mean_inference_ms: 1.7594048535705498\n",
      "    mean_raw_obs_processing_ms: 27.099196533552853\n",
      "  time_since_restore: 36221.65704870224\n",
      "  time_this_iter_s: 504.56388306617737\n",
      "  time_total_s: 36221.65704870224\n",
      "  timers:\n",
      "    learn_throughput: 1183.657\n",
      "    learn_time_ms: 844.84\n",
      "    load_throughput: 44631.698\n",
      "    load_time_ms: 22.406\n",
      "    sample_throughput: 1.552\n",
      "    sample_time_ms: 644285.832\n",
      "    update_time_ms: 3.22\n",
      "  timestamp: 1634879455\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 650000\n",
      "  training_iteration: 650\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   650</td><td style=\"text-align: right;\">         36221.7</td><td style=\"text-align: right;\">650000</td><td style=\"text-align: right;\">   9.516</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -0.11</td><td style=\"text-align: right;\">             32.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 651000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_05-18-02\n",
      "  done: false\n",
      "  episode_len_mean: 38.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 9.381400000000003\n",
      "  episode_reward_min: -4.129999999999926\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 2691\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8445600671424461\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3545977228217654\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009769410075311116\n",
      "          policy_loss: 0.08141325389345487\n",
      "          total_loss: 0.3010880122996039\n",
      "          vf_explained_var: 0.5593178868293762\n",
      "          vf_loss: 0.22496988204721774\n",
      "    num_agent_steps_sampled: 651000\n",
      "    num_agent_steps_trained: 651000\n",
      "    num_steps_sampled: 651000\n",
      "    num_steps_trained: 651000\n",
      "  iterations_since_restore: 651\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.58686371100163\n",
      "    ram_util_percent: 49.51871921182266\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04036956952773101\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.874475084088953\n",
      "    mean_inference_ms: 1.7594793458988516\n",
      "    mean_raw_obs_processing_ms: 27.67108885136374\n",
      "  time_since_restore: 36648.17339468002\n",
      "  time_this_iter_s: 426.5163459777832\n",
      "  time_total_s: 36648.17339468002\n",
      "  timers:\n",
      "    learn_throughput: 1182.505\n",
      "    learn_time_ms: 845.663\n",
      "    load_throughput: 44206.641\n",
      "    load_time_ms: 22.621\n",
      "    sample_throughput: 1.62\n",
      "    sample_time_ms: 617131.454\n",
      "    update_time_ms: 3.344\n",
      "  timestamp: 1634879882\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 651000\n",
      "  training_iteration: 651\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   651</td><td style=\"text-align: right;\">         36648.2</td><td style=\"text-align: right;\">651000</td><td style=\"text-align: right;\">  9.3814</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -4.13</td><td style=\"text-align: right;\">             38.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 652000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_05-28-30\n",
      "  done: false\n",
      "  episode_len_mean: 34.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 9.516800000000003\n",
      "  episode_reward_min: -4.129999999999926\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 2722\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8445600671424461\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0352092213100856\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010059717126059967\n",
      "          policy_loss: -0.017102098133828906\n",
      "          total_loss: 0.4467578956650363\n",
      "          vf_explained_var: 0.9703674912452698\n",
      "          vf_loss: 0.4657160555322965\n",
      "    num_agent_steps_sampled: 652000\n",
      "    num_agent_steps_trained: 652000\n",
      "    num_steps_sampled: 652000\n",
      "    num_steps_trained: 652000\n",
      "  iterations_since_restore: 652\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.00759776536313\n",
      "    ram_util_percent: 49.62122905027933\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04037214508516901\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.878167015663266\n",
      "    mean_inference_ms: 1.7595869602590044\n",
      "    mean_raw_obs_processing_ms: 28.51967141072261\n",
      "  time_since_restore: 37275.88098406792\n",
      "  time_this_iter_s: 627.7075893878937\n",
      "  time_total_s: 37275.88098406792\n",
      "  timers:\n",
      "    learn_throughput: 1187.372\n",
      "    learn_time_ms: 842.196\n",
      "    load_throughput: 46857.606\n",
      "    load_time_ms: 21.341\n",
      "    sample_throughput: 1.672\n",
      "    sample_time_ms: 598247.569\n",
      "    update_time_ms: 2.625\n",
      "  timestamp: 1634880510\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 652000\n",
      "  training_iteration: 652\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   652</td><td style=\"text-align: right;\">         37275.9</td><td style=\"text-align: right;\">652000</td><td style=\"text-align: right;\">  9.5168</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -4.13</td><td style=\"text-align: right;\">             34.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 653000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_05-42-03\n",
      "  done: false\n",
      "  episode_len_mean: 31.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 9.543000000000005\n",
      "  episode_reward_min: -4.129999999999926\n",
      "  episodes_this_iter: 42\n",
      "  episodes_total: 2764\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8445600671424461\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8316127293639712\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02715366996582465\n",
      "          policy_loss: 0.019435849454667833\n",
      "          total_loss: 0.32188892662525176\n",
      "          vf_explained_var: 0.9759241938591003\n",
      "          vf_loss: 0.2878363059212764\n",
      "    num_agent_steps_sampled: 653000\n",
      "    num_agent_steps_trained: 653000\n",
      "    num_steps_sampled: 653000\n",
      "    num_steps_trained: 653000\n",
      "  iterations_since_restore: 653\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.0452196382429\n",
      "    ram_util_percent: 49.619724375538325\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040376604720138334\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.883188668252746\n",
      "    mean_inference_ms: 1.759727952164887\n",
      "    mean_raw_obs_processing_ms: 29.760519013186457\n",
      "  time_since_restore: 38089.4816429615\n",
      "  time_this_iter_s: 813.6006588935852\n",
      "  time_total_s: 38089.4816429615\n",
      "  timers:\n",
      "    learn_throughput: 1180.364\n",
      "    learn_time_ms: 847.196\n",
      "    load_throughput: 48102.023\n",
      "    load_time_ms: 20.789\n",
      "    sample_throughput: 1.655\n",
      "    sample_time_ms: 604254.602\n",
      "    update_time_ms: 2.564\n",
      "  timestamp: 1634881323\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 653000\n",
      "  training_iteration: 653\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   653</td><td style=\"text-align: right;\">         38089.5</td><td style=\"text-align: right;\">653000</td><td style=\"text-align: right;\">   9.543</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">               -4.13</td><td style=\"text-align: right;\">             31.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 654000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_05-54-46\n",
      "  done: false\n",
      "  episode_len_mean: 25.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 9.681900000000004\n",
      "  episode_reward_min: -2.1899999999999733\n",
      "  episodes_this_iter: 38\n",
      "  episodes_total: 2802\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.2668401007136691\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8744705379009247\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0038224250129465175\n",
      "          policy_loss: -0.19393511331743665\n",
      "          total_loss: -0.10533341252141529\n",
      "          vf_explained_var: 0.9912302494049072\n",
      "          vf_loss: 0.09250400521688991\n",
      "    num_agent_steps_sampled: 654000\n",
      "    num_agent_steps_trained: 654000\n",
      "    num_steps_sampled: 654000\n",
      "    num_steps_trained: 654000\n",
      "  iterations_since_restore: 654\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.96326905417814\n",
      "    ram_util_percent: 49.76345270890725\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04038053989024237\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.892755186688447\n",
      "    mean_inference_ms: 1.7598306675132276\n",
      "    mean_raw_obs_processing_ms: 30.86564312250056\n",
      "  time_since_restore: 38852.54063653946\n",
      "  time_this_iter_s: 763.0589935779572\n",
      "  time_total_s: 38852.54063653946\n",
      "  timers:\n",
      "    learn_throughput: 1180.97\n",
      "    learn_time_ms: 846.761\n",
      "    load_throughput: 49119.094\n",
      "    load_time_ms: 20.359\n",
      "    sample_throughput: 1.632\n",
      "    sample_time_ms: 612563.636\n",
      "    update_time_ms: 3.374\n",
      "  timestamp: 1634882086\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 654000\n",
      "  training_iteration: 654\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   654</td><td style=\"text-align: right;\">         38852.5</td><td style=\"text-align: right;\">654000</td><td style=\"text-align: right;\">  9.6819</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">              25.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 655000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_06-04-36\n",
      "  done: false\n",
      "  episode_len_mean: 23.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 9.721400000000001\n",
      "  episode_reward_min: 2.440000000000074\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 2832\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6334200503568346\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.060004633002811\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01194406770237898\n",
      "          policy_loss: -0.07710387259721756\n",
      "          total_loss: 0.14763955908517043\n",
      "          vf_explained_var: 0.9861479997634888\n",
      "          vf_loss: 0.227777866481079\n",
      "    num_agent_steps_sampled: 655000\n",
      "    num_agent_steps_trained: 655000\n",
      "    num_steps_sampled: 655000\n",
      "    num_steps_trained: 655000\n",
      "  iterations_since_restore: 655\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.10844233055886\n",
      "    ram_util_percent: 49.81105826397146\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040383034520927646\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.899561630786174\n",
      "    mean_inference_ms: 1.7599150908214904\n",
      "    mean_raw_obs_processing_ms: 31.656133771109726\n",
      "  time_since_restore: 39441.6330177784\n",
      "  time_this_iter_s: 589.0923812389374\n",
      "  time_total_s: 39441.6330177784\n",
      "  timers:\n",
      "    learn_throughput: 1127.85\n",
      "    learn_time_ms: 886.643\n",
      "    load_throughput: 48979.653\n",
      "    load_time_ms: 20.417\n",
      "    sample_throughput: 1.565\n",
      "    sample_time_ms: 638891.243\n",
      "    update_time_ms: 4.196\n",
      "  timestamp: 1634882676\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 655000\n",
      "  training_iteration: 655\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   655</td><td style=\"text-align: right;\">         39441.6</td><td style=\"text-align: right;\">655000</td><td style=\"text-align: right;\">  9.7214</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">                2.44</td><td style=\"text-align: right;\">             23.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 656000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_06-17-54\n",
      "  done: false\n",
      "  episode_len_mean: 27.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.92\n",
      "  episode_reward_mean: 9.632900000000003\n",
      "  episode_reward_min: -3.0999999999999552\n",
      "  episodes_this_iter: 41\n",
      "  episodes_total: 2873\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6334200503568346\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8239151047335731\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010390150010047665\n",
      "          policy_loss: 0.09625570492611991\n",
      "          total_loss: 0.1149227480093638\n",
      "          vf_explained_var: 0.9965494871139526\n",
      "          vf_loss: 0.02032486089091334\n",
      "    num_agent_steps_sampled: 656000\n",
      "    num_agent_steps_trained: 656000\n",
      "    num_steps_sampled: 656000\n",
      "    num_steps_trained: 656000\n",
      "  iterations_since_restore: 656\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.76259877085161\n",
      "    ram_util_percent: 49.97559262510974\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0403856314885783\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.90792810010926\n",
      "    mean_inference_ms: 1.760021482395324\n",
      "    mean_raw_obs_processing_ms: 32.78498934023469\n",
      "  time_since_restore: 40239.732166051865\n",
      "  time_this_iter_s: 798.099148273468\n",
      "  time_total_s: 40239.732166051865\n",
      "  timers:\n",
      "    learn_throughput: 1124.318\n",
      "    learn_time_ms: 889.428\n",
      "    load_throughput: 48504.091\n",
      "    load_time_ms: 20.617\n",
      "    sample_throughput: 1.573\n",
      "    sample_time_ms: 635544.032\n",
      "    update_time_ms: 4.123\n",
      "  timestamp: 1634883474\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 656000\n",
      "  training_iteration: 656\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   656</td><td style=\"text-align: right;\">         40239.7</td><td style=\"text-align: right;\">656000</td><td style=\"text-align: right;\">  9.6329</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">                -3.1</td><td style=\"text-align: right;\">             27.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 657000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_06-36-21\n",
      "  done: false\n",
      "  episode_len_mean: 24.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 9.719700000000001\n",
      "  episode_reward_min: -3.0999999999999552\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 2929\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6334200503568346\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7637151731385126\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004888338383689719\n",
      "          policy_loss: 0.007555161830451754\n",
      "          total_loss: 0.0683435980644491\n",
      "          vf_explained_var: 0.9935985207557678\n",
      "          vf_loss: 0.06532921737266911\n",
      "    num_agent_steps_sampled: 657000\n",
      "    num_agent_steps_trained: 657000\n",
      "    num_steps_sampled: 657000\n",
      "    num_steps_trained: 657000\n",
      "  iterations_since_restore: 657\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.3547181760608\n",
      "    ram_util_percent: 50.343001899936674\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040387910997745\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.919727848381935\n",
      "    mean_inference_ms: 1.7601394632425365\n",
      "    mean_raw_obs_processing_ms: 34.52853785832088\n",
      "  time_since_restore: 41346.591920137405\n",
      "  time_this_iter_s: 1106.8597540855408\n",
      "  time_total_s: 41346.591920137405\n",
      "  timers:\n",
      "    learn_throughput: 1133.423\n",
      "    learn_time_ms: 882.283\n",
      "    load_throughput: 50375.616\n",
      "    load_time_ms: 19.851\n",
      "    sample_throughput: 1.468\n",
      "    sample_time_ms: 681245.844\n",
      "    update_time_ms: 5.181\n",
      "  timestamp: 1634884581\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 657000\n",
      "  training_iteration: 657\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   657</td><td style=\"text-align: right;\">         41346.6</td><td style=\"text-align: right;\">657000</td><td style=\"text-align: right;\">  9.7197</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">                -3.1</td><td style=\"text-align: right;\">             24.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 658000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_06-46-09\n",
      "  done: false\n",
      "  episode_len_mean: 22.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.920000000000002\n",
      "  episode_reward_mean: 9.736\n",
      "  episode_reward_min: 2.05000000000008\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 2959\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3167100251784173\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3358636524942187\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025103837365192463\n",
      "          policy_loss: 0.05715130666891734\n",
      "          total_loss: 0.21674030708769956\n",
      "          vf_explained_var: 0.904036283493042\n",
      "          vf_loss: 0.16499700190292466\n",
      "    num_agent_steps_sampled: 658000\n",
      "    num_agent_steps_trained: 658000\n",
      "    num_steps_sampled: 658000\n",
      "    num_steps_trained: 658000\n",
      "  iterations_since_restore: 658\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.30821428571429\n",
      "    ram_util_percent: 50.4925\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040389332234902306\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.927731985851924\n",
      "    mean_inference_ms: 1.7602133195298137\n",
      "    mean_raw_obs_processing_ms: 35.27120921607557\n",
      "  time_since_restore: 41934.95011353493\n",
      "  time_this_iter_s: 588.358193397522\n",
      "  time_total_s: 41934.95011353493\n",
      "  timers:\n",
      "    learn_throughput: 1139.707\n",
      "    learn_time_ms: 877.418\n",
      "    load_throughput: 49497.321\n",
      "    load_time_ms: 20.203\n",
      "    sample_throughput: 1.476\n",
      "    sample_time_ms: 677608.21\n",
      "    update_time_ms: 5.139\n",
      "  timestamp: 1634885169\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 658000\n",
      "  training_iteration: 658\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   658</td><td style=\"text-align: right;\">           41935</td><td style=\"text-align: right;\">658000</td><td style=\"text-align: right;\">   9.736</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">                2.05</td><td style=\"text-align: right;\">             22.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 659000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_06-57-18\n",
      "  done: false\n",
      "  episode_len_mean: 22.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.920000000000002\n",
      "  episode_reward_mean: 9.705800000000002\n",
      "  episode_reward_min: 2.05000000000008\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 2992\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.47506503776762615\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9982755124568939\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021209225270797008\n",
      "          policy_loss: -0.03061943170097139\n",
      "          total_loss: 0.1851247446404563\n",
      "          vf_explained_var: 0.9814234375953674\n",
      "          vf_loss: 0.21565116834309367\n",
      "    num_agent_steps_sampled: 659000\n",
      "    num_agent_steps_trained: 659000\n",
      "    num_steps_sampled: 659000\n",
      "    num_steps_trained: 659000\n",
      "  iterations_since_restore: 659\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.62774869109947\n",
      "    ram_util_percent: 50.620209424083775\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040400979617417045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.937423540114768\n",
      "    mean_inference_ms: 1.7603348691362992\n",
      "    mean_raw_obs_processing_ms: 36.0509030117683\n",
      "  time_since_restore: 42604.11935019493\n",
      "  time_this_iter_s: 669.1692366600037\n",
      "  time_total_s: 42604.11935019493\n",
      "  timers:\n",
      "    learn_throughput: 1143.368\n",
      "    learn_time_ms: 874.609\n",
      "    load_throughput: 49585.036\n",
      "    load_time_ms: 20.167\n",
      "    sample_throughput: 1.454\n",
      "    sample_time_ms: 687796.844\n",
      "    update_time_ms: 5.613\n",
      "  timestamp: 1634885838\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 659000\n",
      "  training_iteration: 659\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   659</td><td style=\"text-align: right;\">         42604.1</td><td style=\"text-align: right;\">659000</td><td style=\"text-align: right;\">  9.7058</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">                2.05</td><td style=\"text-align: right;\">             22.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 660000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_07-05-34\n",
      "  done: false\n",
      "  episode_len_mean: 32.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.920000000000002\n",
      "  episode_reward_mean: 9.469200000000003\n",
      "  episode_reward_min: -0.6299999999999408\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 3018\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7125975566514392\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0845109323660533\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007419203132967514\n",
      "          policy_loss: -0.16277889421002734\n",
      "          total_loss: -0.0016262662079599168\n",
      "          vf_explained_var: 0.9854551553726196\n",
      "          vf_loss: 0.16671083230111333\n",
      "    num_agent_steps_sampled: 660000\n",
      "    num_agent_steps_trained: 660000\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "  iterations_since_restore: 660\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.12743988684583\n",
      "    ram_util_percent: 50.6944837340877\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040410187951782764\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.94607954471457\n",
      "    mean_inference_ms: 1.7604557262777027\n",
      "    mean_raw_obs_processing_ms: 36.65840931398594\n",
      "  time_since_restore: 43099.72123670578\n",
      "  time_this_iter_s: 495.601886510849\n",
      "  time_total_s: 43099.72123670578\n",
      "  timers:\n",
      "    learn_throughput: 1146.951\n",
      "    learn_time_ms: 871.877\n",
      "    load_throughput: 50684.861\n",
      "    load_time_ms: 19.73\n",
      "    sample_throughput: 1.456\n",
      "    sample_time_ms: 686903.479\n",
      "    update_time_ms: 5.738\n",
      "  timestamp: 1634886334\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 660\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   660</td><td style=\"text-align: right;\">         43099.7</td><td style=\"text-align: right;\">660000</td><td style=\"text-align: right;\">  9.4692</td><td style=\"text-align: right;\">                9.92</td><td style=\"text-align: right;\">               -0.63</td><td style=\"text-align: right;\">             32.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 661000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_07-23-10\n",
      "  done: false\n",
      "  episode_len_mean: 27.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 9.570900000000002\n",
      "  episode_reward_min: -0.6299999999999408\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 3071\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7125975566514392\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7566842337449392\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006314049733166232\n",
      "          policy_loss: -0.03515816488199764\n",
      "          total_loss: 0.02894158818655544\n",
      "          vf_explained_var: 0.9939906597137451\n",
      "          vf_loss: 0.06716721991284026\n",
      "    num_agent_steps_sampled: 661000\n",
      "    num_agent_steps_trained: 661000\n",
      "    num_steps_sampled: 661000\n",
      "    num_steps_trained: 661000\n",
      "  iterations_since_restore: 661\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.34996682149968\n",
      "    ram_util_percent: 50.73702720637027\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040424866914313684\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.963938134417596\n",
      "    mean_inference_ms: 1.7606859389230436\n",
      "    mean_raw_obs_processing_ms: 38.24518719771638\n",
      "  time_since_restore: 44155.58959031105\n",
      "  time_this_iter_s: 1055.8683536052704\n",
      "  time_total_s: 44155.58959031105\n",
      "  timers:\n",
      "    learn_throughput: 1148.514\n",
      "    learn_time_ms: 870.69\n",
      "    load_throughput: 53907.482\n",
      "    load_time_ms: 18.55\n",
      "    sample_throughput: 1.334\n",
      "    sample_time_ms: 749840.982\n",
      "    update_time_ms: 5.628\n",
      "  timestamp: 1634887390\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 661000\n",
      "  training_iteration: 661\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   661</td><td style=\"text-align: right;\">         44155.6</td><td style=\"text-align: right;\">661000</td><td style=\"text-align: right;\">  9.5709</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">               -0.63</td><td style=\"text-align: right;\">             27.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 662000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_07-41-59\n",
      "  done: false\n",
      "  episode_len_mean: 18.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 9.828300000000002\n",
      "  episode_reward_min: 8.790000000000003\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 3128\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7125975566514392\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.733229876226849\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006789631043932667\n",
      "          policy_loss: -0.017061716690659524\n",
      "          total_loss: 0.032740483929713564\n",
      "          vf_explained_var: 0.9951433539390564\n",
      "          vf_loss: 0.052296226678623096\n",
      "    num_agent_steps_sampled: 662000\n",
      "    num_agent_steps_trained: 662000\n",
      "    num_steps_sampled: 662000\n",
      "    num_steps_trained: 662000\n",
      "  iterations_since_restore: 662\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.50031036623216\n",
      "    ram_util_percent: 50.804220980757286\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04042673827645075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.98014961142397\n",
      "    mean_inference_ms: 1.7608589774836225\n",
      "    mean_raw_obs_processing_ms: 39.985240728651334\n",
      "  time_since_restore: 45285.03888821602\n",
      "  time_this_iter_s: 1129.4492979049683\n",
      "  time_total_s: 45285.03888821602\n",
      "  timers:\n",
      "    learn_throughput: 1141.945\n",
      "    learn_time_ms: 875.699\n",
      "    load_throughput: 51252.178\n",
      "    load_time_ms: 19.511\n",
      "    sample_throughput: 1.25\n",
      "    sample_time_ms: 800009.33\n",
      "    update_time_ms: 5.539\n",
      "  timestamp: 1634888519\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 662000\n",
      "  training_iteration: 662\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   662</td><td style=\"text-align: right;\">           45285</td><td style=\"text-align: right;\">662000</td><td style=\"text-align: right;\">  9.8283</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">                8.79</td><td style=\"text-align: right;\">             18.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 663000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_07-59-58\n",
      "  done: false\n",
      "  episode_len_mean: 17.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 9.813600000000001\n",
      "  episode_reward_min: 7.680000000000005\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 3182\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7125975566514392\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7216582192314995\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006407803821678714\n",
      "          policy_loss: -0.012524190462297864\n",
      "          total_loss: 0.09642268700732125\n",
      "          vf_explained_var: 0.9898824691772461\n",
      "          vf_loss: 0.11159727869348393\n",
      "    num_agent_steps_sampled: 663000\n",
      "    num_agent_steps_trained: 663000\n",
      "    num_steps_sampled: 663000\n",
      "    num_steps_trained: 663000\n",
      "  iterations_since_restore: 663\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.79954515919428\n",
      "    ram_util_percent: 50.98869395711501\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.040431416553805236\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.993735764760203\n",
      "    mean_inference_ms: 1.7609975904286954\n",
      "    mean_raw_obs_processing_ms: 41.48457414346535\n",
      "  time_since_restore: 46363.66435956955\n",
      "  time_this_iter_s: 1078.6254713535309\n",
      "  time_total_s: 46363.66435956955\n",
      "  timers:\n",
      "    learn_throughput: 1142.956\n",
      "    learn_time_ms: 874.925\n",
      "    load_throughput: 50597.609\n",
      "    load_time_ms: 19.764\n",
      "    sample_throughput: 1.21\n",
      "    sample_time_ms: 826512.264\n",
      "    update_time_ms: 5.55\n",
      "  timestamp: 1634889598\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 663000\n",
      "  training_iteration: 663\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   663</td><td style=\"text-align: right;\">         46363.7</td><td style=\"text-align: right;\">663000</td><td style=\"text-align: right;\">  9.8136</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">                7.68</td><td style=\"text-align: right;\">              17.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 664000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_08-17-33\n",
      "  done: false\n",
      "  episode_len_mean: 18.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 9.765500000000003\n",
      "  episode_reward_min: 7.680000000000005\n",
      "  episodes_this_iter: 53\n",
      "  episodes_total: 3235\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7125975566514392\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7512330035368602\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005789155934714726\n",
      "          policy_loss: 0.015682384454541735\n",
      "          total_loss: 0.1572984381682343\n",
      "          vf_explained_var: 0.9873506426811218\n",
      "          vf_loss: 0.1450030432186193\n",
      "    num_agent_steps_sampled: 664000\n",
      "    num_agent_steps_trained: 664000\n",
      "    num_steps_sampled: 664000\n",
      "    num_steps_trained: 664000\n",
      "  iterations_since_restore: 664\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.72682602921647\n",
      "    ram_util_percent: 51.09701195219124\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04043560644456225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.006726396359472\n",
      "    mean_inference_ms: 1.7611339136203719\n",
      "    mean_raw_obs_processing_ms: 42.958996639102715\n",
      "  time_since_restore: 47419.07588529587\n",
      "  time_this_iter_s: 1055.4115257263184\n",
      "  time_total_s: 47419.07588529587\n",
      "  timers:\n",
      "    learn_throughput: 1145.354\n",
      "    learn_time_ms: 873.093\n",
      "    load_throughput: 50979.454\n",
      "    load_time_ms: 19.616\n",
      "    sample_throughput: 1.169\n",
      "    sample_time_ms: 855750.597\n",
      "    update_time_ms: 4.787\n",
      "  timestamp: 1634890653\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 664000\n",
      "  training_iteration: 664\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   664</td><td style=\"text-align: right;\">         47419.1</td><td style=\"text-align: right;\">664000</td><td style=\"text-align: right;\">  9.7655</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">                7.68</td><td style=\"text-align: right;\">             18.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 665000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_08-35-33\n",
      "  done: false\n",
      "  episode_len_mean: 18.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 9.693800000000001\n",
      "  episode_reward_min: 3.3500000000000085\n",
      "  episodes_this_iter: 54\n",
      "  episodes_total: 3289\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7125975566514392\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7897934973239898\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007035060853624763\n",
      "          policy_loss: -0.044890537692440884\n",
      "          total_loss: 0.09121894902653165\n",
      "          vf_explained_var: 0.99031001329422\n",
      "          vf_loss: 0.13899425755565364\n",
      "    num_agent_steps_sampled: 665000\n",
      "    num_agent_steps_trained: 665000\n",
      "    num_steps_sampled: 665000\n",
      "    num_steps_trained: 665000\n",
      "  iterations_since_restore: 665\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.3268656716418\n",
      "    ram_util_percent: 51.20564568462037\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04043909318607927\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.02017658350108\n",
      "    mean_inference_ms: 1.761299720257243\n",
      "    mean_raw_obs_processing_ms: 44.4619131997908\n",
      "  time_since_restore: 48499.018471479416\n",
      "  time_this_iter_s: 1079.942586183548\n",
      "  time_total_s: 48499.018471479416\n",
      "  timers:\n",
      "    learn_throughput: 1198.223\n",
      "    learn_time_ms: 834.569\n",
      "    load_throughput: 48751.533\n",
      "    load_time_ms: 20.512\n",
      "    sample_throughput: 1.105\n",
      "    sample_time_ms: 904873.943\n",
      "    update_time_ms: 4.344\n",
      "  timestamp: 1634891733\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 665000\n",
      "  training_iteration: 665\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   665</td><td style=\"text-align: right;\">           48499</td><td style=\"text-align: right;\">665000</td><td style=\"text-align: right;\">  9.6938</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">                3.35</td><td style=\"text-align: right;\">             18.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_0b57e_00000:\n",
      "  agent_timesteps_total: 666000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-22_08-54-28\n",
      "  done: false\n",
      "  episode_len_mean: 18.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.91\n",
      "  episode_reward_mean: 9.740700000000002\n",
      "  episode_reward_min: 3.3500000000000085\n",
      "  episodes_this_iter: 57\n",
      "  episodes_total: 3346\n",
      "  experiment_id: e6d1265ae3d54b7b8f850cfb5dadc7e7\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7125975566514392\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.73597548339102\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006207044876574308\n",
      "          policy_loss: -0.0427838691820701\n",
      "          total_loss: 0.007410994379056825\n",
      "          vf_explained_var: 0.995331883430481\n",
      "          vf_loss: 0.053131493512127134\n",
      "    num_agent_steps_sampled: 666000\n",
      "    num_agent_steps_trained: 666000\n",
      "    num_steps_sampled: 666000\n",
      "    num_steps_trained: 666000\n",
      "  iterations_since_restore: 666\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.46672839506174\n",
      "    ram_util_percent: 51.33123456790123\n",
      "  pid: 11962\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04044242031611086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.03506000480437\n",
      "    mean_inference_ms: 1.7614927340375561\n",
      "    mean_raw_obs_processing_ms: 46.053551094068716\n",
      "  time_since_restore: 49634.141154527664\n",
      "  time_this_iter_s: 1135.1226830482483\n",
      "  time_total_s: 49634.141154527664\n",
      "  timers:\n",
      "    learn_throughput: 1197.895\n",
      "    learn_time_ms: 834.798\n",
      "    load_throughput: 49767.602\n",
      "    load_time_ms: 20.093\n",
      "    sample_throughput: 1.065\n",
      "    sample_time_ms: 938576.943\n",
      "    update_time_ms: 4.487\n",
      "  timestamp: 1634892868\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 666000\n",
      "  training_iteration: 666\n",
      "  trial_id: 0b57e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 24.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/23.54 GiB heap, 0.0/11.77 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-21_19-06-49<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_0b57e_00000</td><td>RUNNING </td><td>192.168.3.5:11962</td><td style=\"text-align: right;\">   666</td><td style=\"text-align: right;\">         49634.1</td><td style=\"text-align: right;\">666000</td><td style=\"text-align: right;\">  9.7407</td><td style=\"text-align: right;\">                9.91</td><td style=\"text-align: right;\">                3.35</td><td style=\"text-align: right;\">             18.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=11957)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "2021-10-22 09:01:29,336\tWARNING tune.py:519 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2021-10-22 09:01:29,336\tWARNING tune.py:519 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "Process _WandbLoggingProcess-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/tune/integration/wandb.py\", line 200, in run\n",
      "    result = self.queue.get()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/queues.py\", line 94, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/util.py\", line 322, in _exit_function\n",
      "    p.join()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/process.py\", line 140, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/popen_fork.py\", line 48, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/popen_fork.py\", line 28, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt\n",
      "Process wandb_internal:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/site-packages/wandb/sdk/internal/internal.py\", line 152, in wandb_internal\n",
      "    thread.join()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/threading.py\", line 1032, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/root/miniconda/envs/py37/lib/python3.7/threading.py\", line 1048, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "2021-10-22 09:01:30,010\tWARNING tune.py:519 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11848/3010183041.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         },\n\u001b[0;32m---> 29\u001b[0;31m         loggers=[WandbLogger])\n\u001b[0m",
      "\u001b[0;32m~/miniconda/envs/py37/lib/python3.7/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint, _remote)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0mtune_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVerbosity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV1_EXPERIMENT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0m_report_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_reporter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py37/lib/python3.7/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    552\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_staging_grace_period\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_no_available_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py37/lib/python3.7/site-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36m_process_events\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0;31m#  fetch_result functionality so that we don't timeout on fetch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m             trial = self.trial_executor.get_next_available_trial(\n\u001b[0;32m--> 675\u001b[0;31m                 timeout=timeout)  # blocking\n\u001b[0m\u001b[1;32m    676\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py37/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\u001b[0m in \u001b[0;36mget_next_available_trial\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;31m# See https://github.com/ray-project/ray/issues/4211 for details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m         \u001b[0mready\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffled_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclient_mode_should_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda/envs/py37/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_refs, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   1791\u001b[0m             \u001b[0mtimeout_milliseconds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m             \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_task_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m             \u001b[0mfetch_local\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1794\u001b[0m         )\n\u001b[1;32m   1795\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mready_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "analysis = tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 1,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             #\"gamma\": 0.99,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO MultiTask (C32) pretrained (AnnaCNN) (3 noops after placement) r: -0.01\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
