{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *\n",
    "from wrappers_2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9fd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=2, stride=2, padding=0),  \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(), \n",
    "            nn.Conv2d(128, 256, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import one_hot\n",
    "\n",
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        visual_features_dim = 512\n",
    "        target_features_dim = 9 * 11 * 11 \n",
    "        self.visual_encoder = VisualEncoder()\n",
    "        self.visual_encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AngelaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.target_encoder = nn.Sequential(\n",
    "            nn.Conv3d(7, 1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        policy_hidden_dim = 256 \n",
    "        self.policy_network = nn.Sequential(\n",
    "            nn.Linear(visual_features_dim + target_features_dim, 1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, policy_hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "            nn.ELU(),\n",
    "            #nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "            #nn.ELU(),\n",
    "        )\n",
    "        self.action_head = nn.Linear(policy_hidden_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(policy_hidden_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.visual_encoder.cuda()\n",
    "            self.target_encoder.cuda()\n",
    "            self.policy_network.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs']\n",
    "        pov = obs['pov'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        target = one_hot(obs['target_grid'].long(), num_classes=7).permute(0, 4, 1, 2, 3).float()\n",
    "        if self.use_cuda:\n",
    "            pov.cuda()\n",
    "            target.cuda()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            visual_features = self.visual_encoder(pov)\n",
    "            \n",
    "        target_features = self.target_encoder(target)\n",
    "        target_features = target_features.reshape(target_features.shape[0], -1)\n",
    "        features = torch.cat([visual_features, target_features], dim=1)\n",
    "        features = self.policy_network(features)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc09c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualObservationWrapper(ObsWrapper):\n",
    "    def __init__(self, env, include_target=False):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = {   \n",
    "            'pov': gym.spaces.Box(low=0, high=255, shape=(64, 64, 3)),\n",
    "            'inventory': gym.spaces.Box(low=0.0, high=20.0, shape=(6,)),\n",
    "            'compass': gym.spaces.Box(low=-180.0, high=180.0, shape=(1,))\n",
    "        }\n",
    "        if include_target:\n",
    "            self.observation_space['target_grid'] = \\\n",
    "                gym.spaces.Box(low=0, high=6, shape=(9, 11, 11))\n",
    "        self.observation_space = gym.spaces.Dict(self.observation_space)\n",
    "\n",
    "    def observation(self, obs, reward=None, done=None, info=None):\n",
    "        if info is not None:\n",
    "            if 'target_grid' in info:\n",
    "                target_grid = info['target_grid']\n",
    "                del info['target_grid']\n",
    "            else:\n",
    "                logger.error(f'info: {info}')\n",
    "                if hasattr(self.unwrapped, 'should_reset'):\n",
    "                    self.unwrapped.should_reset(True)\n",
    "                target_grid = self.env.unwrapped.tasks.current.target_grid\n",
    "        else:\n",
    "            target_grid = self.env.unwrapped.tasks.current.target_grid\n",
    "        return {\n",
    "            'pov': obs['pov'].astype(np.float32),\n",
    "            'inventory': obs['inventory'],\n",
    "            'compass': np.array([obs['compass']['angle'].item()]),\n",
    "            'target_grid': target_grid\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def reward(self, rew):\n",
    "        if rew == 0:\n",
    "            rew = -0.01\n",
    "        if abs(rew) == 1:\n",
    "            rew /= 10\n",
    "        return rew\n",
    "    \n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=250)\n",
    "    env.update_taskset(TaskSet(preset=[\"C139\"]))\n",
    "    #env = PovOnlyWrapper(env)\n",
    "    env = VisualObservationWrapper(env, include_target=True)\n",
    "    env = SelectAndPlace(env)\n",
    "    env = Discretization(env, flat_action_space('human-level'))\n",
    "    env = RewardWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n",
      "2021-11-10 10:35:37,367\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-11-10 10:35:37,382\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 4.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to auto resume run with id 9ef95_00000 but id f147d_00000 is set.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=128236)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=128236)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO MultiTask (C139) pretrained (AngelaCNN) (3 noops after placement) r: -0.01 div10</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/f147d_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/f147d_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20211110_103538-f147d_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=128236)\u001b[0m 2021-11-10 10:35:40,945\tWARNING ppo.py:143 -- `train_batch_size` (1000) cannot be achieved with your other settings (num_workers=3 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 333.\n",
      "\u001b[2m\u001b[36m(pid=128236)\u001b[0m 2021-11-10 10:35:40,945\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=128236)\u001b[0m 2021-11-10 10:35:40,945\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=128240)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=128240)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=128243)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=128243)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=128238)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=128238)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=128236)\u001b[0m 2021-11-10 10:35:46,789\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=128240)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=128240)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=128243)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=128243)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=128238)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=128238)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=128240)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=128240)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=128240)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=128240)\u001b[0m   warn(RuntimeWarning(msg))\n",
      "\u001b[2m\u001b[36m(pid=128243)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=128243)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=128243)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=128243)\u001b[0m   warn(RuntimeWarning(msg))\n",
      "\u001b[2m\u001b[36m(pid=128238)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=128238)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=128238)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=128238)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 1998\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-37-13\n",
      "  done: false\n",
      "  episode_len_mean: 99.83333333333333\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.7499999999999996\n",
      "  episode_reward_mean: -0.8227777777777784\n",
      "  episode_reward_min: -1.0900000000000007\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 18\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8846336500985283\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0059371544273840345\n",
      "          policy_loss: 0.01761572343252954\n",
      "          total_loss: 0.0038777662529831843\n",
      "          vf_explained_var: -0.03886004909873009\n",
      "          vf_loss: 0.013920948527465086\n",
      "    num_agent_steps_sampled: 1998\n",
      "    num_agent_steps_trained: 1998\n",
      "    num_steps_sampled: 1998\n",
      "    num_steps_trained: 1998\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.71612903225804\n",
      "    ram_util_percent: 24.95241935483871\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04369696636667017\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 117.17439245903627\n",
      "    mean_inference_ms: 2.5807798415169243\n",
      "    mean_raw_obs_processing_ms: 0.6230071685958779\n",
      "  time_since_restore: 86.59278345108032\n",
      "  time_this_iter_s: 86.59278345108032\n",
      "  time_total_s: 86.59278345108032\n",
      "  timers:\n",
      "    learn_throughput: 1158.835\n",
      "    learn_time_ms: 1724.145\n",
      "    load_throughput: 57351.232\n",
      "    load_time_ms: 34.838\n",
      "    sample_throughput: 23.558\n",
      "    sample_time_ms: 84811.85\n",
      "    update_time_ms: 6.329\n",
      "  timestamp: 1636540633\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1998\n",
      "  training_iteration: 1\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         86.5928</td><td style=\"text-align: right;\">1998</td><td style=\"text-align: right;\">-0.822778</td><td style=\"text-align: right;\">                0.75</td><td style=\"text-align: right;\">               -1.09</td><td style=\"text-align: right;\">           99.8333</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 3996\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-37-49\n",
      "  done: false\n",
      "  episode_len_mean: 99.6923076923077\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.7499999999999996\n",
      "  episode_reward_mean: -0.8494871794871801\n",
      "  episode_reward_min: -1.3400000000000007\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 39\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8730496065957207\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008612706028534482\n",
      "          policy_loss: -0.03290103610072817\n",
      "          total_loss: -0.025930298687446684\n",
      "          vf_explained_var: 0.3067989945411682\n",
      "          vf_loss: 0.03397869317060603\n",
      "    num_agent_steps_sampled: 3996\n",
      "    num_agent_steps_trained: 3996\n",
      "    num_steps_sampled: 3996\n",
      "    num_steps_trained: 3996\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.43076923076924\n",
      "    ram_util_percent: 30.446153846153848\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04368065283245729\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 97.41851421877239\n",
      "    mean_inference_ms: 2.56335518464645\n",
      "    mean_raw_obs_processing_ms: 0.6327024571599527\n",
      "  time_since_restore: 122.68599462509155\n",
      "  time_this_iter_s: 36.09321117401123\n",
      "  time_total_s: 122.68599462509155\n",
      "  timers:\n",
      "    learn_throughput: 1070.021\n",
      "    learn_time_ms: 1867.254\n",
      "    load_throughput: 63249.087\n",
      "    load_time_ms: 31.589\n",
      "    sample_throughput: 33.624\n",
      "    sample_time_ms: 59422.241\n",
      "    update_time_ms: 8.703\n",
      "  timestamp: 1636540669\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3996\n",
      "  training_iteration: 2\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         122.686</td><td style=\"text-align: right;\">3996</td><td style=\"text-align: right;\">-0.849487</td><td style=\"text-align: right;\">                0.75</td><td style=\"text-align: right;\">               -1.34</td><td style=\"text-align: right;\">           99.6923</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 5994\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-38-22\n",
      "  done: false\n",
      "  episode_len_mean: 98.91525423728814\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.7899999999999994\n",
      "  episode_reward_mean: -0.8749152542372889\n",
      "  episode_reward_min: -1.910000000000001\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 59\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8548530101776124\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008013804037931435\n",
      "          policy_loss: -0.038929452108485356\n",
      "          total_loss: -0.041990378747383754\n",
      "          vf_explained_var: 0.19605733454227448\n",
      "          vf_loss: 0.02388484131855269\n",
      "    num_agent_steps_sampled: 5994\n",
      "    num_agent_steps_trained: 5994\n",
      "    num_steps_sampled: 5994\n",
      "    num_steps_trained: 5994\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 94.32173913043479\n",
      "    ram_util_percent: 30.66304347826087\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04369044040994682\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 87.14197214771626\n",
      "    mean_inference_ms: 2.5524045723542703\n",
      "    mean_raw_obs_processing_ms: 0.6265698594329314\n",
      "  time_since_restore: 155.40443634986877\n",
      "  time_this_iter_s: 32.71844172477722\n",
      "  time_total_s: 155.40443634986877\n",
      "  timers:\n",
      "    learn_throughput: 1095.881\n",
      "    learn_time_ms: 1823.191\n",
      "    load_throughput: 61819.264\n",
      "    load_time_ms: 32.32\n",
      "    sample_throughput: 40.02\n",
      "    sample_time_ms: 49924.999\n",
      "    update_time_ms: 8.239\n",
      "  timestamp: 1636540702\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5994\n",
      "  training_iteration: 3\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         155.404</td><td style=\"text-align: right;\">5994</td><td style=\"text-align: right;\">-0.874915</td><td style=\"text-align: right;\">                0.79</td><td style=\"text-align: right;\">               -1.91</td><td style=\"text-align: right;\">           98.9153</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 7992\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-38-57\n",
      "  done: false\n",
      "  episode_len_mean: 97.90123456790124\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.3500000000000036\n",
      "  episode_reward_mean: -0.7911111111111117\n",
      "  episode_reward_min: -1.910000000000001\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 81\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.827227659452529\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008567488513662734\n",
      "          policy_loss: -0.06361453935858749\n",
      "          total_loss: 0.09278932550833338\n",
      "          vf_explained_var: 0.11655293405056\n",
      "          vf_loss: 0.18296264303582055\n",
      "    num_agent_steps_sampled: 7992\n",
      "    num_agent_steps_trained: 7992\n",
      "    num_steps_sampled: 7992\n",
      "    num_steps_trained: 7992\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.07800000000002\n",
      "    ram_util_percent: 30.503999999999998\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043903986669506595\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 80.18248044399283\n",
      "    mean_inference_ms: 2.545612437430145\n",
      "    mean_raw_obs_processing_ms: 0.6295215430266987\n",
      "  time_since_restore: 190.4130983352661\n",
      "  time_this_iter_s: 35.00866198539734\n",
      "  time_total_s: 190.4130983352661\n",
      "  timers:\n",
      "    learn_throughput: 1099.592\n",
      "    learn_time_ms: 1817.038\n",
      "    load_throughput: 62136.224\n",
      "    load_time_ms: 32.155\n",
      "    sample_throughput: 43.689\n",
      "    sample_time_ms: 45732.387\n",
      "    update_time_ms: 9.113\n",
      "  timestamp: 1636540737\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7992\n",
      "  training_iteration: 4\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         190.413</td><td style=\"text-align: right;\">7992</td><td style=\"text-align: right;\">-0.791111</td><td style=\"text-align: right;\">                2.35</td><td style=\"text-align: right;\">               -1.91</td><td style=\"text-align: right;\">           97.9012</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 9990\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-39-35\n",
      "  done: false\n",
      "  episode_len_mean: 96.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.3500000000000036\n",
      "  episode_reward_mean: -0.7246000000000005\n",
      "  episode_reward_min: -1.910000000000001\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 102\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8010340690612794\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009017899413901484\n",
      "          policy_loss: -0.04689217706521352\n",
      "          total_loss: 0.0741770662367344\n",
      "          vf_explained_var: 0.3636839985847473\n",
      "          vf_loss: 0.1472760030955431\n",
      "    num_agent_steps_sampled: 9990\n",
      "    num_agent_steps_trained: 9990\n",
      "    num_steps_sampled: 9990\n",
      "    num_steps_trained: 9990\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 91.06545454545453\n",
      "    ram_util_percent: 30.45636363636364\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04395244495442237\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 74.94704282780626\n",
      "    mean_inference_ms: 2.537508205583732\n",
      "    mean_raw_obs_processing_ms: 0.6307623810177766\n",
      "  time_since_restore: 228.8005223274231\n",
      "  time_this_iter_s: 38.38742399215698\n",
      "  time_total_s: 228.8005223274231\n",
      "  timers:\n",
      "    learn_throughput: 1111.203\n",
      "    learn_time_ms: 1798.051\n",
      "    load_throughput: 62450.588\n",
      "    load_time_ms: 31.993\n",
      "    sample_throughput: 45.506\n",
      "    sample_time_ms: 43906.399\n",
      "    update_time_ms: 11.834\n",
      "  timestamp: 1636540775\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9990\n",
      "  training_iteration: 5\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         228.801</td><td style=\"text-align: right;\">9990</td><td style=\"text-align: right;\"> -0.7246</td><td style=\"text-align: right;\">                2.35</td><td style=\"text-align: right;\">               -1.91</td><td style=\"text-align: right;\">             96.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 11988\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-40-02\n",
      "  done: false\n",
      "  episode_len_mean: 95.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.950000000000001\n",
      "  episode_reward_mean: -0.5433000000000004\n",
      "  episode_reward_min: -1.910000000000001\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 123\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7760098877407255\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010169733748337316\n",
      "          policy_loss: -0.06027013790749368\n",
      "          total_loss: 0.12585413730925038\n",
      "          vf_explained_var: 0.26121073961257935\n",
      "          vf_loss: 0.21185042609771093\n",
      "    num_agent_steps_sampled: 11988\n",
      "    num_agent_steps_trained: 11988\n",
      "    num_steps_sampled: 11988\n",
      "    num_steps_trained: 11988\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.32307692307693\n",
      "    ram_util_percent: 30.256410256410252\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04400661696872569\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 63.479481708738575\n",
      "    mean_inference_ms: 2.524561714840479\n",
      "    mean_raw_obs_processing_ms: 0.6343104585135728\n",
      "  time_since_restore: 254.6321258544922\n",
      "  time_this_iter_s: 25.831603527069092\n",
      "  time_total_s: 254.6321258544922\n",
      "  timers:\n",
      "    learn_throughput: 1117.852\n",
      "    learn_time_ms: 1787.356\n",
      "    load_throughput: 62299.516\n",
      "    load_time_ms: 32.071\n",
      "    sample_throughput: 49.217\n",
      "    sample_time_ms: 40595.523\n",
      "    update_time_ms: 11.658\n",
      "  timestamp: 1636540802\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11988\n",
      "  training_iteration: 6\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         254.632</td><td style=\"text-align: right;\">11988</td><td style=\"text-align: right;\"> -0.5433</td><td style=\"text-align: right;\">                2.95</td><td style=\"text-align: right;\">               -1.91</td><td style=\"text-align: right;\">             95.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 13986\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-40-28\n",
      "  done: false\n",
      "  episode_len_mean: 94.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.610000000000003\n",
      "  episode_reward_mean: -0.20210000000000009\n",
      "  episode_reward_min: -1.910000000000001\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 145\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7535278763089863\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010370750505143347\n",
      "          policy_loss: -0.015740461718468438\n",
      "          total_loss: 0.32030851720344455\n",
      "          vf_explained_var: 0.37943047285079956\n",
      "          vf_loss: 0.36151010933376493\n",
      "    num_agent_steps_sampled: 13986\n",
      "    num_agent_steps_trained: 13986\n",
      "    num_steps_sampled: 13986\n",
      "    num_steps_trained: 13986\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.68888888888888\n",
      "    ram_util_percent: 30.16388888888889\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04399084028796018\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 57.76060876232542\n",
      "    mean_inference_ms: 2.520721547836378\n",
      "    mean_raw_obs_processing_ms: 0.6405042984022897\n",
      "  time_since_restore: 280.4549705982208\n",
      "  time_this_iter_s: 25.822844743728638\n",
      "  time_total_s: 280.4549705982208\n",
      "  timers:\n",
      "    learn_throughput: 1122.817\n",
      "    learn_time_ms: 1779.454\n",
      "    load_throughput: 61932.043\n",
      "    load_time_ms: 32.261\n",
      "    sample_throughput: 52.264\n",
      "    sample_time_ms: 38229.34\n",
      "    update_time_ms: 11.881\n",
      "  timestamp: 1636540828\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13986\n",
      "  training_iteration: 7\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         280.455</td><td style=\"text-align: right;\">13986</td><td style=\"text-align: right;\"> -0.2021</td><td style=\"text-align: right;\">                4.61</td><td style=\"text-align: right;\">               -1.91</td><td style=\"text-align: right;\">             94.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 15984\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-40-54\n",
      "  done: false\n",
      "  episode_len_mean: 93.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.610000000000003\n",
      "  episode_reward_mean: 0.21490000000000056\n",
      "  episode_reward_min: -1.760000000000001\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 167\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7281327100027175\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012083349209859456\n",
      "          policy_loss: -0.07540986440366222\n",
      "          total_loss: 0.2409613195629347\n",
      "          vf_explained_var: 0.42334964871406555\n",
      "          vf_loss: 0.34123583993031864\n",
      "    num_agent_steps_sampled: 15984\n",
      "    num_agent_steps_trained: 15984\n",
      "    num_steps_sampled: 15984\n",
      "    num_steps_trained: 15984\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.21315789473684\n",
      "    ram_util_percent: 30.22105263157895\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04401188434637703\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 54.168506088318374\n",
      "    mean_inference_ms: 2.520263134041666\n",
      "    mean_raw_obs_processing_ms: 0.6528279647540048\n",
      "  time_since_restore: 306.8904941082001\n",
      "  time_this_iter_s: 26.435523509979248\n",
      "  time_total_s: 306.8904941082001\n",
      "  timers:\n",
      "    learn_throughput: 1124.502\n",
      "    learn_time_ms: 1776.786\n",
      "    load_throughput: 61510.317\n",
      "    load_time_ms: 32.482\n",
      "    sample_throughput: 54.697\n",
      "    sample_time_ms: 36528.844\n",
      "    update_time_ms: 11.387\n",
      "  timestamp: 1636540854\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15984\n",
      "  training_iteration: 8\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">          306.89</td><td style=\"text-align: right;\">15984</td><td style=\"text-align: right;\">  0.2149</td><td style=\"text-align: right;\">                4.61</td><td style=\"text-align: right;\">               -1.76</td><td style=\"text-align: right;\">             93.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 17982\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-41-20\n",
      "  done: false\n",
      "  episode_len_mean: 92.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.730000000000013\n",
      "  episode_reward_mean: 0.6155000000000009\n",
      "  episode_reward_min: -1.760000000000001\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 188\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6759758608681814\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012136201706455636\n",
      "          policy_loss: -0.047241078512299625\n",
      "          total_loss: 0.28339605209018504\n",
      "          vf_explained_var: 0.44020822644233704\n",
      "          vf_loss: 0.35496964752674104\n",
      "    num_agent_steps_sampled: 17982\n",
      "    num_agent_steps_trained: 17982\n",
      "    num_steps_sampled: 17982\n",
      "    num_steps_trained: 17982\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.35945945945946\n",
      "    ram_util_percent: 30.23243243243243\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0440227869905285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 51.26893747397462\n",
      "    mean_inference_ms: 2.523427764173603\n",
      "    mean_raw_obs_processing_ms: 0.6617457879097843\n",
      "  time_since_restore: 332.7347717285156\n",
      "  time_this_iter_s: 25.84427762031555\n",
      "  time_total_s: 332.7347717285156\n",
      "  timers:\n",
      "    learn_throughput: 1127.427\n",
      "    learn_time_ms: 1772.176\n",
      "    load_throughput: 61387.602\n",
      "    load_time_ms: 32.547\n",
      "    sample_throughput: 56.855\n",
      "    sample_time_ms: 35141.881\n",
      "    update_time_ms: 12.532\n",
      "  timestamp: 1636540880\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17982\n",
      "  training_iteration: 9\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         332.735</td><td style=\"text-align: right;\">17982</td><td style=\"text-align: right;\">  0.6155</td><td style=\"text-align: right;\">                4.73</td><td style=\"text-align: right;\">               -1.76</td><td style=\"text-align: right;\">             92.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 19980\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-41-46\n",
      "  done: false\n",
      "  episode_len_mean: 93.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.570000000000007\n",
      "  episode_reward_mean: 1.1367000000000018\n",
      "  episode_reward_min: -1.610000000000001\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 209\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6339369194848197\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012751676368233688\n",
      "          policy_loss: 0.061154713783235774\n",
      "          total_loss: 0.4211538431545099\n",
      "          vf_explained_var: 0.49939966201782227\n",
      "          vf_loss: 0.38378816426155116\n",
      "    num_agent_steps_sampled: 19980\n",
      "    num_agent_steps_trained: 19980\n",
      "    num_steps_sampled: 19980\n",
      "    num_steps_trained: 19980\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.26756756756757\n",
      "    ram_util_percent: 30.22432432432432\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04407652348614507\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 48.75506005391153\n",
      "    mean_inference_ms: 2.5290466424951283\n",
      "    mean_raw_obs_processing_ms: 0.6706915527817192\n",
      "  time_since_restore: 358.57517862319946\n",
      "  time_this_iter_s: 25.840406894683838\n",
      "  time_total_s: 358.57517862319946\n",
      "  timers:\n",
      "    learn_throughput: 1128.663\n",
      "    learn_time_ms: 1770.236\n",
      "    load_throughput: 61053.038\n",
      "    load_time_ms: 32.726\n",
      "    sample_throughput: 58.711\n",
      "    sample_time_ms: 34031.082\n",
      "    update_time_ms: 12.263\n",
      "  timestamp: 1636540906\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19980\n",
      "  training_iteration: 10\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         358.575</td><td style=\"text-align: right;\">19980</td><td style=\"text-align: right;\">  1.1367</td><td style=\"text-align: right;\">                6.57</td><td style=\"text-align: right;\">               -1.61</td><td style=\"text-align: right;\">             93.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 21978\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-42-12\n",
      "  done: false\n",
      "  episode_len_mean: 93.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.770000000000017\n",
      "  episode_reward_mean: 1.6699000000000033\n",
      "  episode_reward_min: -1.5600000000000007\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 230\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6059079578944613\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014474807855054663\n",
      "          policy_loss: -0.04128521340233939\n",
      "          total_loss: 0.4358946313460668\n",
      "          vf_explained_var: 0.5607660412788391\n",
      "          vf_loss: 0.5003439644262904\n",
      "    num_agent_steps_sampled: 21978\n",
      "    num_agent_steps_trained: 21978\n",
      "    num_steps_sampled: 21978\n",
      "    num_steps_trained: 21978\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.34864864864863\n",
      "    ram_util_percent: 30.18648648648649\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04423979277818504\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 46.84678889412019\n",
      "    mean_inference_ms: 2.5336469853536094\n",
      "    mean_raw_obs_processing_ms: 0.6801350696940406\n",
      "  time_since_restore: 384.2058928012848\n",
      "  time_this_iter_s: 25.630714178085327\n",
      "  time_total_s: 384.2058928012848\n",
      "  timers:\n",
      "    learn_throughput: 1129.587\n",
      "    learn_time_ms: 1768.788\n",
      "    load_throughput: 61405.83\n",
      "    load_time_ms: 32.538\n",
      "    sample_throughput: 71.52\n",
      "    sample_time_ms: 27936.336\n",
      "    update_time_ms: 13.043\n",
      "  timestamp: 1636540932\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21978\n",
      "  training_iteration: 11\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         384.206</td><td style=\"text-align: right;\">21978</td><td style=\"text-align: right;\">  1.6699</td><td style=\"text-align: right;\">                6.77</td><td style=\"text-align: right;\">               -1.56</td><td style=\"text-align: right;\">             93.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 23976\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-42-37\n",
      "  done: false\n",
      "  episode_len_mean: 94.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.770000000000017\n",
      "  episode_reward_mean: 1.938300000000004\n",
      "  episode_reward_min: -1.4300000000000006\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 251\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5799201897212436\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013117755277246387\n",
      "          policy_loss: -0.021668550230207896\n",
      "          total_loss: 0.3918440913160642\n",
      "          vf_explained_var: 0.554253876209259\n",
      "          vf_loss: 0.43668829245226726\n",
      "    num_agent_steps_sampled: 23976\n",
      "    num_agent_steps_trained: 23976\n",
      "    num_steps_sampled: 23976\n",
      "    num_steps_trained: 23976\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.22571428571428\n",
      "    ram_util_percent: 30.140000000000004\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04438773938087909\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 45.30682120399425\n",
      "    mean_inference_ms: 2.5369403984724097\n",
      "    mean_raw_obs_processing_ms: 0.6859919679820908\n",
      "  time_since_restore: 409.0990707874298\n",
      "  time_this_iter_s: 24.89317798614502\n",
      "  time_total_s: 409.0990707874298\n",
      "  timers:\n",
      "    learn_throughput: 1146.619\n",
      "    learn_time_ms: 1742.514\n",
      "    load_throughput: 60245.01\n",
      "    load_time_ms: 33.165\n",
      "    sample_throughput: 74.435\n",
      "    sample_time_ms: 26842.116\n",
      "    update_time_ms: 13.019\n",
      "  timestamp: 1636540957\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23976\n",
      "  training_iteration: 12\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         409.099</td><td style=\"text-align: right;\">23976</td><td style=\"text-align: right;\">  1.9383</td><td style=\"text-align: right;\">                6.77</td><td style=\"text-align: right;\">               -1.43</td><td style=\"text-align: right;\">             94.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 25974\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-43-02\n",
      "  done: false\n",
      "  episode_len_mean: 95.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.770000000000017\n",
      "  episode_reward_mean: 2.381300000000005\n",
      "  episode_reward_min: -1.8200000000000007\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 271\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.566561840829395\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017127647586102173\n",
      "          policy_loss: -0.014942598076803343\n",
      "          total_loss: 0.6442029872997885\n",
      "          vf_explained_var: 0.46783003211021423\n",
      "          vf_loss: 0.6813856733696801\n",
      "    num_agent_steps_sampled: 25974\n",
      "    num_agent_steps_trained: 25974\n",
      "    num_steps_sampled: 25974\n",
      "    num_steps_trained: 25974\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.9972222222222\n",
      "    ram_util_percent: 30.14166666666667\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044499887569229984\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 44.037431560169004\n",
      "    mean_inference_ms: 2.539971824403309\n",
      "    mean_raw_obs_processing_ms: 0.6908661321903777\n",
      "  time_since_restore: 434.2375226020813\n",
      "  time_this_iter_s: 25.13845181465149\n",
      "  time_total_s: 434.2375226020813\n",
      "  timers:\n",
      "    learn_throughput: 1146.074\n",
      "    learn_time_ms: 1743.342\n",
      "    load_throughput: 60330.668\n",
      "    load_time_ms: 33.117\n",
      "    sample_throughput: 76.603\n",
      "    sample_time_ms: 26082.651\n",
      "    update_time_ms: 13.634\n",
      "  timestamp: 1636540982\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25974\n",
      "  training_iteration: 13\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         434.238</td><td style=\"text-align: right;\">25974</td><td style=\"text-align: right;\">  2.3813</td><td style=\"text-align: right;\">                6.77</td><td style=\"text-align: right;\">               -1.82</td><td style=\"text-align: right;\">             95.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 27972\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-43-28\n",
      "  done: false\n",
      "  episode_len_mean: 95.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.310000000000013\n",
      "  episode_reward_mean: 3.007800000000006\n",
      "  episode_reward_min: -1.8200000000000007\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 292\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5277411824180964\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015635706187381585\n",
      "          policy_loss: -0.040556291952019645\n",
      "          total_loss: 0.5555570735817864\n",
      "          vf_explained_var: 0.6243453025817871\n",
      "          vf_loss: 0.6182636367423194\n",
      "    num_agent_steps_sampled: 27972\n",
      "    num_agent_steps_trained: 27972\n",
      "    num_steps_sampled: 27972\n",
      "    num_steps_trained: 27972\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.17567567567566\n",
      "    ram_util_percent: 30.113513513513514\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04458674270999565\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 42.91506970115508\n",
      "    mean_inference_ms: 2.5431783954844813\n",
      "    mean_raw_obs_processing_ms: 0.6962200796541298\n",
      "  time_since_restore: 459.9795184135437\n",
      "  time_this_iter_s: 25.741995811462402\n",
      "  time_total_s: 459.9795184135437\n",
      "  timers:\n",
      "    learn_throughput: 1149.557\n",
      "    learn_time_ms: 1738.06\n",
      "    load_throughput: 59488.859\n",
      "    load_time_ms: 33.586\n",
      "    sample_throughput: 79.408\n",
      "    sample_time_ms: 25161.121\n",
      "    update_time_ms: 13.508\n",
      "  timestamp: 1636541008\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27972\n",
      "  training_iteration: 14\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">          459.98</td><td style=\"text-align: right;\">27972</td><td style=\"text-align: right;\">  3.0078</td><td style=\"text-align: right;\">               10.31</td><td style=\"text-align: right;\">               -1.82</td><td style=\"text-align: right;\">             95.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 29970\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-43-54\n",
      "  done: false\n",
      "  episode_len_mean: 95.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.310000000000013\n",
      "  episode_reward_mean: 3.5009000000000072\n",
      "  episode_reward_min: -1.8200000000000007\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 314\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.480413414183117\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014872395669080406\n",
      "          policy_loss: -0.050481128160442625\n",
      "          total_loss: 0.6902895957231522\n",
      "          vf_explained_var: 0.5811328887939453\n",
      "          vf_loss: 0.7626003779116131\n",
      "    num_agent_steps_sampled: 29970\n",
      "    num_agent_steps_trained: 29970\n",
      "    num_steps_sampled: 29970\n",
      "    num_steps_trained: 29970\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.22432432432434\n",
      "    ram_util_percent: 30.035135135135135\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04471235115177721\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 41.92885488822497\n",
      "    mean_inference_ms: 2.54673013174612\n",
      "    mean_raw_obs_processing_ms: 0.7021538570305117\n",
      "  time_since_restore: 486.1404359340668\n",
      "  time_this_iter_s: 26.16091752052307\n",
      "  time_total_s: 486.1404359340668\n",
      "  timers:\n",
      "    learn_throughput: 1149.307\n",
      "    learn_time_ms: 1738.438\n",
      "    load_throughput: 58878.923\n",
      "    load_time_ms: 33.934\n",
      "    sample_throughput: 83.461\n",
      "    sample_time_ms: 23939.312\n",
      "    update_time_ms: 12.025\n",
      "  timestamp: 1636541034\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29970\n",
      "  training_iteration: 15\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">          486.14</td><td style=\"text-align: right;\">29970</td><td style=\"text-align: right;\">  3.5009</td><td style=\"text-align: right;\">               10.31</td><td style=\"text-align: right;\">               -1.82</td><td style=\"text-align: right;\">             95.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 31968\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-44-20\n",
      "  done: false\n",
      "  episode_len_mean: 94.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.540000000000012\n",
      "  episode_reward_mean: 3.9978000000000082\n",
      "  episode_reward_min: -1.8200000000000007\n",
      "  episodes_this_iter: 22\n",
      "  episodes_total: 336\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.411397689864749\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.028979933521667552\n",
      "          policy_loss: -0.004457156768157369\n",
      "          total_loss: 0.8487850526968638\n",
      "          vf_explained_var: 0.6977705359458923\n",
      "          vf_loss: 0.8715602057320732\n",
      "    num_agent_steps_sampled: 31968\n",
      "    num_agent_steps_trained: 31968\n",
      "    num_steps_sampled: 31968\n",
      "    num_steps_trained: 31968\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.21052631578948\n",
      "    ram_util_percent: 30.007894736842097\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044751489251529025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 41.123658495236356\n",
      "    mean_inference_ms: 2.550117375243992\n",
      "    mean_raw_obs_processing_ms: 0.7068406007827622\n",
      "  time_since_restore: 512.2394988536835\n",
      "  time_this_iter_s: 26.0990629196167\n",
      "  time_total_s: 512.2394988536835\n",
      "  timers:\n",
      "    learn_throughput: 1150.155\n",
      "    learn_time_ms: 1737.157\n",
      "    load_throughput: 58580.607\n",
      "    load_time_ms: 34.107\n",
      "    sample_throughput: 83.365\n",
      "    sample_time_ms: 23967.018\n",
      "    update_time_ms: 12.184\n",
      "  timestamp: 1636541060\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31968\n",
      "  training_iteration: 16\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         512.239</td><td style=\"text-align: right;\">31968</td><td style=\"text-align: right;\">  3.9978</td><td style=\"text-align: right;\">               10.54</td><td style=\"text-align: right;\">               -1.82</td><td style=\"text-align: right;\">              94.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=128238)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=128238)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=128240)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=128240)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=128243)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=128243)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 33966\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-45-06\n",
      "  done: false\n",
      "  episode_len_mean: 90.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.540000000000012\n",
      "  episode_reward_mean: 4.33200000000001\n",
      "  episode_reward_min: -1.8200000000000007\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 360\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3758330220267885\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011814456925538685\n",
      "          policy_loss: 0.01262821587068694\n",
      "          total_loss: 0.7465044772341138\n",
      "          vf_explained_var: 0.744027853012085\n",
      "          vf_loss: 0.7540902574857076\n",
      "    num_agent_steps_sampled: 33966\n",
      "    num_agent_steps_trained: 33966\n",
      "    num_steps_sampled: 33966\n",
      "    num_steps_trained: 33966\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.38923076923078\n",
      "    ram_util_percent: 29.12307692307692\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0446595621068083\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.42674198843542\n",
      "    mean_inference_ms: 2.552594391538536\n",
      "    mean_raw_obs_processing_ms: 1.0924786727383837\n",
      "  time_since_restore: 558.0403356552124\n",
      "  time_this_iter_s: 45.80083680152893\n",
      "  time_total_s: 558.0403356552124\n",
      "  timers:\n",
      "    learn_throughput: 1148.987\n",
      "    learn_time_ms: 1738.922\n",
      "    load_throughput: 58711.899\n",
      "    load_time_ms: 34.031\n",
      "    sample_throughput: 76.953\n",
      "    sample_time_ms: 25963.955\n",
      "    update_time_ms: 11.668\n",
      "  timestamp: 1636541106\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33966\n",
      "  training_iteration: 17\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">          558.04</td><td style=\"text-align: right;\">33966</td><td style=\"text-align: right;\">   4.332</td><td style=\"text-align: right;\">               10.54</td><td style=\"text-align: right;\">               -1.82</td><td style=\"text-align: right;\">             90.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 35964\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-45-37\n",
      "  done: false\n",
      "  episode_len_mean: 90.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.540000000000012\n",
      "  episode_reward_mean: 4.75030000000001\n",
      "  episode_reward_min: -0.19\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 381\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3775629088992165\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011298506239298838\n",
      "          policy_loss: -0.06982921574796949\n",
      "          total_loss: 0.6733536081654685\n",
      "          vf_explained_var: 0.7481786608695984\n",
      "          vf_loss: 0.7635689011641911\n",
      "    num_agent_steps_sampled: 35964\n",
      "    num_agent_steps_trained: 35964\n",
      "    num_steps_sampled: 35964\n",
      "    num_steps_trained: 35964\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.16590909090907\n",
      "    ram_util_percent: 30.23863636363636\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04462557630072652\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.00814492206659\n",
      "    mean_inference_ms: 2.5547144146475644\n",
      "    mean_raw_obs_processing_ms: 1.4120415939082296\n",
      "  time_since_restore: 588.972892999649\n",
      "  time_this_iter_s: 30.932557344436646\n",
      "  time_total_s: 588.972892999649\n",
      "  timers:\n",
      "    learn_throughput: 1140.912\n",
      "    learn_time_ms: 1751.231\n",
      "    load_throughput: 58286.596\n",
      "    load_time_ms: 34.279\n",
      "    sample_throughput: 75.681\n",
      "    sample_time_ms: 26400.391\n",
      "    update_time_ms: 12.297\n",
      "  timestamp: 1636541137\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35964\n",
      "  training_iteration: 18\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         588.973</td><td style=\"text-align: right;\">35964</td><td style=\"text-align: right;\">  4.7503</td><td style=\"text-align: right;\">               10.54</td><td style=\"text-align: right;\">               -0.19</td><td style=\"text-align: right;\">             90.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 37962\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-46-04\n",
      "  done: false\n",
      "  episode_len_mean: 91.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.540000000000012\n",
      "  episode_reward_mean: 5.0664000000000105\n",
      "  episode_reward_min: -0.19\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 402\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3426107429322744\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012119988154709814\n",
      "          policy_loss: 0.036767999934298655\n",
      "          total_loss: 0.6634153509423846\n",
      "          vf_explained_var: 0.7865819931030273\n",
      "          vf_loss: 0.6464374584811075\n",
      "    num_agent_steps_sampled: 37962\n",
      "    num_agent_steps_trained: 37962\n",
      "    num_steps_sampled: 37962\n",
      "    num_steps_trained: 37962\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.07631578947367\n",
      "    ram_util_percent: 30.923684210526314\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04462500019315307\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 39.66795213370794\n",
      "    mean_inference_ms: 2.555994121475724\n",
      "    mean_raw_obs_processing_ms: 1.7147321562508722\n",
      "  time_since_restore: 615.5483691692352\n",
      "  time_this_iter_s: 26.57547616958618\n",
      "  time_total_s: 615.5483691692352\n",
      "  timers:\n",
      "    learn_throughput: 1137.792\n",
      "    learn_time_ms: 1756.032\n",
      "    load_throughput: 58169.226\n",
      "    load_time_ms: 34.348\n",
      "    sample_throughput: 75.483\n",
      "    sample_time_ms: 26469.684\n",
      "    update_time_ms: 11.309\n",
      "  timestamp: 1636541164\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37962\n",
      "  training_iteration: 19\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         615.548</td><td style=\"text-align: right;\">37962</td><td style=\"text-align: right;\">  5.0664</td><td style=\"text-align: right;\">               10.54</td><td style=\"text-align: right;\">               -0.19</td><td style=\"text-align: right;\">             91.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 39960\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-46-31\n",
      "  done: false\n",
      "  episode_len_mean: 92.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.540000000000012\n",
      "  episode_reward_mean: 5.210700000000012\n",
      "  episode_reward_min: -0.19\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 423\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3928366592952184\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010503608902106823\n",
      "          policy_loss: -0.029348456079051607\n",
      "          total_loss: 0.49177761620708876\n",
      "          vf_explained_var: 0.7984403967857361\n",
      "          vf_loss: 0.5419033552919116\n",
      "    num_agent_steps_sampled: 39960\n",
      "    num_agent_steps_trained: 39960\n",
      "    num_steps_sampled: 39960\n",
      "    num_steps_trained: 39960\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.45897435897439\n",
      "    ram_util_percent: 31.171794871794866\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04461942885774437\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 39.378633203492456\n",
      "    mean_inference_ms: 2.5567852629634173\n",
      "    mean_raw_obs_processing_ms: 2.002339341723914\n",
      "  time_since_restore: 642.5659918785095\n",
      "  time_this_iter_s: 27.017622709274292\n",
      "  time_total_s: 642.5659918785095\n",
      "  timers:\n",
      "    learn_throughput: 1137.338\n",
      "    learn_time_ms: 1756.734\n",
      "    load_throughput: 58452.795\n",
      "    load_time_ms: 34.181\n",
      "    sample_throughput: 75.151\n",
      "    sample_time_ms: 26586.468\n",
      "    update_time_ms: 11.526\n",
      "  timestamp: 1636541191\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39960\n",
      "  training_iteration: 20\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         642.566</td><td style=\"text-align: right;\">39960</td><td style=\"text-align: right;\">  5.2107</td><td style=\"text-align: right;\">               10.54</td><td style=\"text-align: right;\">               -0.19</td><td style=\"text-align: right;\">              92.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 41958\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-46-58\n",
      "  done: false\n",
      "  episode_len_mean: 92.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.400000000000015\n",
      "  episode_reward_mean: 5.336300000000011\n",
      "  episode_reward_min: -1.4700000000000009\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 443\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.265721947806222\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011252681971815396\n",
      "          policy_loss: 0.006879181201968875\n",
      "          total_loss: 1.016281931882813\n",
      "          vf_explained_var: 0.74711012840271\n",
      "          vf_loss: 1.0286841684863681\n",
      "    num_agent_steps_sampled: 41958\n",
      "    num_agent_steps_trained: 41958\n",
      "    num_steps_sampled: 41958\n",
      "    num_steps_trained: 41958\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.94358974358975\n",
      "    ram_util_percent: 31.32051282051282\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04463439896222268\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 39.14303696752646\n",
      "    mean_inference_ms: 2.5571004752047126\n",
      "    mean_raw_obs_processing_ms: 2.152666333052399\n",
      "  time_since_restore: 669.904842376709\n",
      "  time_this_iter_s: 27.338850498199463\n",
      "  time_total_s: 669.904842376709\n",
      "  timers:\n",
      "    learn_throughput: 1134.854\n",
      "    learn_time_ms: 1760.579\n",
      "    load_throughput: 59321.133\n",
      "    load_time_ms: 33.681\n",
      "    sample_throughput: 74.68\n",
      "    sample_time_ms: 26754.25\n",
      "    update_time_ms: 11.067\n",
      "  timestamp: 1636541218\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 41958\n",
      "  training_iteration: 21\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         669.905</td><td style=\"text-align: right;\">41958</td><td style=\"text-align: right;\">  5.3363</td><td style=\"text-align: right;\">                10.4</td><td style=\"text-align: right;\">               -1.47</td><td style=\"text-align: right;\">             92.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 43956\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-47-24\n",
      "  done: false\n",
      "  episode_len_mean: 96.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.400000000000015\n",
      "  episode_reward_mean: 5.507400000000013\n",
      "  episode_reward_min: -1.4700000000000009\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 463\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.307974105789548\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008839242778811436\n",
      "          policy_loss: 0.023779102769635973\n",
      "          total_loss: 0.576098541773501\n",
      "          vf_explained_var: 0.8117967247962952\n",
      "          vf_loss: 0.5727474097694669\n",
      "    num_agent_steps_sampled: 43956\n",
      "    num_agent_steps_trained: 43956\n",
      "    num_steps_sampled: 43956\n",
      "    num_steps_trained: 43956\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.94166666666666\n",
      "    ram_util_percent: 31.408333333333335\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04466056694038576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.882016681448924\n",
      "    mean_inference_ms: 2.5575287018270902\n",
      "    mean_raw_obs_processing_ms: 2.0844750297905827\n",
      "  time_since_restore: 695.5668413639069\n",
      "  time_this_iter_s: 25.661998987197876\n",
      "  time_total_s: 695.5668413639069\n",
      "  timers:\n",
      "    learn_throughput: 1135.763\n",
      "    learn_time_ms: 1759.17\n",
      "    load_throughput: 59597.545\n",
      "    load_time_ms: 33.525\n",
      "    sample_throughput: 74.46\n",
      "    sample_time_ms: 26833.246\n",
      "    update_time_ms: 10.514\n",
      "  timestamp: 1636541244\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 43956\n",
      "  training_iteration: 22\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         695.567</td><td style=\"text-align: right;\">43956</td><td style=\"text-align: right;\">  5.5074</td><td style=\"text-align: right;\">                10.4</td><td style=\"text-align: right;\">               -1.47</td><td style=\"text-align: right;\">             96.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 45954\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-47-50\n",
      "  done: false\n",
      "  episode_len_mean: 97.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.400000000000015\n",
      "  episode_reward_mean: 5.652000000000015\n",
      "  episode_reward_min: -1.4700000000000009\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 484\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.22153890473502\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011417649633313568\n",
      "          policy_loss: -0.03119412681886128\n",
      "          total_loss: 0.6075078728653136\n",
      "          vf_explained_var: 0.8003586530685425\n",
      "          vf_loss: 0.6574920979284105\n",
      "    num_agent_steps_sampled: 45954\n",
      "    num_agent_steps_trained: 45954\n",
      "    num_steps_sampled: 45954\n",
      "    num_steps_trained: 45954\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.05789473684212\n",
      "    ram_util_percent: 31.507894736842104\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04469492605367192\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.57661918305685\n",
      "    mean_inference_ms: 2.557405173792391\n",
      "    mean_raw_obs_processing_ms: 2.0190003964170784\n",
      "  time_since_restore: 721.6611499786377\n",
      "  time_this_iter_s: 26.094308614730835\n",
      "  time_total_s: 721.6611499786377\n",
      "  timers:\n",
      "    learn_throughput: 1136.595\n",
      "    learn_time_ms: 1757.882\n",
      "    load_throughput: 59313.282\n",
      "    load_time_ms: 33.686\n",
      "    sample_throughput: 74.189\n",
      "    sample_time_ms: 26931.086\n",
      "    update_time_ms: 9.7\n",
      "  timestamp: 1636541270\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45954\n",
      "  training_iteration: 23\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         721.661</td><td style=\"text-align: right;\">45954</td><td style=\"text-align: right;\">   5.652</td><td style=\"text-align: right;\">                10.4</td><td style=\"text-align: right;\">               -1.47</td><td style=\"text-align: right;\">             97.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 47952\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-48-17\n",
      "  done: false\n",
      "  episode_len_mean: 97.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.270000000000014\n",
      "  episode_reward_mean: 5.745800000000014\n",
      "  episode_reward_min: -1.4700000000000009\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 504\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.227656358764285\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012494995685089539\n",
      "          policy_loss: -0.04271360827122061\n",
      "          total_loss: 0.7109435819444202\n",
      "          vf_explained_var: 0.7887855172157288\n",
      "          vf_loss: 0.7721852514005842\n",
      "    num_agent_steps_sampled: 47952\n",
      "    num_agent_steps_trained: 47952\n",
      "    num_steps_sampled: 47952\n",
      "    num_steps_trained: 47952\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.01842105263158\n",
      "    ram_util_percent: 31.528947368421044\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04472279651830641\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.30649302537359\n",
      "    mean_inference_ms: 2.557247678082397\n",
      "    mean_raw_obs_processing_ms: 1.9614170560064497\n",
      "  time_since_restore: 748.379810333252\n",
      "  time_this_iter_s: 26.718660354614258\n",
      "  time_total_s: 748.379810333252\n",
      "  timers:\n",
      "    learn_throughput: 1135.908\n",
      "    learn_time_ms: 1758.945\n",
      "    load_throughput: 59845.373\n",
      "    load_time_ms: 33.386\n",
      "    sample_throughput: 73.922\n",
      "    sample_time_ms: 27028.325\n",
      "    update_time_ms: 9.501\n",
      "  timestamp: 1636541297\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47952\n",
      "  training_iteration: 24\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">          748.38</td><td style=\"text-align: right;\">47952</td><td style=\"text-align: right;\">  5.7458</td><td style=\"text-align: right;\">               10.27</td><td style=\"text-align: right;\">               -1.47</td><td style=\"text-align: right;\">             97.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 49950\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-48-43\n",
      "  done: false\n",
      "  episode_len_mean: 97.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.270000000000014\n",
      "  episode_reward_mean: 6.002800000000013\n",
      "  episode_reward_min: -1.4700000000000009\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 525\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1957642714182537\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01286217273354184\n",
      "          policy_loss: -0.012418724170752933\n",
      "          total_loss: 0.9355167519478571\n",
      "          vf_explained_var: 0.7663384675979614\n",
      "          vf_loss: 0.9660344694341932\n",
      "    num_agent_steps_sampled: 49950\n",
      "    num_agent_steps_trained: 49950\n",
      "    num_steps_sampled: 49950\n",
      "    num_steps_trained: 49950\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.17894736842106\n",
      "    ram_util_percent: 31.557894736842105\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04476399488933783\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.04065910973112\n",
      "    mean_inference_ms: 2.5575274653229116\n",
      "    mean_raw_obs_processing_ms: 1.905581281042555\n",
      "  time_since_restore: 774.8690841197968\n",
      "  time_this_iter_s: 26.4892737865448\n",
      "  time_total_s: 774.8690841197968\n",
      "  timers:\n",
      "    learn_throughput: 1131.782\n",
      "    learn_time_ms: 1765.358\n",
      "    load_throughput: 57828.516\n",
      "    load_time_ms: 34.55\n",
      "    sample_throughput: 73.856\n",
      "    sample_time_ms: 27052.712\n",
      "    update_time_ms: 10.252\n",
      "  timestamp: 1636541323\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49950\n",
      "  training_iteration: 25\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         774.869</td><td style=\"text-align: right;\">49950</td><td style=\"text-align: right;\">  6.0028</td><td style=\"text-align: right;\">               10.27</td><td style=\"text-align: right;\">               -1.47</td><td style=\"text-align: right;\">             97.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 51948\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-49-10\n",
      "  done: false\n",
      "  episode_len_mean: 97.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.270000000000014\n",
      "  episode_reward_mean: 6.208900000000015\n",
      "  episode_reward_min: 1.9900000000000126\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 546\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1293957971391224\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01281589746078513\n",
      "          policy_loss: -0.0315396452943484\n",
      "          total_loss: 0.7530185425565357\n",
      "          vf_explained_var: 0.7764570116996765\n",
      "          vf_loss: 0.8020073797021593\n",
      "    num_agent_steps_sampled: 51948\n",
      "    num_agent_steps_trained: 51948\n",
      "    num_steps_sampled: 51948\n",
      "    num_steps_trained: 51948\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.56052631578946\n",
      "    ram_util_percent: 31.58421052631579\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044782446249089905\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.79094107109668\n",
      "    mean_inference_ms: 2.5582311431328746\n",
      "    mean_raw_obs_processing_ms: 1.8566568041359426\n",
      "  time_since_restore: 801.438029050827\n",
      "  time_this_iter_s: 26.568944931030273\n",
      "  time_total_s: 801.438029050827\n",
      "  timers:\n",
      "    learn_throughput: 1131.113\n",
      "    learn_time_ms: 1766.401\n",
      "    load_throughput: 57883.438\n",
      "    load_time_ms: 34.518\n",
      "    sample_throughput: 73.729\n",
      "    sample_time_ms: 27099.16\n",
      "    update_time_ms: 10.355\n",
      "  timestamp: 1636541350\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 51948\n",
      "  training_iteration: 26\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         801.438</td><td style=\"text-align: right;\">51948</td><td style=\"text-align: right;\">  6.2089</td><td style=\"text-align: right;\">               10.27</td><td style=\"text-align: right;\">                1.99</td><td style=\"text-align: right;\">             97.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 53946\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-49-37\n",
      "  done: false\n",
      "  episode_len_mean: 97.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.270000000000014\n",
      "  episode_reward_mean: 6.478700000000015\n",
      "  episode_reward_min: 1.9900000000000126\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 566\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1299749045144942\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012301247637542718\n",
      "          policy_loss: -0.0253513226551669\n",
      "          total_loss: 0.7402340168044681\n",
      "          vf_explained_var: 0.8115695118904114\n",
      "          vf_loss: 0.7831947122301374\n",
      "    num_agent_steps_sampled: 53946\n",
      "    num_agent_steps_trained: 53946\n",
      "    num_steps_sampled: 53946\n",
      "    num_steps_trained: 53946\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.50263157894737\n",
      "    ram_util_percent: 31.621052631578944\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04480215645254418\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.59147367447159\n",
      "    mean_inference_ms: 2.55878058137902\n",
      "    mean_raw_obs_processing_ms: 1.812998165533507\n",
      "  time_since_restore: 828.3016662597656\n",
      "  time_this_iter_s: 26.8636372089386\n",
      "  time_total_s: 828.3016662597656\n",
      "  timers:\n",
      "    learn_throughput: 1131.223\n",
      "    learn_time_ms: 1766.23\n",
      "    load_throughput: 57761.513\n",
      "    load_time_ms: 34.591\n",
      "    sample_throughput: 79.267\n",
      "    sample_time_ms: 25205.945\n",
      "    update_time_ms: 9.991\n",
      "  timestamp: 1636541377\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 53946\n",
      "  training_iteration: 27\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.9/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         828.302</td><td style=\"text-align: right;\">53946</td><td style=\"text-align: right;\">  6.4787</td><td style=\"text-align: right;\">               10.27</td><td style=\"text-align: right;\">                1.99</td><td style=\"text-align: right;\">             97.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 55944\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-50-03\n",
      "  done: false\n",
      "  episode_len_mean: 97.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.270000000000014\n",
      "  episode_reward_mean: 6.364400000000015\n",
      "  episode_reward_min: 2.3800000000000012\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 586\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.154387102808271\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012668717275979067\n",
      "          policy_loss: -0.02155579665587062\n",
      "          total_loss: 0.7123023685245287\n",
      "          vf_explained_var: 0.8117629289627075\n",
      "          vf_loss: 0.7516014269420079\n",
      "    num_agent_steps_sampled: 55944\n",
      "    num_agent_steps_trained: 55944\n",
      "    num_steps_sampled: 55944\n",
      "    num_steps_trained: 55944\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.05\n",
      "    ram_util_percent: 31.6578947368421\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044820042906201805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.40616429500853\n",
      "    mean_inference_ms: 2.55960383542132\n",
      "    mean_raw_obs_processing_ms: 1.7736311824525004\n",
      "  time_since_restore: 855.0347390174866\n",
      "  time_this_iter_s: 26.733072757720947\n",
      "  time_total_s: 855.0347390174866\n",
      "  timers:\n",
      "    learn_throughput: 1140.286\n",
      "    learn_time_ms: 1752.192\n",
      "    load_throughput: 58301.275\n",
      "    load_time_ms: 34.27\n",
      "    sample_throughput: 80.564\n",
      "    sample_time_ms: 24800.097\n",
      "    update_time_ms: 9.029\n",
      "  timestamp: 1636541403\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55944\n",
      "  training_iteration: 28\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         855.035</td><td style=\"text-align: right;\">55944</td><td style=\"text-align: right;\">  6.3644</td><td style=\"text-align: right;\">               10.27</td><td style=\"text-align: right;\">                2.38</td><td style=\"text-align: right;\">             97.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 57942\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-50-30\n",
      "  done: false\n",
      "  episode_len_mean: 97.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.170000000000016\n",
      "  episode_reward_mean: 6.398000000000016\n",
      "  episode_reward_min: 2.3800000000000012\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 606\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1237727278754823\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011549540168803234\n",
      "          policy_loss: -0.044458790079113984\n",
      "          total_loss: 0.6101092695480301\n",
      "          vf_explained_var: 0.8668228983879089\n",
      "          vf_loss: 0.6723409252507346\n",
      "    num_agent_steps_sampled: 57942\n",
      "    num_agent_steps_trained: 57942\n",
      "    num_steps_sampled: 57942\n",
      "    num_steps_trained: 57942\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.68947368421054\n",
      "    ram_util_percent: 31.663157894736845\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04481140497090861\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.234547924403344\n",
      "    mean_inference_ms: 2.559867891609717\n",
      "    mean_raw_obs_processing_ms: 1.7374895405683082\n",
      "  time_since_restore: 881.7964813709259\n",
      "  time_this_iter_s: 26.76174235343933\n",
      "  time_total_s: 881.7964813709259\n",
      "  timers:\n",
      "    learn_throughput: 1142.393\n",
      "    learn_time_ms: 1748.961\n",
      "    load_throughput: 58396.3\n",
      "    load_time_ms: 34.214\n",
      "    sample_throughput: 80.491\n",
      "    sample_time_ms: 24822.558\n",
      "    update_time_ms: 8.26\n",
      "  timestamp: 1636541430\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 57942\n",
      "  training_iteration: 29\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         881.796</td><td style=\"text-align: right;\">57942</td><td style=\"text-align: right;\">   6.398</td><td style=\"text-align: right;\">               10.17</td><td style=\"text-align: right;\">                2.38</td><td style=\"text-align: right;\">             97.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_f147d_00000:\n",
      "  agent_timesteps_total: 59940\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-10_10-50-58\n",
      "  done: false\n",
      "  episode_len_mean: 98.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.090000000000018\n",
      "  episode_reward_mean: 6.107100000000015\n",
      "  episode_reward_min: 2.0600000000000054\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 627\n",
      "  experiment_id: fc595125a55c42209f18ff09cd4b9a94\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1734012478873845\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013057920636007438\n",
      "          policy_loss: -0.025017070663826805\n",
      "          total_loss: 0.8296772919950031\n",
      "          vf_explained_var: 0.7531847953796387\n",
      "          vf_loss: 0.8725110002926417\n",
      "    num_agent_steps_sampled: 59940\n",
      "    num_agent_steps_trained: 59940\n",
      "    num_steps_sampled: 59940\n",
      "    num_steps_trained: 59940\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.89999999999999\n",
      "    ram_util_percent: 31.700000000000006\n",
      "  pid: 128236\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044816742547480526\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.0720921353505\n",
      "    mean_inference_ms: 2.560070278028003\n",
      "    mean_raw_obs_processing_ms: 1.7025725980000477\n",
      "  time_since_restore: 909.2369358539581\n",
      "  time_this_iter_s: 27.440454483032227\n",
      "  time_total_s: 909.2369358539581\n",
      "  timers:\n",
      "    learn_throughput: 1135.766\n",
      "    learn_time_ms: 1759.166\n",
      "    load_throughput: 58191.402\n",
      "    load_time_ms: 34.335\n",
      "    sample_throughput: 80.39\n",
      "    sample_time_ms: 24853.733\n",
      "    update_time_ms: 9.284\n",
      "  timestamp: 1636541458\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 59940\n",
      "  training_iteration: 30\n",
      "  trial_id: f147d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/47.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 1.0/1 GPUs, 0.0/26.21 GiB heap, 0.0/13.1 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/C139/PPO_2021-11-10_10-35-37<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_f147d_00000</td><td>RUNNING </td><td>192.168.3.5:128236</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         909.237</td><td style=\"text-align: right;\">59940</td><td style=\"text-align: right;\">  6.1071</td><td style=\"text-align: right;\">               10.09</td><td style=\"text-align: right;\">                2.06</td><td style=\"text-align: right;\">             98.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "analysis = tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 3,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             #\"gamma\": 0.99,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO MultiTask (C139) pretrained (AngelaCNN) (3 noops after placement) r: -0.01 div10\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger],\n",
    "        local_dir=\"/IGLU-Minecraft/checkpoints/C139\",\n",
    "        keep_checkpoints_num=50,\n",
    "        checkpoint_freq=5,\n",
    "        checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
