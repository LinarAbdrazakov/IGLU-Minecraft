{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *\n",
    "from wrappers_2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9fd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=2, stride=2, padding=0),  \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(), \n",
    "            nn.Conv2d(128, 256, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf15c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PovTargetFusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PovTargetFusion, self).__init__()\n",
    "        \n",
    "        #self.visual_encoder = VisualEncoder()\n",
    "        self.visual_dim = 512\n",
    "        \n",
    "        self.conv_1 = nn.Conv3d(7, 7, 1)\n",
    "        self.linear_1 = nn.Linear(self.visual_dim, 7)\n",
    "        #self.act_1 = nn.ELU()\n",
    "        \n",
    "        self.conv_2 = nn.Conv3d(7, 16, 3)\n",
    "        self.linear_2 = nn.Linear(self.visual_dim, 16)\n",
    "        self.act_2 = nn.ELU()\n",
    "\n",
    "        self.conv_3 = nn.Conv3d(16, 32, 3)\n",
    "        self.linear_3 = nn.Linear(self.visual_dim, 32)\n",
    "        self.act_3 = nn.ELU()\n",
    "        \n",
    "        self.conv_4 = nn.Conv3d(32, 64, 3)\n",
    "        self.linear_4 = nn.Linear(self.visual_dim, 64)\n",
    "        self.act_4 = nn.ELU()\n",
    "        \n",
    "        self.conv_5 = nn.Conv3d(64, 128, 3)\n",
    "        self.act_5 = nn.ELU()\n",
    "        \n",
    "        self.linear_6 = nn.Linear(128 * 9, 512)\n",
    "        self.act_6 = nn.ELU()\n",
    "        \n",
    "    def forward(self, target, vis_features):\n",
    "        Q = self.linear_1(vis_features).reshape(-1, 1, 7).permute(0, 2, 1)\n",
    "        V = self.conv_1(target).reshape(-1, 9 * 11 * 11, 7)\n",
    "        result = torch.sigmoid(torch.bmm(V, Q)) * V\n",
    "        result = result.reshape(-1, 9, 11, 11, 7).permute(0, 4, 1, 2, 3)\n",
    "        \n",
    "        Q = self.linear_2(vis_features).reshape(-1, 1, 16).permute(0, 2, 1)\n",
    "        V = self.conv_2(result).reshape(-1, 7 * 9 * 9, 16)\n",
    "        result = torch.sigmoid(torch.bmm(V, Q)) * self.act_2(V)\n",
    "        result = result.reshape(-1, 7, 9, 9, 16).permute(0, 4, 1, 2, 3)\n",
    "        \n",
    "        Q = self.linear_3(vis_features).reshape(-1, 1, 32).permute(0, 2, 1)\n",
    "        V = self.conv_3(result).reshape(-1, 5 * 7 * 7, 32)\n",
    "        result = torch.sigmoid(torch.bmm(V, Q)) * self.act_3(V)\n",
    "        result = result.reshape(-1, 5, 7, 7, 32).permute(0, 4, 1, 2, 3)\n",
    "        \n",
    "        Q = self.linear_4(vis_features).reshape(-1, 1, 64).permute(0, 2, 1)\n",
    "        V = self.conv_4(result).reshape(-1, 3 * 5 * 5, 64)\n",
    "        result = torch.sigmoid(torch.bmm(V, Q)) * self.act_4(V)\n",
    "        result = result.reshape(-1, 3, 5, 5, 64).permute(0, 4, 1, 2, 3)\n",
    "        \n",
    "        result = self.act_5(self.conv_5(result))\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "        result = self.act_6(self.linear_6(result))\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import one_hot\n",
    "\n",
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        visual_features_dim = 512\n",
    "        target_features_dim = 9 * 11 * 11 \n",
    "        self.visual_encoder = VisualEncoder()\n",
    "        self.visual_encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AngelaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.target_encoder = PovTargetFusion()\n",
    "        policy_hidden_dim = 256 \n",
    "        self.policy_network = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, policy_hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.action_head = nn.Linear(policy_hidden_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(policy_hidden_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.visual_encoder.cuda()\n",
    "            self.target_encoder.cuda()\n",
    "            self.policy_network.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs']\n",
    "        pov = obs['pov'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        target = one_hot(obs['target_grid'].long(), num_classes=7).permute(0, 4, 1, 2, 3).float()\n",
    "        if self.use_cuda:\n",
    "            pov.cuda()\n",
    "            target.cuda()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            visual_features = self.visual_encoder(pov)\n",
    "            \n",
    "        target_features = self.target_encoder(target, visual_features)\n",
    "        target_features = target_features.reshape(target_features.shape[0], -1)\n",
    "        features = torch.cat([visual_features, target_features], dim=1)\n",
    "        features = self.policy_network(features)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "592760ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2362368"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_features_dim = 512\n",
    "target_features_dim = 9 * 11 * 11\n",
    "policy_hidden_dim = 256 \n",
    "\n",
    "policy_network = nn.Sequential(\n",
    "    nn.Linear(visual_features_dim + target_features_dim, 1024),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(512, policy_hidden_dim),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "    nn.ELU(),\n",
    "    #nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "    #nn.ELU(),\n",
    ")\n",
    "\n",
    "sum(p.numel() for p in policy_network.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc09c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualObservationWrapper(ObsWrapper):\n",
    "    def __init__(self, env, include_target=False):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = {   \n",
    "            'pov': gym.spaces.Box(low=0, high=255, shape=(64, 64, 3)),\n",
    "            'inventory': gym.spaces.Box(low=0.0, high=20.0, shape=(6,)),\n",
    "            'compass': gym.spaces.Box(low=-180.0, high=180.0, shape=(1,))\n",
    "        }\n",
    "        if include_target:\n",
    "            self.observation_space['target_grid'] = \\\n",
    "                gym.spaces.Box(low=0, high=6, shape=(9, 11, 11))\n",
    "        self.observation_space = gym.spaces.Dict(self.observation_space)\n",
    "\n",
    "    def observation(self, obs, reward=None, done=None, info=None):\n",
    "        if info is not None:\n",
    "            if 'target_grid' in info:\n",
    "                target_grid = info['target_grid']\n",
    "                del info['target_grid']\n",
    "            else:\n",
    "                logger.error(f'info: {info}')\n",
    "                if hasattr(self.unwrapped, 'should_reset'):\n",
    "                    self.unwrapped.should_reset(True)\n",
    "                target_grid = self.env.unwrapped.tasks.current.target_grid\n",
    "        else:\n",
    "            target_grid = self.env.unwrapped.tasks.current.target_grid\n",
    "        return {\n",
    "            'pov': obs['pov'].astype(np.float32),\n",
    "            'inventory': obs['inventory'],\n",
    "            'compass': np.array([obs['compass']['angle'].item()]),\n",
    "            'target_grid': target_grid\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "tasks = []\n",
    "for i in range(1,156):\n",
    "    if ('C'+str(i)) == 'C38': continue\n",
    "    tasks.append('C'+str(i))\n",
    "    \n",
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def reward(self, rew):\n",
    "        if rew == 0:\n",
    "            rew = -0.01\n",
    "        if abs(rew) == 1:\n",
    "            rew /= 10\n",
    "            \n",
    "        return rew\n",
    "    \n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=250)\n",
    "    env.update_taskset(TaskSet(preset=tasks))\n",
    "    #env = PovOnlyWrapper(env)\n",
    "    env = VisualObservationWrapper(env, include_target=True)\n",
    "    env = SelectAndPlace(env)\n",
    "    env = Discretization(env, flat_action_space('human-level'))\n",
    "    env = RewardWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 32.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-07 15:39:46,527\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-11-07 15:39:46,549\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to auto resume run with id 3ecf2_00000 but id ef690_00000 is set.\n",
      "\u001b[2m\u001b[36m(pid=9113)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9113)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO All Tasks pretrained (AngelaCNN+fusion) (3 noops after placement) r: -0.01 div10</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/ef690_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/ef690_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20211107_153948-ef690_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=9113)\u001b[0m 2021-11-07 15:39:51,116\tWARNING ppo.py:143 -- `train_batch_size` (5000) cannot be achieved with your other settings (num_workers=3 num_envs_per_worker=1 rollout_fragment_length=200)! Auto-adjusting `rollout_fragment_length` to 1666.\n",
      "\u001b[2m\u001b[36m(pid=9113)\u001b[0m 2021-11-07 15:39:51,117\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=9113)\u001b[0m 2021-11-07 15:39:51,117\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9113)\u001b[0m 2021-11-07 15:40:08,938\tINFO trainable.py:109 -- Trainable.setup took 20.983 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=9113)\u001b[0m 2021-11-07 15:40:08,939\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m   warn(RuntimeWarning(msg))\n",
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m   warn(RuntimeWarning(msg))\n",
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 9996\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_15-44-45\n",
      "  done: false\n",
      "  episode_len_mean: 103.87368421052632\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.690000000000007\n",
      "  episode_reward_mean: -0.7170526315789476\n",
      "  episode_reward_min: -1.6000000000000008\n",
      "  episodes_this_iter: 95\n",
      "  episodes_total: 95\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.881463285185333\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.005726082684665919\n",
      "          policy_loss: -0.014017798388615632\n",
      "          total_loss: 0.024265516103587598\n",
      "          vf_explained_var: -0.32391759753227234\n",
      "          vf_loss: 0.06595273009425777\n",
      "    num_agent_steps_sampled: 9996\n",
      "    num_agent_steps_trained: 9996\n",
      "    num_steps_sampled: 9996\n",
      "    num_steps_trained: 9996\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.03341772151899\n",
      "    ram_util_percent: 33.62987341772152\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08596707202089066\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 56.430154185805854\n",
      "    mean_inference_ms: 6.6963782512208105\n",
      "    mean_raw_obs_processing_ms: 0.9413418935566459\n",
      "  time_since_restore: 276.6725866794586\n",
      "  time_this_iter_s: 276.6725866794586\n",
      "  time_total_s: 276.6725866794586\n",
      "  timers:\n",
      "    learn_throughput: 175.949\n",
      "    learn_time_ms: 56811.936\n",
      "    load_throughput: 49930.704\n",
      "    load_time_ms: 200.197\n",
      "    sample_throughput: 45.521\n",
      "    sample_time_ms: 219590.39\n",
      "    update_time_ms: 41.688\n",
      "  timestamp: 1636299885\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9996\n",
      "  training_iteration: 1\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 45.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         276.673</td><td style=\"text-align: right;\">9996</td><td style=\"text-align: right;\">-0.717053</td><td style=\"text-align: right;\">                4.69</td><td style=\"text-align: right;\">                -1.6</td><td style=\"text-align: right;\">           103.874</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 19992\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_15-48-14\n",
      "  done: false\n",
      "  episode_len_mean: 102.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.860000000000011\n",
      "  episode_reward_mean: -0.29689999999999983\n",
      "  episode_reward_min: -1.540000000000001\n",
      "  episodes_this_iter: 97\n",
      "  episodes_total: 192\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.8669099536716427\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.008004153515828798\n",
      "          policy_loss: -0.021347131272857516\n",
      "          total_loss: 0.04276177956698797\n",
      "          vf_explained_var: 0.13229481875896454\n",
      "          vf_loss: 0.09117717912727091\n",
      "    num_agent_steps_sampled: 19992\n",
      "    num_agent_steps_trained: 19992\n",
      "    num_steps_sampled: 19992\n",
      "    num_steps_trained: 19992\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.33590604026845\n",
      "    ram_util_percent: 36.04026845637584\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0849906531277814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 46.37060735639778\n",
      "    mean_inference_ms: 6.656116537922567\n",
      "    mean_raw_obs_processing_ms: 0.930757850423023\n",
      "  time_since_restore: 485.46531319618225\n",
      "  time_this_iter_s: 208.79272651672363\n",
      "  time_total_s: 485.46531319618225\n",
      "  timers:\n",
      "    learn_throughput: 177.138\n",
      "    learn_time_ms: 56430.583\n",
      "    load_throughput: 39885.425\n",
      "    load_time_ms: 250.618\n",
      "    sample_throughput: 53.745\n",
      "    sample_time_ms: 185989.426\n",
      "    update_time_ms: 33.562\n",
      "  timestamp: 1636300094\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19992\n",
      "  training_iteration: 2\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 45.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         485.465</td><td style=\"text-align: right;\">19992</td><td style=\"text-align: right;\"> -0.2969</td><td style=\"text-align: right;\">                4.86</td><td style=\"text-align: right;\">               -1.54</td><td style=\"text-align: right;\">            102.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 29988\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_15-51-41\n",
      "  done: false\n",
      "  episode_len_mean: 104.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.680000000000005\n",
      "  episode_reward_mean: 0.23140000000000044\n",
      "  episode_reward_min: -2.189999999999997\n",
      "  episodes_this_iter: 96\n",
      "  episodes_total: 288\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.8375419239712576\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01280893030899631\n",
      "          policy_loss: -0.025665039607347586\n",
      "          total_loss: 0.15897202860468473\n",
      "          vf_explained_var: 0.4611608386039734\n",
      "          vf_loss: 0.21045070096659355\n",
      "    num_agent_steps_sampled: 29988\n",
      "    num_agent_steps_trained: 29988\n",
      "    num_steps_sampled: 29988\n",
      "    num_steps_trained: 29988\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.80918367346939\n",
      "    ram_util_percent: 36.11904761904762\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08377250958806934\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 42.852755921549104\n",
      "    mean_inference_ms: 6.665326720137141\n",
      "    mean_raw_obs_processing_ms: 0.9388605519229444\n",
      "  time_since_restore: 692.303454875946\n",
      "  time_this_iter_s: 206.8381416797638\n",
      "  time_total_s: 692.303454875946\n",
      "  timers:\n",
      "    learn_throughput: 177.495\n",
      "    learn_time_ms: 56317.086\n",
      "    load_throughput: 39466.327\n",
      "    load_time_ms: 253.279\n",
      "    sample_throughput: 57.402\n",
      "    sample_time_ms: 174139.458\n",
      "    update_time_ms: 29.984\n",
      "  timestamp: 1636300301\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29988\n",
      "  training_iteration: 3\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 45.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         692.303</td><td style=\"text-align: right;\">29988</td><td style=\"text-align: right;\">  0.2314</td><td style=\"text-align: right;\">                8.68</td><td style=\"text-align: right;\">               -2.19</td><td style=\"text-align: right;\">            104.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 39984\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_15-55-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.690000000000009\n",
      "  episode_reward_mean: 0.2714000000000005\n",
      "  episode_reward_min: -1.910000000000001\n",
      "  episodes_this_iter: 99\n",
      "  episodes_total: 387\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.814265903040894\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01251414466637949\n",
      "          policy_loss: -0.024552390955261186\n",
      "          total_loss: 0.15232630074581402\n",
      "          vf_explained_var: 0.45500150322914124\n",
      "          vf_loss: 0.20251852030603168\n",
      "    num_agent_steps_sampled: 39984\n",
      "    num_agent_steps_trained: 39984\n",
      "    num_steps_sampled: 39984\n",
      "    num_steps_trained: 39984\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.48269794721408\n",
      "    ram_util_percent: 31.838123167155427\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0840863489290953\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 41.524895366106904\n",
      "    mean_inference_ms: 6.636088387281879\n",
      "    mean_raw_obs_processing_ms: 2.4518158370268637\n",
      "  time_since_restore: 931.3420853614807\n",
      "  time_this_iter_s: 239.03863048553467\n",
      "  time_total_s: 931.3420853614807\n",
      "  timers:\n",
      "    learn_throughput: 177.046\n",
      "    learn_time_ms: 56460.016\n",
      "    load_throughput: 41877.266\n",
      "    load_time_ms: 238.698\n",
      "    sample_throughput: 56.769\n",
      "    sample_time_ms: 176080.88\n",
      "    update_time_ms: 26.884\n",
      "  timestamp: 1636300540\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39984\n",
      "  training_iteration: 4\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 37.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         931.342</td><td style=\"text-align: right;\">39984</td><td style=\"text-align: right;\">  0.2714</td><td style=\"text-align: right;\">                6.69</td><td style=\"text-align: right;\">               -1.91</td><td style=\"text-align: right;\">            100.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 49980\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_15-59-21\n",
      "  done: false\n",
      "  episode_len_mean: 104.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.920000000000011\n",
      "  episode_reward_mean: 0.6159000000000014\n",
      "  episode_reward_min: -1.870000000000001\n",
      "  episodes_this_iter: 96\n",
      "  episodes_total: 483\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.7958645411026786\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.012164358488454263\n",
      "          policy_loss: -0.025776770726865173\n",
      "          total_loss: 0.1933178161055996\n",
      "          vf_explained_var: 0.5301831960678101\n",
      "          vf_loss: 0.2446203603512711\n",
      "    num_agent_steps_sampled: 49980\n",
      "    num_agent_steps_trained: 49980\n",
      "    num_steps_sampled: 49980\n",
      "    num_steps_trained: 49980\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.27373417721519\n",
      "    ram_util_percent: 30.35\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0843806067650145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.91044778687628\n",
      "    mean_inference_ms: 6.629760043500461\n",
      "    mean_raw_obs_processing_ms: 2.1902477097720445\n",
      "  time_since_restore: 1152.58460688591\n",
      "  time_this_iter_s: 221.24252152442932\n",
      "  time_total_s: 1152.58460688591\n",
      "  timers:\n",
      "    learn_throughput: 176.84\n",
      "    learn_time_ms: 56525.603\n",
      "    load_throughput: 39735.328\n",
      "    load_time_ms: 251.565\n",
      "    sample_throughput: 57.552\n",
      "    sample_time_ms: 173685.064\n",
      "    update_time_ms: 25.268\n",
      "  timestamp: 1636300761\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49980\n",
      "  training_iteration: 5\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         1152.58</td><td style=\"text-align: right;\">49980</td><td style=\"text-align: right;\">  0.6159</td><td style=\"text-align: right;\">                4.92</td><td style=\"text-align: right;\">               -1.87</td><td style=\"text-align: right;\">            104.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 59976\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_16-02-58\n",
      "  done: false\n",
      "  episode_len_mean: 103.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.700000000000003\n",
      "  episode_reward_mean: 0.7659000000000021\n",
      "  episode_reward_min: -1.920000000000001\n",
      "  episodes_this_iter: 97\n",
      "  episodes_total: 580\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.7729798665413488\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.013606591013315418\n",
      "          policy_loss: -0.026233421842384544\n",
      "          total_loss: 0.19056971003898443\n",
      "          vf_explained_var: 0.5563409924507141\n",
      "          vf_loss: 0.24181161136645027\n",
      "    num_agent_steps_sampled: 59976\n",
      "    num_agent_steps_trained: 59976\n",
      "    num_steps_sampled: 59976\n",
      "    num_steps_trained: 59976\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.5252427184466\n",
      "    ram_util_percent: 30.706796116504854\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08443572504017105\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.579591252388084\n",
      "    mean_inference_ms: 6.623285038649254\n",
      "    mean_raw_obs_processing_ms: 1.9912437880851293\n",
      "  time_since_restore: 1369.623830318451\n",
      "  time_this_iter_s: 217.0392234325409\n",
      "  time_total_s: 1369.623830318451\n",
      "  timers:\n",
      "    learn_throughput: 176.738\n",
      "    learn_time_ms: 56558.412\n",
      "    load_throughput: 38724.549\n",
      "    load_time_ms: 258.131\n",
      "    sample_throughput: 58.319\n",
      "    sample_time_ms: 171402.66\n",
      "    update_time_ms: 23.735\n",
      "  timestamp: 1636300978\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 59976\n",
      "  training_iteration: 6\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 38.8/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         1369.62</td><td style=\"text-align: right;\">59976</td><td style=\"text-align: right;\">  0.7659</td><td style=\"text-align: right;\">                 6.7</td><td style=\"text-align: right;\">               -1.92</td><td style=\"text-align: right;\">            103.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 69972\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_16-06-40\n",
      "  done: false\n",
      "  episode_len_mean: 102.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.760000000000012\n",
      "  episode_reward_mean: 0.7351000000000014\n",
      "  episode_reward_min: -2.179999999999997\n",
      "  episodes_this_iter: 97\n",
      "  episodes_total: 677\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.7532448784917847\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.014182066141346438\n",
      "          policy_loss: -0.027021054394988933\n",
      "          total_loss: 0.23320526017799464\n",
      "          vf_explained_var: 0.5770830512046814\n",
      "          vf_loss: 0.28492235109910496\n",
      "    num_agent_steps_sampled: 69972\n",
      "    num_agent_steps_trained: 69972\n",
      "    num_steps_sampled: 69972\n",
      "    num_steps_trained: 69972\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.30349206349206\n",
      "    ram_util_percent: 30.896825396825395\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08469617593312634\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.48815984638891\n",
      "    mean_inference_ms: 6.627963515072199\n",
      "    mean_raw_obs_processing_ms: 1.8536720019295996\n",
      "  time_since_restore: 1590.7680730819702\n",
      "  time_this_iter_s: 221.1442427635193\n",
      "  time_total_s: 1590.7680730819702\n",
      "  timers:\n",
      "    learn_throughput: 176.824\n",
      "    learn_time_ms: 56530.808\n",
      "    load_throughput: 39284.772\n",
      "    load_time_ms: 254.45\n",
      "    sample_throughput: 58.656\n",
      "    sample_time_ms: 170416.96\n",
      "    update_time_ms: 23.313\n",
      "  timestamp: 1636301200\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 69972\n",
      "  training_iteration: 7\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 39.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         1590.77</td><td style=\"text-align: right;\">69972</td><td style=\"text-align: right;\">  0.7351</td><td style=\"text-align: right;\">                6.76</td><td style=\"text-align: right;\">               -2.18</td><td style=\"text-align: right;\">            102.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 79968\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_16-10-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.08910891089108\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.620000000000015\n",
      "  episode_reward_mean: 1.062574257425745\n",
      "  episode_reward_min: -1.940000000000001\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 778\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.735616473458771\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015148684692153212\n",
      "          policy_loss: -0.02821433074836038\n",
      "          total_loss: 0.22985127607249042\n",
      "          vf_explained_var: 0.5617288947105408\n",
      "          vf_loss: 0.2823920345586589\n",
      "    num_agent_steps_sampled: 79968\n",
      "    num_agent_steps_trained: 79968\n",
      "    num_steps_sampled: 79968\n",
      "    num_steps_trained: 79968\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.69283667621777\n",
      "    ram_util_percent: 31.12292263610315\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08404563032024978\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.625904491667264\n",
      "    mean_inference_ms: 6.625134731916311\n",
      "    mean_raw_obs_processing_ms: 2.4591054387492206\n",
      "  time_since_restore: 1835.0485799312592\n",
      "  time_this_iter_s: 244.28050684928894\n",
      "  time_total_s: 1835.0485799312592\n",
      "  timers:\n",
      "    learn_throughput: 176.908\n",
      "    learn_time_ms: 56503.932\n",
      "    load_throughput: 40495.437\n",
      "    load_time_ms: 246.843\n",
      "    sample_throughput: 57.921\n",
      "    sample_time_ms: 172579.773\n",
      "    update_time_ms: 23.074\n",
      "  timestamp: 1636301444\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79968\n",
      "  training_iteration: 8\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 39.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         1835.05</td><td style=\"text-align: right;\">79968</td><td style=\"text-align: right;\"> 1.06257</td><td style=\"text-align: right;\">                8.62</td><td style=\"text-align: right;\">               -1.94</td><td style=\"text-align: right;\">           100.089</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 89964\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_16-14-25\n",
      "  done: false\n",
      "  episode_len_mean: 102.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0000000000000115\n",
      "  episode_reward_mean: 0.7126000000000018\n",
      "  episode_reward_min: -2.12\n",
      "  episodes_this_iter: 96\n",
      "  episodes_total: 874\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.725383094029549\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.014758684166334132\n",
      "          policy_loss: -0.030482410080730916\n",
      "          total_loss: 0.23640633389895033\n",
      "          vf_explained_var: 0.526335597038269\n",
      "          vf_loss: 0.29119083863706924\n",
      "    num_agent_steps_sampled: 89964\n",
      "    num_agent_steps_trained: 89964\n",
      "    num_steps_sampled: 89964\n",
      "    num_steps_trained: 89964\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.18730158730159\n",
      "    ram_util_percent: 31.416507936507937\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08425614816635509\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.52725799203431\n",
      "    mean_inference_ms: 6.630633576602407\n",
      "    mean_raw_obs_processing_ms: 2.308140876885916\n",
      "  time_since_restore: 2055.861280441284\n",
      "  time_this_iter_s: 220.81270051002502\n",
      "  time_total_s: 2055.861280441284\n",
      "  timers:\n",
      "    learn_throughput: 176.917\n",
      "    learn_time_ms: 56500.918\n",
      "    load_throughput: 40446.912\n",
      "    load_time_ms: 247.139\n",
      "    sample_throughput: 58.242\n",
      "    sample_time_ms: 171629.445\n",
      "    update_time_ms: 23.748\n",
      "  timestamp: 1636301665\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 89964\n",
      "  training_iteration: 9\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 39.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         2055.86</td><td style=\"text-align: right;\">89964</td><td style=\"text-align: right;\">  0.7126</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">               -2.12</td><td style=\"text-align: right;\">            102.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 99960\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_16-18-10\n",
      "  done: false\n",
      "  episode_len_mean: 99.76237623762377\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.310000000000013\n",
      "  episode_reward_mean: 1.1178217821782208\n",
      "  episode_reward_min: -2.0300000000000002\n",
      "  episodes_this_iter: 101\n",
      "  episodes_total: 975\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.7161481140006303\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.015336657994412804\n",
      "          policy_loss: -0.02860308372312122\n",
      "          total_loss: 0.2253956856094619\n",
      "          vf_explained_var: 0.5550938844680786\n",
      "          vf_loss: 0.2780929193537459\n",
      "    num_agent_steps_sampled: 99960\n",
      "    num_agent_steps_trained: 99960\n",
      "    num_steps_sampled: 99960\n",
      "    num_steps_trained: 99960\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.16947040498442\n",
      "    ram_util_percent: 31.472274143302176\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08415010797834879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.66303014207558\n",
      "    mean_inference_ms: 6.635667411379072\n",
      "    mean_raw_obs_processing_ms: 2.1771668970761047\n",
      "  time_since_restore: 2281.2967178821564\n",
      "  time_this_iter_s: 225.4354374408722\n",
      "  time_total_s: 2281.2967178821564\n",
      "  timers:\n",
      "    learn_throughput: 176.938\n",
      "    learn_time_ms: 56494.394\n",
      "    load_throughput: 40332.407\n",
      "    load_time_ms: 247.84\n",
      "    sample_throughput: 58.341\n",
      "    sample_time_ms: 171337.828\n",
      "    update_time_ms: 22.769\n",
      "  timestamp: 1636301890\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 99960\n",
      "  training_iteration: 10\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 39.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">          2281.3</td><td style=\"text-align: right;\">99960</td><td style=\"text-align: right;\"> 1.11782</td><td style=\"text-align: right;\">               10.31</td><td style=\"text-align: right;\">               -2.03</td><td style=\"text-align: right;\">           99.7624</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 109956\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_16-22-10\n",
      "  done: false\n",
      "  episode_len_mean: 101.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.7900000000000125\n",
      "  episode_reward_mean: 0.9363000000000028\n",
      "  episode_reward_min: -2.0599999999999996\n",
      "  episodes_this_iter: 98\n",
      "  episodes_total: 1073\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.7050572075395505\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01627367809150793\n",
      "          policy_loss: -0.03365043133783799\n",
      "          total_loss: 0.22872364655152982\n",
      "          vf_explained_var: 0.566852867603302\n",
      "          vf_loss: 0.28616991638182065\n",
      "    num_agent_steps_sampled: 109956\n",
      "    num_agent_steps_trained: 109956\n",
      "    num_steps_sampled: 109956\n",
      "    num_steps_trained: 109956\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.23684210526315\n",
      "    ram_util_percent: 31.49561403508772\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08418578821727472\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.52472704148967\n",
      "    mean_inference_ms: 6.6415999894465765\n",
      "    mean_raw_obs_processing_ms: 2.6047975539428943\n",
      "  time_since_restore: 2521.3580510616302\n",
      "  time_this_iter_s: 240.06133317947388\n",
      "  time_total_s: 2521.3580510616302\n",
      "  timers:\n",
      "    learn_throughput: 177.168\n",
      "    learn_time_ms: 56420.906\n",
      "    load_throughput: 39542.752\n",
      "    load_time_ms: 252.79\n",
      "    sample_throughput: 59.589\n",
      "    sample_time_ms: 167748.419\n",
      "    update_time_ms: 20.238\n",
      "  timestamp: 1636302130\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 109956\n",
      "  training_iteration: 11\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 39.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         2521.36</td><td style=\"text-align: right;\">109956</td><td style=\"text-align: right;\">  0.9363</td><td style=\"text-align: right;\">                6.79</td><td style=\"text-align: right;\">               -2.06</td><td style=\"text-align: right;\">            101.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 119952\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_16-26-00\n",
      "  done: false\n",
      "  episode_len_mean: 101.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.610000000000014\n",
      "  episode_reward_mean: 1.2251000000000032\n",
      "  episode_reward_min: -2.179999999999997\n",
      "  episodes_this_iter: 99\n",
      "  episodes_total: 1172\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.685304493170518\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01812668949846224\n",
      "          policy_loss: -0.031576821488192956\n",
      "          total_loss: 0.2543524573278478\n",
      "          vf_explained_var: 0.5433704257011414\n",
      "          vf_loss: 0.3091569850937678\n",
      "    num_agent_steps_sampled: 119952\n",
      "    num_agent_steps_trained: 119952\n",
      "    num_steps_sampled: 119952\n",
      "    num_steps_trained: 119952\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.32018348623853\n",
      "    ram_util_percent: 31.63241590214067\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08408731575299537\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.61098271894996\n",
      "    mean_inference_ms: 6.642751510946564\n",
      "    mean_raw_obs_processing_ms: 2.4828740739292123\n",
      "  time_since_restore: 2750.5196759700775\n",
      "  time_this_iter_s: 229.16162490844727\n",
      "  time_total_s: 2750.5196759700775\n",
      "  timers:\n",
      "    learn_throughput: 177.015\n",
      "    learn_time_ms: 56469.901\n",
      "    load_throughput: 40217.683\n",
      "    load_time_ms: 248.547\n",
      "    sample_throughput: 58.89\n",
      "    sample_time_ms: 169740.658\n",
      "    update_time_ms: 19.866\n",
      "  timestamp: 1636302360\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 119952\n",
      "  training_iteration: 12\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 40.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         2750.52</td><td style=\"text-align: right;\">119952</td><td style=\"text-align: right;\">  1.2251</td><td style=\"text-align: right;\">                8.61</td><td style=\"text-align: right;\">               -2.18</td><td style=\"text-align: right;\">            101.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 129948\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_16-29-41\n",
      "  done: false\n",
      "  episode_len_mean: 102.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.620000000000012\n",
      "  episode_reward_mean: 1.1395000000000028\n",
      "  episode_reward_min: -2.22\n",
      "  episodes_this_iter: 97\n",
      "  episodes_total: 1269\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.675288236854423\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.019158277029229495\n",
      "          policy_loss: -0.03342935424775649\n",
      "          total_loss: 0.24424498225602076\n",
      "          vf_explained_var: 0.5621176362037659\n",
      "          vf_loss: 0.30059556403221227\n",
      "    num_agent_steps_sampled: 129948\n",
      "    num_agent_steps_trained: 129948\n",
      "    num_steps_sampled: 129948\n",
      "    num_steps_trained: 129948\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.13417721518987\n",
      "    ram_util_percent: 31.685126582278482\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0842493959764131\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.54227113100673\n",
      "    mean_inference_ms: 6.642302304374727\n",
      "    mean_raw_obs_processing_ms: 2.3737610994429854\n",
      "  time_since_restore: 2972.1585941314697\n",
      "  time_this_iter_s: 221.6389181613922\n",
      "  time_total_s: 2972.1585941314697\n",
      "  timers:\n",
      "    learn_throughput: 176.984\n",
      "    learn_time_ms: 56479.58\n",
      "    load_throughput: 40347.063\n",
      "    load_time_ms: 247.75\n",
      "    sample_throughput: 58.384\n",
      "    sample_time_ms: 171212.062\n",
      "    update_time_ms: 19.683\n",
      "  timestamp: 1636302581\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 129948\n",
      "  training_iteration: 13\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 39.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         2972.16</td><td style=\"text-align: right;\">129948</td><td style=\"text-align: right;\">  1.1395</td><td style=\"text-align: right;\">                8.62</td><td style=\"text-align: right;\">               -2.22</td><td style=\"text-align: right;\">            102.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 139944\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_16-33-27\n",
      "  done: false\n",
      "  episode_len_mean: 101.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.360000000000012\n",
      "  episode_reward_mean: 1.2552000000000034\n",
      "  episode_reward_min: -2.2099999999999986\n",
      "  episodes_this_iter: 99\n",
      "  episodes_total: 1368\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999998\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.6580861760000896\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.02210920791495533\n",
      "          policy_loss: -0.029866728506600247\n",
      "          total_loss: 0.2913779709042392\n",
      "          vf_explained_var: 0.5548118352890015\n",
      "          vf_loss: 0.34340371924460444\n",
      "    num_agent_steps_sampled: 139944\n",
      "    num_agent_steps_trained: 139944\n",
      "    num_steps_sampled: 139944\n",
      "    num_steps_trained: 139944\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.86739130434782\n",
      "    ram_util_percent: 31.694099378881987\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08421741196354389\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.57748764973112\n",
      "    mean_inference_ms: 6.638248745623635\n",
      "    mean_raw_obs_processing_ms: 2.275809502572864\n",
      "  time_since_restore: 3197.7771451473236\n",
      "  time_this_iter_s: 225.61855101585388\n",
      "  time_total_s: 3197.7771451473236\n",
      "  timers:\n",
      "    learn_throughput: 177.199\n",
      "    learn_time_ms: 56411.188\n",
      "    load_throughput: 40280.514\n",
      "    load_time_ms: 248.16\n",
      "    sample_throughput: 58.821\n",
      "    sample_time_ms: 169938.811\n",
      "    update_time_ms: 19.932\n",
      "  timestamp: 1636302807\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 139944\n",
      "  training_iteration: 14\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 39.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         3197.78</td><td style=\"text-align: right;\">139944</td><td style=\"text-align: right;\">  1.2552</td><td style=\"text-align: right;\">                8.36</td><td style=\"text-align: right;\">               -2.21</td><td style=\"text-align: right;\">            101.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 149940\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_16-37-39\n",
      "  done: false\n",
      "  episode_len_mean: 99.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.520000000000014\n",
      "  episode_reward_mean: 1.4560000000000044\n",
      "  episode_reward_min: -2.0199999999999996\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 1468\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.6538626375361387\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.018803956699661593\n",
      "          policy_loss: -0.036514532742783046\n",
      "          total_loss: 0.22197361307807711\n",
      "          vf_explained_var: 0.6335610151290894\n",
      "          vf_loss: 0.2793855861784556\n",
      "    num_agent_steps_sampled: 149940\n",
      "    num_agent_steps_trained: 149940\n",
      "    num_steps_sampled: 149940\n",
      "    num_steps_trained: 149940\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.13833333333334\n",
      "    ram_util_percent: 31.80222222222222\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08405748677447869\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.703586124266685\n",
      "    mean_inference_ms: 6.636009093333244\n",
      "    mean_raw_obs_processing_ms: 2.551870701972782\n",
      "  time_since_restore: 3450.0260515213013\n",
      "  time_this_iter_s: 252.24890637397766\n",
      "  time_total_s: 3450.0260515213013\n",
      "  timers:\n",
      "    learn_throughput: 177.289\n",
      "    learn_time_ms: 56382.538\n",
      "    load_throughput: 42007.228\n",
      "    load_time_ms: 237.959\n",
      "    sample_throughput: 57.755\n",
      "    sample_time_ms: 173076.755\n",
      "    update_time_ms: 21.654\n",
      "  timestamp: 1636303059\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 149940\n",
      "  training_iteration: 15\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 40.3/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         3450.03</td><td style=\"text-align: right;\">149940</td><td style=\"text-align: right;\">   1.456</td><td style=\"text-align: right;\">                6.52</td><td style=\"text-align: right;\">               -2.02</td><td style=\"text-align: right;\">             99.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 159936\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_16-41-30\n",
      "  done: false\n",
      "  episode_len_mean: 102.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.490000000000014\n",
      "  episode_reward_mean: 1.6574000000000049\n",
      "  episode_reward_min: -2.279999999999999\n",
      "  episodes_this_iter: 98\n",
      "  episodes_total: 1566\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.6275053209728663\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.021212963988494506\n",
      "          policy_loss: -0.03393929168327242\n",
      "          total_loss: 0.27409146750648306\n",
      "          vf_explained_var: 0.6249567270278931\n",
      "          vf_loss: 0.3279419239005472\n",
      "    num_agent_steps_sampled: 159936\n",
      "    num_agent_steps_trained: 159936\n",
      "    num_steps_sampled: 159936\n",
      "    num_steps_trained: 159936\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.86109422492402\n",
      "    ram_util_percent: 32.04498480243161\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08394648981541714\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.72595381760081\n",
      "    mean_inference_ms: 6.634264673797117\n",
      "    mean_raw_obs_processing_ms: 2.4623244036462735\n",
      "  time_since_restore: 3680.772578239441\n",
      "  time_this_iter_s: 230.74652671813965\n",
      "  time_total_s: 3680.772578239441\n",
      "  timers:\n",
      "    learn_throughput: 177.438\n",
      "    learn_time_ms: 56335.11\n",
      "    load_throughput: 42706.602\n",
      "    load_time_ms: 234.062\n",
      "    sample_throughput: 57.285\n",
      "    sample_time_ms: 174497.039\n",
      "    update_time_ms: 22.696\n",
      "  timestamp: 1636303290\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 159936\n",
      "  training_iteration: 16\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 40.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         3680.77</td><td style=\"text-align: right;\">159936</td><td style=\"text-align: right;\">  1.6574</td><td style=\"text-align: right;\">               10.49</td><td style=\"text-align: right;\">               -2.28</td><td style=\"text-align: right;\">             102.2</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 169932\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_16-45-21\n",
      "  done: false\n",
      "  episode_len_mean: 103.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.130000000000017\n",
      "  episode_reward_mean: 1.7559000000000051\n",
      "  episode_reward_min: -2.1400000000000006\n",
      "  episodes_this_iter: 96\n",
      "  episodes_total: 1662\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.623352229900849\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.019678593991341163\n",
      "          policy_loss: -0.035520751268053666\n",
      "          total_loss: 0.24357310466659374\n",
      "          vf_explained_var: 0.586357831954956\n",
      "          vf_loss: 0.29647201208604707\n",
      "    num_agent_steps_sampled: 169932\n",
      "    num_agent_steps_trained: 169932\n",
      "    num_steps_sampled: 169932\n",
      "    num_steps_trained: 169932\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.74255319148935\n",
      "    ram_util_percent: 32.06960486322188\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0841508181926713\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.70408772691177\n",
      "    mean_inference_ms: 6.639878498833476\n",
      "    mean_raw_obs_processing_ms: 2.3829942980776324\n",
      "  time_since_restore: 3911.2639513015747\n",
      "  time_this_iter_s: 230.4913730621338\n",
      "  time_total_s: 3911.2639513015747\n",
      "  timers:\n",
      "    learn_throughput: 177.531\n",
      "    learn_time_ms: 56305.685\n",
      "    load_throughput: 43308.609\n",
      "    load_time_ms: 230.809\n",
      "    sample_throughput: 56.968\n",
      "    sample_time_ms: 175465.55\n",
      "    update_time_ms: 22.006\n",
      "  timestamp: 1636303521\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 169932\n",
      "  training_iteration: 17\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 40.5/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         3911.26</td><td style=\"text-align: right;\">169932</td><td style=\"text-align: right;\">  1.7559</td><td style=\"text-align: right;\">               10.13</td><td style=\"text-align: right;\">               -2.14</td><td style=\"text-align: right;\">             103.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 179928\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_16-49-05\n",
      "  done: false\n",
      "  episode_len_mean: 101.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.500000000000011\n",
      "  episode_reward_mean: 1.3445000000000054\n",
      "  episode_reward_min: -2.04\n",
      "  episodes_this_iter: 99\n",
      "  episodes_total: 1761\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000001\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.6283575642822137\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.020804582247598857\n",
      "          policy_loss: -0.03765451446430296\n",
      "          total_loss: 0.21547712894976456\n",
      "          vf_explained_var: 0.6893129944801331\n",
      "          vf_loss: 0.27005315610868297\n",
      "    num_agent_steps_sampled: 179928\n",
      "    num_agent_steps_trained: 179928\n",
      "    num_steps_sampled: 179928\n",
      "    num_steps_trained: 179928\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.7525\n",
      "    ram_util_percent: 32.069062499999994\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08395852027258666\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.73556642465861\n",
      "    mean_inference_ms: 6.640000109694765\n",
      "    mean_raw_obs_processing_ms: 2.303229565798446\n",
      "  time_since_restore: 4135.755361557007\n",
      "  time_this_iter_s: 224.49141025543213\n",
      "  time_total_s: 4135.755361557007\n",
      "  timers:\n",
      "    learn_throughput: 177.65\n",
      "    learn_time_ms: 56267.801\n",
      "    load_throughput: 42277.364\n",
      "    load_time_ms: 236.439\n",
      "    sample_throughput: 57.607\n",
      "    sample_time_ms: 173520.286\n",
      "    update_time_ms: 21.303\n",
      "  timestamp: 1636303745\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 179928\n",
      "  training_iteration: 18\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 40.4/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         4135.76</td><td style=\"text-align: right;\">179928</td><td style=\"text-align: right;\">  1.3445</td><td style=\"text-align: right;\">                 6.5</td><td style=\"text-align: right;\">               -2.04</td><td style=\"text-align: right;\">            101.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 189924\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_16-53-10\n",
      "  done: false\n",
      "  episode_len_mean: 99.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.410000000000014\n",
      "  episode_reward_mean: 1.3250000000000042\n",
      "  episode_reward_min: -1.9100000000000008\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 1861\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999999\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.6159864511245337\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01864839982678148\n",
      "          policy_loss: -0.03840073677432588\n",
      "          total_loss: 0.2902961689564917\n",
      "          vf_explained_var: 0.6033046841621399\n",
      "          vf_loss: 0.3422691006499987\n",
      "    num_agent_steps_sampled: 189924\n",
      "    num_agent_steps_trained: 189924\n",
      "    num_steps_sampled: 189924\n",
      "    num_steps_trained: 189924\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.98137535816619\n",
      "    ram_util_percent: 32.12893982808023\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08384288309174387\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.8094019696563\n",
      "    mean_inference_ms: 6.644875101772077\n",
      "    mean_raw_obs_processing_ms: 2.5176811298853066\n",
      "  time_since_restore: 4380.774795532227\n",
      "  time_this_iter_s: 245.01943397521973\n",
      "  time_total_s: 4380.774795532227\n",
      "  timers:\n",
      "    learn_throughput: 177.774\n",
      "    learn_time_ms: 56228.567\n",
      "    load_throughput: 42217.5\n",
      "    load_time_ms: 236.774\n",
      "    sample_throughput: 56.803\n",
      "    sample_time_ms: 175977.446\n",
      "    update_time_ms: 23.507\n",
      "  timestamp: 1636303990\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 189924\n",
      "  training_iteration: 19\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 40.7/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         4380.77</td><td style=\"text-align: right;\">189924</td><td style=\"text-align: right;\">   1.325</td><td style=\"text-align: right;\">                6.41</td><td style=\"text-align: right;\">               -1.91</td><td style=\"text-align: right;\">             99.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 199920\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_16-56-53\n",
      "  done: false\n",
      "  episode_len_mean: 102.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.010000000000014\n",
      "  episode_reward_mean: 1.8736000000000055\n",
      "  episode_reward_min: -2.1799999999999975\n",
      "  episodes_this_iter: 98\n",
      "  episodes_total: 1959\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999999\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.599187805509975\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01914685640908187\n",
      "          policy_loss: -0.03634006816766456\n",
      "          total_loss: 0.28411460114467857\n",
      "          vf_explained_var: 0.6516804099082947\n",
      "          vf_loss: 0.3335224202077868\n",
      "    num_agent_steps_sampled: 199920\n",
      "    num_agent_steps_trained: 199920\n",
      "    num_steps_sampled: 199920\n",
      "    num_steps_trained: 199920\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.61037735849057\n",
      "    ram_util_percent: 32.24025157232704\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08383362274451939\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.73685061779367\n",
      "    mean_inference_ms: 6.647495238242438\n",
      "    mean_raw_obs_processing_ms: 2.449328386517318\n",
      "  time_since_restore: 4603.570039272308\n",
      "  time_this_iter_s: 222.7952437400818\n",
      "  time_total_s: 4603.570039272308\n",
      "  timers:\n",
      "    learn_throughput: 177.839\n",
      "    learn_time_ms: 56208.07\n",
      "    load_throughput: 42244.325\n",
      "    load_time_ms: 236.624\n",
      "    sample_throughput: 56.882\n",
      "    sample_time_ms: 175731.783\n",
      "    update_time_ms: 25.369\n",
      "  timestamp: 1636304213\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 199920\n",
      "  training_iteration: 20\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 40.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         4603.57</td><td style=\"text-align: right;\">199920</td><td style=\"text-align: right;\">  1.8736</td><td style=\"text-align: right;\">                8.01</td><td style=\"text-align: right;\">               -2.18</td><td style=\"text-align: right;\">            102.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 209916\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_17-00-41\n",
      "  done: false\n",
      "  episode_len_mean: 101.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.040000000000015\n",
      "  episode_reward_mean: 1.5591000000000046\n",
      "  episode_reward_min: -2.1100000000000008\n",
      "  episodes_this_iter: 99\n",
      "  episodes_total: 2058\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999999\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.5891009259427715\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.018690139271363874\n",
      "          policy_loss: -0.04415796919383554\n",
      "          total_loss: 0.16790224770800424\n",
      "          vf_explained_var: 0.7462381720542908\n",
      "          vf_loss: 0.22533538005290887\n",
      "    num_agent_steps_sampled: 209916\n",
      "    num_agent_steps_trained: 209916\n",
      "    num_steps_sampled: 209916\n",
      "    num_steps_trained: 209916\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.15630769230769\n",
      "    ram_util_percent: 32.227384615384615\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08384723479132586\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.77941589627724\n",
      "    mean_inference_ms: 6.661071864199139\n",
      "    mean_raw_obs_processing_ms: 2.383685646042561\n",
      "  time_since_restore: 4831.212926864624\n",
      "  time_this_iter_s: 227.64288759231567\n",
      "  time_total_s: 4831.212926864624\n",
      "  timers:\n",
      "    learn_throughput: 177.604\n",
      "    learn_time_ms: 56282.671\n",
      "    load_throughput: 42135.863\n",
      "    load_time_ms: 237.233\n",
      "    sample_throughput: 57.312\n",
      "    sample_time_ms: 174414.592\n",
      "    update_time_ms: 25.238\n",
      "  timestamp: 1636304441\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 209916\n",
      "  training_iteration: 21\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 40.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         4831.21</td><td style=\"text-align: right;\">209916</td><td style=\"text-align: right;\">  1.5591</td><td style=\"text-align: right;\">                8.04</td><td style=\"text-align: right;\">               -2.11</td><td style=\"text-align: right;\">            101.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 219912\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_17-04-46\n",
      "  done: false\n",
      "  episode_len_mean: 98.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.340000000000016\n",
      "  episode_reward_mean: 1.5392000000000043\n",
      "  episode_reward_min: -2.1\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 2158\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999999\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.601542722873199\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.019367127363236075\n",
      "          policy_loss: -0.04049163330664747\n",
      "          total_loss: 0.22793413317308595\n",
      "          vf_explained_var: 0.6623491644859314\n",
      "          vf_loss: 0.2813683819375996\n",
      "    num_agent_steps_sampled: 219912\n",
      "    num_agent_steps_trained: 219912\n",
      "    num_steps_sampled: 219912\n",
      "    num_steps_trained: 219912\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.41575931232092\n",
      "    ram_util_percent: 32.19169054441261\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08379137977052128\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.79818280472874\n",
      "    mean_inference_ms: 6.6695118584661195\n",
      "    mean_raw_obs_processing_ms: 2.5811405824888016\n",
      "  time_since_restore: 5076.064461708069\n",
      "  time_this_iter_s: 244.85153484344482\n",
      "  time_total_s: 5076.064461708069\n",
      "  timers:\n",
      "    learn_throughput: 177.662\n",
      "    learn_time_ms: 56264.023\n",
      "    load_throughput: 42548.099\n",
      "    load_time_ms: 234.934\n",
      "    sample_throughput: 56.794\n",
      "    sample_time_ms: 176005.738\n",
      "    update_time_ms: 24.797\n",
      "  timestamp: 1636304686\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 219912\n",
      "  training_iteration: 22\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 40.6/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         5076.06</td><td style=\"text-align: right;\">219912</td><td style=\"text-align: right;\">  1.5392</td><td style=\"text-align: right;\">                8.34</td><td style=\"text-align: right;\">                -2.1</td><td style=\"text-align: right;\">             98.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 229908\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_17-08-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.580000000000014\n",
      "  episode_reward_mean: 1.9033000000000058\n",
      "  episode_reward_min: -1.750000000000001\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 2258\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999999\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.5885871310519355\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.021262941129300942\n",
      "          policy_loss: -0.03870657231722377\n",
      "          total_loss: 0.2352805390030655\n",
      "          vf_explained_var: 0.7006276845932007\n",
      "          vf_loss: 0.28552049693898257\n",
      "    num_agent_steps_sampled: 229908\n",
      "    num_agent_steps_trained: 229908\n",
      "    num_steps_sampled: 229908\n",
      "    num_steps_trained: 229908\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.06292834890967\n",
      "    ram_util_percent: 32.400000000000006\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08385703887142039\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.82115462900338\n",
      "    mean_inference_ms: 6.67152737731816\n",
      "    mean_raw_obs_processing_ms: 2.5146511610971323\n",
      "  time_since_restore: 5301.553765535355\n",
      "  time_this_iter_s: 225.48930382728577\n",
      "  time_total_s: 5301.553765535355\n",
      "  timers:\n",
      "    learn_throughput: 177.601\n",
      "    learn_time_ms: 56283.386\n",
      "    load_throughput: 42856.935\n",
      "    load_time_ms: 233.241\n",
      "    sample_throughput: 56.675\n",
      "    sample_time_ms: 176373.753\n",
      "    update_time_ms: 24.656\n",
      "  timestamp: 1636304911\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 229908\n",
      "  training_iteration: 23\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 41.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         5301.55</td><td style=\"text-align: right;\">229908</td><td style=\"text-align: right;\">  1.9033</td><td style=\"text-align: right;\">               10.58</td><td style=\"text-align: right;\">               -1.75</td><td style=\"text-align: right;\">            100.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 239904\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_17-12-15\n",
      "  done: false\n",
      "  episode_len_mean: 102.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.03000000000002\n",
      "  episode_reward_mean: 1.6201000000000059\n",
      "  episode_reward_min: -2.1599999999999957\n",
      "  episodes_this_iter: 98\n",
      "  episodes_total: 2356\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.5825857755465385\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.018385470489326015\n",
      "          policy_loss: -0.04014591830822392\n",
      "          total_loss: 0.1955351237924053\n",
      "          vf_explained_var: 0.7560774087905884\n",
      "          vf_loss: 0.24289160979768404\n",
      "    num_agent_steps_sampled: 239904\n",
      "    num_agent_steps_trained: 239904\n",
      "    num_steps_sampled: 239904\n",
      "    num_steps_trained: 239904\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.84576802507839\n",
      "    ram_util_percent: 32.52225705329154\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08380088307286354\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.79730740179611\n",
      "    mean_inference_ms: 6.67258682173863\n",
      "    mean_raw_obs_processing_ms: 2.4562241323494836\n",
      "  time_since_restore: 5524.941787242889\n",
      "  time_this_iter_s: 223.3880217075348\n",
      "  time_total_s: 5524.941787242889\n",
      "  timers:\n",
      "    learn_throughput: 177.655\n",
      "    learn_time_ms: 56266.229\n",
      "    load_throughput: 41892.887\n",
      "    load_time_ms: 238.609\n",
      "    sample_throughput: 56.743\n",
      "    sample_time_ms: 176162.309\n",
      "    update_time_ms: 24.594\n",
      "  timestamp: 1636305135\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 239904\n",
      "  training_iteration: 24\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 40.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         5524.94</td><td style=\"text-align: right;\">239904</td><td style=\"text-align: right;\">  1.6201</td><td style=\"text-align: right;\">               10.03</td><td style=\"text-align: right;\">               -2.16</td><td style=\"text-align: right;\">            102.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 249900\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_17-15-52\n",
      "  done: false\n",
      "  episode_len_mean: 103.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 7.980000000000015\n",
      "  episode_reward_mean: 1.6977000000000055\n",
      "  episode_reward_min: -2.000000000000001\n",
      "  episodes_this_iter: 96\n",
      "  episodes_total: 2452\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.5827013498697524\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.016713561168718756\n",
      "          policy_loss: -0.04357304242240567\n",
      "          total_loss: 0.19037485939936125\n",
      "          vf_explained_var: 0.7218834757804871\n",
      "          vf_loss: 0.24285243227759487\n",
      "    num_agent_steps_sampled: 249900\n",
      "    num_agent_steps_trained: 249900\n",
      "    num_steps_sampled: 249900\n",
      "    num_steps_trained: 249900\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.63516129032259\n",
      "    ram_util_percent: 32.498387096774195\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08381534351160652\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.689810762642445\n",
      "    mean_inference_ms: 6.670238758411695\n",
      "    mean_raw_obs_processing_ms: 2.4032331218246745\n",
      "  time_since_restore: 5742.408999919891\n",
      "  time_this_iter_s: 217.46721267700195\n",
      "  time_total_s: 5742.408999919891\n",
      "  timers:\n",
      "    learn_throughput: 177.788\n",
      "    learn_time_ms: 56224.116\n",
      "    load_throughput: 41034.208\n",
      "    load_time_ms: 243.602\n",
      "    sample_throughput: 57.872\n",
      "    sample_time_ms: 172724.525\n",
      "    update_time_ms: 22.441\n",
      "  timestamp: 1636305352\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 249900\n",
      "  training_iteration: 25\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 40.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         5742.41</td><td style=\"text-align: right;\">249900</td><td style=\"text-align: right;\">  1.6977</td><td style=\"text-align: right;\">                7.98</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            103.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 259896\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_17-19-58\n",
      "  done: false\n",
      "  episode_len_mean: 101.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.650000000000016\n",
      "  episode_reward_mean: 1.716600000000005\n",
      "  episode_reward_min: -1.800000000000001\n",
      "  episodes_this_iter: 98\n",
      "  episodes_total: 2550\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.5849983394655407\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.0182502747045965\n",
      "          policy_loss: -0.03970588273408576\n",
      "          total_loss: 0.1886894057608313\n",
      "          vf_explained_var: 0.7178239822387695\n",
      "          vf_loss: 0.23576686814363695\n",
      "    num_agent_steps_sampled: 259896\n",
      "    num_agent_steps_trained: 259896\n",
      "    num_steps_sampled: 259896\n",
      "    num_steps_trained: 259896\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.53742857142858\n",
      "    ram_util_percent: 32.41742857142858\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08348981302487109\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.72551110563254\n",
      "    mean_inference_ms: 6.66202487457558\n",
      "    mean_raw_obs_processing_ms: 2.545579926694021\n",
      "  time_since_restore: 5987.540562868118\n",
      "  time_this_iter_s: 245.13156294822693\n",
      "  time_total_s: 5987.540562868118\n",
      "  timers:\n",
      "    learn_throughput: 177.634\n",
      "    learn_time_ms: 56272.887\n",
      "    load_throughput: 41053.591\n",
      "    load_time_ms: 243.487\n",
      "    sample_throughput: 57.41\n",
      "    sample_time_ms: 174116.433\n",
      "    update_time_ms: 21.185\n",
      "  timestamp: 1636305598\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 259896\n",
      "  training_iteration: 26\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 40.9/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         5987.54</td><td style=\"text-align: right;\">259896</td><td style=\"text-align: right;\">  1.7166</td><td style=\"text-align: right;\">                8.65</td><td style=\"text-align: right;\">                -1.8</td><td style=\"text-align: right;\">             101.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 269892\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_17-23-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.15000000000002\n",
      "  episode_reward_mean: 1.666000000000006\n",
      "  episode_reward_min: -2.2699999999999974\n",
      "  episodes_this_iter: 99\n",
      "  episodes_total: 2649\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.5730708503315594\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01772320087068398\n",
      "          policy_loss: -0.04475393624514596\n",
      "          total_loss: 0.17393132797482178\n",
      "          vf_explained_var: 0.7575363516807556\n",
      "          vf_loss: 0.22647123055325613\n",
      "    num_agent_steps_sampled: 269892\n",
      "    num_agent_steps_trained: 269892\n",
      "    num_steps_sampled: 269892\n",
      "    num_steps_trained: 269892\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.35675675675677\n",
      "    ram_util_percent: 32.608108108108105\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08342406189311374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.685191848672034\n",
      "    mean_inference_ms: 6.6576676758784235\n",
      "    mean_raw_obs_processing_ms: 2.49198412281525\n",
      "  time_since_restore: 6220.884608268738\n",
      "  time_this_iter_s: 233.3440454006195\n",
      "  time_total_s: 6220.884608268738\n",
      "  timers:\n",
      "    learn_throughput: 177.557\n",
      "    learn_time_ms: 56297.406\n",
      "    load_throughput: 40175.31\n",
      "    load_time_ms: 248.81\n",
      "    sample_throughput: 57.326\n",
      "    sample_time_ms: 174371.871\n",
      "    update_time_ms: 21.271\n",
      "  timestamp: 1636305831\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 269892\n",
      "  training_iteration: 27\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 41.2/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         6220.88</td><td style=\"text-align: right;\">269892</td><td style=\"text-align: right;\">   1.666</td><td style=\"text-align: right;\">               12.15</td><td style=\"text-align: right;\">               -2.27</td><td style=\"text-align: right;\">            100.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 279888\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_17-27-43\n",
      "  done: false\n",
      "  episode_len_mean: 101.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 12.120000000000015\n",
      "  episode_reward_mean: 1.9893000000000063\n",
      "  episode_reward_min: -2.0200000000000005\n",
      "  episodes_this_iter: 98\n",
      "  episodes_total: 2747\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.5676373936172223\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.01945985881261614\n",
      "          policy_loss: -0.0434726883442356\n",
      "          total_loss: 0.20954847002449709\n",
      "          vf_explained_var: 0.7567579746246338\n",
      "          vf_loss: 0.25899442442270937\n",
      "    num_agent_steps_sampled: 279888\n",
      "    num_agent_steps_trained: 279888\n",
      "    num_steps_sampled: 279888\n",
      "    num_steps_trained: 279888\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.39486404833836\n",
      "    ram_util_percent: 32.63021148036254\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08320847562861794\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.74315527786754\n",
      "    mean_inference_ms: 6.65303179340266\n",
      "    mean_raw_obs_processing_ms: 2.441142621546411\n",
      "  time_since_restore: 6453.43466424942\n",
      "  time_this_iter_s: 232.55005598068237\n",
      "  time_total_s: 6453.43466424942\n",
      "  timers:\n",
      "    learn_throughput: 177.467\n",
      "    learn_time_ms: 56325.89\n",
      "    load_throughput: 40132.896\n",
      "    load_time_ms: 249.072\n",
      "    sample_throughput: 57.071\n",
      "    sample_time_ms: 175148.832\n",
      "    update_time_ms: 21.473\n",
      "  timestamp: 1636306063\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 279888\n",
      "  training_iteration: 28\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 41.1/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         6453.43</td><td style=\"text-align: right;\">279888</td><td style=\"text-align: right;\">  1.9893</td><td style=\"text-align: right;\">               12.12</td><td style=\"text-align: right;\">               -2.02</td><td style=\"text-align: right;\">            101.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9115)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9108)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_ef690_00000:\n",
      "  agent_timesteps_total: 289884\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-07_17-31-49\n",
      "  done: false\n",
      "  episode_len_mean: 99.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.360000000000017\n",
      "  episode_reward_mean: 1.7643000000000049\n",
      "  episode_reward_min: -2.0699999999999994\n",
      "  episodes_this_iter: 100\n",
      "  episodes_total: 2847\n",
      "  experiment_id: 5986ad5d0d3246ff80964fd8aadc2f01\n",
      "  hostname: cds2\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125\n",
      "          cur_lr: 0.00010000000000000002\n",
      "          entropy: 2.574681407162267\n",
      "          entropy_coeff: 0.01\n",
      "          kl: 0.0186158744570433\n",
      "          policy_loss: -0.04669317620464115\n",
      "          total_loss: 0.16800117368738238\n",
      "          vf_explained_var: 0.780113697052002\n",
      "          vf_loss: 0.22159259046435867\n",
      "    num_agent_steps_sampled: 289884\n",
      "    num_agent_steps_trained: 289884\n",
      "    num_steps_sampled: 289884\n",
      "    num_steps_trained: 289884\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 10.55.229.87\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.5945868945869\n",
      "    ram_util_percent: 32.6042735042735\n",
      "  pid: 9113\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08318402978412129\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.72409602843658\n",
      "    mean_inference_ms: 6.65131486762858\n",
      "    mean_raw_obs_processing_ms: 2.50975608403765\n",
      "  time_since_restore: 6699.268540859222\n",
      "  time_this_iter_s: 245.83387660980225\n",
      "  time_total_s: 6699.268540859222\n",
      "  timers:\n",
      "    learn_throughput: 177.455\n",
      "    learn_time_ms: 56329.835\n",
      "    load_throughput: 39811.969\n",
      "    load_time_ms: 251.08\n",
      "    sample_throughput: 57.054\n",
      "    sample_time_ms: 175202.536\n",
      "    update_time_ms: 18.496\n",
      "  timestamp: 1636306309\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 289884\n",
      "  training_iteration: 29\n",
      "  trial_id: ef690_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 41.0/125.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/16 CPUs, 1.0/1 GPUs, 0.0/73.94 GiB heap, 0.0/35.68 GiB objects (0.0/1.0 accelerator_type:V100)<br>Result logdir: /IGLU-Minecraft/checkpoints/all_tasks/PPO_2021-11-07_15-39-46<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_ef690_00000</td><td>RUNNING </td><td>10.55.229.87:9113</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         6699.27</td><td style=\"text-align: right;\">289884</td><td style=\"text-align: right;\">  1.7643</td><td style=\"text-align: right;\">                8.36</td><td style=\"text-align: right;\">               -2.07</td><td style=\"text-align: right;\">             99.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=9117)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "analysis = tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 3,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 5_000,\n",
    "             \"lr\": 1e-4,\n",
    "             #\"gamma\": 0.99,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO All Tasks pretrained (AngelaCNN+fusion) (3 noops after placement) r: -0.01 div10\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger],\n",
    "        local_dir=\"/IGLU-Minecraft/checkpoints/all_tasks\",\n",
    "        keep_checkpoints_num=50,\n",
    "        checkpoint_freq=5,\n",
    "        checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
