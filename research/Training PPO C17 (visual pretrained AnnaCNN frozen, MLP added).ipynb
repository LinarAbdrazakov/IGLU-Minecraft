{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9fd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4, padding=0),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(64, 512, kernel_size=2, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        features_dim = 512\n",
    "        self.encoder = VisualEncoder()\n",
    "        self.encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AnnaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        mlp_feature_dim = 128\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(features_dim, mlp_feature_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(mlp_feature_dim, mlp_feature_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(mlp_feature_dim, mlp_feature_dim),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.action_head = nn.Linear(mlp_feature_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(mlp_feature_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.encoder.cuda()\n",
    "            self.mlp.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        if self.use_cuda:\n",
    "            obs.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = self.encoder(obs)\n",
    "            \n",
    "        features = self.mlp(features)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C17']))\n",
    "    env = PovOnlyWrapper(env)\n",
    "    env = IgluActionWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n",
      "2021-09-27 09:45:26,222\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-09-27 09:45:26,238\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[2m\u001b[36m(pid=457)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=457)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO C17 pretrained and frozen (AnnaCNN), MLP added (128)</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/a44fc_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/a44fc_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20210927_094526-a44fc_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=457)\u001b[0m 2021-09-27 09:45:30,315\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=457)\u001b[0m 2021-09-27 09:45:30,315\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=452)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=452)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=457)\u001b[0m 2021-09-27 09:45:37,146\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=452)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=452)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=452)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=452)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=452)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=452)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-46-36\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.805732337633769\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010329088540200829\n",
      "          policy_loss: 0.04055147626333767\n",
      "          total_loss: 0.015616235468122694\n",
      "          vf_explained_var: 0.22965596616268158\n",
      "          vf_loss: 0.0010562666378165079\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.41529411764706\n",
      "    ram_util_percent: 76.1\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.05098656340912506\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 56.86052624400441\n",
      "    mean_inference_ms: 1.7448214741496297\n",
      "    mean_raw_obs_processing_ms: 0.1474501012445806\n",
      "  time_since_restore: 59.54776453971863\n",
      "  time_this_iter_s: 59.54776453971863\n",
      "  time_total_s: 59.54776453971863\n",
      "  timers:\n",
      "    learn_throughput: 1698.007\n",
      "    learn_time_ms: 588.926\n",
      "    load_throughput: 57213.259\n",
      "    load_time_ms: 17.478\n",
      "    sample_throughput: 16.968\n",
      "    sample_time_ms: 58935.545\n",
      "    update_time_ms: 2.067\n",
      "  timestamp: 1632735996\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         59.5478</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-46-48\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8035645617379084\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013295291665596388\n",
      "          policy_loss: 0.04517248993118604\n",
      "          total_loss: 0.020256566173500485\n",
      "          vf_explained_var: 0.374893456697464\n",
      "          vf_loss: 0.00046066646367156255\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.89411764705882\n",
      "    ram_util_percent: 83.7764705882353\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.049803695867750714\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 44.894239871954255\n",
      "    mean_inference_ms: 1.7372605496982578\n",
      "    mean_raw_obs_processing_ms: 0.14265923720789153\n",
      "  time_since_restore: 71.05102133750916\n",
      "  time_this_iter_s: 11.503256797790527\n",
      "  time_total_s: 71.05102133750916\n",
      "  timers:\n",
      "    learn_throughput: 1755.083\n",
      "    learn_time_ms: 569.774\n",
      "    load_throughput: 95695.913\n",
      "    load_time_ms: 10.45\n",
      "    sample_throughput: 28.622\n",
      "    sample_time_ms: 34938.342\n",
      "    update_time_ms: 2.121\n",
      "  timestamp: 1632736008\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">          71.051</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-46-59\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -1.0\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 3\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8037554873360526\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010338222757646814\n",
      "          policy_loss: -0.05599654912948608\n",
      "          total_loss: -0.06599820686711205\n",
      "          vf_explained_var: -0.4692392647266388\n",
      "          vf_loss: 0.01596825069670255\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.39375\n",
      "    ram_util_percent: 83.29374999999999\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.049001011663576066\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 38.21253436019621\n",
      "    mean_inference_ms: 1.7250578981470426\n",
      "    mean_raw_obs_processing_ms: 0.1427067995300896\n",
      "  time_since_restore: 82.17550206184387\n",
      "  time_this_iter_s: 11.124480724334717\n",
      "  time_total_s: 82.17550206184387\n",
      "  timers:\n",
      "    learn_throughput: 1781.614\n",
      "    learn_time_ms: 561.289\n",
      "    load_throughput: 121425.034\n",
      "    load_time_ms: 8.236\n",
      "    sample_throughput: 37.291\n",
      "    sample_time_ms: 26816.107\n",
      "    update_time_ms: 1.903\n",
      "  timestamp: 1632736019\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         82.1755</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">      -1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-47-10\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.75\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 4\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7964626815583973\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010901198233542731\n",
      "          policy_loss: 0.03405804791384273\n",
      "          total_loss: 0.008928658937414487\n",
      "          vf_explained_var: 0.1497691422700882\n",
      "          vf_loss: 0.0006549998982033382\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.96000000000001\n",
      "    ram_util_percent: 83.18666666666668\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04848174538311853\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.838228155667\n",
      "    mean_inference_ms: 1.7134318283945\n",
      "    mean_raw_obs_processing_ms: 0.14142244150350947\n",
      "  time_since_restore: 92.92891764640808\n",
      "  time_this_iter_s: 10.753415584564209\n",
      "  time_total_s: 92.92891764640808\n",
      "  timers:\n",
      "    learn_throughput: 1744.499\n",
      "    learn_time_ms: 573.23\n",
      "    load_throughput: 142922.266\n",
      "    load_time_ms: 6.997\n",
      "    sample_throughput: 44.162\n",
      "    sample_time_ms: 22644.125\n",
      "    update_time_ms: 1.841\n",
      "  timestamp: 1632736030\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         92.9289</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">   -0.75</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-47-21\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 5\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7887028376261394\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012098019660501065\n",
      "          policy_loss: -0.030468663490480847\n",
      "          total_loss: -0.05558622462881936\n",
      "          vf_explained_var: -0.0752684697508812\n",
      "          vf_loss: 0.00034986095399492317\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.675\n",
      "    ram_util_percent: 83.23124999999999\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0480941138629103\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.740391899526884\n",
      "    mean_inference_ms: 1.7057948011145219\n",
      "    mean_raw_obs_processing_ms: 0.14004647187443947\n",
      "  time_since_restore: 104.2532594203949\n",
      "  time_this_iter_s: 11.324341773986816\n",
      "  time_total_s: 104.2532594203949\n",
      "  timers:\n",
      "    learn_throughput: 1756.422\n",
      "    learn_time_ms: 569.339\n",
      "    load_throughput: 156821.034\n",
      "    load_time_ms: 6.377\n",
      "    sample_throughput: 49.34\n",
      "    sample_time_ms: 20267.582\n",
      "    update_time_ms: 1.778\n",
      "  timestamp: 1632736041\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         104.253</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">    -0.6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-47-32\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 6\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.77143173482683\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012219150867285405\n",
      "          policy_loss: 0.06433627787563535\n",
      "          total_loss: 0.03963676417867343\n",
      "          vf_explained_var: -0.4600197672843933\n",
      "          vf_loss: 0.0005709710395118843\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.80588235294117\n",
      "    ram_util_percent: 83.12352941176471\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047769708548272345\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.416275533883464\n",
      "    mean_inference_ms: 1.699896782844249\n",
      "    mean_raw_obs_processing_ms: 0.1388628529464796\n",
      "  time_since_restore: 115.70749139785767\n",
      "  time_this_iter_s: 11.454231977462769\n",
      "  time_total_s: 115.70749139785767\n",
      "  timers:\n",
      "    learn_throughput: 1768.603\n",
      "    learn_time_ms: 565.418\n",
      "    load_throughput: 171149.51\n",
      "    load_time_ms: 5.843\n",
      "    sample_throughput: 53.458\n",
      "    sample_time_ms: 18706.4\n",
      "    update_time_ms: 1.738\n",
      "  timestamp: 1632736052\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         115.707</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">    -0.5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-47-43\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.42857142857142855\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 7\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7427472008599176\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009640759892151456\n",
      "          policy_loss: -0.04384344079428249\n",
      "          total_loss: -0.06899384649263487\n",
      "          vf_explained_var: -0.739227294921875\n",
      "          vf_loss: 0.0003489170508045289\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 7000\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 7000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.32666666666667\n",
      "    ram_util_percent: 82.75999999999999\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04747196570264193\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.582821387168405\n",
      "    mean_inference_ms: 1.6942146346583384\n",
      "    mean_raw_obs_processing_ms: 0.13802522533551634\n",
      "  time_since_restore: 126.43784499168396\n",
      "  time_this_iter_s: 10.730353593826294\n",
      "  time_total_s: 126.43784499168396\n",
      "  timers:\n",
      "    learn_throughput: 1763.307\n",
      "    learn_time_ms: 567.116\n",
      "    load_throughput: 181904.587\n",
      "    load_time_ms: 5.497\n",
      "    sample_throughput: 57.197\n",
      "    sample_time_ms: 17483.345\n",
      "    update_time_ms: 1.7\n",
      "  timestamp: 1632736063\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         126.438</td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\">-0.428571</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-47-54\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.375\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 8\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.714538672235277\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010679104646621848\n",
      "          policy_loss: 0.024115999622477426\n",
      "          total_loss: -0.0006353208588229286\n",
      "          vf_explained_var: -0.5791343450546265\n",
      "          vf_loss: 0.0002582480940165826\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.137499999999996\n",
      "    ram_util_percent: 82.80625\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04721814885867317\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.099651752813834\n",
      "    mean_inference_ms: 1.6894787256563228\n",
      "    mean_raw_obs_processing_ms: 0.13733678104445057\n",
      "  time_since_restore: 137.55194473266602\n",
      "  time_this_iter_s: 11.114099740982056\n",
      "  time_total_s: 137.55194473266602\n",
      "  timers:\n",
      "    learn_throughput: 1759.222\n",
      "    learn_time_ms: 568.433\n",
      "    load_throughput: 191860.209\n",
      "    load_time_ms: 5.212\n",
      "    sample_throughput: 60.191\n",
      "    sample_time_ms: 16613.861\n",
      "    update_time_ms: 1.68\n",
      "  timestamp: 1632736074\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         137.552</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">  -0.375</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-48-05\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3333333333333333\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 9\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.718460604879591\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01222960937883593\n",
      "          policy_loss: -0.03984375206960572\n",
      "          total_loss: -0.06441102460440662\n",
      "          vf_explained_var: -0.8434838056564331\n",
      "          vf_loss: 0.00017141049271837498\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 9000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.986666666666665\n",
      "    ram_util_percent: 83.00666666666667\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047000115796665805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.867198101293987\n",
      "    mean_inference_ms: 1.685010049486854\n",
      "    mean_raw_obs_processing_ms: 0.13668190348143727\n",
      "  time_since_restore: 148.23350191116333\n",
      "  time_this_iter_s: 10.681557178497314\n",
      "  time_total_s: 148.23350191116333\n",
      "  timers:\n",
      "    learn_throughput: 1765.809\n",
      "    learn_time_ms: 566.313\n",
      "    load_throughput: 200510.647\n",
      "    load_time_ms: 4.987\n",
      "    sample_throughput: 62.922\n",
      "    sample_time_ms: 15892.735\n",
      "    update_time_ms: 1.663\n",
      "  timestamp: 1632736085\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         148.234</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">-0.333333</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-48-16\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 10\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.682468605041504\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010493182629358168\n",
      "          policy_loss: -0.03741591659684976\n",
      "          total_loss: -0.06200905599527889\n",
      "          vf_explained_var: -0.6192070245742798\n",
      "          vf_loss: 0.0001329097007126418\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.10625\n",
      "    ram_util_percent: 82.775\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046807342969137196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.826741160147463\n",
      "    mean_inference_ms: 1.681268829412495\n",
      "    mean_raw_obs_processing_ms: 0.13615007621283642\n",
      "  time_since_restore: 159.2066912651062\n",
      "  time_this_iter_s: 10.973189353942871\n",
      "  time_total_s: 159.2066912651062\n",
      "  timers:\n",
      "    learn_throughput: 1764.613\n",
      "    learn_time_ms: 566.697\n",
      "    load_throughput: 205659.593\n",
      "    load_time_ms: 4.862\n",
      "    sample_throughput: 65.177\n",
      "    sample_time_ms: 15342.928\n",
      "    update_time_ms: 1.642\n",
      "  timestamp: 1632736096\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         159.207</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">    -0.3</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 11000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-48-27\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2727272727272727\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 11\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.681708264350891\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01291343847153707\n",
      "          policy_loss: 0.008813033087386025\n",
      "          total_loss: -0.015226711829503378\n",
      "          vf_explained_var: -0.40170586109161377\n",
      "          vf_loss: 0.00019465037880258429\n",
      "    num_agent_steps_sampled: 11000\n",
      "    num_agent_steps_trained: 11000\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 11000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.675\n",
      "    ram_util_percent: 82.74375\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046629237214457435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.9346625802761\n",
      "    mean_inference_ms: 1.6779417442706228\n",
      "    mean_raw_obs_processing_ms: 0.13565442351638424\n",
      "  time_since_restore: 170.1006977558136\n",
      "  time_this_iter_s: 10.894006490707397\n",
      "  time_total_s: 170.1006977558136\n",
      "  timers:\n",
      "    learn_throughput: 1777.467\n",
      "    learn_time_ms: 562.598\n",
      "    load_throughput: 290667.572\n",
      "    load_time_ms: 3.44\n",
      "    sample_throughput: 95.391\n",
      "    sample_time_ms: 10483.181\n",
      "    update_time_ms: 1.59\n",
      "  timestamp: 1632736107\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         170.101</td><td style=\"text-align: right;\">11000</td><td style=\"text-align: right;\">-0.272727</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-48-38\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.25\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 12\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6555524932013617\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012537563103542008\n",
      "          policy_loss: -0.020764565136697558\n",
      "          total_loss: -0.044588534409801164\n",
      "          vf_explained_var: -0.57622230052948\n",
      "          vf_loss: 0.00022404208042037984\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.973333333333336\n",
      "    ram_util_percent: 82.98666666666666\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04646872215079567\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.160500374267766\n",
      "    mean_inference_ms: 1.6749439648354958\n",
      "    mean_raw_obs_processing_ms: 0.1352801149064358\n",
      "  time_since_restore: 181.05522227287292\n",
      "  time_this_iter_s: 10.954524517059326\n",
      "  time_total_s: 181.05522227287292\n",
      "  timers:\n",
      "    learn_throughput: 1780.63\n",
      "    learn_time_ms: 561.599\n",
      "    load_throughput: 292104.827\n",
      "    load_time_ms: 3.423\n",
      "    sample_throughput: 95.881\n",
      "    sample_time_ms: 10429.617\n",
      "    update_time_ms: 1.518\n",
      "  timestamp: 1632736118\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         181.055</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   -0.25</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 13000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-48-49\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.23076923076923078\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 13\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.689887107743157\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00840642058701544\n",
      "          policy_loss: 0.07942151054739952\n",
      "          total_loss: 0.054325954119364424\n",
      "          vf_explained_var: -0.03170866519212723\n",
      "          vf_loss: 0.00012202818033983931\n",
      "    num_agent_steps_sampled: 13000\n",
      "    num_agent_steps_trained: 13000\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 13000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.98125\n",
      "    ram_util_percent: 83.08125\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04631547421956149\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.481192515130626\n",
      "    mean_inference_ms: 1.6720120785383632\n",
      "    mean_raw_obs_processing_ms: 0.1349037649197379\n",
      "  time_since_restore: 191.92525959014893\n",
      "  time_this_iter_s: 10.870037317276001\n",
      "  time_total_s: 191.92525959014893\n",
      "  timers:\n",
      "    learn_throughput: 1784.224\n",
      "    learn_time_ms: 560.468\n",
      "    load_throughput: 298151.368\n",
      "    load_time_ms: 3.354\n",
      "    sample_throughput: 96.104\n",
      "    sample_time_ms: 10405.382\n",
      "    update_time_ms: 1.518\n",
      "  timestamp: 1632736129\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         191.925</td><td style=\"text-align: right;\">13000</td><td style=\"text-align: right;\">-0.230769</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 14000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-49-00\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.21428571428571427\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 14\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.677726886007521\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012382958547477932\n",
      "          policy_loss: 0.08099757788909806\n",
      "          total_loss: 0.05687437421745724\n",
      "          vf_explained_var: -0.8712305426597595\n",
      "          vf_loss: 0.00017747031419680247\n",
      "    num_agent_steps_sampled: 14000\n",
      "    num_agent_steps_trained: 14000\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.68666666666666\n",
      "    ram_util_percent: 83.18000000000002\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04617520803508727\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.87863970224534\n",
      "    mean_inference_ms: 1.6693844090915648\n",
      "    mean_raw_obs_processing_ms: 0.13456925416967921\n",
      "  time_since_restore: 202.65058994293213\n",
      "  time_this_iter_s: 10.725330352783203\n",
      "  time_total_s: 202.65058994293213\n",
      "  timers:\n",
      "    learn_throughput: 1805.255\n",
      "    learn_time_ms: 553.938\n",
      "    load_throughput: 294744.594\n",
      "    load_time_ms: 3.393\n",
      "    sample_throughput: 96.063\n",
      "    sample_time_ms: 10409.876\n",
      "    update_time_ms: 1.506\n",
      "  timestamp: 1632736140\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         202.651</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\">-0.214286</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-49-10\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 15\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6460800303353205\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010132130489357847\n",
      "          policy_loss: 0.06351352532704671\n",
      "          total_loss: 0.03927651767929395\n",
      "          vf_explained_var: -0.7520787715911865\n",
      "          vf_loss: 0.0001973647232969395\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_agent_steps_trained: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.506666666666675\n",
      "    ram_util_percent: 83.20666666666669\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04604464328730649\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.339274219510987\n",
      "    mean_inference_ms: 1.6668449898028095\n",
      "    mean_raw_obs_processing_ms: 0.13424107661922732\n",
      "  time_since_restore: 213.15076661109924\n",
      "  time_this_iter_s: 10.500176668167114\n",
      "  time_total_s: 213.15076661109924\n",
      "  timers:\n",
      "    learn_throughput: 1810.942\n",
      "    learn_time_ms: 552.199\n",
      "    load_throughput: 299818.006\n",
      "    load_time_ms: 3.335\n",
      "    sample_throughput: 96.812\n",
      "    sample_time_ms: 10329.279\n",
      "    update_time_ms: 1.505\n",
      "  timestamp: 1632736150\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         213.151</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">    -0.2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-49-21\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.1875\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 16\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6588867876264786\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01185849966711733\n",
      "          policy_loss: 0.05703977818290393\n",
      "          total_loss: 0.032991356154282885\n",
      "          vf_explained_var: -0.8823341131210327\n",
      "          vf_loss: 0.0001687442731458254\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.07333333333333\n",
      "    ram_util_percent: 83.11333333333332\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04592080204753099\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.853027184986445\n",
      "    mean_inference_ms: 1.6643766475277118\n",
      "    mean_raw_obs_processing_ms: 0.13391371372329727\n",
      "  time_since_restore: 223.58920907974243\n",
      "  time_this_iter_s: 10.438442468643188\n",
      "  time_total_s: 223.58920907974243\n",
      "  timers:\n",
      "    learn_throughput: 1811.562\n",
      "    learn_time_ms: 552.01\n",
      "    load_throughput: 300217.166\n",
      "    load_time_ms: 3.331\n",
      "    sample_throughput: 97.773\n",
      "    sample_time_ms: 10227.812\n",
      "    update_time_ms: 1.501\n",
      "  timestamp: 1632736161\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         223.589</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> -0.1875</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 17000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-49-31\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.17647058823529413\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 17\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6784042252434626\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011205888207052055\n",
      "          policy_loss: 0.042711418991287546\n",
      "          total_loss: 0.018338595165146722\n",
      "          vf_explained_var: -0.6857379674911499\n",
      "          vf_loss: 0.00017004440746354197\n",
      "    num_agent_steps_sampled: 17000\n",
      "    num_agent_steps_trained: 17000\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 17000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.412499999999994\n",
      "    ram_util_percent: 83.06875\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04580606324027094\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.413216953384822\n",
      "    mean_inference_ms: 1.662068252429316\n",
      "    mean_raw_obs_processing_ms: 0.13360245166188264\n",
      "  time_since_restore: 234.40231323242188\n",
      "  time_this_iter_s: 10.813104152679443\n",
      "  time_total_s: 234.40231323242188\n",
      "  timers:\n",
      "    learn_throughput: 1815.084\n",
      "    learn_time_ms: 550.939\n",
      "    load_throughput: 301260.837\n",
      "    load_time_ms: 3.319\n",
      "    sample_throughput: 97.691\n",
      "    sample_time_ms: 10236.395\n",
      "    update_time_ms: 1.505\n",
      "  timestamp: 1632736171\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         234.402</td><td style=\"text-align: right;\">17000</td><td style=\"text-align: right;\">-0.176471</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-49-43\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.16666666666666666\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 18\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.704558401637607\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01229910693371786\n",
      "          policy_loss: 0.06567061502072546\n",
      "          total_loss: 0.04123717885878351\n",
      "          vf_explained_var: -0.9646249413490295\n",
      "          vf_loss: 0.00015232594353922952\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.81333333333335\n",
      "    ram_util_percent: 83.66666666666667\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04570326778900209\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.013943186356816\n",
      "    mean_inference_ms: 1.66000635417626\n",
      "    mean_raw_obs_processing_ms: 0.1333024599584397\n",
      "  time_since_restore: 245.4608132839203\n",
      "  time_this_iter_s: 11.058500051498413\n",
      "  time_total_s: 245.4608132839203\n",
      "  timers:\n",
      "    learn_throughput: 1821.929\n",
      "    learn_time_ms: 548.869\n",
      "    load_throughput: 300930.132\n",
      "    load_time_ms: 3.323\n",
      "    sample_throughput: 97.723\n",
      "    sample_time_ms: 10232.989\n",
      "    update_time_ms: 1.502\n",
      "  timestamp: 1632736183\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         245.461</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">-0.166667</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 19000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-49-54\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.15789473684210525\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 19\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6941337082121106\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009575957764662756\n",
      "          policy_loss: -0.035940580483939916\n",
      "          total_loss: -0.060781961182753245\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0001847643524039692\n",
      "    num_agent_steps_sampled: 19000\n",
      "    num_agent_steps_trained: 19000\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 19000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.18125\n",
      "    ram_util_percent: 83.30000000000001\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04561018293165682\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.64975854345091\n",
      "    mean_inference_ms: 1.6581390860835339\n",
      "    mean_raw_obs_processing_ms: 0.13302189707699133\n",
      "  time_since_restore: 256.5575997829437\n",
      "  time_this_iter_s: 11.096786499023438\n",
      "  time_total_s: 256.5575997829437\n",
      "  timers:\n",
      "    learn_throughput: 1821.551\n",
      "    learn_time_ms: 548.983\n",
      "    load_throughput: 301185.121\n",
      "    load_time_ms: 3.32\n",
      "    sample_throughput: 97.329\n",
      "    sample_time_ms: 10274.428\n",
      "    update_time_ms: 1.492\n",
      "  timestamp: 1632736194\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 19\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         256.558</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">-0.157895</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-50-05\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.15\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 20\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6732290347417194\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013424420483415665\n",
      "          policy_loss: 0.011729190829727384\n",
      "          total_loss: -0.012116028202904595\n",
      "          vf_explained_var: -0.8670452833175659\n",
      "          vf_loss: 0.00020218941289284784\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.275\n",
      "    ram_util_percent: 83.26875\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045523499607279264\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.316007330181385\n",
      "    mean_inference_ms: 1.6563847016866817\n",
      "    mean_raw_obs_processing_ms: 0.13276270944300445\n",
      "  time_since_restore: 267.6331503391266\n",
      "  time_this_iter_s: 11.075550556182861\n",
      "  time_total_s: 267.6331503391266\n",
      "  timers:\n",
      "    learn_throughput: 1820.3\n",
      "    learn_time_ms: 549.36\n",
      "    load_throughput: 306084.317\n",
      "    load_time_ms: 3.267\n",
      "    sample_throughput: 97.236\n",
      "    sample_time_ms: 10284.307\n",
      "    update_time_ms: 1.498\n",
      "  timestamp: 1632736205\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 20\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         267.633</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">   -0.15</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-50-16\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.14285714285714285\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 21\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.635540599293179\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007592134645080743\n",
      "          policy_loss: -0.01641912263714605\n",
      "          total_loss: -0.04114143513143063\n",
      "          vf_explained_var: -0.9792525172233582\n",
      "          vf_loss: 0.00011466684817504656\n",
      "    num_agent_steps_sampled: 21000\n",
      "    num_agent_steps_trained: 21000\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.724999999999994\n",
      "    ram_util_percent: 83.40625\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04544344058701397\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.00894498411726\n",
      "    mean_inference_ms: 1.6547699809182597\n",
      "    mean_raw_obs_processing_ms: 0.13252408744697342\n",
      "  time_since_restore: 278.7487807273865\n",
      "  time_this_iter_s: 11.115630388259888\n",
      "  time_total_s: 278.7487807273865\n",
      "  timers:\n",
      "    learn_throughput: 1816.716\n",
      "    learn_time_ms: 550.444\n",
      "    load_throughput: 306516.026\n",
      "    load_time_ms: 3.262\n",
      "    sample_throughput: 97.041\n",
      "    sample_time_ms: 10304.928\n",
      "    update_time_ms: 1.711\n",
      "  timestamp: 1632736216\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         278.749</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\">-0.142857</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 22000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-50-27\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.13636363636363635\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 22\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6332542763815985\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012893105649885585\n",
      "          policy_loss: 0.05193514298233721\n",
      "          total_loss: 0.028266762528154583\n",
      "          vf_explained_var: -0.7702014446258545\n",
      "          vf_loss: 8.554076507102258e-05\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 22000\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.90625\n",
      "    ram_util_percent: 83.40625\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04537195771712565\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.72554511075138\n",
      "    mean_inference_ms: 1.6533177766496048\n",
      "    mean_raw_obs_processing_ms: 0.1322941531945552\n",
      "  time_since_restore: 289.9305350780487\n",
      "  time_this_iter_s: 11.181754350662231\n",
      "  time_total_s: 289.9305350780487\n",
      "  timers:\n",
      "    learn_throughput: 1817.739\n",
      "    learn_time_ms: 550.134\n",
      "    load_throughput: 303137.689\n",
      "    load_time_ms: 3.299\n",
      "    sample_throughput: 96.825\n",
      "    sample_time_ms: 10327.959\n",
      "    update_time_ms: 1.717\n",
      "  timestamp: 1632736227\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 22\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         289.931</td><td style=\"text-align: right;\">22000</td><td style=\"text-align: right;\">-0.136364</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 23000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-50-38\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5652173913043478\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 23\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6612953186035155\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01282809208543719\n",
      "          policy_loss: 0.06806365168756909\n",
      "          total_loss: 0.327099633961916\n",
      "          vf_explained_var: 0.18259534239768982\n",
      "          vf_loss: 0.283083319498433\n",
      "    num_agent_steps_sampled: 23000\n",
      "    num_agent_steps_trained: 23000\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 23000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.675\n",
      "    ram_util_percent: 82.85624999999999\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045309190409460805\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.46308810436243\n",
      "    mean_inference_ms: 1.6521339077619532\n",
      "    mean_raw_obs_processing_ms: 0.1321078457439271\n",
      "  time_since_restore: 301.24184703826904\n",
      "  time_this_iter_s: 11.311311960220337\n",
      "  time_total_s: 301.24184703826904\n",
      "  timers:\n",
      "    learn_throughput: 1812.096\n",
      "    learn_time_ms: 551.847\n",
      "    load_throughput: 302235.545\n",
      "    load_time_ms: 3.309\n",
      "    sample_throughput: 96.429\n",
      "    sample_time_ms: 10370.283\n",
      "    update_time_ms: 1.715\n",
      "  timestamp: 1632736238\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 23\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         301.242</td><td style=\"text-align: right;\">23000</td><td style=\"text-align: right;\">-0.565217</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-50-49\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -1.0416666666666667\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 24\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.626789771185981\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01289673387667578\n",
      "          policy_loss: -0.05931640391548475\n",
      "          total_loss: 0.1053502360979716\n",
      "          vf_explained_var: -0.06013857573270798\n",
      "          vf_loss: 0.1883551905112755\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.325\n",
      "    ram_util_percent: 82.225\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04525065302158804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.21867270838217\n",
      "    mean_inference_ms: 1.6510383661706285\n",
      "    mean_raw_obs_processing_ms: 0.13194543222156244\n",
      "  time_since_restore: 312.14056277275085\n",
      "  time_this_iter_s: 10.898715734481812\n",
      "  time_total_s: 312.14056277275085\n",
      "  timers:\n",
      "    learn_throughput: 1800.239\n",
      "    learn_time_ms: 555.482\n",
      "    load_throughput: 300234.358\n",
      "    load_time_ms: 3.331\n",
      "    sample_throughput: 96.303\n",
      "    sample_time_ms: 10383.925\n",
      "    update_time_ms: 1.718\n",
      "  timestamp: 1632736249\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         312.141</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">-1.04167</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 25000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-51-01\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -1.0\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 25\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6125179290771485\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011932781526772206\n",
      "          policy_loss: 0.07222083885636595\n",
      "          total_loss: 0.052778958901762964\n",
      "          vf_explained_var: -0.4050467312335968\n",
      "          vf_loss: 0.004296745892821086\n",
      "    num_agent_steps_sampled: 25000\n",
      "    num_agent_steps_trained: 25000\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 25000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.431250000000006\n",
      "    ram_util_percent: 81.53750000000001\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045196117746764505\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.990899814586664\n",
      "    mean_inference_ms: 1.6500270832495252\n",
      "    mean_raw_obs_processing_ms: 0.13181157917229933\n",
      "  time_since_restore: 323.3514356613159\n",
      "  time_this_iter_s: 11.210872888565063\n",
      "  time_total_s: 323.3514356613159\n",
      "  timers:\n",
      "    learn_throughput: 1785.341\n",
      "    learn_time_ms: 560.117\n",
      "    load_throughput: 300187.084\n",
      "    load_time_ms: 3.331\n",
      "    sample_throughput: 95.69\n",
      "    sample_time_ms: 10450.374\n",
      "    update_time_ms: 1.721\n",
      "  timestamp: 1632736261\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 25\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         323.351</td><td style=\"text-align: right;\">25000</td><td style=\"text-align: right;\">      -1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 26000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-51-12\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.9615384615384616\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 26\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6309716860453287\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00879371616945538\n",
      "          policy_loss: -0.001782646444108751\n",
      "          total_loss: -0.024680161310566797\n",
      "          vf_explained_var: -0.3862658739089966\n",
      "          vf_loss: 0.0016534580221761845\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 26000\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.7125\n",
      "    ram_util_percent: 81.6625\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045145760015506894\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.778407746630169\n",
      "    mean_inference_ms: 1.64911251379136\n",
      "    mean_raw_obs_processing_ms: 0.13171869559434282\n",
      "  time_since_restore: 334.7966833114624\n",
      "  time_this_iter_s: 11.445247650146484\n",
      "  time_total_s: 334.7966833114624\n",
      "  timers:\n",
      "    learn_throughput: 1781.935\n",
      "    learn_time_ms: 561.188\n",
      "    load_throughput: 299516.124\n",
      "    load_time_ms: 3.339\n",
      "    sample_throughput: 94.786\n",
      "    sample_time_ms: 10550.038\n",
      "    update_time_ms: 1.724\n",
      "  timestamp: 1632736272\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 26\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         334.797</td><td style=\"text-align: right;\">26000</td><td style=\"text-align: right;\">-0.961538</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-51-23\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.9259259259259259\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 27\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6059876441955567\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011444530826733586\n",
      "          policy_loss: 0.06297398010889689\n",
      "          total_loss: 0.03997249808162451\n",
      "          vf_explained_var: -0.006823228672146797\n",
      "          vf_loss: 0.0007694867328003359\n",
      "    num_agent_steps_sampled: 27000\n",
      "    num_agent_steps_trained: 27000\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 27000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.66875\n",
      "    ram_util_percent: 81.6125\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04509714552576271\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.57897121169393\n",
      "    mean_inference_ms: 1.6482101264707767\n",
      "    mean_raw_obs_processing_ms: 0.13163647324984074\n",
      "  time_since_restore: 345.642746925354\n",
      "  time_this_iter_s: 10.846063613891602\n",
      "  time_total_s: 345.642746925354\n",
      "  timers:\n",
      "    learn_throughput: 1790.805\n",
      "    learn_time_ms: 558.408\n",
      "    load_throughput: 300300.995\n",
      "    load_time_ms: 3.33\n",
      "    sample_throughput: 94.725\n",
      "    sample_time_ms: 10556.879\n",
      "    update_time_ms: 1.722\n",
      "  timestamp: 1632736283\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 27\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         345.643</td><td style=\"text-align: right;\">27000</td><td style=\"text-align: right;\">-0.925926</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-51-34\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.8928571428571429\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 28\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.560286913977729\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00855171924867418\n",
      "          policy_loss: 0.03706726150380241\n",
      "          total_loss: 0.014142593120535214\n",
      "          vf_explained_var: -0.013870496302843094\n",
      "          vf_loss: 0.000967857293936605\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.50666666666666\n",
      "    ram_util_percent: 81.45333333333333\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04504972320607543\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.391565421869776\n",
      "    mean_inference_ms: 1.6473108518480617\n",
      "    mean_raw_obs_processing_ms: 0.131579816211093\n",
      "  time_since_restore: 356.64790558815\n",
      "  time_this_iter_s: 11.00515866279602\n",
      "  time_total_s: 356.64790558815\n",
      "  timers:\n",
      "    learn_throughput: 1795.76\n",
      "    learn_time_ms: 556.867\n",
      "    load_throughput: 301312.778\n",
      "    load_time_ms: 3.319\n",
      "    sample_throughput: 94.759\n",
      "    sample_time_ms: 10553.1\n",
      "    update_time_ms: 1.725\n",
      "  timestamp: 1632736294\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 28\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         356.648</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">-0.892857</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 29000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-51-45\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.8620689655172413\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 29\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5998176760143705\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010702356087869678\n",
      "          policy_loss: -0.011283529922366142\n",
      "          total_loss: -0.03259314489033487\n",
      "          vf_explained_var: -0.464955598115921\n",
      "          vf_loss: 0.0025480872647474623\n",
      "    num_agent_steps_sampled: 29000\n",
      "    num_agent_steps_trained: 29000\n",
      "    num_steps_sampled: 29000\n",
      "    num_steps_trained: 29000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.45882352941177\n",
      "    ram_util_percent: 81.13529411764706\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04500363694021842\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.215568547184672\n",
      "    mean_inference_ms: 1.6464261828210613\n",
      "    mean_raw_obs_processing_ms: 0.13152173745783902\n",
      "  time_since_restore: 368.0588231086731\n",
      "  time_this_iter_s: 11.410917520523071\n",
      "  time_total_s: 368.0588231086731\n",
      "  timers:\n",
      "    learn_throughput: 1791.285\n",
      "    learn_time_ms: 558.258\n",
      "    load_throughput: 301826.66\n",
      "    load_time_ms: 3.313\n",
      "    sample_throughput: 94.49\n",
      "    sample_time_ms: 10583.077\n",
      "    update_time_ms: 1.726\n",
      "  timestamp: 1632736305\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 29\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         368.059</td><td style=\"text-align: right;\">29000</td><td style=\"text-align: right;\">-0.862069</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=452)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=452)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-52-14\n",
      "  done: false\n",
      "  episode_len_mean: 996.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.8333333333333334\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 30\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5421253628200953\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012744015329957737\n",
      "          policy_loss: 0.031650001472896996\n",
      "          total_loss: 0.009462033046616448\n",
      "          vf_explained_var: -0.2103954255580902\n",
      "          vf_loss: 0.0006844784796056855\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.90731707317073\n",
      "    ram_util_percent: 79.67317073170733\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04495908281641533\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.049649600421294\n",
      "    mean_inference_ms: 1.6455677155809714\n",
      "    mean_raw_obs_processing_ms: 0.1508873981010517\n",
      "  time_since_restore: 396.717896938324\n",
      "  time_this_iter_s: 28.65907382965088\n",
      "  time_total_s: 396.717896938324\n",
      "  timers:\n",
      "    learn_throughput: 1796.643\n",
      "    learn_time_ms: 556.594\n",
      "    load_throughput: 213220.546\n",
      "    load_time_ms: 4.69\n",
      "    sample_throughput: 81.026\n",
      "    sample_time_ms: 12341.695\n",
      "    update_time_ms: 1.719\n",
      "  timestamp: 1632736334\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 30\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         396.718</td><td style=\"text-align: right;\">30000</td><td style=\"text-align: right;\">-0.833333</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">             996.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 31000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-52-25\n",
      "  done: false\n",
      "  episode_len_mean: 996.2258064516129\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.8064516129032258\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 31\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5372197839948867\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010848983692036832\n",
      "          policy_loss: -0.07751996401283476\n",
      "          total_loss: -0.10026671174499724\n",
      "          vf_explained_var: -0.8695618510246277\n",
      "          vf_loss: 0.00045565398007359664\n",
      "    num_agent_steps_sampled: 31000\n",
      "    num_agent_steps_trained: 31000\n",
      "    num_steps_sampled: 31000\n",
      "    num_steps_trained: 31000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.93125\n",
      "    ram_util_percent: 82.2\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044917183167126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.892973337458498\n",
      "    mean_inference_ms: 1.6447442406969572\n",
      "    mean_raw_obs_processing_ms: 0.16839147243718047\n",
      "  time_since_restore: 407.92218685150146\n",
      "  time_this_iter_s: 11.20428991317749\n",
      "  time_total_s: 407.92218685150146\n",
      "  timers:\n",
      "    learn_throughput: 1801.696\n",
      "    learn_time_ms: 555.033\n",
      "    load_throughput: 213471.226\n",
      "    load_time_ms: 4.684\n",
      "    sample_throughput: 80.955\n",
      "    sample_time_ms: 12352.505\n",
      "    update_time_ms: 1.509\n",
      "  timestamp: 1632736345\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 31\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         407.922</td><td style=\"text-align: right;\">31000</td><td style=\"text-align: right;\">-0.806452</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           996.226</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-52-36\n",
      "  done: false\n",
      "  episode_len_mean: 996.34375\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.78125\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 32\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5658591667811077\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009350220738246498\n",
      "          policy_loss: 0.026071756415896947\n",
      "          total_loss: 0.003117231527964274\n",
      "          vf_explained_var: -0.7615180015563965\n",
      "          vf_loss: 0.0008340235657265617\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.88\n",
      "    ram_util_percent: 81.53333333333333\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04487698628967836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.744417761731935\n",
      "    mean_inference_ms: 1.6439357243734944\n",
      "    mean_raw_obs_processing_ms: 0.18424969456584936\n",
      "  time_since_restore: 418.7425928115845\n",
      "  time_this_iter_s: 10.820405960083008\n",
      "  time_total_s: 418.7425928115845\n",
      "  timers:\n",
      "    learn_throughput: 1799.34\n",
      "    learn_time_ms: 555.759\n",
      "    load_throughput: 214430.527\n",
      "    load_time_ms: 4.664\n",
      "    sample_throughput: 81.198\n",
      "    sample_time_ms: 12315.625\n",
      "    update_time_ms: 1.514\n",
      "  timestamp: 1632736356\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 32\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         418.743</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">-0.78125</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           996.344</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 33000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-52-47\n",
      "  done: false\n",
      "  episode_len_mean: 996.4545454545455\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.7575757575757576\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 33\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6012832429673938\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010765904485337844\n",
      "          policy_loss: 0.056229699154694876\n",
      "          total_loss: 0.03291211985051632\n",
      "          vf_explained_var: -0.25554513931274414\n",
      "          vf_loss: 0.0005420734788963779\n",
      "    num_agent_steps_sampled: 33000\n",
      "    num_agent_steps_trained: 33000\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 33000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.675\n",
      "    ram_util_percent: 81.6125\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044838216073812526\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.603330847846735\n",
      "    mean_inference_ms: 1.6431373512453582\n",
      "    mean_raw_obs_processing_ms: 0.19863865273571643\n",
      "  time_since_restore: 429.53452825546265\n",
      "  time_this_iter_s: 10.791935443878174\n",
      "  time_total_s: 429.53452825546265\n",
      "  timers:\n",
      "    learn_throughput: 1802.19\n",
      "    learn_time_ms: 554.881\n",
      "    load_throughput: 214287.013\n",
      "    load_time_ms: 4.667\n",
      "    sample_throughput: 81.535\n",
      "    sample_time_ms: 12264.618\n",
      "    update_time_ms: 1.513\n",
      "  timestamp: 1632736367\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 33\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         429.535</td><td style=\"text-align: right;\">33000</td><td style=\"text-align: right;\">-0.757576</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           996.455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 34000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-52-58\n",
      "  done: false\n",
      "  episode_len_mean: 996.5588235294117\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.7352941176470589\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 34\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5600968493355647\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009862764160171688\n",
      "          policy_loss: 0.01375560752219624\n",
      "          total_loss: -0.009270123951137067\n",
      "          vf_explained_var: -0.6111779808998108\n",
      "          vf_loss: 0.0006026825279049162\n",
      "    num_agent_steps_sampled: 34000\n",
      "    num_agent_steps_trained: 34000\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 34000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.606249999999996\n",
      "    ram_util_percent: 81.99375\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044802218036566754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.469644078706954\n",
      "    mean_inference_ms: 1.6423915136513219\n",
      "    mean_raw_obs_processing_ms: 0.21172342310321007\n",
      "  time_since_restore: 440.96343445777893\n",
      "  time_this_iter_s: 11.428906202316284\n",
      "  time_total_s: 440.96343445777893\n",
      "  timers:\n",
      "    learn_throughput: 1814.206\n",
      "    learn_time_ms: 551.205\n",
      "    load_throughput: 217196.535\n",
      "    load_time_ms: 4.604\n",
      "    sample_throughput: 81.16\n",
      "    sample_time_ms: 12321.398\n",
      "    update_time_ms: 1.514\n",
      "  timestamp: 1632736378\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 34\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         440.963</td><td style=\"text-align: right;\">34000</td><td style=\"text-align: right;\">-0.735294</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           996.559</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 35000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-53-10\n",
      "  done: false\n",
      "  episode_len_mean: 996.6571428571428\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.7142857142857143\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 35\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5193190813064574\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009229464555117714\n",
      "          policy_loss: 0.08516740451256434\n",
      "          total_loss: 0.06200485146707958\n",
      "          vf_explained_var: -0.6238638162612915\n",
      "          vf_loss: 0.00018474565553737598\n",
      "    num_agent_steps_sampled: 35000\n",
      "    num_agent_steps_trained: 35000\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 35000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.275000000000006\n",
      "    ram_util_percent: 82.31875\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04476827945095378\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.34249163940405\n",
      "    mean_inference_ms: 1.64170112707872\n",
      "    mean_raw_obs_processing_ms: 0.22364532068210405\n",
      "  time_since_restore: 452.1084189414978\n",
      "  time_this_iter_s: 11.144984483718872\n",
      "  time_total_s: 452.1084189414978\n",
      "  timers:\n",
      "    learn_throughput: 1812.81\n",
      "    learn_time_ms: 551.63\n",
      "    load_throughput: 217043.68\n",
      "    load_time_ms: 4.607\n",
      "    sample_throughput: 81.206\n",
      "    sample_time_ms: 12314.355\n",
      "    update_time_ms: 1.541\n",
      "  timestamp: 1632736390\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 35\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         452.108</td><td style=\"text-align: right;\">35000</td><td style=\"text-align: right;\">-0.714286</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           996.657</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-53-21\n",
      "  done: false\n",
      "  episode_len_mean: 996.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6944444444444444\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 36\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.425547210375468\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011397095534852974\n",
      "          policy_loss: -0.046511819917294715\n",
      "          total_loss: -0.06831131544378069\n",
      "          vf_explained_var: -0.9791341423988342\n",
      "          vf_loss: 0.0001765593396460948\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.475\n",
      "    ram_util_percent: 82.75625\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044738556944980606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.221749069454756\n",
      "    mean_inference_ms: 1.6410875392244642\n",
      "    mean_raw_obs_processing_ms: 0.234520001814439\n",
      "  time_since_restore: 463.72366762161255\n",
      "  time_this_iter_s: 11.615248680114746\n",
      "  time_total_s: 463.72366762161255\n",
      "  timers:\n",
      "    learn_throughput: 1811.701\n",
      "    learn_time_ms: 551.967\n",
      "    load_throughput: 216033.088\n",
      "    load_time_ms: 4.629\n",
      "    sample_throughput: 81.097\n",
      "    sample_time_ms: 12330.909\n",
      "    update_time_ms: 1.548\n",
      "  timestamp: 1632736401\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 36\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         463.724</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">-0.694444</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            996.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 37000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-53-33\n",
      "  done: false\n",
      "  episode_len_mean: 996.8378378378378\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6756756756756757\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 37\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4133993599149917\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010674142281382018\n",
      "          policy_loss: 0.06781306047406462\n",
      "          total_loss: 0.045999745031197865\n",
      "          vf_explained_var: -0.09480413794517517\n",
      "          vf_loss: 0.00018584886044360852\n",
      "    num_agent_steps_sampled: 37000\n",
      "    num_agent_steps_trained: 37000\n",
      "    num_steps_sampled: 37000\n",
      "    num_steps_trained: 37000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.24117647058824\n",
      "    ram_util_percent: 82.7235294117647\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04471212390834464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.10702151920345\n",
      "    mean_inference_ms: 1.6405491066915705\n",
      "    mean_raw_obs_processing_ms: 0.24446270801312417\n",
      "  time_since_restore: 475.47454142570496\n",
      "  time_this_iter_s: 11.750873804092407\n",
      "  time_total_s: 475.47454142570496\n",
      "  timers:\n",
      "    learn_throughput: 1810.329\n",
      "    learn_time_ms: 552.386\n",
      "    load_throughput: 215794.12\n",
      "    load_time_ms: 4.634\n",
      "    sample_throughput: 80.509\n",
      "    sample_time_ms: 12420.967\n",
      "    update_time_ms: 1.543\n",
      "  timestamp: 1632736413\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 37\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         475.475</td><td style=\"text-align: right;\">37000</td><td style=\"text-align: right;\">-0.675676</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           996.838</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 38000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-53-44\n",
      "  done: false\n",
      "  episode_len_mean: 996.921052631579\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6578947368421053\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 38\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4197953833474055\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010851279022951053\n",
      "          policy_loss: -0.0388837653833131\n",
      "          total_loss: -0.06065206453204155\n",
      "          vf_explained_var: -0.8988217115402222\n",
      "          vf_loss: 0.0002593981540283292\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 38000\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.212500000000006\n",
      "    ram_util_percent: 82.86250000000001\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04468679893070643\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.997511264892589\n",
      "    mean_inference_ms: 1.6400200823648625\n",
      "    mean_raw_obs_processing_ms: 0.2535561915051902\n",
      "  time_since_restore: 486.64460802078247\n",
      "  time_this_iter_s: 11.170066595077515\n",
      "  time_total_s: 486.64460802078247\n",
      "  timers:\n",
      "    learn_throughput: 1801.145\n",
      "    learn_time_ms: 555.203\n",
      "    load_throughput: 215939.661\n",
      "    load_time_ms: 4.631\n",
      "    sample_throughput: 80.421\n",
      "    sample_time_ms: 12434.601\n",
      "    update_time_ms: 1.549\n",
      "  timestamp: 1632736424\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 38\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         486.645</td><td style=\"text-align: right;\">38000</td><td style=\"text-align: right;\">-0.657895</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           996.921</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 39000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-53-56\n",
      "  done: false\n",
      "  episode_len_mean: 997.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6410256410256411\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 39\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.335504333178202\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007261495072691076\n",
      "          policy_loss: 0.022592040648063024\n",
      "          total_loss: 0.0008774920056263606\n",
      "          vf_explained_var: -0.9510140419006348\n",
      "          vf_loss: 0.00018819585950243184\n",
      "    num_agent_steps_sampled: 39000\n",
      "    num_agent_steps_trained: 39000\n",
      "    num_steps_sampled: 39000\n",
      "    num_steps_trained: 39000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.75882352941177\n",
      "    ram_util_percent: 83.08235294117647\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0446628933878879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.892984513344471\n",
      "    mean_inference_ms: 1.63951311751819\n",
      "    mean_raw_obs_processing_ms: 0.26187792413270117\n",
      "  time_since_restore: 498.0295512676239\n",
      "  time_this_iter_s: 11.38494324684143\n",
      "  time_total_s: 498.0295512676239\n",
      "  timers:\n",
      "    learn_throughput: 1797.121\n",
      "    learn_time_ms: 556.445\n",
      "    load_throughput: 214393.261\n",
      "    load_time_ms: 4.664\n",
      "    sample_throughput: 80.445\n",
      "    sample_time_ms: 12430.807\n",
      "    update_time_ms: 1.557\n",
      "  timestamp: 1632736436\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 39\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">          498.03</td><td style=\"text-align: right;\">39000</td><td style=\"text-align: right;\">-0.641026</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">               997</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-54-07\n",
      "  done: false\n",
      "  episode_len_mean: 997.075\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.625\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 40\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.303238291210598\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00634435874746901\n",
      "          policy_loss: 0.11674577436513371\n",
      "          total_loss: 0.09517655463682281\n",
      "          vf_explained_var: -0.9069872498512268\n",
      "          vf_loss: 0.00019429188056771334\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.8\n",
      "    ram_util_percent: 83.25999999999999\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04463963811065656\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.792951941288091\n",
      "    mean_inference_ms: 1.6390154090203741\n",
      "    mean_raw_obs_processing_ms: 0.2694994568016668\n",
      "  time_since_restore: 509.11971855163574\n",
      "  time_this_iter_s: 11.09016728401184\n",
      "  time_total_s: 509.11971855163574\n",
      "  timers:\n",
      "    learn_throughput: 1802.758\n",
      "    learn_time_ms: 554.706\n",
      "    load_throughput: 302015.741\n",
      "    load_time_ms: 3.311\n",
      "    sample_throughput: 93.659\n",
      "    sample_time_ms: 10677.062\n",
      "    update_time_ms: 1.561\n",
      "  timestamp: 1632736447\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 40\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">          509.12</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  -0.625</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           997.075</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 41000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-54-19\n",
      "  done: false\n",
      "  episode_len_mean: 997.1463414634146\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6097560975609756\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 41\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.462312939431932\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008451272066230532\n",
      "          policy_loss: 0.09995489037699169\n",
      "          total_loss: 0.07707338051663505\n",
      "          vf_explained_var: -0.06280287355184555\n",
      "          vf_loss: 5.136595641993659e-05\n",
      "    num_agent_steps_sampled: 41000\n",
      "    num_agent_steps_trained: 41000\n",
      "    num_steps_sampled: 41000\n",
      "    num_steps_trained: 41000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.34117647058824\n",
      "    ram_util_percent: 83.26470588235294\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044619151922706844\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.697473229277346\n",
      "    mean_inference_ms: 1.6385732154834094\n",
      "    mean_raw_obs_processing_ms: 0.27650873037287976\n",
      "  time_since_restore: 521.0451855659485\n",
      "  time_this_iter_s: 11.925467014312744\n",
      "  time_total_s: 521.0451855659485\n",
      "  timers:\n",
      "    learn_throughput: 1774.122\n",
      "    learn_time_ms: 563.659\n",
      "    load_throughput: 297962.86\n",
      "    load_time_ms: 3.356\n",
      "    sample_throughput: 93.109\n",
      "    sample_time_ms: 10740.08\n",
      "    update_time_ms: 1.615\n",
      "  timestamp: 1632736459\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 41000\n",
      "  training_iteration: 41\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         521.045</td><td style=\"text-align: right;\">41000</td><td style=\"text-align: right;\">-0.609756</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           997.146</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 42000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-54-31\n",
      "  done: false\n",
      "  episode_len_mean: 997.2142857142857\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5952380952380952\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 42\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3450508541531034\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013110836210966889\n",
      "          policy_loss: 0.1479820965892739\n",
      "          total_loss: 0.1273753491954671\n",
      "          vf_explained_var: -0.9861538410186768\n",
      "          vf_loss: 0.00022159139827838064\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.4111111111111\n",
      "    ram_util_percent: 83.77222222222223\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0446023162122521\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.606604859531801\n",
      "    mean_inference_ms: 1.6382011877271008\n",
      "    mean_raw_obs_processing_ms: 0.28295680802004464\n",
      "  time_since_restore: 533.5694167613983\n",
      "  time_this_iter_s: 12.524231195449829\n",
      "  time_total_s: 533.5694167613983\n",
      "  timers:\n",
      "    learn_throughput: 1766.013\n",
      "    learn_time_ms: 566.247\n",
      "    load_throughput: 299522.541\n",
      "    load_time_ms: 3.339\n",
      "    sample_throughput: 91.678\n",
      "    sample_time_ms: 10907.79\n",
      "    update_time_ms: 1.655\n",
      "  timestamp: 1632736471\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 42\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         533.569</td><td style=\"text-align: right;\">42000</td><td style=\"text-align: right;\">-0.595238</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           997.214</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 43000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-54-43\n",
      "  done: false\n",
      "  episode_len_mean: 997.2790697674419\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5813953488372093\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 43\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3652758704291448\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017469043071418434\n",
      "          policy_loss: 0.04572704508900642\n",
      "          total_loss: 0.025751821200052896\n",
      "          vf_explained_var: -0.39358407258987427\n",
      "          vf_loss: 0.00018372369862239187\n",
      "    num_agent_steps_sampled: 43000\n",
      "    num_agent_steps_trained: 43000\n",
      "    num_steps_sampled: 43000\n",
      "    num_steps_trained: 43000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.90588235294118\n",
      "    ram_util_percent: 83.84117647058824\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04458578241303859\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.519460530532982\n",
      "    mean_inference_ms: 1.6378386588166784\n",
      "    mean_raw_obs_processing_ms: 0.2888771708461861\n",
      "  time_since_restore: 544.9522910118103\n",
      "  time_this_iter_s: 11.382874250411987\n",
      "  time_total_s: 544.9522910118103\n",
      "  timers:\n",
      "    learn_throughput: 1744.331\n",
      "    learn_time_ms: 573.286\n",
      "    load_throughput: 298043.318\n",
      "    load_time_ms: 3.355\n",
      "    sample_throughput: 91.242\n",
      "    sample_time_ms: 10959.826\n",
      "    update_time_ms: 1.662\n",
      "  timestamp: 1632736483\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 43000\n",
      "  training_iteration: 43\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         544.952</td><td style=\"text-align: right;\">43000</td><td style=\"text-align: right;\">-0.581395</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           997.279</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-54-54\n",
      "  done: false\n",
      "  episode_len_mean: 997.3409090909091\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5681818181818182\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 44\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.39381652408176\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01197811427685773\n",
      "          policy_loss: 0.04195513427257538\n",
      "          total_loss: 0.020592757024698788\n",
      "          vf_explained_var: -0.6965802907943726\n",
      "          vf_loss: 0.00018016247461976793\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.7\n",
      "    ram_util_percent: 83.89375\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044569679139793345\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.435854986751211\n",
      "    mean_inference_ms: 1.6374928738919217\n",
      "    mean_raw_obs_processing_ms: 0.2943149877098112\n",
      "  time_since_restore: 556.3715546131134\n",
      "  time_this_iter_s: 11.4192636013031\n",
      "  time_total_s: 556.3715546131134\n",
      "  timers:\n",
      "    learn_throughput: 1744.083\n",
      "    learn_time_ms: 573.367\n",
      "    load_throughput: 299569.605\n",
      "    load_time_ms: 3.338\n",
      "    sample_throughput: 91.251\n",
      "    sample_time_ms: 10958.802\n",
      "    update_time_ms: 1.654\n",
      "  timestamp: 1632736494\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 44\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         556.372</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">-0.568182</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           997.341</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 45000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-55-05\n",
      "  done: false\n",
      "  episode_len_mean: 997.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5555555555555556\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 45\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.442754626274109\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011006570879549598\n",
      "          policy_loss: -0.15853114128112794\n",
      "          total_loss: -0.18060511963235007\n",
      "          vf_explained_var: -0.4502853751182556\n",
      "          vf_loss: 0.00015225416734918125\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_agent_steps_trained: 45000\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.568749999999994\n",
      "    ram_util_percent: 83.4125\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0445538870919162\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.35545147754622\n",
      "    mean_inference_ms: 1.6371526992134928\n",
      "    mean_raw_obs_processing_ms: 0.29931215465074607\n",
      "  time_since_restore: 567.5210585594177\n",
      "  time_this_iter_s: 11.149503946304321\n",
      "  time_total_s: 567.5210585594177\n",
      "  timers:\n",
      "    learn_throughput: 1758.622\n",
      "    learn_time_ms: 568.627\n",
      "    load_throughput: 299462.663\n",
      "    load_time_ms: 3.339\n",
      "    sample_throughput: 91.208\n",
      "    sample_time_ms: 10963.989\n",
      "    update_time_ms: 1.627\n",
      "  timestamp: 1632736505\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 45\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         567.521</td><td style=\"text-align: right;\">45000</td><td style=\"text-align: right;\">-0.555556</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">             997.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 46000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-55-16\n",
      "  done: false\n",
      "  episode_len_mean: 997.4565217391304\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5434782608695652\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 46\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4526505443784927\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0104088920965168\n",
      "          policy_loss: 0.029996430418557592\n",
      "          total_loss: 0.007829383843474917\n",
      "          vf_explained_var: 0.5549829006195068\n",
      "          vf_loss: 0.00027768181842273204\n",
      "    num_agent_steps_sampled: 46000\n",
      "    num_agent_steps_trained: 46000\n",
      "    num_steps_sampled: 46000\n",
      "    num_steps_trained: 46000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.287499999999994\n",
      "    ram_util_percent: 83.25625\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044538049611411126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.277984383478383\n",
      "    mean_inference_ms: 1.6368059104783597\n",
      "    mean_raw_obs_processing_ms: 0.3039046181463662\n",
      "  time_since_restore: 578.4759511947632\n",
      "  time_this_iter_s: 10.954892635345459\n",
      "  time_total_s: 578.4759511947632\n",
      "  timers:\n",
      "    learn_throughput: 1764.248\n",
      "    learn_time_ms: 566.814\n",
      "    load_throughput: 297535.895\n",
      "    load_time_ms: 3.361\n",
      "    sample_throughput: 91.745\n",
      "    sample_time_ms: 10899.825\n",
      "    update_time_ms: 1.613\n",
      "  timestamp: 1632736516\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 46\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         578.476</td><td style=\"text-align: right;\">46000</td><td style=\"text-align: right;\">-0.543478</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           997.457</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 47000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-55-27\n",
      "  done: false\n",
      "  episode_len_mean: 997.5106382978723\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5319148936170213\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 47\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4116566287146672\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007567296366049366\n",
      "          policy_loss: 0.007689807646804386\n",
      "          total_loss: -0.01470093722972605\n",
      "          vf_explained_var: -0.6077679395675659\n",
      "          vf_loss: 0.00021236082710755807\n",
      "    num_agent_steps_sampled: 47000\n",
      "    num_agent_steps_trained: 47000\n",
      "    num_steps_sampled: 47000\n",
      "    num_steps_trained: 47000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.3\n",
      "    ram_util_percent: 83.31333333333332\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044522215087611915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.20330950790973\n",
      "    mean_inference_ms: 1.6364553531074935\n",
      "    mean_raw_obs_processing_ms: 0.3081335410845691\n",
      "  time_since_restore: 589.493524312973\n",
      "  time_this_iter_s: 11.017573118209839\n",
      "  time_total_s: 589.493524312973\n",
      "  timers:\n",
      "    learn_throughput: 1765.377\n",
      "    learn_time_ms: 566.451\n",
      "    load_throughput: 297192.254\n",
      "    load_time_ms: 3.365\n",
      "    sample_throughput: 92.363\n",
      "    sample_time_ms: 10826.835\n",
      "    update_time_ms: 1.63\n",
      "  timestamp: 1632736527\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47000\n",
      "  training_iteration: 47\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         589.494</td><td style=\"text-align: right;\">47000</td><td style=\"text-align: right;\">-0.531915</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           997.511</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-55-38\n",
      "  done: false\n",
      "  episode_len_mean: 997.5625\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5208333333333334\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 48\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.423216618431939\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01231654551090868\n",
      "          policy_loss: -0.13129470373193422\n",
      "          total_loss: -0.15300621746314896\n",
      "          vf_explained_var: -0.9094326496124268\n",
      "          vf_loss: 5.734250260300339e-05\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.18125\n",
      "    ram_util_percent: 83.4125\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04450617457305723\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.131203208181683\n",
      "    mean_inference_ms: 1.6360961480259533\n",
      "    mean_raw_obs_processing_ms: 0.3120252659064852\n",
      "  time_since_restore: 600.3290288448334\n",
      "  time_this_iter_s: 10.835504531860352\n",
      "  time_total_s: 600.3290288448334\n",
      "  timers:\n",
      "    learn_throughput: 1774.993\n",
      "    learn_time_ms: 563.382\n",
      "    load_throughput: 294989.204\n",
      "    load_time_ms: 3.39\n",
      "    sample_throughput: 92.623\n",
      "    sample_time_ms: 10796.463\n",
      "    update_time_ms: 1.621\n",
      "  timestamp: 1632736538\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 48\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         600.329</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">-0.520833</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           997.562</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 49000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-55-49\n",
      "  done: false\n",
      "  episode_len_mean: 997.6122448979592\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5102040816326531\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 49\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.430307242605421\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00496159334063673\n",
      "          policy_loss: -0.0004397918780644735\n",
      "          total_loss: -0.02363187554809782\n",
      "          vf_explained_var: -0.4644320607185364\n",
      "          vf_loss: 0.00011867137555883447\n",
      "    num_agent_steps_sampled: 49000\n",
      "    num_agent_steps_trained: 49000\n",
      "    num_steps_sampled: 49000\n",
      "    num_steps_trained: 49000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.53333333333333\n",
      "    ram_util_percent: 83.22\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04449017369155059\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.061483018557091\n",
      "    mean_inference_ms: 1.6357287292012637\n",
      "    mean_raw_obs_processing_ms: 0.31561017853040996\n",
      "  time_since_restore: 611.057163476944\n",
      "  time_this_iter_s: 10.728134632110596\n",
      "  time_total_s: 611.057163476944\n",
      "  timers:\n",
      "    learn_throughput: 1787.042\n",
      "    learn_time_ms: 559.584\n",
      "    load_throughput: 297223.844\n",
      "    load_time_ms: 3.364\n",
      "    sample_throughput: 93.157\n",
      "    sample_time_ms: 10734.613\n",
      "    update_time_ms: 1.614\n",
      "  timestamp: 1632736549\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49000\n",
      "  training_iteration: 49\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         611.057</td><td style=\"text-align: right;\">49000</td><td style=\"text-align: right;\">-0.510204</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           997.612</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 50000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-56-00\n",
      "  done: false\n",
      "  episode_len_mean: 997.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 50\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.446855311923557\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009555488702940273\n",
      "          policy_loss: 0.0405749106572734\n",
      "          total_loss: 0.017156982297698655\n",
      "          vf_explained_var: -0.6736918687820435\n",
      "          vf_loss: 9.507641591173726e-05\n",
      "    num_agent_steps_sampled: 50000\n",
      "    num_agent_steps_trained: 50000\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.9625\n",
      "    ram_util_percent: 83.07499999999999\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044474325249910435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.994084755491413\n",
      "    mean_inference_ms: 1.6353572384913397\n",
      "    mean_raw_obs_processing_ms: 0.31890944673946037\n",
      "  time_since_restore: 621.9306609630585\n",
      "  time_this_iter_s: 10.873497486114502\n",
      "  time_total_s: 621.9306609630585\n",
      "  timers:\n",
      "    learn_throughput: 1786.688\n",
      "    learn_time_ms: 559.695\n",
      "    load_throughput: 300178.491\n",
      "    load_time_ms: 3.331\n",
      "    sample_throughput: 93.346\n",
      "    sample_time_ms: 10712.874\n",
      "    update_time_ms: 1.611\n",
      "  timestamp: 1632736560\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 50\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         621.931</td><td style=\"text-align: right;\">50000</td><td style=\"text-align: right;\">    -0.5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            997.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 51000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-56-11\n",
      "  done: false\n",
      "  episode_len_mean: 997.7058823529412\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.49019607843137253\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 51\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.444496046172248\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012641017251916884\n",
      "          policy_loss: 0.011608008129729164\n",
      "          total_loss: -0.011327644892864757\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0002452062804271312\n",
      "    num_agent_steps_sampled: 51000\n",
      "    num_agent_steps_trained: 51000\n",
      "    num_steps_sampled: 51000\n",
      "    num_steps_trained: 51000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.32\n",
      "    ram_util_percent: 82.89333333333333\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044458397221763896\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.928878719932756\n",
      "    mean_inference_ms: 1.6349805079552766\n",
      "    mean_raw_obs_processing_ms: 0.3219418323473901\n",
      "  time_since_restore: 632.7572274208069\n",
      "  time_this_iter_s: 10.826566457748413\n",
      "  time_total_s: 632.7572274208069\n",
      "  timers:\n",
      "    learn_throughput: 1817.376\n",
      "    learn_time_ms: 550.244\n",
      "    load_throughput: 303181.513\n",
      "    load_time_ms: 3.298\n",
      "    sample_throughput: 94.227\n",
      "    sample_time_ms: 10612.626\n",
      "    update_time_ms: 1.545\n",
      "  timestamp: 1632736571\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 51000\n",
      "  training_iteration: 51\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         632.757</td><td style=\"text-align: right;\">51000</td><td style=\"text-align: right;\">-0.490196</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           997.706</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-56-22\n",
      "  done: false\n",
      "  episode_len_mean: 997.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.4807692307692308\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 52\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3554792351192897\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01785307893249457\n",
      "          policy_loss: -0.06105543532305294\n",
      "          total_loss: -0.0819253938893477\n",
      "          vf_explained_var: -0.1517515629529953\n",
      "          vf_loss: 0.0008995237494269127\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.66875\n",
      "    ram_util_percent: 82.85\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044442815738330685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.865852274223066\n",
      "    mean_inference_ms: 1.6346115393398781\n",
      "    mean_raw_obs_processing_ms: 0.3247296048089709\n",
      "  time_since_restore: 643.9149265289307\n",
      "  time_this_iter_s: 11.15769910812378\n",
      "  time_total_s: 643.9149265289307\n",
      "  timers:\n",
      "    learn_throughput: 1817.721\n",
      "    learn_time_ms: 550.14\n",
      "    load_throughput: 299120.959\n",
      "    load_time_ms: 3.343\n",
      "    sample_throughput: 95.455\n",
      "    sample_time_ms: 10476.149\n",
      "    update_time_ms: 1.493\n",
      "  timestamp: 1632736582\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 52\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         643.915</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">-0.480769</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            997.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 53000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-56-33\n",
      "  done: false\n",
      "  episode_len_mean: 997.7924528301887\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.4716981132075472\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 53\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5008601559533012\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011555833318225364\n",
      "          policy_loss: -0.062133844362364875\n",
      "          total_loss: -0.08595593141184912\n",
      "          vf_explained_var: -0.6297535300254822\n",
      "          vf_loss: 3.0931659325182815e-05\n",
      "    num_agent_steps_sampled: 53000\n",
      "    num_agent_steps_trained: 53000\n",
      "    num_steps_sampled: 53000\n",
      "    num_steps_trained: 53000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.51176470588236\n",
      "    ram_util_percent: 82.9470588235294\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04442796162977469\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.805027222217012\n",
      "    mean_inference_ms: 1.634259731558156\n",
      "    mean_raw_obs_processing_ms: 0.32729483837628454\n",
      "  time_since_restore: 655.467719078064\n",
      "  time_this_iter_s: 11.5527925491333\n",
      "  time_total_s: 655.467719078064\n",
      "  timers:\n",
      "    learn_throughput: 1838.795\n",
      "    learn_time_ms: 543.834\n",
      "    load_throughput: 297007.06\n",
      "    load_time_ms: 3.367\n",
      "    sample_throughput: 95.243\n",
      "    sample_time_ms: 10499.419\n",
      "    update_time_ms: 1.488\n",
      "  timestamp: 1632736593\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 53000\n",
      "  training_iteration: 53\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         655.468</td><td style=\"text-align: right;\">53000</td><td style=\"text-align: right;\">-0.471698</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           997.792</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 54000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-56-44\n",
      "  done: false\n",
      "  episode_len_mean: 997.8333333333334\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.46296296296296297\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 54\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.437099239561293\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010910973129386221\n",
      "          policy_loss: -0.06450833943155077\n",
      "          total_loss: -0.08678555265069007\n",
      "          vf_explained_var: -0.4642900228500366\n",
      "          vf_loss: 0.0010026760875512586\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 54000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.199999999999996\n",
      "    ram_util_percent: 82.83333333333331\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04441329942752776\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.746112720000985\n",
      "    mean_inference_ms: 1.6339101600970276\n",
      "    mean_raw_obs_processing_ms: 0.3296500151923896\n",
      "  time_since_restore: 666.4417073726654\n",
      "  time_this_iter_s: 10.97398829460144\n",
      "  time_total_s: 666.4417073726654\n",
      "  timers:\n",
      "    learn_throughput: 1840.686\n",
      "    learn_time_ms: 543.276\n",
      "    load_throughput: 296589.119\n",
      "    load_time_ms: 3.372\n",
      "    sample_throughput: 95.644\n",
      "    sample_time_ms: 10455.457\n",
      "    update_time_ms: 1.485\n",
      "  timestamp: 1632736604\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 54\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         666.442</td><td style=\"text-align: right;\">54000</td><td style=\"text-align: right;\">-0.462963</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           997.833</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 55000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-56-55\n",
      "  done: false\n",
      "  episode_len_mean: 997.8727272727273\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.45454545454545453\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 55\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4554296758439804\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011565075720524436\n",
      "          policy_loss: 0.10992081173592144\n",
      "          total_loss: 0.08658607262704107\n",
      "          vf_explained_var: -0.7791928648948669\n",
      "          vf_loss: 6.304904309217818e-05\n",
      "    num_agent_steps_sampled: 55000\n",
      "    num_agent_steps_trained: 55000\n",
      "    num_steps_sampled: 55000\n",
      "    num_steps_trained: 55000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.475\n",
      "    ram_util_percent: 82.6125\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044398708818447506\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.68903423293972\n",
      "    mean_inference_ms: 1.6335640094330828\n",
      "    mean_raw_obs_processing_ms: 0.33181085685186185\n",
      "  time_since_restore: 677.4717135429382\n",
      "  time_this_iter_s: 11.030006170272827\n",
      "  time_total_s: 677.4717135429382\n",
      "  timers:\n",
      "    learn_throughput: 1840.525\n",
      "    learn_time_ms: 543.323\n",
      "    load_throughput: 298963.185\n",
      "    load_time_ms: 3.345\n",
      "    sample_throughput: 95.753\n",
      "    sample_time_ms: 10443.499\n",
      "    update_time_ms: 1.483\n",
      "  timestamp: 1632736615\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55000\n",
      "  training_iteration: 55\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         677.472</td><td style=\"text-align: right;\">55000</td><td style=\"text-align: right;\">-0.454545</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           997.873</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-57-07\n",
      "  done: false\n",
      "  episode_len_mean: 997.9107142857143\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.44642857142857145\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 56\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4960766659842597\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008763134070471029\n",
      "          policy_loss: -0.09444023788803153\n",
      "          total_loss: -0.1184612524178293\n",
      "          vf_explained_var: -0.2177165150642395\n",
      "          vf_loss: 6.34363065425229e-05\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.2375\n",
      "    ram_util_percent: 82.4625\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04438419843222151\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.633732982646121\n",
      "    mean_inference_ms: 1.6332163171187055\n",
      "    mean_raw_obs_processing_ms: 0.3337917688054331\n",
      "  time_since_restore: 688.5720810890198\n",
      "  time_this_iter_s: 11.100367546081543\n",
      "  time_total_s: 688.5720810890198\n",
      "  timers:\n",
      "    learn_throughput: 1842.026\n",
      "    learn_time_ms: 542.88\n",
      "    load_throughput: 304354.111\n",
      "    load_time_ms: 3.286\n",
      "    sample_throughput: 95.615\n",
      "    sample_time_ms: 10458.586\n",
      "    update_time_ms: 1.486\n",
      "  timestamp: 1632736627\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 56\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         688.572</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">-0.446429</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           997.911</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 57000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-57-18\n",
      "  done: false\n",
      "  episode_len_mean: 997.9473684210526\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.43859649122807015\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 57\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4860143370098537\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008532767298662078\n",
      "          policy_loss: 0.08870763277841939\n",
      "          total_loss: 0.06473080389615563\n",
      "          vf_explained_var: -0.8163315653800964\n",
      "          vf_loss: 3.0038851946301293e-05\n",
      "    num_agent_steps_sampled: 57000\n",
      "    num_agent_steps_trained: 57000\n",
      "    num_steps_sampled: 57000\n",
      "    num_steps_trained: 57000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.0\n",
      "    ram_util_percent: 82.4875\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04436967034688547\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.580051601820792\n",
      "    mean_inference_ms: 1.6328693486692303\n",
      "    mean_raw_obs_processing_ms: 0.3356117001738735\n",
      "  time_since_restore: 699.463189125061\n",
      "  time_this_iter_s: 10.89110803604126\n",
      "  time_total_s: 699.463189125061\n",
      "  timers:\n",
      "    learn_throughput: 1842.816\n",
      "    learn_time_ms: 542.648\n",
      "    load_throughput: 305511.334\n",
      "    load_time_ms: 3.273\n",
      "    sample_throughput: 95.729\n",
      "    sample_time_ms: 10446.167\n",
      "    update_time_ms: 1.471\n",
      "  timestamp: 1632736638\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 57000\n",
      "  training_iteration: 57\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         699.463</td><td style=\"text-align: right;\">57000</td><td style=\"text-align: right;\">-0.438596</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           997.947</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 58000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-57-29\n",
      "  done: false\n",
      "  episode_len_mean: 997.9827586206897\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.43103448275862066\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 58\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5146480056974623\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011519127022995502\n",
      "          policy_loss: 0.1503887211283048\n",
      "          total_loss: 0.12643591024809414\n",
      "          vf_explained_var: -0.7380331158638\n",
      "          vf_loss: 4.1754657356957774e-05\n",
      "    num_agent_steps_sampled: 58000\n",
      "    num_agent_steps_trained: 58000\n",
      "    num_steps_sampled: 58000\n",
      "    num_steps_trained: 58000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.85333333333334\n",
      "    ram_util_percent: 82.46000000000001\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0443552453163851\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.527992853480571\n",
      "    mean_inference_ms: 1.632522075436817\n",
      "    mean_raw_obs_processing_ms: 0.33727913613615257\n",
      "  time_since_restore: 710.597704410553\n",
      "  time_this_iter_s: 11.134515285491943\n",
      "  time_total_s: 710.597704410553\n",
      "  timers:\n",
      "    learn_throughput: 1842.445\n",
      "    learn_time_ms: 542.757\n",
      "    load_throughput: 307626.592\n",
      "    load_time_ms: 3.251\n",
      "    sample_throughput: 95.456\n",
      "    sample_time_ms: 10475.988\n",
      "    update_time_ms: 1.474\n",
      "  timestamp: 1632736649\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 58000\n",
      "  training_iteration: 58\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         710.598</td><td style=\"text-align: right;\">58000</td><td style=\"text-align: right;\">-0.431034</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           997.983</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 59000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-57-40\n",
      "  done: false\n",
      "  episode_len_mean: 998.0169491525423\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.423728813559322\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 59\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.51912649207645\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01738736821466611\n",
      "          policy_loss: 0.25651738229725096\n",
      "          total_loss: 0.23309455033805634\n",
      "          vf_explained_var: -0.5351142287254333\n",
      "          vf_loss: 2.9694063798362752e-05\n",
      "    num_agent_steps_sampled: 59000\n",
      "    num_agent_steps_trained: 59000\n",
      "    num_steps_sampled: 59000\n",
      "    num_steps_trained: 59000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.72941176470588\n",
      "    ram_util_percent: 82.45882352941177\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04434125703150498\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.47758211135838\n",
      "    mean_inference_ms: 1.6321874017864655\n",
      "    mean_raw_obs_processing_ms: 0.33880718612763844\n",
      "  time_since_restore: 722.1425387859344\n",
      "  time_this_iter_s: 11.54483437538147\n",
      "  time_total_s: 722.1425387859344\n",
      "  timers:\n",
      "    learn_throughput: 1840.711\n",
      "    learn_time_ms: 543.268\n",
      "    load_throughput: 304845.19\n",
      "    load_time_ms: 3.28\n",
      "    sample_throughput: 94.723\n",
      "    sample_time_ms: 10557.099\n",
      "    update_time_ms: 1.479\n",
      "  timestamp: 1632736660\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 59000\n",
      "  training_iteration: 59\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         722.143</td><td style=\"text-align: right;\">59000</td><td style=\"text-align: right;\">-0.423729</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           998.017</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=452)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=452)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-58-08\n",
      "  done: false\n",
      "  episode_len_mean: 995.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.4166666666666667\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 60\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.480437080065409\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013200492190084508\n",
      "          policy_loss: -0.03543841656711366\n",
      "          total_loss: -0.05854247560103734\n",
      "          vf_explained_var: 0.1686076521873474\n",
      "          vf_loss: 0.00038026271839852494\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.6825\n",
      "    ram_util_percent: 82.5025\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04432759582926673\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.42872070650251\n",
      "    mean_inference_ms: 1.631856979622102\n",
      "    mean_raw_obs_processing_ms: 0.34472954524775284\n",
      "  time_since_restore: 749.8774657249451\n",
      "  time_this_iter_s: 27.73492693901062\n",
      "  time_total_s: 749.8774657249451\n",
      "  timers:\n",
      "    learn_throughput: 1841.326\n",
      "    learn_time_ms: 543.087\n",
      "    load_throughput: 213584.279\n",
      "    load_time_ms: 4.682\n",
      "    sample_throughput: 81.686\n",
      "    sample_time_ms: 12241.973\n",
      "    update_time_ms: 1.48\n",
      "  timestamp: 1632736688\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 60\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         749.877</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">-0.416667</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">             995.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 61000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-58-19\n",
      "  done: false\n",
      "  episode_len_mean: 995.672131147541\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.4098360655737705\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 61\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3208749188317195\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011709556786032721\n",
      "          policy_loss: -0.01681977692577574\n",
      "          total_loss: -0.038464849773380494\n",
      "          vf_explained_var: -0.39424043893814087\n",
      "          vf_loss: 0.0003927216927978508\n",
      "    num_agent_steps_sampled: 61000\n",
      "    num_agent_steps_trained: 61000\n",
      "    num_steps_sampled: 61000\n",
      "    num_steps_trained: 61000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.88666666666667\n",
      "    ram_util_percent: 82.58666666666666\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04431406167453967\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.381221634658878\n",
      "    mean_inference_ms: 1.6315265377096708\n",
      "    mean_raw_obs_processing_ms: 0.35030807441773515\n",
      "  time_since_restore: 760.8577377796173\n",
      "  time_this_iter_s: 10.980272054672241\n",
      "  time_total_s: 760.8577377796173\n",
      "  timers:\n",
      "    learn_throughput: 1841.949\n",
      "    learn_time_ms: 542.903\n",
      "    load_throughput: 214068.278\n",
      "    load_time_ms: 4.671\n",
      "    sample_throughput: 81.582\n",
      "    sample_time_ms: 12257.545\n",
      "    update_time_ms: 1.487\n",
      "  timestamp: 1632736699\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 61000\n",
      "  training_iteration: 61\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         760.858</td><td style=\"text-align: right;\">61000</td><td style=\"text-align: right;\">-0.409836</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           995.672</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 62000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-58-30\n",
      "  done: false\n",
      "  episode_len_mean: 995.741935483871\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.4032258064516129\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 62\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.479464544190301\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011652095583870726\n",
      "          policy_loss: -0.006349134465886487\n",
      "          total_loss: -0.029935549034012687\n",
      "          vf_explained_var: -0.4277023673057556\n",
      "          vf_loss: 4.301853481997063e-05\n",
      "    num_agent_steps_sampled: 62000\n",
      "    num_agent_steps_trained: 62000\n",
      "    num_steps_sampled: 62000\n",
      "    num_steps_trained: 62000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.375\n",
      "    ram_util_percent: 82.5625\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04430068545887527\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.335011779303679\n",
      "    mean_inference_ms: 1.6311949452584928\n",
      "    mean_raw_obs_processing_ms: 0.3555670038589197\n",
      "  time_since_restore: 771.7912366390228\n",
      "  time_this_iter_s: 10.933498859405518\n",
      "  time_total_s: 771.7912366390228\n",
      "  timers:\n",
      "    learn_throughput: 1852.12\n",
      "    learn_time_ms: 539.922\n",
      "    load_throughput: 214787.405\n",
      "    load_time_ms: 4.656\n",
      "    sample_throughput: 81.712\n",
      "    sample_time_ms: 12238.133\n",
      "    update_time_ms: 1.49\n",
      "  timestamp: 1632736710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 62000\n",
      "  training_iteration: 62\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         771.791</td><td style=\"text-align: right;\">62000</td><td style=\"text-align: right;\">-0.403226</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           995.742</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 63000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-58-41\n",
      "  done: false\n",
      "  episode_len_mean: 995.8095238095239\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3968253968253968\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 63\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3266677776972453\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012239088318622806\n",
      "          policy_loss: -0.09594119857582781\n",
      "          total_loss: -0.11785151896377405\n",
      "          vf_explained_var: -0.9996626973152161\n",
      "          vf_loss: 0.00013244915803119916\n",
      "    num_agent_steps_sampled: 63000\n",
      "    num_agent_steps_trained: 63000\n",
      "    num_steps_sampled: 63000\n",
      "    num_steps_trained: 63000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.59375\n",
      "    ram_util_percent: 82.79374999999999\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04428741250235032\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.29003844911221\n",
      "    mean_inference_ms: 1.630864887925441\n",
      "    mean_raw_obs_processing_ms: 0.3605248047313021\n",
      "  time_since_restore: 782.7346086502075\n",
      "  time_this_iter_s: 10.943372011184692\n",
      "  time_total_s: 782.7346086502075\n",
      "  timers:\n",
      "    learn_throughput: 1855.166\n",
      "    learn_time_ms: 539.035\n",
      "    load_throughput: 217076.256\n",
      "    load_time_ms: 4.607\n",
      "    sample_throughput: 82.114\n",
      "    sample_time_ms: 12178.123\n",
      "    update_time_ms: 1.496\n",
      "  timestamp: 1632736721\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63000\n",
      "  training_iteration: 63\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         782.735</td><td style=\"text-align: right;\">63000</td><td style=\"text-align: right;\">-0.396825</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            995.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-58-52\n",
      "  done: false\n",
      "  episode_len_mean: 995.875\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.390625\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 64\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4324774636162654\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00812802860384066\n",
      "          policy_loss: -0.022891997711526023\n",
      "          total_loss: -0.046236639552646216\n",
      "          vf_explained_var: -0.25196540355682373\n",
      "          vf_loss: 0.00016732867394845621\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.14375\n",
      "    ram_util_percent: 82.8875\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04427427159917477\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.246344300722368\n",
      "    mean_inference_ms: 1.6305379038038814\n",
      "    mean_raw_obs_processing_ms: 0.36519495815319025\n",
      "  time_since_restore: 794.0445880889893\n",
      "  time_this_iter_s: 11.309979438781738\n",
      "  time_total_s: 794.0445880889893\n",
      "  timers:\n",
      "    learn_throughput: 1854.416\n",
      "    learn_time_ms: 539.253\n",
      "    load_throughput: 217242.658\n",
      "    load_time_ms: 4.603\n",
      "    sample_throughput: 81.89\n",
      "    sample_time_ms: 12211.5\n",
      "    update_time_ms: 1.501\n",
      "  timestamp: 1632736732\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 64\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         794.045</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">-0.390625</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           995.875</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 65000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-59-04\n",
      "  done: false\n",
      "  episode_len_mean: 995.9384615384615\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.38461538461538464\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 65\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.339511587884691\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015580974991816104\n",
      "          policy_loss: 0.014180869112412136\n",
      "          total_loss: -0.007497233235173755\n",
      "          vf_explained_var: 0.019973674789071083\n",
      "          vf_loss: 0.000158916115227233\n",
      "    num_agent_steps_sampled: 65000\n",
      "    num_agent_steps_trained: 65000\n",
      "    num_steps_sampled: 65000\n",
      "    num_steps_trained: 65000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.9375\n",
      "    ram_util_percent: 82.9\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044261324075926096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.203880276222769\n",
      "    mean_inference_ms: 1.6302153536893769\n",
      "    mean_raw_obs_processing_ms: 0.36959497720019235\n",
      "  time_since_restore: 805.389191865921\n",
      "  time_this_iter_s: 11.344603776931763\n",
      "  time_total_s: 805.389191865921\n",
      "  timers:\n",
      "    learn_throughput: 1854.246\n",
      "    learn_time_ms: 539.303\n",
      "    load_throughput: 215345.406\n",
      "    load_time_ms: 4.644\n",
      "    sample_throughput: 81.68\n",
      "    sample_time_ms: 12242.876\n",
      "    update_time_ms: 1.499\n",
      "  timestamp: 1632736744\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 65000\n",
      "  training_iteration: 65\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         805.389</td><td style=\"text-align: right;\">65000</td><td style=\"text-align: right;\">-0.384615</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           995.938</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 66000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-59-15\n",
      "  done: false\n",
      "  episode_len_mean: 996.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3787878787878788\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 66\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9590937693913777\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011116876959542794\n",
      "          policy_loss: -0.09436257142159674\n",
      "          total_loss: -0.11263168222374387\n",
      "          vf_explained_var: -0.778717041015625\n",
      "          vf_loss: 0.00021013704552185825\n",
      "    num_agent_steps_sampled: 66000\n",
      "    num_agent_steps_trained: 66000\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 66000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.19375\n",
      "    ram_util_percent: 83.00625\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044248476199094576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.16257681039752\n",
      "    mean_inference_ms: 1.6298953727797116\n",
      "    mean_raw_obs_processing_ms: 0.37374083193424273\n",
      "  time_since_restore: 816.6455812454224\n",
      "  time_this_iter_s: 11.256389379501343\n",
      "  time_total_s: 816.6455812454224\n",
      "  timers:\n",
      "    learn_throughput: 1853.889\n",
      "    learn_time_ms: 539.407\n",
      "    load_throughput: 214439.298\n",
      "    load_time_ms: 4.663\n",
      "    sample_throughput: 81.577\n",
      "    sample_time_ms: 12258.333\n",
      "    update_time_ms: 1.499\n",
      "  timestamp: 1632736755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 66\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         816.646</td><td style=\"text-align: right;\">66000</td><td style=\"text-align: right;\">-0.378788</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">               996</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 67000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-59-26\n",
      "  done: false\n",
      "  episode_len_mean: 996.0597014925373\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.373134328358209\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 67\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0707662290996973\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01720860961393666\n",
      "          policy_loss: 0.04426366960008939\n",
      "          total_loss: 0.025507788194550407\n",
      "          vf_explained_var: -0.8826278448104858\n",
      "          vf_loss: 0.00023091819991047184\n",
      "    num_agent_steps_sampled: 67000\n",
      "    num_agent_steps_trained: 67000\n",
      "    num_steps_sampled: 67000\n",
      "    num_steps_trained: 67000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.98125\n",
      "    ram_util_percent: 83.125\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04423576328988237\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.12240267791381\n",
      "    mean_inference_ms: 1.629578509925697\n",
      "    mean_raw_obs_processing_ms: 0.3776482938751866\n",
      "  time_since_restore: 828.0126755237579\n",
      "  time_this_iter_s: 11.367094278335571\n",
      "  time_total_s: 828.0126755237579\n",
      "  timers:\n",
      "    learn_throughput: 1844.243\n",
      "    learn_time_ms: 542.228\n",
      "    load_throughput: 214008.205\n",
      "    load_time_ms: 4.673\n",
      "    sample_throughput: 81.28\n",
      "    sample_time_ms: 12303.116\n",
      "    update_time_ms: 1.499\n",
      "  timestamp: 1632736766\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 67000\n",
      "  training_iteration: 67\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         828.013</td><td style=\"text-align: right;\">67000</td><td style=\"text-align: right;\">-0.373134</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            996.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-59-38\n",
      "  done: false\n",
      "  episode_len_mean: 996.1176470588235\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.36764705882352944\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 68\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.255882183710734\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014320747189460508\n",
      "          policy_loss: -0.004952885458866755\n",
      "          total_loss: -0.025895218716727363\n",
      "          vf_explained_var: 0.010173287242650986\n",
      "          vf_loss: 0.0001844172966634182\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.48823529411765\n",
      "    ram_util_percent: 83.3235294117647\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04422327244453922\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.08330954133203\n",
      "    mean_inference_ms: 1.6292674479136091\n",
      "    mean_raw_obs_processing_ms: 0.3813306796981196\n",
      "  time_since_restore: 839.3625173568726\n",
      "  time_this_iter_s: 11.349841833114624\n",
      "  time_total_s: 839.3625173568726\n",
      "  timers:\n",
      "    learn_throughput: 1843.84\n",
      "    learn_time_ms: 542.346\n",
      "    load_throughput: 214196.184\n",
      "    load_time_ms: 4.669\n",
      "    sample_throughput: 81.139\n",
      "    sample_time_ms: 12324.508\n",
      "    update_time_ms: 1.499\n",
      "  timestamp: 1632736778\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 68\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         839.363</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">-0.367647</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           996.118</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 69000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_09-59-49\n",
      "  done: false\n",
      "  episode_len_mean: 996.1739130434783\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.36231884057971014\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 69\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2852277358373008\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014434838022348585\n",
      "          policy_loss: -0.027509624593787724\n",
      "          total_loss: -0.048668758736716375\n",
      "          vf_explained_var: 0.10451435297727585\n",
      "          vf_loss: 0.0002496580206449532\n",
      "    num_agent_steps_sampled: 69000\n",
      "    num_agent_steps_trained: 69000\n",
      "    num_steps_sampled: 69000\n",
      "    num_steps_trained: 69000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.733333333333334\n",
      "    ram_util_percent: 83.37333333333335\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04421091076144339\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.045210716036536\n",
      "    mean_inference_ms: 1.628959377473353\n",
      "    mean_raw_obs_processing_ms: 0.3848003984278578\n",
      "  time_since_restore: 850.4995453357697\n",
      "  time_this_iter_s: 11.137027978897095\n",
      "  time_total_s: 850.4995453357697\n",
      "  timers:\n",
      "    learn_throughput: 1841.836\n",
      "    learn_time_ms: 542.936\n",
      "    load_throughput: 215253.677\n",
      "    load_time_ms: 4.646\n",
      "    sample_throughput: 81.412\n",
      "    sample_time_ms: 12283.136\n",
      "    update_time_ms: 1.5\n",
      "  timestamp: 1632736789\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 69000\n",
      "  training_iteration: 69\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">           850.5</td><td style=\"text-align: right;\">69000</td><td style=\"text-align: right;\">-0.362319</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           996.174</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 70000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-00-00\n",
      "  done: false\n",
      "  episode_len_mean: 996.2285714285714\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.35714285714285715\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 70\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8632776379585265\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01785937241368909\n",
      "          policy_loss: -0.07228041895561749\n",
      "          total_loss: -0.08876885943528678\n",
      "          vf_explained_var: -0.46342891454696655\n",
      "          vf_loss: 0.0003583956301251116\n",
      "    num_agent_steps_sampled: 70000\n",
      "    num_agent_steps_trained: 70000\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.858823529411765\n",
      "    ram_util_percent: 83.57058823529411\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04419875721111888\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.008083914249092\n",
      "    mean_inference_ms: 1.6286553098339065\n",
      "    mean_raw_obs_processing_ms: 0.3880699696911571\n",
      "  time_since_restore: 861.7091357707977\n",
      "  time_this_iter_s: 11.209590435028076\n",
      "  time_total_s: 861.7091357707977\n",
      "  timers:\n",
      "    learn_throughput: 1843.314\n",
      "    learn_time_ms: 542.501\n",
      "    load_throughput: 307827.529\n",
      "    load_time_ms: 3.249\n",
      "    sample_throughput: 94.052\n",
      "    sample_time_ms: 10632.467\n",
      "    update_time_ms: 1.501\n",
      "  timestamp: 1632736800\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 70\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         861.709</td><td style=\"text-align: right;\">70000</td><td style=\"text-align: right;\">-0.357143</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           996.229</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 71000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-00-11\n",
      "  done: false\n",
      "  episode_len_mean: 996.2816901408451\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.352112676056338\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 71\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9351907425456576\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009590987518400264\n",
      "          policy_loss: 0.024472063634958532\n",
      "          total_loss: 0.006205180142488744\n",
      "          vf_explained_var: -0.906122088432312\n",
      "          vf_loss: 0.00012592437932552357\n",
      "    num_agent_steps_sampled: 71000\n",
      "    num_agent_steps_trained: 71000\n",
      "    num_steps_sampled: 71000\n",
      "    num_steps_trained: 71000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.5375\n",
      "    ram_util_percent: 83.6625\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04418679860921114\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.971907588614643\n",
      "    mean_inference_ms: 1.6283575976020332\n",
      "    mean_raw_obs_processing_ms: 0.3911503245727584\n",
      "  time_since_restore: 873.0237476825714\n",
      "  time_this_iter_s: 11.314611911773682\n",
      "  time_total_s: 873.0237476825714\n",
      "  timers:\n",
      "    learn_throughput: 1842.286\n",
      "    learn_time_ms: 542.804\n",
      "    load_throughput: 304597.24\n",
      "    load_time_ms: 3.283\n",
      "    sample_throughput: 93.76\n",
      "    sample_time_ms: 10665.542\n",
      "    update_time_ms: 1.493\n",
      "  timestamp: 1632736811\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 71000\n",
      "  training_iteration: 71\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         873.024</td><td style=\"text-align: right;\">71000</td><td style=\"text-align: right;\">-0.352113</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           996.282</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-00-23\n",
      "  done: false\n",
      "  episode_len_mean: 996.3333333333334\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3472222222222222\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 72\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.966591571436988\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012904804081262721\n",
      "          policy_loss: -0.02659039224187533\n",
      "          total_loss: -0.044868030357691976\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 9.779808367764215e-05\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.8625\n",
      "    ram_util_percent: 83.76875\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04417512501328778\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.936671359544695\n",
      "    mean_inference_ms: 1.628067003361541\n",
      "    mean_raw_obs_processing_ms: 0.39405238032048484\n",
      "  time_since_restore: 884.588742017746\n",
      "  time_this_iter_s: 11.56499433517456\n",
      "  time_total_s: 884.588742017746\n",
      "  timers:\n",
      "    learn_throughput: 1812.487\n",
      "    learn_time_ms: 551.728\n",
      "    load_throughput: 304029.806\n",
      "    load_time_ms: 3.289\n",
      "    sample_throughput: 93.286\n",
      "    sample_time_ms: 10719.719\n",
      "    update_time_ms: 1.496\n",
      "  timestamp: 1632736823\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 72\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         884.589</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">-0.347222</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           996.333</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 73000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-00-35\n",
      "  done: false\n",
      "  episode_len_mean: 996.3835616438356\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3424657534246575\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 73\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.994081864092085\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009875687115463922\n",
      "          policy_loss: 0.044892171894510585\n",
      "          total_loss: 0.02602264554136329\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 8.372129769769445e-05\n",
      "    num_agent_steps_sampled: 73000\n",
      "    num_agent_steps_trained: 73000\n",
      "    num_steps_sampled: 73000\n",
      "    num_steps_trained: 73000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.24117647058824\n",
      "    ram_util_percent: 83.88235294117646\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044163965071983766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.902428465539561\n",
      "    mean_inference_ms: 1.6277900406206063\n",
      "    mean_raw_obs_processing_ms: 0.39678684605735426\n",
      "  time_since_restore: 896.6281111240387\n",
      "  time_this_iter_s: 12.039369106292725\n",
      "  time_total_s: 896.6281111240387\n",
      "  timers:\n",
      "    learn_throughput: 1801.087\n",
      "    learn_time_ms: 555.22\n",
      "    load_throughput: 299524.68\n",
      "    load_time_ms: 3.339\n",
      "    sample_throughput: 92.372\n",
      "    sample_time_ms: 10825.769\n",
      "    update_time_ms: 1.499\n",
      "  timestamp: 1632736835\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 73000\n",
      "  training_iteration: 73\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         896.628</td><td style=\"text-align: right;\">73000</td><td style=\"text-align: right;\">-0.342466</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           996.384</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 74000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-00-46\n",
      "  done: false\n",
      "  episode_len_mean: 996.4324324324324\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.33783783783783783\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 74\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9646952960226272\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010049037633409528\n",
      "          policy_loss: -0.014385247520274586\n",
      "          total_loss: -0.03293280504229996\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 9.449231870854015e-05\n",
      "    num_agent_steps_sampled: 74000\n",
      "    num_agent_steps_trained: 74000\n",
      "    num_steps_sampled: 74000\n",
      "    num_steps_trained: 74000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.14705882352942\n",
      "    ram_util_percent: 83.55882352941177\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04415288523228718\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.869037881730163\n",
      "    mean_inference_ms: 1.627516105247483\n",
      "    mean_raw_obs_processing_ms: 0.3993618064204115\n",
      "  time_since_restore: 907.9961547851562\n",
      "  time_this_iter_s: 11.368043661117554\n",
      "  time_total_s: 907.9961547851562\n",
      "  timers:\n",
      "    learn_throughput: 1800.625\n",
      "    learn_time_ms: 555.363\n",
      "    load_throughput: 299580.304\n",
      "    load_time_ms: 3.338\n",
      "    sample_throughput: 92.324\n",
      "    sample_time_ms: 10831.437\n",
      "    update_time_ms: 1.498\n",
      "  timestamp: 1632736846\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 74000\n",
      "  training_iteration: 74\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         907.996</td><td style=\"text-align: right;\">74000</td><td style=\"text-align: right;\">-0.337838</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           996.432</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 75000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-00-58\n",
      "  done: false\n",
      "  episode_len_mean: 996.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3333333333333333\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 75\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.976941998799642\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011440577891174936\n",
      "          policy_loss: -0.026971268985006545\n",
      "          total_loss: -0.04549882171882524\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 9.780802061464379e-05\n",
      "    num_agent_steps_sampled: 75000\n",
      "    num_agent_steps_trained: 75000\n",
      "    num_steps_sampled: 75000\n",
      "    num_steps_trained: 75000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.46875\n",
      "    ram_util_percent: 83.34375\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04414189358611181\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.836440222141118\n",
      "    mean_inference_ms: 1.6272461170120451\n",
      "    mean_raw_obs_processing_ms: 0.4017858683334783\n",
      "  time_since_restore: 919.2194933891296\n",
      "  time_this_iter_s: 11.223338603973389\n",
      "  time_total_s: 919.2194933891296\n",
      "  timers:\n",
      "    learn_throughput: 1801.002\n",
      "    learn_time_ms: 555.247\n",
      "    load_throughput: 301965.731\n",
      "    load_time_ms: 3.312\n",
      "    sample_throughput: 92.427\n",
      "    sample_time_ms: 10819.386\n",
      "    update_time_ms: 1.491\n",
      "  timestamp: 1632736858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75000\n",
      "  training_iteration: 75\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         919.219</td><td style=\"text-align: right;\">75000</td><td style=\"text-align: right;\">-0.333333</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            996.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-01-09\n",
      "  done: false\n",
      "  episode_len_mean: 996.5263157894736\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.32894736842105265\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 76\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0939615779452856\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013585545328337586\n",
      "          policy_loss: 0.04455562076634831\n",
      "          total_loss: 0.025076691351003117\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00010213162232604291\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.0875\n",
      "    ram_util_percent: 83.3125\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044131017254931675\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.804587959702612\n",
      "    mean_inference_ms: 1.6269802171988055\n",
      "    mean_raw_obs_processing_ms: 0.4040670459035473\n",
      "  time_since_restore: 930.3236424922943\n",
      "  time_this_iter_s: 11.104149103164673\n",
      "  time_total_s: 930.3236424922943\n",
      "  timers:\n",
      "    learn_throughput: 1801.62\n",
      "    learn_time_ms: 555.056\n",
      "    load_throughput: 301909.218\n",
      "    load_time_ms: 3.312\n",
      "    sample_throughput: 92.555\n",
      "    sample_time_ms: 10804.353\n",
      "    update_time_ms: 1.486\n",
      "  timestamp: 1632736869\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 76\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         930.324</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">-0.328947</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           996.526</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 77000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-01-20\n",
      "  done: false\n",
      "  episode_len_mean: 996.5714285714286\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3246753246753247\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 77\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4851537624994915\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013092339605042917\n",
      "          policy_loss: 0.10282069312201605\n",
      "          total_loss: 0.07951652606328329\n",
      "          vf_explained_var: -0.4075332283973694\n",
      "          vf_loss: 0.00023813745259152104\n",
      "    num_agent_steps_sampled: 77000\n",
      "    num_agent_steps_trained: 77000\n",
      "    num_steps_sampled: 77000\n",
      "    num_steps_trained: 77000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.2\n",
      "    ram_util_percent: 83.2625\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04412023304314549\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.773487340983733\n",
      "    mean_inference_ms: 1.6267168493267297\n",
      "    mean_raw_obs_processing_ms: 0.4062129120825592\n",
      "  time_since_restore: 941.6188945770264\n",
      "  time_this_iter_s: 11.295252084732056\n",
      "  time_total_s: 941.6188945770264\n",
      "  timers:\n",
      "    learn_throughput: 1809.518\n",
      "    learn_time_ms: 552.633\n",
      "    load_throughput: 301807.114\n",
      "    load_time_ms: 3.313\n",
      "    sample_throughput: 92.596\n",
      "    sample_time_ms: 10799.621\n",
      "    update_time_ms: 1.485\n",
      "  timestamp: 1632736880\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 77000\n",
      "  training_iteration: 77\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         941.619</td><td style=\"text-align: right;\">77000</td><td style=\"text-align: right;\">-0.324675</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           996.571</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 78000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-01-31\n",
      "  done: false\n",
      "  episode_len_mean: 996.6153846153846\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.32051282051282054\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 78\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.154319405555725\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013544718833497048\n",
      "          policy_loss: -0.06992851009385453\n",
      "          total_loss: -0.08994569844669766\n",
      "          vf_explained_var: -0.7919886112213135\n",
      "          vf_loss: 0.00017153082246497637\n",
      "    num_agent_steps_sampled: 78000\n",
      "    num_agent_steps_trained: 78000\n",
      "    num_steps_sampled: 78000\n",
      "    num_steps_trained: 78000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.243750000000006\n",
      "    ram_util_percent: 83.10624999999999\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04410955202348577\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.74307979239249\n",
      "    mean_inference_ms: 1.6264558632916588\n",
      "    mean_raw_obs_processing_ms: 0.40823051401217303\n",
      "  time_since_restore: 952.7196016311646\n",
      "  time_this_iter_s: 11.100707054138184\n",
      "  time_total_s: 952.7196016311646\n",
      "  timers:\n",
      "    learn_throughput: 1810.04\n",
      "    learn_time_ms: 552.474\n",
      "    load_throughput: 301935.298\n",
      "    load_time_ms: 3.312\n",
      "    sample_throughput: 92.808\n",
      "    sample_time_ms: 10774.881\n",
      "    update_time_ms: 1.477\n",
      "  timestamp: 1632736891\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 78000\n",
      "  training_iteration: 78\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">          952.72</td><td style=\"text-align: right;\">78000</td><td style=\"text-align: right;\">-0.320513</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           996.615</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 79000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-01-42\n",
      "  done: false\n",
      "  episode_len_mean: 996.6582278481013\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.31645569620253167\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 79\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0754912296930947\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01210729897673125\n",
      "          policy_loss: 0.0015200641627113023\n",
      "          total_loss: -0.0178614120102591\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00016270438354695215\n",
      "    num_agent_steps_sampled: 79000\n",
      "    num_agent_steps_trained: 79000\n",
      "    num_steps_sampled: 79000\n",
      "    num_steps_trained: 79000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.1125\n",
      "    ram_util_percent: 82.89375000000001\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04409896097173266\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.71335158388887\n",
      "    mean_inference_ms: 1.6261971051194437\n",
      "    mean_raw_obs_processing_ms: 0.4101264731507533\n",
      "  time_since_restore: 963.8793833255768\n",
      "  time_this_iter_s: 11.159781694412231\n",
      "  time_total_s: 963.8793833255768\n",
      "  timers:\n",
      "    learn_throughput: 1813.857\n",
      "    learn_time_ms: 551.311\n",
      "    load_throughput: 303313.061\n",
      "    load_time_ms: 3.297\n",
      "    sample_throughput: 92.778\n",
      "    sample_time_ms: 10778.359\n",
      "    update_time_ms: 1.469\n",
      "  timestamp: 1632736902\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79000\n",
      "  training_iteration: 79\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         963.879</td><td style=\"text-align: right;\">79000</td><td style=\"text-align: right;\">-0.316456</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           996.658</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-01-54\n",
      "  done: false\n",
      "  episode_len_mean: 996.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3125\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 80\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1470501979192096\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011002565900558252\n",
      "          policy_loss: -0.033908497500750756\n",
      "          total_loss: -0.054203688104947405\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 7.505360699724405e-05\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.1\n",
      "    ram_util_percent: 82.81875\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04408847475199038\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.684285015671724\n",
      "    mean_inference_ms: 1.6259396526090881\n",
      "    mean_raw_obs_processing_ms: 0.4119072700109584\n",
      "  time_since_restore: 975.0716669559479\n",
      "  time_this_iter_s: 11.192283630371094\n",
      "  time_total_s: 975.0716669559479\n",
      "  timers:\n",
      "    learn_throughput: 1811.83\n",
      "    learn_time_ms: 551.928\n",
      "    load_throughput: 302311.79\n",
      "    load_time_ms: 3.308\n",
      "    sample_throughput: 92.799\n",
      "    sample_time_ms: 10776.003\n",
      "    update_time_ms: 1.475\n",
      "  timestamp: 1632736914\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 80\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         975.072</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> -0.3125</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">             996.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 81000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-02-05\n",
      "  done: false\n",
      "  episode_len_mean: 996.7407407407408\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.30864197530864196\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 81\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4441994455125595\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013881351340386613\n",
      "          policy_loss: -0.03801036609543695\n",
      "          total_loss: -0.06093440494603581\n",
      "          vf_explained_var: -0.6968477964401245\n",
      "          vf_loss: 0.00012982220008173802\n",
      "    num_agent_steps_sampled: 81000\n",
      "    num_agent_steps_trained: 81000\n",
      "    num_steps_sampled: 81000\n",
      "    num_steps_trained: 81000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.55333333333333\n",
      "    ram_util_percent: 82.77333333333333\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04407805246509492\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.655800583219365\n",
      "    mean_inference_ms: 1.625684015560127\n",
      "    mean_raw_obs_processing_ms: 0.41358214996129733\n",
      "  time_since_restore: 985.9115746021271\n",
      "  time_this_iter_s: 10.8399076461792\n",
      "  time_total_s: 985.9115746021271\n",
      "  timers:\n",
      "    learn_throughput: 1812.521\n",
      "    learn_time_ms: 551.718\n",
      "    load_throughput: 305903.495\n",
      "    load_time_ms: 3.269\n",
      "    sample_throughput: 93.207\n",
      "    sample_time_ms: 10728.802\n",
      "    update_time_ms: 1.479\n",
      "  timestamp: 1632736925\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 81000\n",
      "  training_iteration: 81\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         985.912</td><td style=\"text-align: right;\">81000</td><td style=\"text-align: right;\">-0.308642</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           996.741</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 82000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-02-15\n",
      "  done: false\n",
      "  episode_len_mean: 996.780487804878\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3048780487804878\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 82\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.445192337036133\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010098829825171051\n",
      "          policy_loss: -0.04714887930701176\n",
      "          total_loss: -0.07055884848038356\n",
      "          vf_explained_var: -0.9317458868026733\n",
      "          vf_loss: 3.2071062853194436e-05\n",
      "    num_agent_steps_sampled: 82000\n",
      "    num_agent_steps_trained: 82000\n",
      "    num_steps_sampled: 82000\n",
      "    num_steps_trained: 82000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.912499999999994\n",
      "    ram_util_percent: 82.65\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04406773104072684\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.627870966761757\n",
      "    mean_inference_ms: 1.6254286364898471\n",
      "    mean_raw_obs_processing_ms: 0.41515652768931893\n",
      "  time_since_restore: 996.6939604282379\n",
      "  time_this_iter_s: 10.78238582611084\n",
      "  time_total_s: 996.6939604282379\n",
      "  timers:\n",
      "    learn_throughput: 1837.755\n",
      "    learn_time_ms: 544.142\n",
      "    load_throughput: 309679.858\n",
      "    load_time_ms: 3.229\n",
      "    sample_throughput: 93.825\n",
      "    sample_time_ms: 10658.185\n",
      "    update_time_ms: 1.483\n",
      "  timestamp: 1632736935\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 82000\n",
      "  training_iteration: 82\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         996.694</td><td style=\"text-align: right;\">82000</td><td style=\"text-align: right;\">-0.304878</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            996.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 83000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-02-26\n",
      "  done: false\n",
      "  episode_len_mean: 996.8192771084338\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.30120481927710846\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 83\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4451661242379084\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01068461461128142\n",
      "          policy_loss: -0.023661728410257234\n",
      "          total_loss: -0.04695396356077658\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 9.096311504334962e-05\n",
      "    num_agent_steps_sampled: 83000\n",
      "    num_agent_steps_trained: 83000\n",
      "    num_steps_sampled: 83000\n",
      "    num_steps_trained: 83000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.29333333333334\n",
      "    ram_util_percent: 82.47333333333334\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044057470026666615\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.6005025317749\n",
      "    mean_inference_ms: 1.6251735262995395\n",
      "    mean_raw_obs_processing_ms: 0.4166367198847343\n",
      "  time_since_restore: 1007.6287214756012\n",
      "  time_this_iter_s: 10.934761047363281\n",
      "  time_total_s: 1007.6287214756012\n",
      "  timers:\n",
      "    learn_throughput: 1849.914\n",
      "    learn_time_ms: 540.566\n",
      "    load_throughput: 314361.392\n",
      "    load_time_ms: 3.181\n",
      "    sample_throughput: 94.774\n",
      "    sample_time_ms: 10551.369\n",
      "    update_time_ms: 1.469\n",
      "  timestamp: 1632736946\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 83000\n",
      "  training_iteration: 83\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         1007.63</td><td style=\"text-align: right;\">83000</td><td style=\"text-align: right;\">-0.301205</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           996.819</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-02-37\n",
      "  done: false\n",
      "  episode_len_mean: 996.8571428571429\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2976190476190476\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 84\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5142599211798773\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011256014261884733\n",
      "          policy_loss: -0.06689576643208663\n",
      "          total_loss: -0.09085810639792019\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 5.465846065817459e-05\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.74374999999999\n",
      "    ram_util_percent: 82.15625\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04404733447538918\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.573710216852865\n",
      "    mean_inference_ms: 1.624920230776748\n",
      "    mean_raw_obs_processing_ms: 0.41802578187033324\n",
      "  time_since_restore: 1018.7926971912384\n",
      "  time_this_iter_s: 11.163975715637207\n",
      "  time_total_s: 1018.7926971912384\n",
      "  timers:\n",
      "    learn_throughput: 1850.397\n",
      "    learn_time_ms: 540.424\n",
      "    load_throughput: 313066.169\n",
      "    load_time_ms: 3.194\n",
      "    sample_throughput: 94.957\n",
      "    sample_time_ms: 10531.088\n",
      "    update_time_ms: 1.47\n",
      "  timestamp: 1632736957\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 84\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         1018.79</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">-0.297619</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           996.857</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 85000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-02-49\n",
      "  done: false\n",
      "  episode_len_mean: 996.8941176470588\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.29411764705882354\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 85\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.515455635388692\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011675284378133198\n",
      "          policy_loss: 0.0023936600734790164\n",
      "          total_loss: -0.021519931240214243\n",
      "          vf_explained_var: -0.9217342138290405\n",
      "          vf_loss: 7.343765913295405e-05\n",
      "    num_agent_steps_sampled: 85000\n",
      "    num_agent_steps_trained: 85000\n",
      "    num_steps_sampled: 85000\n",
      "    num_steps_trained: 85000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.48823529411765\n",
      "    ram_util_percent: 81.85882352941178\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04403729318840959\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.54751574614722\n",
      "    mean_inference_ms: 1.6246698538426028\n",
      "    mean_raw_obs_processing_ms: 0.41932862035835683\n",
      "  time_since_restore: 1030.2803642749786\n",
      "  time_this_iter_s: 11.487667083740234\n",
      "  time_total_s: 1030.2803642749786\n",
      "  timers:\n",
      "    learn_throughput: 1847.242\n",
      "    learn_time_ms: 541.348\n",
      "    load_throughput: 314006.019\n",
      "    load_time_ms: 3.185\n",
      "    sample_throughput: 94.727\n",
      "    sample_time_ms: 10556.623\n",
      "    update_time_ms: 1.478\n",
      "  timestamp: 1632736969\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 85000\n",
      "  training_iteration: 85\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         1030.28</td><td style=\"text-align: right;\">85000</td><td style=\"text-align: right;\">-0.294118</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           996.894</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 86000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-03-00\n",
      "  done: false\n",
      "  episode_len_mean: 996.9302325581396\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.29069767441860467\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 86\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5860971980624727\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008264924803426589\n",
      "          policy_loss: -0.08598445666333039\n",
      "          total_loss: -0.11098221581843164\n",
      "          vf_explained_var: -0.9505621790885925\n",
      "          vf_loss: 3.672012190792076e-05\n",
      "    num_agent_steps_sampled: 86000\n",
      "    num_agent_steps_trained: 86000\n",
      "    num_steps_sampled: 86000\n",
      "    num_steps_trained: 86000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.1375\n",
      "    ram_util_percent: 81.64374999999998\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04402736156880255\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.521894769449771\n",
      "    mean_inference_ms: 1.6244242958353963\n",
      "    mean_raw_obs_processing_ms: 0.42054676837430516\n",
      "  time_since_restore: 1041.7071497440338\n",
      "  time_this_iter_s: 11.426785469055176\n",
      "  time_total_s: 1041.7071497440338\n",
      "  timers:\n",
      "    learn_throughput: 1846.022\n",
      "    learn_time_ms: 541.705\n",
      "    load_throughput: 314745.91\n",
      "    load_time_ms: 3.177\n",
      "    sample_throughput: 94.442\n",
      "    sample_time_ms: 10588.531\n",
      "    update_time_ms: 1.478\n",
      "  timestamp: 1632736980\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 86000\n",
      "  training_iteration: 86\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         1041.71</td><td style=\"text-align: right;\">86000</td><td style=\"text-align: right;\">-0.290698</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            996.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 87000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-03-12\n",
      "  done: false\n",
      "  episode_len_mean: 996.9655172413793\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.28735632183908044\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 87\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5881185399161444\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010886381061822093\n",
      "          policy_loss: -0.18314331707855067\n",
      "          total_loss: -0.20787292338079877\n",
      "          vf_explained_var: 0.1907006800174713\n",
      "          vf_loss: 6.294369890787897e-05\n",
      "    num_agent_steps_sampled: 87000\n",
      "    num_agent_steps_trained: 87000\n",
      "    num_steps_sampled: 87000\n",
      "    num_steps_trained: 87000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.3125\n",
      "    ram_util_percent: 81.725\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04401753080607616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.496800273932847\n",
      "    mean_inference_ms: 1.6241806504617076\n",
      "    mean_raw_obs_processing_ms: 0.42168709872256954\n",
      "  time_since_restore: 1052.9202916622162\n",
      "  time_this_iter_s: 11.213141918182373\n",
      "  time_total_s: 1052.9202916622162\n",
      "  timers:\n",
      "    learn_throughput: 1846.923\n",
      "    learn_time_ms: 541.441\n",
      "    load_throughput: 316064.625\n",
      "    load_time_ms: 3.164\n",
      "    sample_throughput: 94.513\n",
      "    sample_time_ms: 10580.57\n",
      "    update_time_ms: 1.482\n",
      "  timestamp: 1632736992\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 87000\n",
      "  training_iteration: 87\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         1052.92</td><td style=\"text-align: right;\">87000</td><td style=\"text-align: right;\">-0.287356</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           996.966</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-03-23\n",
      "  done: false\n",
      "  episode_len_mean: 997.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2840909090909091\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 88\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0860192404852973\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015539483375167418\n",
      "          policy_loss: -0.08680013169844945\n",
      "          total_loss: -0.10587001680913899\n",
      "          vf_explained_var: -0.5601653456687927\n",
      "          vf_loss: 0.00023635876350454056\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.21875\n",
      "    ram_util_percent: 81.34375\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04400777000034418\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.472262936020448\n",
      "    mean_inference_ms: 1.6239384460942683\n",
      "    mean_raw_obs_processing_ms: 0.4227506425297742\n",
      "  time_since_restore: 1064.4703390598297\n",
      "  time_this_iter_s: 11.550047397613525\n",
      "  time_total_s: 1064.4703390598297\n",
      "  timers:\n",
      "    learn_throughput: 1848.295\n",
      "    learn_time_ms: 541.039\n",
      "    load_throughput: 316009.855\n",
      "    load_time_ms: 3.164\n",
      "    sample_throughput: 94.109\n",
      "    sample_time_ms: 10625.921\n",
      "    update_time_ms: 1.481\n",
      "  timestamp: 1632737003\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 88\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         1064.47</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">-0.284091</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">               997</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 89000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-03-35\n",
      "  done: false\n",
      "  episode_len_mean: 997.0337078651686\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2808988764044944\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 89\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3523712184694077\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013333466181854912\n",
      "          policy_loss: 0.010362384302748575\n",
      "          total_loss: -0.01168900157014529\n",
      "          vf_explained_var: 0.19320610165596008\n",
      "          vf_loss: 0.00013897816946458383\n",
      "    num_agent_steps_sampled: 89000\n",
      "    num_agent_steps_trained: 89000\n",
      "    num_steps_sampled: 89000\n",
      "    num_steps_trained: 89000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.1235294117647\n",
      "    ram_util_percent: 81.11764705882354\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043998082822100514\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.448236492800612\n",
      "    mean_inference_ms: 1.6236979875831556\n",
      "    mean_raw_obs_processing_ms: 0.4237436774896921\n",
      "  time_since_restore: 1075.828164100647\n",
      "  time_this_iter_s: 11.35782504081726\n",
      "  time_total_s: 1075.828164100647\n",
      "  timers:\n",
      "    learn_throughput: 1848.031\n",
      "    learn_time_ms: 541.116\n",
      "    load_throughput: 315917.027\n",
      "    load_time_ms: 3.165\n",
      "    sample_throughput: 93.936\n",
      "    sample_time_ms: 10645.548\n",
      "    update_time_ms: 1.492\n",
      "  timestamp: 1632737015\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 89000\n",
      "  training_iteration: 89\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         1075.83</td><td style=\"text-align: right;\">89000</td><td style=\"text-align: right;\">-0.280899</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           997.034</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=452)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=452)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 90000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-04-02\n",
      "  done: false\n",
      "  episode_len_mean: 995.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2777777777777778\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 90\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4662816921869912\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014618149111113067\n",
      "          policy_loss: -0.06823080480098724\n",
      "          total_loss: -0.09134889791409175\n",
      "          vf_explained_var: 0.13050125539302826\n",
      "          vf_loss: 8.290317738202349e-05\n",
      "    num_agent_steps_sampled: 90000\n",
      "    num_agent_steps_trained: 90000\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.0025641025641\n",
      "    ram_util_percent: 81.49743589743588\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04398847090224613\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.424657461676942\n",
      "    mean_inference_ms: 1.6234590510218092\n",
      "    mean_raw_obs_processing_ms: 0.42669081858511876\n",
      "  time_since_restore: 1103.1797106266022\n",
      "  time_this_iter_s: 27.3515465259552\n",
      "  time_total_s: 1103.1797106266022\n",
      "  timers:\n",
      "    learn_throughput: 1850.449\n",
      "    learn_time_ms: 540.409\n",
      "    load_throughput: 223261.597\n",
      "    load_time_ms: 4.479\n",
      "    sample_throughput: 81.561\n",
      "    sample_time_ms: 12260.825\n",
      "    update_time_ms: 1.494\n",
      "  timestamp: 1632737042\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 90\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         1103.18</td><td style=\"text-align: right;\">90000</td><td style=\"text-align: right;\">-0.277778</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">             995.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 91000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-04-14\n",
      "  done: false\n",
      "  episode_len_mean: 995.4505494505495\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.27472527472527475\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 91\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1958285437689886\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014575347260994714\n",
      "          policy_loss: 0.04505645682414373\n",
      "          total_loss: 0.02469144579437044\n",
      "          vf_explained_var: -0.9798324704170227\n",
      "          vf_loss: 0.0001357409550918318\n",
      "    num_agent_steps_sampled: 91000\n",
      "    num_agent_steps_trained: 91000\n",
      "    num_steps_sampled: 91000\n",
      "    num_steps_trained: 91000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.05294117647058\n",
      "    ram_util_percent: 81.84705882352942\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04397892973591168\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.401642217489522\n",
      "    mean_inference_ms: 1.6232207299479353\n",
      "    mean_raw_obs_processing_ms: 0.4295048373335948\n",
      "  time_since_restore: 1115.1903021335602\n",
      "  time_this_iter_s: 12.010591506958008\n",
      "  time_total_s: 1115.1903021335602\n",
      "  timers:\n",
      "    learn_throughput: 1850.087\n",
      "    learn_time_ms: 540.515\n",
      "    load_throughput: 223106.024\n",
      "    load_time_ms: 4.482\n",
      "    sample_throughput: 80.79\n",
      "    sample_time_ms: 12377.771\n",
      "    update_time_ms: 1.49\n",
      "  timestamp: 1632737054\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 91000\n",
      "  training_iteration: 91\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         1115.19</td><td style=\"text-align: right;\">91000</td><td style=\"text-align: right;\">-0.274725</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           995.451</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-04-25\n",
      "  done: false\n",
      "  episode_len_mean: 995.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2717391304347826\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 92\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3614769697189333\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014678004714618175\n",
      "          policy_loss: -0.041372941931088765\n",
      "          total_loss: -0.06321064001984067\n",
      "          vf_explained_var: 0.14604657888412476\n",
      "          vf_loss: 0.00030927007553853197\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.45333333333333\n",
      "    ram_util_percent: 81.97999999999999\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04396946744522547\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.379024639973533\n",
      "    mean_inference_ms: 1.622985033301757\n",
      "    mean_raw_obs_processing_ms: 0.43219213943845763\n",
      "  time_since_restore: 1125.9779691696167\n",
      "  time_this_iter_s: 10.787667036056519\n",
      "  time_total_s: 1125.9779691696167\n",
      "  timers:\n",
      "    learn_throughput: 1855.71\n",
      "    learn_time_ms: 538.877\n",
      "    load_throughput: 223104.837\n",
      "    load_time_ms: 4.482\n",
      "    sample_throughput: 80.776\n",
      "    sample_time_ms: 12379.938\n",
      "    update_time_ms: 1.491\n",
      "  timestamp: 1632737065\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 92\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         1125.98</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">-0.271739</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">             995.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 93000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-04-36\n",
      "  done: false\n",
      "  episode_len_mean: 995.5483870967741\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.26881720430107525\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 93\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5284757534662883\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012323660227782751\n",
      "          policy_loss: -0.024588466187318168\n",
      "          total_loss: -0.04837231164177259\n",
      "          vf_explained_var: -0.5373808741569519\n",
      "          vf_loss: 0.0002685445061716665\n",
      "    num_agent_steps_sampled: 93000\n",
      "    num_agent_steps_trained: 93000\n",
      "    num_steps_sampled: 93000\n",
      "    num_steps_trained: 93000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.84375\n",
      "    ram_util_percent: 82.11875\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04396005483531105\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.356784364030856\n",
      "    mean_inference_ms: 1.622750025825283\n",
      "    mean_raw_obs_processing_ms: 0.43475761619615133\n",
      "  time_since_restore: 1136.6597559452057\n",
      "  time_this_iter_s: 10.68178677558899\n",
      "  time_total_s: 1136.6597559452057\n",
      "  timers:\n",
      "    learn_throughput: 1856.118\n",
      "    learn_time_ms: 538.759\n",
      "    load_throughput: 223252.091\n",
      "    load_time_ms: 4.479\n",
      "    sample_throughput: 80.941\n",
      "    sample_time_ms: 12354.748\n",
      "    update_time_ms: 1.514\n",
      "  timestamp: 1632737076\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 93000\n",
      "  training_iteration: 93\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         1136.66</td><td style=\"text-align: right;\">93000</td><td style=\"text-align: right;\">-0.268817</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           995.548</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 94000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-04-46\n",
      "  done: false\n",
      "  episode_len_mean: 995.5957446808511\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.26595744680851063\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 94\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5352438926696776\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010031122698881036\n",
      "          policy_loss: 0.020282725079192057\n",
      "          total_loss: -0.003952838646041022\n",
      "          vf_explained_var: -0.7528144717216492\n",
      "          vf_loss: 0.00011376222028047778\n",
      "    num_agent_steps_sampled: 94000\n",
      "    num_agent_steps_trained: 94000\n",
      "    num_steps_sampled: 94000\n",
      "    num_steps_trained: 94000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.166666666666664\n",
      "    ram_util_percent: 82.22666666666667\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04395068713650328\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.334904466467878\n",
      "    mean_inference_ms: 1.622515498689428\n",
      "    mean_raw_obs_processing_ms: 0.4372064766095873\n",
      "  time_since_restore: 1147.286788225174\n",
      "  time_this_iter_s: 10.627032279968262\n",
      "  time_total_s: 1147.286788225174\n",
      "  timers:\n",
      "    learn_throughput: 1853.686\n",
      "    learn_time_ms: 539.466\n",
      "    load_throughput: 221681.571\n",
      "    load_time_ms: 4.511\n",
      "    sample_throughput: 81.299\n",
      "    sample_time_ms: 12300.29\n",
      "    update_time_ms: 1.512\n",
      "  timestamp: 1632737086\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 94000\n",
      "  training_iteration: 94\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         1147.29</td><td style=\"text-align: right;\">94000</td><td style=\"text-align: right;\">-0.265957</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           995.596</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 95000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-04-57\n",
      "  done: false\n",
      "  episode_len_mean: 995.6421052631579\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2631578947368421\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 95\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5749624013900756\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01204502987278201\n",
      "          policy_loss: -0.18602746840980317\n",
      "          total_loss: -0.21048158076074389\n",
      "          vf_explained_var: -0.37137654423713684\n",
      "          vf_loss: 9.100819018688829e-05\n",
      "    num_agent_steps_sampled: 95000\n",
      "    num_agent_steps_trained: 95000\n",
      "    num_steps_sampled: 95000\n",
      "    num_steps_trained: 95000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.19333333333334\n",
      "    ram_util_percent: 82.38000000000001\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04394139216138845\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.313364634067037\n",
      "    mean_inference_ms: 1.6222813876160198\n",
      "    mean_raw_obs_processing_ms: 0.4395437539061774\n",
      "  time_since_restore: 1157.7978196144104\n",
      "  time_this_iter_s: 10.51103138923645\n",
      "  time_total_s: 1157.7978196144104\n",
      "  timers:\n",
      "    learn_throughput: 1858.694\n",
      "    learn_time_ms: 538.012\n",
      "    load_throughput: 220681.992\n",
      "    load_time_ms: 4.531\n",
      "    sample_throughput: 81.94\n",
      "    sample_time_ms: 12204.03\n",
      "    update_time_ms: 1.503\n",
      "  timestamp: 1632737097\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 95000\n",
      "  training_iteration: 95\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">          1157.8</td><td style=\"text-align: right;\">95000</td><td style=\"text-align: right;\">-0.263158</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           995.642</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-05-08\n",
      "  done: false\n",
      "  episode_len_mean: 995.6875\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2604166666666667\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 96\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4106970522138806\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012784088820001309\n",
      "          policy_loss: -0.05201982864075237\n",
      "          total_loss: -0.07465417550669776\n",
      "          vf_explained_var: -0.546211838722229\n",
      "          vf_loss: 0.00019421665015720968\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.14\n",
      "    ram_util_percent: 82.49333333333333\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04393216001116903\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.292180221294705\n",
      "    mean_inference_ms: 1.6220489901451571\n",
      "    mean_raw_obs_processing_ms: 0.44177412210731665\n",
      "  time_since_restore: 1168.5373678207397\n",
      "  time_this_iter_s: 10.739548206329346\n",
      "  time_total_s: 1168.5373678207397\n",
      "  timers:\n",
      "    learn_throughput: 1859.122\n",
      "    learn_time_ms: 537.888\n",
      "    load_throughput: 220714.507\n",
      "    load_time_ms: 4.531\n",
      "    sample_throughput: 82.403\n",
      "    sample_time_ms: 12135.418\n",
      "    update_time_ms: 1.507\n",
      "  timestamp: 1632737108\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 96\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         1168.54</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">-0.260417</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           995.688</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 97000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-05-18\n",
      "  done: false\n",
      "  episode_len_mean: 995.7319587628866\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.25773195876288657\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 97\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4636710405349733\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008966200105216012\n",
      "          policy_loss: 0.026558161661442783\n",
      "          total_loss: 0.0029882450277606645\n",
      "          vf_explained_var: -0.7326455116271973\n",
      "          vf_loss: 0.00017017332429531963\n",
      "    num_agent_steps_sampled: 97000\n",
      "    num_agent_steps_trained: 97000\n",
      "    num_steps_sampled: 97000\n",
      "    num_steps_trained: 97000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.93333333333335\n",
      "    ram_util_percent: 82.59333333333333\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043922979680270854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.271322788584376\n",
      "    mean_inference_ms: 1.6218174313464537\n",
      "    mean_raw_obs_processing_ms: 0.44390203500061726\n",
      "  time_since_restore: 1179.0890152454376\n",
      "  time_this_iter_s: 10.551647424697876\n",
      "  time_total_s: 1179.0890152454376\n",
      "  timers:\n",
      "    learn_throughput: 1858.923\n",
      "    learn_time_ms: 537.946\n",
      "    load_throughput: 220600.744\n",
      "    load_time_ms: 4.533\n",
      "    sample_throughput: 82.856\n",
      "    sample_time_ms: 12069.17\n",
      "    update_time_ms: 1.51\n",
      "  timestamp: 1632737118\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 97000\n",
      "  training_iteration: 97\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         1179.09</td><td style=\"text-align: right;\">97000</td><td style=\"text-align: right;\">-0.257732</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           995.732</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 98000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-05-29\n",
      "  done: false\n",
      "  episode_len_mean: 995.7755102040817\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.25510204081632654\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 98\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4576113250520493\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01505977275867563\n",
      "          policy_loss: 0.003390168315834469\n",
      "          total_loss: -0.01936850580904219\n",
      "          vf_explained_var: -0.465974897146225\n",
      "          vf_loss: 0.0003114600219608595\n",
      "    num_agent_steps_sampled: 98000\n",
      "    num_agent_steps_trained: 98000\n",
      "    num_steps_sampled: 98000\n",
      "    num_steps_trained: 98000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.5\n",
      "    ram_util_percent: 82.7\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04391384932730618\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.25078337842271\n",
      "    mean_inference_ms: 1.6215860492789969\n",
      "    mean_raw_obs_processing_ms: 0.44593227196912527\n",
      "  time_since_restore: 1189.6267311573029\n",
      "  time_this_iter_s: 10.537715911865234\n",
      "  time_total_s: 1189.6267311573029\n",
      "  timers:\n",
      "    learn_throughput: 1857.952\n",
      "    learn_time_ms: 538.227\n",
      "    load_throughput: 220123.752\n",
      "    load_time_ms: 4.543\n",
      "    sample_throughput: 83.559\n",
      "    sample_time_ms: 11967.623\n",
      "    update_time_ms: 1.513\n",
      "  timestamp: 1632737129\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 98000\n",
      "  training_iteration: 98\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         1189.63</td><td style=\"text-align: right;\">98000</td><td style=\"text-align: right;\">-0.255102</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           995.776</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 99000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-05-39\n",
      "  done: false\n",
      "  episode_len_mean: 995.8181818181819\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.25252525252525254\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 99\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.43654474152459\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014700124339958789\n",
      "          policy_loss: -0.03249659689350261\n",
      "          total_loss: -0.05517716362244553\n",
      "          vf_explained_var: -0.9641844034194946\n",
      "          vf_loss: 0.00021486807777869722\n",
      "    num_agent_steps_sampled: 99000\n",
      "    num_agent_steps_trained: 99000\n",
      "    num_steps_sampled: 99000\n",
      "    num_steps_trained: 99000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.193333333333335\n",
      "    ram_util_percent: 82.80666666666667\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043904800762402246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.230555892264924\n",
      "    mean_inference_ms: 1.6213553985314042\n",
      "    mean_raw_obs_processing_ms: 0.44786840862921584\n",
      "  time_since_restore: 1200.1807112693787\n",
      "  time_this_iter_s: 10.553980112075806\n",
      "  time_total_s: 1200.1807112693787\n",
      "  timers:\n",
      "    learn_throughput: 1856.967\n",
      "    learn_time_ms: 538.513\n",
      "    load_throughput: 220007.134\n",
      "    load_time_ms: 4.545\n",
      "    sample_throughput: 84.125\n",
      "    sample_time_ms: 11887.056\n",
      "    update_time_ms: 1.504\n",
      "  timestamp: 1632737139\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 99000\n",
      "  training_iteration: 99\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         1200.18</td><td style=\"text-align: right;\">99000</td><td style=\"text-align: right;\">-0.252525</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">           995.818</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-05-50\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.25\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 100\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.512313355339898\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009706112236933276\n",
      "          policy_loss: -0.09734418193499247\n",
      "          total_loss: -0.12133587135208977\n",
      "          vf_explained_var: -0.8946531414985657\n",
      "          vf_loss: 0.00016083191981629676\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.53125\n",
      "    ram_util_percent: 82.92500000000001\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043895831585780073\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.21064512260449\n",
      "    mean_inference_ms: 1.621125813873\n",
      "    mean_raw_obs_processing_ms: 0.44971452701755515\n",
      "  time_since_restore: 1210.861846446991\n",
      "  time_this_iter_s: 10.681135177612305\n",
      "  time_total_s: 1210.861846446991\n",
      "  timers:\n",
      "    learn_throughput: 1855.304\n",
      "    learn_time_ms: 538.995\n",
      "    load_throughput: 309769.056\n",
      "    load_time_ms: 3.228\n",
      "    sample_throughput: 97.839\n",
      "    sample_time_ms: 10220.865\n",
      "    update_time_ms: 1.502\n",
      "  timestamp: 1632737150\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 100\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         1210.86</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">   -0.25</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 101000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-06-01\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.25\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 101\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.500488583246867\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011554061339139299\n",
      "          policy_loss: -0.07617067458728949\n",
      "          total_loss: -0.09992490427361594\n",
      "          vf_explained_var: -0.09894564747810364\n",
      "          vf_loss: 9.524845206922489e-05\n",
      "    num_agent_steps_sampled: 101000\n",
      "    num_agent_steps_trained: 101000\n",
      "    num_steps_sampled: 101000\n",
      "    num_steps_trained: 101000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.04666666666667\n",
      "    ram_util_percent: 83.02666666666666\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043815920330932547\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.73433955455862\n",
      "    mean_inference_ms: 1.6196571286374022\n",
      "    mean_raw_obs_processing_ms: 0.45451431723179675\n",
      "  time_since_restore: 1221.4418017864227\n",
      "  time_this_iter_s: 10.579955339431763\n",
      "  time_total_s: 1221.4418017864227\n",
      "  timers:\n",
      "    learn_throughput: 1855.463\n",
      "    learn_time_ms: 538.949\n",
      "    load_throughput: 310014.043\n",
      "    load_time_ms: 3.226\n",
      "    sample_throughput: 99.227\n",
      "    sample_time_ms: 10077.864\n",
      "    update_time_ms: 1.51\n",
      "  timestamp: 1632737161\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 101000\n",
      "  training_iteration: 101\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         1221.44</td><td style=\"text-align: right;\">101000</td><td style=\"text-align: right;\">   -0.25</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 102000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-06-11\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.25\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 102\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5123241980870565\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01145668517560153\n",
      "          policy_loss: -0.08064234219491481\n",
      "          total_loss: -0.10453213651974996\n",
      "          vf_explained_var: -0.18743263185024261\n",
      "          vf_loss: 8.777698039921233e-05\n",
      "    num_agent_steps_sampled: 102000\n",
      "    num_agent_steps_trained: 102000\n",
      "    num_steps_sampled: 102000\n",
      "    num_steps_trained: 102000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.94666666666668\n",
      "    ram_util_percent: 83.11999999999999\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04375956556895894\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.497266573102152\n",
      "    mean_inference_ms: 1.618335781913038\n",
      "    mean_raw_obs_processing_ms: 0.45936041725575305\n",
      "  time_since_restore: 1232.0192189216614\n",
      "  time_this_iter_s: 10.577417135238647\n",
      "  time_total_s: 1232.0192189216614\n",
      "  timers:\n",
      "    learn_throughput: 1855.498\n",
      "    learn_time_ms: 538.939\n",
      "    load_throughput: 310587.957\n",
      "    load_time_ms: 3.22\n",
      "    sample_throughput: 99.435\n",
      "    sample_time_ms: 10056.87\n",
      "    update_time_ms: 1.505\n",
      "  timestamp: 1632737171\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 102000\n",
      "  training_iteration: 102\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         1232.02</td><td style=\"text-align: right;\">102000</td><td style=\"text-align: right;\">   -0.25</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 103000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-06-22\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.22\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 103\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.542198716269599\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011692136675641684\n",
      "          policy_loss: -0.08370613381266594\n",
      "          total_loss: -0.10783207602798939\n",
      "          vf_explained_var: -0.38231921195983887\n",
      "          vf_loss: 0.00012683175957338083\n",
      "    num_agent_steps_sampled: 103000\n",
      "    num_agent_steps_trained: 103000\n",
      "    num_steps_sampled: 103000\n",
      "    num_steps_trained: 103000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.30666666666666\n",
      "    ram_util_percent: 83.21333333333335\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04371534973092175\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.340880717386412\n",
      "    mean_inference_ms: 1.6173012015419093\n",
      "    mean_raw_obs_processing_ms: 0.46411097001873086\n",
      "  time_since_restore: 1242.5241963863373\n",
      "  time_this_iter_s: 10.504977464675903\n",
      "  time_total_s: 1242.5241963863373\n",
      "  timers:\n",
      "    learn_throughput: 1854.528\n",
      "    learn_time_ms: 539.221\n",
      "    load_throughput: 310624.759\n",
      "    load_time_ms: 3.219\n",
      "    sample_throughput: 99.612\n",
      "    sample_time_ms: 10038.93\n",
      "    update_time_ms: 1.485\n",
      "  timestamp: 1632737182\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 103000\n",
      "  training_iteration: 103\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         1242.52</td><td style=\"text-align: right;\">103000</td><td style=\"text-align: right;\">   -0.22</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-06-32\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.22\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 104\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4987660937839085\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013131179469850086\n",
      "          policy_loss: -0.06456632419592805\n",
      "          total_loss: -0.08803767419109741\n",
      "          vf_explained_var: -0.46517160534858704\n",
      "          vf_loss: 0.00020319122547031535\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.20666666666668\n",
      "    ram_util_percent: 83.31333333333333\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043675745560957165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.225729357136503\n",
      "    mean_inference_ms: 1.6164838427255313\n",
      "    mean_raw_obs_processing_ms: 0.4688676017580022\n",
      "  time_since_restore: 1252.9687390327454\n",
      "  time_this_iter_s: 10.444542646408081\n",
      "  time_total_s: 1252.9687390327454\n",
      "  timers:\n",
      "    learn_throughput: 1859.416\n",
      "    learn_time_ms: 537.803\n",
      "    load_throughput: 313527.187\n",
      "    load_time_ms: 3.19\n",
      "    sample_throughput: 99.779\n",
      "    sample_time_ms: 10022.119\n",
      "    update_time_ms: 1.482\n",
      "  timestamp: 1632737192\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 104\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         1252.97</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">   -0.22</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 105000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-06-43\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.22\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 105\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4741125716103447\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011676632973626531\n",
      "          policy_loss: -0.06076458651158545\n",
      "          total_loss: -0.08416639566421509\n",
      "          vf_explained_var: -0.5485209822654724\n",
      "          vf_loss: 0.00017165386889246293\n",
      "    num_agent_steps_sampled: 105000\n",
      "    num_agent_steps_trained: 105000\n",
      "    num_steps_sampled: 105000\n",
      "    num_steps_trained: 105000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.25333333333334\n",
      "    ram_util_percent: 83.44666666666667\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043639848292742124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.134166161226775\n",
      "    mean_inference_ms: 1.6156967468611436\n",
      "    mean_raw_obs_processing_ms: 0.4736102353969751\n",
      "  time_since_restore: 1263.7224719524384\n",
      "  time_this_iter_s: 10.753732919692993\n",
      "  time_total_s: 1263.7224719524384\n",
      "  timers:\n",
      "    learn_throughput: 1857.498\n",
      "    learn_time_ms: 538.359\n",
      "    load_throughput: 316057.48\n",
      "    load_time_ms: 3.164\n",
      "    sample_throughput: 99.543\n",
      "    sample_time_ms: 10045.942\n",
      "    update_time_ms: 1.492\n",
      "  timestamp: 1632737203\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 105000\n",
      "  training_iteration: 105\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         1263.72</td><td style=\"text-align: right;\">105000</td><td style=\"text-align: right;\">   -0.22</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 106000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-06-54\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.22\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 106\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5359444936116535\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012554039221025744\n",
      "          policy_loss: -0.05826359134581354\n",
      "          total_loss: -0.08220881014648411\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.00015881840111736285\n",
      "    num_agent_steps_sampled: 106000\n",
      "    num_agent_steps_trained: 106000\n",
      "    num_steps_sampled: 106000\n",
      "    num_steps_trained: 106000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.08125\n",
      "    ram_util_percent: 83.57499999999999\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04360781949449415\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.05807340629527\n",
      "    mean_inference_ms: 1.6149574316373536\n",
      "    mean_raw_obs_processing_ms: 0.47832312403178195\n",
      "  time_since_restore: 1274.578993320465\n",
      "  time_this_iter_s: 10.856521368026733\n",
      "  time_total_s: 1274.578993320465\n",
      "  timers:\n",
      "    learn_throughput: 1857.053\n",
      "    learn_time_ms: 538.488\n",
      "    load_throughput: 316162.306\n",
      "    load_time_ms: 3.163\n",
      "    sample_throughput: 99.428\n",
      "    sample_time_ms: 10057.509\n",
      "    update_time_ms: 1.488\n",
      "  timestamp: 1632737214\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 106000\n",
      "  training_iteration: 106\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         1274.58</td><td style=\"text-align: right;\">106000</td><td style=\"text-align: right;\">   -0.22</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 107000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-07-05\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.22\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 107\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.412741658422682\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010499231353629313\n",
      "          policy_loss: 0.015522919512457318\n",
      "          total_loss: -0.007456866568989224\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 9.77083774665112e-05\n",
      "    num_agent_steps_sampled: 107000\n",
      "    num_agent_steps_trained: 107000\n",
      "    num_steps_sampled: 107000\n",
      "    num_steps_trained: 107000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.69333333333334\n",
      "    ram_util_percent: 83.66000000000003\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04358032337885201\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.994051605972667\n",
      "    mean_inference_ms: 1.6143185434027827\n",
      "    mean_raw_obs_processing_ms: 0.48299138310478024\n",
      "  time_since_restore: 1285.3942685127258\n",
      "  time_this_iter_s: 10.815275192260742\n",
      "  time_total_s: 1285.3942685127258\n",
      "  timers:\n",
      "    learn_throughput: 1856.513\n",
      "    learn_time_ms: 538.644\n",
      "    load_throughput: 316295.821\n",
      "    load_time_ms: 3.162\n",
      "    sample_throughput: 99.17\n",
      "    sample_time_ms: 10083.74\n",
      "    update_time_ms: 1.485\n",
      "  timestamp: 1632737225\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 107000\n",
      "  training_iteration: 107\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         1285.39</td><td style=\"text-align: right;\">107000</td><td style=\"text-align: right;\">   -0.22</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-07-15\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.22\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 108\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.316991596751743\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01066190524365545\n",
      "          policy_loss: -0.014652973123722607\n",
      "          total_loss: -0.03655144208007389\n",
      "          vf_explained_var: -0.33423781394958496\n",
      "          vf_loss: 0.00020525490391365666\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.86875\n",
      "    ram_util_percent: 83.78125\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043555165737184556\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.938596096531226\n",
      "    mean_inference_ms: 1.6137144960494234\n",
      "    mean_raw_obs_processing_ms: 0.4876211680899406\n",
      "  time_since_restore: 1296.0290048122406\n",
      "  time_this_iter_s: 10.63473629951477\n",
      "  time_total_s: 1296.0290048122406\n",
      "  timers:\n",
      "    learn_throughput: 1857.805\n",
      "    learn_time_ms: 538.27\n",
      "    load_throughput: 317219.201\n",
      "    load_time_ms: 3.152\n",
      "    sample_throughput: 99.07\n",
      "    sample_time_ms: 10093.836\n",
      "    update_time_ms: 1.483\n",
      "  timestamp: 1632737235\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 108\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         1296.03</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">   -0.22</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 109000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-07-26\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.22\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 109\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3871677849027844\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009367949348666273\n",
      "          policy_loss: -0.0905936581393083\n",
      "          total_loss: -0.11334290264381303\n",
      "          vf_explained_var: -0.5333758592605591\n",
      "          vf_loss: 0.00018563646359931834\n",
      "    num_agent_steps_sampled: 109000\n",
      "    num_agent_steps_trained: 109000\n",
      "    num_steps_sampled: 109000\n",
      "    num_steps_trained: 109000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.46\n",
      "    ram_util_percent: 83.82\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04353176219767573\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.890172572389925\n",
      "    mean_inference_ms: 1.6131784039312325\n",
      "    mean_raw_obs_processing_ms: 0.4922200362261839\n",
      "  time_since_restore: 1306.8000905513763\n",
      "  time_this_iter_s: 10.771085739135742\n",
      "  time_total_s: 1306.8000905513763\n",
      "  timers:\n",
      "    learn_throughput: 1858.486\n",
      "    learn_time_ms: 538.072\n",
      "    load_throughput: 316553.634\n",
      "    load_time_ms: 3.159\n",
      "    sample_throughput: 98.856\n",
      "    sample_time_ms: 10115.719\n",
      "    update_time_ms: 1.486\n",
      "  timestamp: 1632737246\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 109000\n",
      "  training_iteration: 109\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">          1306.8</td><td style=\"text-align: right;\">109000</td><td style=\"text-align: right;\">   -0.22</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 110000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-07-37\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.22\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 110\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4652015580071343\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009110927573655787\n",
      "          policy_loss: -0.005454500123030609\n",
      "          total_loss: -0.029091759812500742\n",
      "          vf_explained_var: -0.3377518653869629\n",
      "          vf_loss: 0.00010366387448609911\n",
      "    num_agent_steps_sampled: 110000\n",
      "    num_agent_steps_trained: 110000\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.55333333333333\n",
      "    ram_util_percent: 83.92666666666668\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04351009626823476\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.847112470952123\n",
      "    mean_inference_ms: 1.6126563591257042\n",
      "    mean_raw_obs_processing_ms: 0.49677936264666256\n",
      "  time_since_restore: 1317.3582215309143\n",
      "  time_this_iter_s: 10.558130979537964\n",
      "  time_total_s: 1317.3582215309143\n",
      "  timers:\n",
      "    learn_throughput: 1856.92\n",
      "    learn_time_ms: 538.526\n",
      "    load_throughput: 317938.176\n",
      "    load_time_ms: 3.145\n",
      "    sample_throughput: 98.981\n",
      "    sample_time_ms: 10102.975\n",
      "    update_time_ms: 1.483\n",
      "  timestamp: 1632737257\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 110\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         1317.36</td><td style=\"text-align: right;\">110000</td><td style=\"text-align: right;\">   -0.22</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 111000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-07-48\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.22\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 111\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4393127547370064\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010423565351376871\n",
      "          policy_loss: -0.031075719330045912\n",
      "          total_loss: -0.054172804123825496\n",
      "          vf_explained_var: -0.7887703776359558\n",
      "          vf_loss: 0.00025368434548403863\n",
      "    num_agent_steps_sampled: 111000\n",
      "    num_agent_steps_trained: 111000\n",
      "    num_steps_sampled: 111000\n",
      "    num_steps_trained: 111000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.325\n",
      "    ram_util_percent: 84.11875\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04349059298930892\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.808499922189194\n",
      "    mean_inference_ms: 1.6121622817218766\n",
      "    mean_raw_obs_processing_ms: 0.5013046024353315\n",
      "  time_since_restore: 1328.4113328456879\n",
      "  time_this_iter_s: 11.05311131477356\n",
      "  time_total_s: 1328.4113328456879\n",
      "  timers:\n",
      "    learn_throughput: 1856.756\n",
      "    learn_time_ms: 538.574\n",
      "    load_throughput: 318532.155\n",
      "    load_time_ms: 3.139\n",
      "    sample_throughput: 98.52\n",
      "    sample_time_ms: 10150.269\n",
      "    update_time_ms: 1.477\n",
      "  timestamp: 1632737268\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 111000\n",
      "  training_iteration: 111\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         1328.41</td><td style=\"text-align: right;\">111000</td><td style=\"text-align: right;\">   -0.22</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-07-59\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.22\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 112\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4308849228752982\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0124228629374006\n",
      "          policy_loss: 0.027183395396504138\n",
      "          total_loss: 0.004371066091375219\n",
      "          vf_explained_var: -0.6992344856262207\n",
      "          vf_loss: 0.0002542324799125911\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.48\n",
      "    ram_util_percent: 84.22000000000001\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04347245441800411\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.773519391721972\n",
      "    mean_inference_ms: 1.6116925520551206\n",
      "    mean_raw_obs_processing_ms: 0.5057843423112865\n",
      "  time_since_restore: 1339.2157180309296\n",
      "  time_this_iter_s: 10.8043851852417\n",
      "  time_total_s: 1339.2157180309296\n",
      "  timers:\n",
      "    learn_throughput: 1854.553\n",
      "    learn_time_ms: 539.214\n",
      "    load_throughput: 318648.312\n",
      "    load_time_ms: 3.138\n",
      "    sample_throughput: 98.306\n",
      "    sample_time_ms: 10172.324\n",
      "    update_time_ms: 1.473\n",
      "  timestamp: 1632737279\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 112\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         1339.22</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">   -0.22</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 113000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-08-09\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.22\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 113\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4488657659954494\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010524038661440244\n",
      "          policy_loss: -0.0005680052977469233\n",
      "          total_loss: -0.023695984275804625\n",
      "          vf_explained_var: -0.6912607550621033\n",
      "          vf_loss: 0.0003082723378207043\n",
      "    num_agent_steps_sampled: 113000\n",
      "    num_agent_steps_trained: 113000\n",
      "    num_steps_sampled: 113000\n",
      "    num_steps_trained: 113000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.14375\n",
      "    ram_util_percent: 84.29374999999999\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04345651463586757\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.741636873020584\n",
      "    mean_inference_ms: 1.6112724322971972\n",
      "    mean_raw_obs_processing_ms: 0.5102315946420437\n",
      "  time_since_restore: 1350.0738544464111\n",
      "  time_this_iter_s: 10.858136415481567\n",
      "  time_total_s: 1350.0738544464111\n",
      "  timers:\n",
      "    learn_throughput: 1853.594\n",
      "    learn_time_ms: 539.493\n",
      "    load_throughput: 318186.604\n",
      "    load_time_ms: 3.143\n",
      "    sample_throughput: 97.969\n",
      "    sample_time_ms: 10207.33\n",
      "    update_time_ms: 1.479\n",
      "  timestamp: 1632737289\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 113000\n",
      "  training_iteration: 113\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         1350.07</td><td style=\"text-align: right;\">113000</td><td style=\"text-align: right;\">   -0.22</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 114000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-08-20\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.22\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 114\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.375048679775662\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009759398024871052\n",
      "          policy_loss: 0.01174226962029934\n",
      "          total_loss: -0.010745615636308988\n",
      "          vf_explained_var: -0.972567081451416\n",
      "          vf_loss: 0.00028666068570196835\n",
      "    num_agent_steps_sampled: 114000\n",
      "    num_agent_steps_trained: 114000\n",
      "    num_steps_sampled: 114000\n",
      "    num_steps_trained: 114000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.12666666666667\n",
      "    ram_util_percent: 84.31333333333332\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04344172939941951\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.712532267439377\n",
      "    mean_inference_ms: 1.6108657270502695\n",
      "    mean_raw_obs_processing_ms: 0.5146410565657695\n",
      "  time_since_restore: 1360.8204972743988\n",
      "  time_this_iter_s: 10.746642827987671\n",
      "  time_total_s: 1360.8204972743988\n",
      "  timers:\n",
      "    learn_throughput: 1853.528\n",
      "    learn_time_ms: 539.512\n",
      "    load_throughput: 318980.31\n",
      "    load_time_ms: 3.135\n",
      "    sample_throughput: 97.692\n",
      "    sample_time_ms: 10236.215\n",
      "    update_time_ms: 1.483\n",
      "  timestamp: 1632737300\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 114000\n",
      "  training_iteration: 114\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         1360.82</td><td style=\"text-align: right;\">114000</td><td style=\"text-align: right;\">   -0.22</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 115000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-08-31\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.22\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 115\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4308475547366672\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01156606003930294\n",
      "          policy_loss: 0.00013366316755612692\n",
      "          total_loss: -0.02271705377433035\n",
      "          vf_explained_var: -0.7011638879776001\n",
      "          vf_loss: 0.0003011547160794079\n",
      "    num_agent_steps_sampled: 115000\n",
      "    num_agent_steps_trained: 115000\n",
      "    num_steps_sampled: 115000\n",
      "    num_steps_trained: 115000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.93333333333333\n",
      "    ram_util_percent: 84.33999999999999\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043428200348842705\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.685922824429598\n",
      "    mean_inference_ms: 1.6104953221562448\n",
      "    mean_raw_obs_processing_ms: 0.5190174543811521\n",
      "  time_since_restore: 1371.3649797439575\n",
      "  time_this_iter_s: 10.544482469558716\n",
      "  time_total_s: 1371.3649797439575\n",
      "  timers:\n",
      "    learn_throughput: 1853.936\n",
      "    learn_time_ms: 539.393\n",
      "    load_throughput: 318856.639\n",
      "    load_time_ms: 3.136\n",
      "    sample_throughput: 97.892\n",
      "    sample_time_ms: 10215.377\n",
      "    update_time_ms: 1.477\n",
      "  timestamp: 1632737311\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 115000\n",
      "  training_iteration: 115\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         1371.36</td><td style=\"text-align: right;\">115000</td><td style=\"text-align: right;\">   -0.22</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-08-40\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.22\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 116\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.453119407759772\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02203691001039147\n",
      "          policy_loss: -0.26220006942749025\n",
      "          total_loss: -0.28421656820509167\n",
      "          vf_explained_var: 0.02681116759777069\n",
      "          vf_loss: 0.00031100030690949\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.44285714285714\n",
      "    ram_util_percent: 84.35714285714285\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043416108650762245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.661445323679317\n",
      "    mean_inference_ms: 1.610161064138201\n",
      "    mean_raw_obs_processing_ms: 0.5233629477206437\n",
      "  time_since_restore: 1380.9886374473572\n",
      "  time_this_iter_s: 9.623657703399658\n",
      "  time_total_s: 1380.9886374473572\n",
      "  timers:\n",
      "    learn_throughput: 1853.719\n",
      "    learn_time_ms: 539.456\n",
      "    load_throughput: 315034.325\n",
      "    load_time_ms: 3.174\n",
      "    sample_throughput: 99.089\n",
      "    sample_time_ms: 10091.982\n",
      "    update_time_ms: 1.484\n",
      "  timestamp: 1632737320\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 116\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         1380.99</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">   -0.22</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 117000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-08-51\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.22\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 117\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.459447087181939\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013564407329556901\n",
      "          policy_loss: -0.05766338308652242\n",
      "          total_loss: -0.07980961269802517\n",
      "          vf_explained_var: -0.2659725844860077\n",
      "          vf_loss: 0.0004135823834480511\n",
      "    num_agent_steps_sampled: 117000\n",
      "    num_agent_steps_trained: 117000\n",
      "    num_steps_sampled: 117000\n",
      "    num_steps_trained: 117000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.20625\n",
      "    ram_util_percent: 84.34375\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04340486404379522\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.63875852028392\n",
      "    mean_inference_ms: 1.6098471797002116\n",
      "    mean_raw_obs_processing_ms: 0.5276757204650847\n",
      "  time_since_restore: 1391.9633264541626\n",
      "  time_this_iter_s: 10.97468900680542\n",
      "  time_total_s: 1391.9633264541626\n",
      "  timers:\n",
      "    learn_throughput: 1853.788\n",
      "    learn_time_ms: 539.436\n",
      "    load_throughput: 311212.483\n",
      "    load_time_ms: 3.213\n",
      "    sample_throughput: 98.932\n",
      "    sample_time_ms: 10107.924\n",
      "    update_time_ms: 1.484\n",
      "  timestamp: 1632737331\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 117000\n",
      "  training_iteration: 117\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         1391.96</td><td style=\"text-align: right;\">117000</td><td style=\"text-align: right;\">   -0.22</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 118000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-09-03\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.22\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 118\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3399159537421332\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010984441265901168\n",
      "          policy_loss: -0.09490214006768333\n",
      "          total_loss: -0.11642094014419449\n",
      "          vf_explained_var: -0.21758867800235748\n",
      "          vf_loss: 0.0002326896220135192\n",
      "    num_agent_steps_sampled: 118000\n",
      "    num_agent_steps_trained: 118000\n",
      "    num_steps_sampled: 118000\n",
      "    num_steps_trained: 118000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.03125\n",
      "    ram_util_percent: 84.38125\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04339369023238648\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.61754825160541\n",
      "    mean_inference_ms: 1.6095353329957485\n",
      "    mean_raw_obs_processing_ms: 0.5319558346303175\n",
      "  time_since_restore: 1403.1405465602875\n",
      "  time_this_iter_s: 11.177220106124878\n",
      "  time_total_s: 1403.1405465602875\n",
      "  timers:\n",
      "    learn_throughput: 1852.07\n",
      "    learn_time_ms: 539.937\n",
      "    load_throughput: 306464.515\n",
      "    load_time_ms: 3.263\n",
      "    sample_throughput: 98.41\n",
      "    sample_time_ms: 10161.609\n",
      "    update_time_ms: 1.489\n",
      "  timestamp: 1632737343\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 118000\n",
      "  training_iteration: 118\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         1403.14</td><td style=\"text-align: right;\">118000</td><td style=\"text-align: right;\">   -0.22</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 119000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-09-14\n",
      "  done: false\n",
      "  episode_len_mean: 995.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.22\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 119\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3349675840801662\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013339080889397082\n",
      "          policy_loss: 0.0661469676428371\n",
      "          total_loss: 0.045084558779166804\n",
      "          vf_explained_var: 0.19147713482379913\n",
      "          vf_loss: 0.000286403959358318\n",
      "    num_agent_steps_sampled: 119000\n",
      "    num_agent_steps_trained: 119000\n",
      "    num_steps_sampled: 119000\n",
      "    num_steps_trained: 119000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.8625\n",
      "    ram_util_percent: 84.4\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04338266166000995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.597629333674552\n",
      "    mean_inference_ms: 1.6092261576647087\n",
      "    mean_raw_obs_processing_ms: 0.536202437533702\n",
      "  time_since_restore: 1414.2426927089691\n",
      "  time_this_iter_s: 11.10214614868164\n",
      "  time_total_s: 1414.2426927089691\n",
      "  timers:\n",
      "    learn_throughput: 1853.748\n",
      "    learn_time_ms: 539.448\n",
      "    load_throughput: 306003.925\n",
      "    load_time_ms: 3.268\n",
      "    sample_throughput: 98.085\n",
      "    sample_time_ms: 10195.214\n",
      "    update_time_ms: 1.486\n",
      "  timestamp: 1632737354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 119000\n",
      "  training_iteration: 119\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         1414.24</td><td style=\"text-align: right;\">119000</td><td style=\"text-align: right;\">   -0.22</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            995.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=452)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=452)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-09-41\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.22\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 120\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.323055312368605\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011627117998634917\n",
      "          policy_loss: -0.016850759088993073\n",
      "          total_loss: -0.038081653705901566\n",
      "          vf_explained_var: 0.0623636469244957\n",
      "          vf_loss: 0.0002555872740534445\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.58461538461538\n",
      "    ram_util_percent: 84.37435897435901\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0433721398620828\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.57885681211169\n",
      "    mean_inference_ms: 1.608929315156385\n",
      "    mean_raw_obs_processing_ms: 0.5418100645470468\n",
      "  time_since_restore: 1441.8001687526703\n",
      "  time_this_iter_s: 27.557476043701172\n",
      "  time_total_s: 1441.8001687526703\n",
      "  timers:\n",
      "    learn_throughput: 1856.961\n",
      "    learn_time_ms: 538.514\n",
      "    load_throughput: 211172.289\n",
      "    load_time_ms: 4.735\n",
      "    sample_throughput: 84.072\n",
      "    sample_time_ms: 11894.604\n",
      "    update_time_ms: 1.483\n",
      "  timestamp: 1632737381\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 120\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">          1441.8</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">   -0.22</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 121000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-09-54\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.22\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 121\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.270085334777832\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009752515961991613\n",
      "          policy_loss: -0.07205371757348379\n",
      "          total_loss: -0.0930008331934611\n",
      "          vf_explained_var: 0.003582184901461005\n",
      "          vf_loss: 0.0002908626417694096\n",
      "    num_agent_steps_sampled: 121000\n",
      "    num_agent_steps_trained: 121000\n",
      "    num_steps_sampled: 121000\n",
      "    num_steps_trained: 121000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.62222222222222\n",
      "    ram_util_percent: 84.3\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043361885056861935\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.561240564753598\n",
      "    mean_inference_ms: 1.608635552704317\n",
      "    mean_raw_obs_processing_ms: 0.5473717587717302\n",
      "  time_since_restore: 1454.2477066516876\n",
      "  time_this_iter_s: 12.447537899017334\n",
      "  time_total_s: 1454.2477066516876\n",
      "  timers:\n",
      "    learn_throughput: 1857.828\n",
      "    learn_time_ms: 538.263\n",
      "    load_throughput: 210920.611\n",
      "    load_time_ms: 4.741\n",
      "    sample_throughput: 83.103\n",
      "    sample_time_ms: 12033.194\n",
      "    update_time_ms: 1.504\n",
      "  timestamp: 1632737394\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 121000\n",
      "  training_iteration: 121\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         1454.25</td><td style=\"text-align: right;\">121000</td><td style=\"text-align: right;\">   -0.22</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 122000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-10-05\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.22\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 122\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2555505408181085\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009595969256593959\n",
      "          policy_loss: -0.021736368536949158\n",
      "          total_loss: -0.042629948423968424\n",
      "          vf_explained_var: -0.40540796518325806\n",
      "          vf_loss: 0.00022252813966285127\n",
      "    num_agent_steps_sampled: 122000\n",
      "    num_agent_steps_trained: 122000\n",
      "    num_steps_sampled: 122000\n",
      "    num_steps_trained: 122000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.15\n",
      "    ram_util_percent: 84.2875\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043351274666745224\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.544522877241523\n",
      "    mean_inference_ms: 1.6083363964655368\n",
      "    mean_raw_obs_processing_ms: 0.5528922179445793\n",
      "  time_since_restore: 1465.2454800605774\n",
      "  time_this_iter_s: 10.99777340888977\n",
      "  time_total_s: 1465.2454800605774\n",
      "  timers:\n",
      "    learn_throughput: 1857.013\n",
      "    learn_time_ms: 538.499\n",
      "    load_throughput: 210826.254\n",
      "    load_time_ms: 4.743\n",
      "    sample_throughput: 82.972\n",
      "    sample_time_ms: 12052.262\n",
      "    update_time_ms: 1.511\n",
      "  timestamp: 1632737405\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 122000\n",
      "  training_iteration: 122\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         1465.25</td><td style=\"text-align: right;\">122000</td><td style=\"text-align: right;\">   -0.22</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 123000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-10-16\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 123\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.247271778848436\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008996578306628302\n",
      "          policy_loss: 0.027954529432786836\n",
      "          total_loss: 0.006962629449036386\n",
      "          vf_explained_var: -0.6644667983055115\n",
      "          vf_loss: 0.000131331504114011\n",
      "    num_agent_steps_sampled: 123000\n",
      "    num_agent_steps_trained: 123000\n",
      "    num_steps_sampled: 123000\n",
      "    num_steps_trained: 123000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.313333333333325\n",
      "    ram_util_percent: 84.32666666666667\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043340035755069256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.528623398253037\n",
      "    mean_inference_ms: 1.6080031273234239\n",
      "    mean_raw_obs_processing_ms: 0.5583627926817369\n",
      "  time_since_restore: 1476.272964477539\n",
      "  time_this_iter_s: 11.02748441696167\n",
      "  time_total_s: 1476.272964477539\n",
      "  timers:\n",
      "    learn_throughput: 1858.085\n",
      "    learn_time_ms: 538.188\n",
      "    load_throughput: 211025.669\n",
      "    load_time_ms: 4.739\n",
      "    sample_throughput: 82.854\n",
      "    sample_time_ms: 12069.49\n",
      "    update_time_ms: 1.511\n",
      "  timestamp: 1632737416\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 123000\n",
      "  training_iteration: 123\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         1476.27</td><td style=\"text-align: right;\">123000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-10-27\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 124\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.270600414276123\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010725383851992268\n",
      "          policy_loss: -0.11296452801260683\n",
      "          total_loss: -0.13395766309565968\n",
      "          vf_explained_var: -0.6531471610069275\n",
      "          vf_loss: 0.00010406360140930499\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.247058823529414\n",
      "    ram_util_percent: 84.36470588235294\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04332897723651749\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.513637049738135\n",
      "    mean_inference_ms: 1.6076703556210188\n",
      "    mean_raw_obs_processing_ms: 0.5637884186787808\n",
      "  time_since_restore: 1487.6307661533356\n",
      "  time_this_iter_s: 11.357801675796509\n",
      "  time_total_s: 1487.6307661533356\n",
      "  timers:\n",
      "    learn_throughput: 1856.33\n",
      "    learn_time_ms: 538.697\n",
      "    load_throughput: 211472.537\n",
      "    load_time_ms: 4.729\n",
      "    sample_throughput: 82.431\n",
      "    sample_time_ms: 12131.423\n",
      "    update_time_ms: 1.51\n",
      "  timestamp: 1632737427\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 124\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         1487.63</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 125000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-10-39\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 125\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2950886461469864\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009341220624485608\n",
      "          policy_loss: -0.030236192958222496\n",
      "          total_loss: -0.05167242387930552\n",
      "          vf_explained_var: -0.4263309836387634\n",
      "          vf_loss: 0.00011346935643814504\n",
      "    num_agent_steps_sampled: 125000\n",
      "    num_agent_steps_trained: 125000\n",
      "    num_steps_sampled: 125000\n",
      "    num_steps_trained: 125000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.150000000000006\n",
      "    ram_util_percent: 84.4625\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043318013127190344\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.499359174636552\n",
      "    mean_inference_ms: 1.6073371270411292\n",
      "    mean_raw_obs_processing_ms: 0.5691672881259856\n",
      "  time_since_restore: 1498.8157353401184\n",
      "  time_this_iter_s: 11.184969186782837\n",
      "  time_total_s: 1498.8157353401184\n",
      "  timers:\n",
      "    learn_throughput: 1857.617\n",
      "    learn_time_ms: 538.324\n",
      "    load_throughput: 211429.896\n",
      "    load_time_ms: 4.73\n",
      "    sample_throughput: 81.995\n",
      "    sample_time_ms: 12195.845\n",
      "    update_time_ms: 1.513\n",
      "  timestamp: 1632737439\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 125000\n",
      "  training_iteration: 125\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         1498.82</td><td style=\"text-align: right;\">125000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 126000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-10-50\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 126\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2260587983661226\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01099889403702796\n",
      "          policy_loss: -0.03831673661867777\n",
      "          total_loss: -0.058837314446767174\n",
      "          vf_explained_var: -0.44266512989997864\n",
      "          vf_loss: 9.017281602912893e-05\n",
      "    num_agent_steps_sampled: 126000\n",
      "    num_agent_steps_trained: 126000\n",
      "    num_steps_sampled: 126000\n",
      "    num_steps_trained: 126000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.15625\n",
      "    ram_util_percent: 84.4375\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0433069824178086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.48565793018924\n",
      "    mean_inference_ms: 1.606997724946271\n",
      "    mean_raw_obs_processing_ms: 0.5744975408690509\n",
      "  time_since_restore: 1510.186861038208\n",
      "  time_this_iter_s: 11.3711256980896\n",
      "  time_total_s: 1510.186861038208\n",
      "  timers:\n",
      "    learn_throughput: 1859.033\n",
      "    learn_time_ms: 537.914\n",
      "    load_throughput: 212678.88\n",
      "    load_time_ms: 4.702\n",
      "    sample_throughput: 80.834\n",
      "    sample_time_ms: 12371.057\n",
      "    update_time_ms: 1.507\n",
      "  timestamp: 1632737450\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 126000\n",
      "  training_iteration: 126\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         1510.19</td><td style=\"text-align: right;\">126000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 127000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-11-01\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 127\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1929963297314115\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009330250123228892\n",
      "          policy_loss: -0.024379999190568925\n",
      "          total_loss: -0.04479401947723494\n",
      "          vf_explained_var: -0.9590843915939331\n",
      "          vf_loss: 0.00011640513942741866\n",
      "    num_agent_steps_sampled: 127000\n",
      "    num_agent_steps_trained: 127000\n",
      "    num_steps_sampled: 127000\n",
      "    num_steps_trained: 127000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.7125\n",
      "    ram_util_percent: 84.5\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04329641534804684\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.472674916558526\n",
      "    mean_inference_ms: 1.606671980860711\n",
      "    mean_raw_obs_processing_ms: 0.579785860374348\n",
      "  time_since_restore: 1521.5357325077057\n",
      "  time_this_iter_s: 11.34887146949768\n",
      "  time_total_s: 1521.5357325077057\n",
      "  timers:\n",
      "    learn_throughput: 1859.74\n",
      "    learn_time_ms: 537.71\n",
      "    load_throughput: 214707.141\n",
      "    load_time_ms: 4.658\n",
      "    sample_throughput: 80.588\n",
      "    sample_time_ms: 12408.722\n",
      "    update_time_ms: 1.506\n",
      "  timestamp: 1632737461\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 127000\n",
      "  training_iteration: 127\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         1521.54</td><td style=\"text-align: right;\">127000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-11-13\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 128\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2151292112138536\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012395373805143963\n",
      "          policy_loss: -0.054587363327542944\n",
      "          total_loss: -0.07469122991379765\n",
      "          vf_explained_var: -0.9204661250114441\n",
      "          vf_loss: 0.00018812077077099174\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.26875\n",
      "    ram_util_percent: 84.4625\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043286414295101855\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.460294411796182\n",
      "    mean_inference_ms: 1.606361602800302\n",
      "    mean_raw_obs_processing_ms: 0.5850279632997678\n",
      "  time_since_restore: 1532.7339844703674\n",
      "  time_this_iter_s: 11.198251962661743\n",
      "  time_total_s: 1532.7339844703674\n",
      "  timers:\n",
      "    learn_throughput: 1859.003\n",
      "    learn_time_ms: 537.923\n",
      "    load_throughput: 216513.731\n",
      "    load_time_ms: 4.619\n",
      "    sample_throughput: 80.576\n",
      "    sample_time_ms: 12410.658\n",
      "    update_time_ms: 1.51\n",
      "  timestamp: 1632737473\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 128\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">         1532.73</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 129000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-11-24\n",
      "  done: false\n",
      "  episode_len_mean: 994.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 129\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1980379237069023\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010149024645018863\n",
      "          policy_loss: -0.03005354909433259\n",
      "          total_loss: -0.05038167279627588\n",
      "          vf_explained_var: -0.4944753348827362\n",
      "          vf_loss: 0.00012989847972575162\n",
      "    num_agent_steps_sampled: 129000\n",
      "    num_agent_steps_trained: 129000\n",
      "    num_steps_sampled: 129000\n",
      "    num_steps_trained: 129000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.25\n",
      "    ram_util_percent: 84.5\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043276904277206735\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.448339623975748\n",
      "    mean_inference_ms: 1.6060642605596098\n",
      "    mean_raw_obs_processing_ms: 0.5902308088781759\n",
      "  time_since_restore: 1543.9759502410889\n",
      "  time_this_iter_s: 11.241965770721436\n",
      "  time_total_s: 1543.9759502410889\n",
      "  timers:\n",
      "    learn_throughput: 1858.916\n",
      "    learn_time_ms: 537.948\n",
      "    load_throughput: 217105.47\n",
      "    load_time_ms: 4.606\n",
      "    sample_throughput: 80.485\n",
      "    sample_time_ms: 12424.607\n",
      "    update_time_ms: 1.51\n",
      "  timestamp: 1632737484\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 129000\n",
      "  training_iteration: 129\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         1543.98</td><td style=\"text-align: right;\">129000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             994.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 130000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-11-35\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 130\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.168603293100993\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011139769462456832\n",
      "          policy_loss: 0.010121205697456996\n",
      "          total_loss: -0.009749749965137906\n",
      "          vf_explained_var: -0.7690477967262268\n",
      "          vf_loss: 0.00014411172439091993\n",
      "    num_agent_steps_sampled: 130000\n",
      "    num_agent_steps_trained: 130000\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.13125\n",
      "    ram_util_percent: 84.54375\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04326780306964018\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.436852894448617\n",
      "    mean_inference_ms: 1.6057753687760388\n",
      "    mean_raw_obs_processing_ms: 0.5895686761028063\n",
      "  time_since_restore: 1555.0340597629547\n",
      "  time_this_iter_s: 11.058109521865845\n",
      "  time_total_s: 1555.0340597629547\n",
      "  timers:\n",
      "    learn_throughput: 1857.407\n",
      "    learn_time_ms: 538.385\n",
      "    load_throughput: 317276.791\n",
      "    load_time_ms: 3.152\n",
      "    sample_throughput: 92.801\n",
      "    sample_time_ms: 10775.715\n",
      "    update_time_ms: 1.511\n",
      "  timestamp: 1632737495\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 130\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">         1555.03</td><td style=\"text-align: right;\">130000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 131000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-11-46\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 131\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.180068940586514\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009920696732209904\n",
      "          policy_loss: 0.007277895592980915\n",
      "          total_loss: -0.012970662117004395\n",
      "          vf_explained_var: -0.9949599504470825\n",
      "          vf_loss: 6.402681489513877e-05\n",
      "    num_agent_steps_sampled: 131000\n",
      "    num_agent_steps_trained: 131000\n",
      "    num_steps_sampled: 131000\n",
      "    num_steps_trained: 131000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.98125\n",
      "    ram_util_percent: 84.57499999999999\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04325871765298973\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.425808025094573\n",
      "    mean_inference_ms: 1.605492436809345\n",
      "    mean_raw_obs_processing_ms: 0.5890572781666981\n",
      "  time_since_restore: 1566.3138723373413\n",
      "  time_this_iter_s: 11.279812574386597\n",
      "  time_total_s: 1566.3138723373413\n",
      "  timers:\n",
      "    learn_throughput: 1857.601\n",
      "    learn_time_ms: 538.329\n",
      "    load_throughput: 311644.896\n",
      "    load_time_ms: 3.209\n",
      "    sample_throughput: 93.809\n",
      "    sample_time_ms: 10659.992\n",
      "    update_time_ms: 1.495\n",
      "  timestamp: 1632737506\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 131000\n",
      "  training_iteration: 131\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         1566.31</td><td style=\"text-align: right;\">131000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-11-58\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 132\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1536112705866497\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011125395037270221\n",
      "          policy_loss: -0.049575287890103126\n",
      "          total_loss: -0.0693201421863503\n",
      "          vf_explained_var: -0.9962068200111389\n",
      "          vf_loss: 0.00012244741985543644\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.15625\n",
      "    ram_util_percent: 84.6\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0432498641771432\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.415296278695273\n",
      "    mean_inference_ms: 1.6052199185922833\n",
      "    mean_raw_obs_processing_ms: 0.5886851144777949\n",
      "  time_since_restore: 1577.7188878059387\n",
      "  time_this_iter_s: 11.405015468597412\n",
      "  time_total_s: 1577.7188878059387\n",
      "  timers:\n",
      "    learn_throughput: 1861.105\n",
      "    learn_time_ms: 537.315\n",
      "    load_throughput: 311730.596\n",
      "    load_time_ms: 3.208\n",
      "    sample_throughput: 93.443\n",
      "    sample_time_ms: 10701.726\n",
      "    update_time_ms: 1.492\n",
      "  timestamp: 1632737518\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 132\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         1577.72</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 133000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-12-09\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 133\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.065454708205329\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009798131735522159\n",
      "          policy_loss: -0.03954739218784703\n",
      "          total_loss: -0.05862853272507588\n",
      "          vf_explained_var: -0.8764340877532959\n",
      "          vf_loss: 0.0001036855329503952\n",
      "    num_agent_steps_sampled: 133000\n",
      "    num_agent_steps_trained: 133000\n",
      "    num_steps_sampled: 133000\n",
      "    num_steps_trained: 133000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.49411764705883\n",
      "    ram_util_percent: 84.62352941176471\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04324129781047834\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.405282308935778\n",
      "    mean_inference_ms: 1.6049600921979155\n",
      "    mean_raw_obs_processing_ms: 0.5884420588184482\n",
      "  time_since_restore: 1589.0389733314514\n",
      "  time_this_iter_s: 11.320085525512695\n",
      "  time_total_s: 1589.0389733314514\n",
      "  timers:\n",
      "    learn_throughput: 1858.166\n",
      "    learn_time_ms: 538.165\n",
      "    load_throughput: 311145.532\n",
      "    load_time_ms: 3.214\n",
      "    sample_throughput: 93.195\n",
      "    sample_time_ms: 10730.164\n",
      "    update_time_ms: 1.491\n",
      "  timestamp: 1632737529\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 133000\n",
      "  training_iteration: 133\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">         1589.04</td><td style=\"text-align: right;\">133000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 134000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-12-21\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 134\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.156803486082289\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015034947357320935\n",
      "          policy_loss: -0.018491632863879205\n",
      "          total_loss: -0.03770326893362734\n",
      "          vf_explained_var: -0.4988389015197754\n",
      "          vf_loss: 0.00010115566049838284\n",
      "    num_agent_steps_sampled: 134000\n",
      "    num_agent_steps_trained: 134000\n",
      "    num_steps_sampled: 134000\n",
      "    num_steps_trained: 134000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.39411764705881\n",
      "    ram_util_percent: 84.7235294117647\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04323260397596739\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.3956195316238\n",
      "    mean_inference_ms: 1.6047007909433566\n",
      "    mean_raw_obs_processing_ms: 0.5883187443737224\n",
      "  time_since_restore: 1601.2179560661316\n",
      "  time_this_iter_s: 12.178982734680176\n",
      "  time_total_s: 1601.2179560661316\n",
      "  timers:\n",
      "    learn_throughput: 1844.027\n",
      "    learn_time_ms: 542.291\n",
      "    load_throughput: 296937.672\n",
      "    load_time_ms: 3.368\n",
      "    sample_throughput: 92.525\n",
      "    sample_time_ms: 10807.92\n",
      "    update_time_ms: 1.51\n",
      "  timestamp: 1632737541\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 134000\n",
      "  training_iteration: 134\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         1601.22</td><td style=\"text-align: right;\">134000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 135000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-12-33\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 135\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.185128492779202\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007821342538149503\n",
      "          policy_loss: -0.09094371994336446\n",
      "          total_loss: -0.11154021095070574\n",
      "          vf_explained_var: -0.9347925782203674\n",
      "          vf_loss: 8.159318056439386e-05\n",
      "    num_agent_steps_sampled: 135000\n",
      "    num_agent_steps_trained: 135000\n",
      "    num_steps_sampled: 135000\n",
      "    num_steps_trained: 135000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.85882352941177\n",
      "    ram_util_percent: 84.93529411764706\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04322404181674937\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.386391298825655\n",
      "    mean_inference_ms: 1.6044414705131314\n",
      "    mean_raw_obs_processing_ms: 0.5883043322098068\n",
      "  time_since_restore: 1613.3946137428284\n",
      "  time_this_iter_s: 12.176657676696777\n",
      "  time_total_s: 1613.3946137428284\n",
      "  timers:\n",
      "    learn_throughput: 1835.747\n",
      "    learn_time_ms: 544.737\n",
      "    load_throughput: 296082.451\n",
      "    load_time_ms: 3.377\n",
      "    sample_throughput: 91.704\n",
      "    sample_time_ms: 10904.672\n",
      "    update_time_ms: 1.51\n",
      "  timestamp: 1632737553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 135000\n",
      "  training_iteration: 135\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         1613.39</td><td style=\"text-align: right;\">135000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-12-45\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 136\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1960924175050525\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011506190059546522\n",
      "          policy_loss: -0.0629605116115676\n",
      "          total_loss: -0.08313107722335392\n",
      "          vf_explained_var: -0.5047528147697449\n",
      "          vf_loss: 6.443079340291054e-05\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.05555555555556\n",
      "    ram_util_percent: 84.72777777777777\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043214663155424785\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.377434900266072\n",
      "    mean_inference_ms: 1.6041698019195323\n",
      "    mean_raw_obs_processing_ms: 0.5883931304134159\n",
      "  time_since_restore: 1625.3842749595642\n",
      "  time_this_iter_s: 11.98966121673584\n",
      "  time_total_s: 1625.3842749595642\n",
      "  timers:\n",
      "    learn_throughput: 1828.081\n",
      "    learn_time_ms: 547.022\n",
      "    load_throughput: 293488.58\n",
      "    load_time_ms: 3.407\n",
      "    sample_throughput: 91.206\n",
      "    sample_time_ms: 10964.205\n",
      "    update_time_ms: 1.518\n",
      "  timestamp: 1632737565\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 136\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         1625.38</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 137000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-12-57\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 137\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.179061847262912\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00978318530491839\n",
      "          policy_loss: -0.01571765591700872\n",
      "          total_loss: -0.035933285703261694\n",
      "          vf_explained_var: -0.4583781063556671\n",
      "          vf_loss: 0.00010751232912298292\n",
      "    num_agent_steps_sampled: 137000\n",
      "    num_agent_steps_trained: 137000\n",
      "    num_steps_sampled: 137000\n",
      "    num_steps_trained: 137000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.3375\n",
      "    ram_util_percent: 84.9125\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04320470792062678\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.36868150738481\n",
      "    mean_inference_ms: 1.6038845640157522\n",
      "    mean_raw_obs_processing_ms: 0.5885740852617807\n",
      "  time_since_restore: 1637.065177679062\n",
      "  time_this_iter_s: 11.68090271949768\n",
      "  time_total_s: 1637.065177679062\n",
      "  timers:\n",
      "    learn_throughput: 1824.38\n",
      "    learn_time_ms: 548.131\n",
      "    load_throughput: 293168.563\n",
      "    load_time_ms: 3.411\n",
      "    sample_throughput: 90.94\n",
      "    sample_time_ms: 10996.289\n",
      "    update_time_ms: 1.521\n",
      "  timestamp: 1632737577\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 137000\n",
      "  training_iteration: 137\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         1637.07</td><td style=\"text-align: right;\">137000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 138000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-13-08\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 138\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.128486484951443\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009095514882196959\n",
      "          policy_loss: 0.002421041371093856\n",
      "          total_loss: -0.01728616551392608\n",
      "          vf_explained_var: -0.4781325161457062\n",
      "          vf_loss: 0.00021332968131496777\n",
      "    num_agent_steps_sampled: 138000\n",
      "    num_agent_steps_trained: 138000\n",
      "    num_steps_sampled: 138000\n",
      "    num_steps_trained: 138000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.50000000000001\n",
      "    ram_util_percent: 84.78823529411764\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04319483422315642\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.360238021715261\n",
      "    mean_inference_ms: 1.60360578097286\n",
      "    mean_raw_obs_processing_ms: 0.588843131140707\n",
      "  time_since_restore: 1648.4725966453552\n",
      "  time_this_iter_s: 11.407418966293335\n",
      "  time_total_s: 1648.4725966453552\n",
      "  timers:\n",
      "    learn_throughput: 1824.714\n",
      "    learn_time_ms: 548.031\n",
      "    load_throughput: 293844.289\n",
      "    load_time_ms: 3.403\n",
      "    sample_throughput: 90.766\n",
      "    sample_time_ms: 11017.317\n",
      "    update_time_ms: 1.518\n",
      "  timestamp: 1632737588\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 138000\n",
      "  training_iteration: 138\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         1648.47</td><td style=\"text-align: right;\">138000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 139000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-13-20\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 139\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1003341065512764\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010835912028468937\n",
      "          policy_loss: -0.03099995921883318\n",
      "          total_loss: -0.05019443883664078\n",
      "          vf_explained_var: -0.676210343837738\n",
      "          vf_loss: 0.00018347512878891494\n",
      "    num_agent_steps_sampled: 139000\n",
      "    num_agent_steps_trained: 139000\n",
      "    num_steps_sampled: 139000\n",
      "    num_steps_trained: 139000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.49375\n",
      "    ram_util_percent: 84.68125\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04318485654247659\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.352035178940627\n",
      "    mean_inference_ms: 1.6033279958607707\n",
      "    mean_raw_obs_processing_ms: 0.5891960817170183\n",
      "  time_since_restore: 1659.8190486431122\n",
      "  time_this_iter_s: 11.346451997756958\n",
      "  time_total_s: 1659.8190486431122\n",
      "  timers:\n",
      "    learn_throughput: 1823.18\n",
      "    learn_time_ms: 548.492\n",
      "    load_throughput: 293355.155\n",
      "    load_time_ms: 3.409\n",
      "    sample_throughput: 90.684\n",
      "    sample_time_ms: 11027.279\n",
      "    update_time_ms: 1.525\n",
      "  timestamp: 1632737600\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 139000\n",
      "  training_iteration: 139\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         1659.82</td><td style=\"text-align: right;\">139000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-13-32\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 140\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1023738980293274\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010601222695464221\n",
      "          policy_loss: -0.07213304510547056\n",
      "          total_loss: -0.09139892558256785\n",
      "          vf_explained_var: -0.3365122377872467\n",
      "          vf_loss: 0.0001676770193929163\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.3764705882353\n",
      "    ram_util_percent: 84.92941176470588\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04317515637425793\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.344162747572762\n",
      "    mean_inference_ms: 1.6030589684424514\n",
      "    mean_raw_obs_processing_ms: 0.5896285106614308\n",
      "  time_since_restore: 1671.8314661979675\n",
      "  time_this_iter_s: 12.012417554855347\n",
      "  time_total_s: 1671.8314661979675\n",
      "  timers:\n",
      "    learn_throughput: 1821.605\n",
      "    learn_time_ms: 548.966\n",
      "    load_throughput: 294357.779\n",
      "    load_time_ms: 3.397\n",
      "    sample_throughput: 89.91\n",
      "    sample_time_ms: 11122.235\n",
      "    update_time_ms: 1.528\n",
      "  timestamp: 1632737612\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 140\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         1671.83</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 141000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-13-44\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 141\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.153299452198876\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008987602084954242\n",
      "          policy_loss: -0.09230061651517947\n",
      "          total_loss: -0.1123015594151285\n",
      "          vf_explained_var: -0.9773854613304138\n",
      "          vf_loss: 0.00018390873930103328\n",
      "    num_agent_steps_sampled: 141000\n",
      "    num_agent_steps_trained: 141000\n",
      "    num_steps_sampled: 141000\n",
      "    num_steps_trained: 141000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.65882352941177\n",
      "    ram_util_percent: 85.02941176470588\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04316487002914895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.336465257716311\n",
      "    mean_inference_ms: 1.6027795958832711\n",
      "    mean_raw_obs_processing_ms: 0.590125641407074\n",
      "  time_since_restore: 1683.9021048545837\n",
      "  time_this_iter_s: 12.070638656616211\n",
      "  time_total_s: 1683.9021048545837\n",
      "  timers:\n",
      "    learn_throughput: 1820.415\n",
      "    learn_time_ms: 549.325\n",
      "    load_throughput: 299434.87\n",
      "    load_time_ms: 3.34\n",
      "    sample_throughput: 89.278\n",
      "    sample_time_ms: 11201.004\n",
      "    update_time_ms: 1.529\n",
      "  timestamp: 1632737624\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 141000\n",
      "  training_iteration: 141\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">          1683.9</td><td style=\"text-align: right;\">141000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 142000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-13-55\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 142\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.103549999660916\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009268246148892798\n",
      "          policy_loss: -0.028026428570350013\n",
      "          total_loss: -0.04754615492290921\n",
      "          vf_explained_var: -0.8501831889152527\n",
      "          vf_loss: 0.00012553793704783958\n",
      "    num_agent_steps_sampled: 142000\n",
      "    num_agent_steps_trained: 142000\n",
      "    num_steps_sampled: 142000\n",
      "    num_steps_trained: 142000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.1875\n",
      "    ram_util_percent: 84.925\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04315340898338782\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.328734161529875\n",
      "    mean_inference_ms: 1.6024786042905348\n",
      "    mean_raw_obs_processing_ms: 0.5906843986889079\n",
      "  time_since_restore: 1695.2424139976501\n",
      "  time_this_iter_s: 11.340309143066406\n",
      "  time_total_s: 1695.2424139976501\n",
      "  timers:\n",
      "    learn_throughput: 1819.921\n",
      "    learn_time_ms: 549.474\n",
      "    load_throughput: 299781.577\n",
      "    load_time_ms: 3.336\n",
      "    sample_throughput: 89.33\n",
      "    sample_time_ms: 11194.406\n",
      "    update_time_ms: 1.527\n",
      "  timestamp: 1632737635\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 142000\n",
      "  training_iteration: 142\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         1695.24</td><td style=\"text-align: right;\">142000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_a44fc_00000:\n",
      "  agent_timesteps_total: 143000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-27_10-14-07\n",
      "  done: false\n",
      "  episode_len_mean: 995.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 143\n",
      "  experiment_id: 03be24429ec34374bfc1b78d8a5a014c\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1419659548335606\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008303343883149314\n",
      "          policy_loss: -0.08544610804981656\n",
      "          total_loss: -0.10540715588463677\n",
      "          vf_explained_var: -0.555293619632721\n",
      "          vf_loss: 0.00021311044470672237\n",
      "    num_agent_steps_sampled: 143000\n",
      "    num_agent_steps_trained: 143000\n",
      "    num_steps_sampled: 143000\n",
      "    num_steps_trained: 143000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.65294117647059\n",
      "    ram_util_percent: 84.92352941176472\n",
      "  pid: 457\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04314211570987668\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.321209790452276\n",
      "    mean_inference_ms: 1.6021797068489498\n",
      "    mean_raw_obs_processing_ms: 0.5913077654214883\n",
      "  time_since_restore: 1706.544551372528\n",
      "  time_this_iter_s: 11.30213737487793\n",
      "  time_total_s: 1706.544551372528\n",
      "  timers:\n",
      "    learn_throughput: 1822.324\n",
      "    learn_time_ms: 548.75\n",
      "    load_throughput: 299576.024\n",
      "    load_time_ms: 3.338\n",
      "    sample_throughput: 89.339\n",
      "    sample_time_ms: 11193.311\n",
      "    update_time_ms: 1.524\n",
      "  timestamp: 1632737647\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 143000\n",
      "  training_iteration: 143\n",
      "  trial_id: a44fc_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.23 GiB heap, 0.0/4.11 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-27_09-45-25<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_a44fc_00000</td><td>RUNNING </td><td>192.168.1.100:457</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         1706.54</td><td style=\"text-align: right;\">143000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "analysis = tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 1,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO C17 pretrained and frozen (AnnaCNN), MLP added (128)\",\n",
    "                      \"notes\": \"camera noop removed from actions\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
