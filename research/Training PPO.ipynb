{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ef090c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install ray torch torchvision tabulate tensorboard\n",
    "#!pip3 install 'ray[rllib]'\n",
    "#!pip3 install ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf02f4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 uninstall -y iglu && pip3 install git+https://github.com/iglu-contest/iglu.git\n",
    "#!pip3 install 'ray[rllib]'\n",
    "#!pip3 install wandb\n",
    "#!pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "from models import VisualEncoder\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        features_dim = 512\n",
    "        self.encoder = VisualEncoder(features_dim)\n",
    "        #self.encoder.load_state_dict(\n",
    "        #    torch.load(\"Visual Autoencoder weights and models/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        #)\n",
    "        self.action_head = nn.Linear(features_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(features_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.encoder.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        if self.use_cuda:\n",
    "            obs.cuda()\n",
    "            \n",
    "        features = self.encoder(obs)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C22']))\n",
    "    env = PovOnlyWrapper(env)\n",
    "    env = IgluActionWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-17 10:30:48,371\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-09-17 10:30:48,386\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[2m\u001b[36m(pid=1260)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1260)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO C22 not pretrained</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/52b7f_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/52b7f_00000</a><br/>\n",
       "                Run data is saved locally in <code>/src/wandb/run-20210917_103048-52b7f_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1260)\u001b[0m 2021-09-17 10:30:52,488\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=1260)\u001b[0m 2021-09-17 10:30:52,488\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=1256)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1256)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=1260)\u001b[0m 2021-09-17 10:30:59,280\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=1256)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1256)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=1256)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1256)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=1256)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=1256)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-32-01\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.0\n",
      "  episode_reward_mean: -1.0\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8840800762176513\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0052462765865284805\n",
      "          policy_loss: 0.10100602741456693\n",
      "          total_loss: 0.3638703312828309\n",
      "          vf_explained_var: -0.24777162075042725\n",
      "          vf_loss: 0.29065584903906305\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.64719101123596\n",
      "    ram_util_percent: 67.11460674157304\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04946863972819173\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 59.69329027981906\n",
      "    mean_inference_ms: 1.6730269947490255\n",
      "    mean_raw_obs_processing_ms: 0.17040807169515057\n",
      "  time_since_restore: 62.32992887496948\n",
      "  time_this_iter_s: 62.32992887496948\n",
      "  time_total_s: 62.32992887496948\n",
      "  timers:\n",
      "    learn_throughput: 1755.042\n",
      "    learn_time_ms: 569.787\n",
      "    load_throughput: 47176.838\n",
      "    load_time_ms: 21.197\n",
      "    sample_throughput: 16.199\n",
      "    sample_time_ms: 61732.169\n",
      "    update_time_ms: 2.655\n",
      "  timestamp: 1631874721\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         62.3299</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">      -1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-32-13\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.0\n",
      "  episode_reward_mean: -1.5\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.869246580865648\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011319737190402036\n",
      "          policy_loss: -0.03667076254884402\n",
      "          total_loss: 0.09371679541137483\n",
      "          vf_explained_var: -0.03631535544991493\n",
      "          vf_loss: 0.15681607491957644\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.747058823529414\n",
      "    ram_util_percent: 74.66470588235293\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04829283975271113\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 47.02796399824541\n",
      "    mean_inference_ms: 1.6413993096830497\n",
      "    mean_raw_obs_processing_ms: 0.1620158653949903\n",
      "  time_since_restore: 73.720782995224\n",
      "  time_this_iter_s: 11.390854120254517\n",
      "  time_total_s: 73.720782995224\n",
      "  timers:\n",
      "    learn_throughput: 1745.104\n",
      "    learn_time_ms: 573.032\n",
      "    load_throughput: 80451.601\n",
      "    load_time_ms: 12.43\n",
      "    sample_throughput: 27.572\n",
      "    sample_time_ms: 36268.143\n",
      "    update_time_ms: 2.632\n",
      "  timestamp: 1631874733\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         73.7208</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">    -1.5</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-32-24\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -1.0\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 3\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8521761867735123\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010228719906339508\n",
      "          policy_loss: -0.15951182527674568\n",
      "          total_loss: -0.1851644312342008\n",
      "          vf_explained_var: 0.005779870320111513\n",
      "          vf_loss: 0.000823413884306016\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.25\n",
      "    ram_util_percent: 72.85000000000001\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04800656512033919\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 39.99839207769883\n",
      "    mean_inference_ms: 1.6341102475892653\n",
      "    mean_raw_obs_processing_ms: 0.15665832157792284\n",
      "  time_since_restore: 85.29310607910156\n",
      "  time_this_iter_s: 11.572323083877563\n",
      "  time_total_s: 85.29310607910156\n",
      "  timers:\n",
      "    learn_throughput: 1699.82\n",
      "    learn_time_ms: 588.298\n",
      "    load_throughput: 107031.226\n",
      "    load_time_ms: 9.343\n",
      "    sample_throughput: 35.936\n",
      "    sample_time_ms: 27827.013\n",
      "    update_time_ms: 2.551\n",
      "  timestamp: 1631874744\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         85.2931</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">      -1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-32-35\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.75\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 4\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.848890770806207\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011207164392573866\n",
      "          policy_loss: -0.13432129621505737\n",
      "          total_loss: -0.16001200137866867\n",
      "          vf_explained_var: -0.06027766317129135\n",
      "          vf_loss: 0.0005567707907175645\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.3125\n",
      "    ram_util_percent: 72.10624999999999\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.047680581395287025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 35.40143335423302\n",
      "    mean_inference_ms: 1.6246596630544352\n",
      "    mean_raw_obs_processing_ms: 0.15272114396371325\n",
      "  time_since_restore: 96.21026134490967\n",
      "  time_this_iter_s: 10.917155265808105\n",
      "  time_total_s: 96.21026134490967\n",
      "  timers:\n",
      "    learn_throughput: 1720.126\n",
      "    learn_time_ms: 581.353\n",
      "    load_throughput: 128006.836\n",
      "    load_time_ms: 7.812\n",
      "    sample_throughput: 42.631\n",
      "    sample_time_ms: 23457.289\n",
      "    update_time_ms: 2.419\n",
      "  timestamp: 1631874755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         96.2103</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">   -0.75</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-32-44\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 5\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.842140785853068\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010743191855565807\n",
      "          policy_loss: -0.17705184523430134\n",
      "          total_loss: -0.20304970575703515\n",
      "          vf_explained_var: -0.377010315656662\n",
      "          vf_loss: 0.0002749076003561236\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.723076923076924\n",
      "    ram_util_percent: 71.80000000000001\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04732813329592376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 32.05609867904075\n",
      "    mean_inference_ms: 1.6129945925282847\n",
      "    mean_raw_obs_processing_ms: 0.14953870088572707\n",
      "  time_since_restore: 105.35331010818481\n",
      "  time_this_iter_s: 9.143048763275146\n",
      "  time_total_s: 105.35331010818481\n",
      "  timers:\n",
      "    learn_throughput: 1728.856\n",
      "    learn_time_ms: 578.417\n",
      "    load_throughput: 143329.347\n",
      "    load_time_ms: 6.977\n",
      "    sample_throughput: 48.83\n",
      "    sample_time_ms: 20479.353\n",
      "    update_time_ms: 2.339\n",
      "  timestamp: 1631874764\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         105.353</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">    -0.6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-32-54\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 6\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8443995979097156\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008190267362769493\n",
      "          policy_loss: -0.15388307869434356\n",
      "          total_loss: -0.18053890532917446\n",
      "          vf_explained_var: -0.48836278915405273\n",
      "          vf_loss: 0.000150114481706017\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.83846153846154\n",
      "    ram_util_percent: 71.56923076923077\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04702000227014988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.50071810783051\n",
      "    mean_inference_ms: 1.602523494208777\n",
      "    mean_raw_obs_processing_ms: 0.14817365858314646\n",
      "  time_since_restore: 114.66870975494385\n",
      "  time_this_iter_s: 9.315399646759033\n",
      "  time_total_s: 114.66870975494385\n",
      "  timers:\n",
      "    learn_throughput: 1713.395\n",
      "    learn_time_ms: 583.636\n",
      "    load_throughput: 156774.922\n",
      "    load_time_ms: 6.379\n",
      "    sample_throughput: 54.009\n",
      "    sample_time_ms: 18515.295\n",
      "    update_time_ms: 2.422\n",
      "  timestamp: 1631874774\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         114.669</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">    -0.5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-33-03\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.42857142857142855\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 7\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.847299607594808\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008962885047440219\n",
      "          policy_loss: -0.13267004895541404\n",
      "          total_loss: -0.15923279730810058\n",
      "          vf_explained_var: -0.30060911178588867\n",
      "          vf_loss: 0.0001176707019112655\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 7000\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 7000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.85714285714285\n",
      "    ram_util_percent: 72.07857142857144\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046775266494368764\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.479327448706027\n",
      "    mean_inference_ms: 1.5934189646453916\n",
      "    mean_raw_obs_processing_ms: 0.1468702251910152\n",
      "  time_since_restore: 124.11809015274048\n",
      "  time_this_iter_s: 9.44938039779663\n",
      "  time_total_s: 124.11809015274048\n",
      "  timers:\n",
      "    learn_throughput: 1700.288\n",
      "    learn_time_ms: 588.136\n",
      "    load_throughput: 167237.002\n",
      "    load_time_ms: 5.98\n",
      "    sample_throughput: 58.376\n",
      "    sample_time_ms: 17130.384\n",
      "    update_time_ms: 2.968\n",
      "  timestamp: 1631874783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         124.118</td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\">-0.428571</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-33-13\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.375\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 8\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.851990720960829\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00574726114064239\n",
      "          policy_loss: -0.09691248519553078\n",
      "          total_loss: -0.12419385214646657\n",
      "          vf_explained_var: -0.5367058515548706\n",
      "          vf_loss: 8.908802262036867e-05\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.23571428571428\n",
      "    ram_util_percent: 72.83571428571427\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04662054961107674\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.83793318041677\n",
      "    mean_inference_ms: 1.5869919450893364\n",
      "    mean_raw_obs_processing_ms: 0.14580408350893043\n",
      "  time_since_restore: 133.88343214988708\n",
      "  time_this_iter_s: 9.765341997146606\n",
      "  time_total_s: 133.88343214988708\n",
      "  timers:\n",
      "    learn_throughput: 1693.379\n",
      "    learn_time_ms: 590.535\n",
      "    load_throughput: 170884.827\n",
      "    load_time_ms: 5.852\n",
      "    sample_throughput: 61.987\n",
      "    sample_time_ms: 16132.525\n",
      "    update_time_ms: 2.849\n",
      "  timestamp: 1631874793\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         133.883</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">  -0.375</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-33-23\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3333333333333333\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 9\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.836118952433268\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010634361182345446\n",
      "          policy_loss: -0.101078635868099\n",
      "          total_loss: -0.1271533771107594\n",
      "          vf_explained_var: -0.5536850690841675\n",
      "          vf_loss: 0.00015957660200203666\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 9000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.53333333333334\n",
      "    ram_util_percent: 73.23333333333332\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04648473569373585\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.48437706685786\n",
      "    mean_inference_ms: 1.581743992497434\n",
      "    mean_raw_obs_processing_ms: 0.1448202293343172\n",
      "  time_since_restore: 144.38439965248108\n",
      "  time_this_iter_s: 10.500967502593994\n",
      "  time_total_s: 144.38439965248108\n",
      "  timers:\n",
      "    learn_throughput: 1680.499\n",
      "    learn_time_ms: 595.061\n",
      "    load_throughput: 177847.833\n",
      "    load_time_ms: 5.623\n",
      "    sample_throughput: 64.785\n",
      "    sample_time_ms: 15435.668\n",
      "    update_time_ms: 2.764\n",
      "  timestamp: 1631874803\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         144.384</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">-0.333333</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-33-33\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 10\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.822963325182597\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008276352381990865\n",
      "          policy_loss: -0.1356684045659171\n",
      "          total_loss: -0.16218406214482253\n",
      "          vf_explained_var: -0.39555680751800537\n",
      "          vf_loss: 5.870492162406056e-05\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.76428571428571\n",
      "    ram_util_percent: 73.2\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046365144734911025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.341182475920924\n",
      "    mean_inference_ms: 1.5772944527525543\n",
      "    mean_raw_obs_processing_ms: 0.1440893108436639\n",
      "  time_since_restore: 154.41019558906555\n",
      "  time_this_iter_s: 10.025795936584473\n",
      "  time_total_s: 154.41019558906555\n",
      "  timers:\n",
      "    learn_throughput: 1663.121\n",
      "    learn_time_ms: 601.279\n",
      "    load_throughput: 186145.462\n",
      "    load_time_ms: 5.372\n",
      "    sample_throughput: 67.441\n",
      "    sample_time_ms: 14827.818\n",
      "    update_time_ms: 2.885\n",
      "  timestamp: 1631874813\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">          154.41</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">    -0.3</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 11000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-33-45\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2727272727272727\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 11\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8284642722871567\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011423514236280501\n",
      "          policy_loss: -0.1200908087607887\n",
      "          total_loss: -0.14596797774235407\n",
      "          vf_explained_var: -0.5722293853759766\n",
      "          vf_loss: 0.0001227692067105737\n",
      "    num_agent_steps_sampled: 11000\n",
      "    num_agent_steps_trained: 11000\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 11000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.48750000000001\n",
      "    ram_util_percent: 73.21875\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04628170364687649\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.371575498697155\n",
      "    mean_inference_ms: 1.5737574535848253\n",
      "    mean_raw_obs_processing_ms: 0.14349540020465287\n",
      "  time_since_restore: 165.74800562858582\n",
      "  time_this_iter_s: 11.337810039520264\n",
      "  time_total_s: 165.74800562858582\n",
      "  timers:\n",
      "    learn_throughput: 1643.653\n",
      "    learn_time_ms: 608.401\n",
      "    load_throughput: 271713.407\n",
      "    load_time_ms: 3.68\n",
      "    sample_throughput: 102.849\n",
      "    sample_time_ms: 9723.013\n",
      "    update_time_ms: 3.144\n",
      "  timestamp: 1631874825\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         165.748</td><td style=\"text-align: right;\">11000</td><td style=\"text-align: right;\">-0.272727</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-33-55\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.5\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 12\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8150553676817154\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011145365266840804\n",
      "          policy_loss: -0.062455154872602885\n",
      "          total_loss: 0.03384215591682328\n",
      "          vf_explained_var: 0.2707104980945587\n",
      "          vf_loss: 0.12221878661463657\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.38666666666666\n",
      "    ram_util_percent: 73.25333333333333\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04620446654273395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.532556001195903\n",
      "    mean_inference_ms: 1.570353134855752\n",
      "    mean_raw_obs_processing_ms: 0.14296063905291403\n",
      "  time_since_restore: 176.23526859283447\n",
      "  time_this_iter_s: 10.487262964248657\n",
      "  time_total_s: 176.23526859283447\n",
      "  timers:\n",
      "    learn_throughput: 1640.009\n",
      "    learn_time_ms: 609.753\n",
      "    load_throughput: 275745.128\n",
      "    load_time_ms: 3.627\n",
      "    sample_throughput: 103.827\n",
      "    sample_time_ms: 9631.412\n",
      "    update_time_ms: 3.098\n",
      "  timestamp: 1631874835\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         176.235</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">    -0.5</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 13000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-34-06\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.46153846153846156\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 13\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.817117346657647\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014412033919385382\n",
      "          policy_loss: -0.12370517436001036\n",
      "          total_loss: -0.14470642333229383\n",
      "          vf_explained_var: 0.47035571932792664\n",
      "          vf_loss: 0.004287515378867586\n",
      "    num_agent_steps_sampled: 13000\n",
      "    num_agent_steps_trained: 13000\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 13000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.4625\n",
      "    ram_util_percent: 73.0375\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046134342564258995\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.801232289334134\n",
      "    mean_inference_ms: 1.567251768925946\n",
      "    mean_raw_obs_processing_ms: 0.14242607015429173\n",
      "  time_since_restore: 187.2164306640625\n",
      "  time_this_iter_s: 10.981162071228027\n",
      "  time_total_s: 187.2164306640625\n",
      "  timers:\n",
      "    learn_throughput: 1648.776\n",
      "    learn_time_ms: 606.51\n",
      "    load_throughput: 273972.775\n",
      "    load_time_ms: 3.65\n",
      "    sample_throughput: 104.433\n",
      "    sample_time_ms: 9575.533\n",
      "    update_time_ms: 3.066\n",
      "  timestamp: 1631874846\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         187.216</td><td style=\"text-align: right;\">13000</td><td style=\"text-align: right;\">-0.461538</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 14000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-34-18\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.42857142857142855\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 14\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7930385616090563\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013853290038673935\n",
      "          policy_loss: 0.006681902623838849\n",
      "          total_loss: -0.01710263482398457\n",
      "          vf_explained_var: -0.07106836140155792\n",
      "          vf_loss: 0.001375188631290156\n",
      "    num_agent_steps_sampled: 14000\n",
      "    num_agent_steps_trained: 14000\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.48125\n",
      "    ram_util_percent: 73.04374999999999\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046070834206943904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.158933130048286\n",
      "    mean_inference_ms: 1.5645886939244362\n",
      "    mean_raw_obs_processing_ms: 0.1419100941771723\n",
      "  time_since_restore: 198.51947331428528\n",
      "  time_this_iter_s: 11.303042650222778\n",
      "  time_total_s: 198.51947331428528\n",
      "  timers:\n",
      "    learn_throughput: 1646.416\n",
      "    learn_time_ms: 607.38\n",
      "    load_throughput: 274334.75\n",
      "    load_time_ms: 3.645\n",
      "    sample_throughput: 104.023\n",
      "    sample_time_ms: 9613.242\n",
      "    update_time_ms: 3.063\n",
      "  timestamp: 1631874858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         198.519</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\">-0.428571</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-34-28\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.4\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 15\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7728970766067507\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01076944931168558\n",
      "          policy_loss: -0.09502497704492675\n",
      "          total_loss: -0.11989402166671223\n",
      "          vf_explained_var: -0.7383517622947693\n",
      "          vf_loss: 0.0007060359705873352\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_agent_steps_trained: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.42666666666666\n",
      "    ram_util_percent: 72.99333333333334\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.046004505813828596\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.587386324322686\n",
      "    mean_inference_ms: 1.5618680259262092\n",
      "    mean_raw_obs_processing_ms: 0.1414164095262866\n",
      "  time_since_restore: 209.1883430480957\n",
      "  time_this_iter_s: 10.668869733810425\n",
      "  time_total_s: 209.1883430480957\n",
      "  timers:\n",
      "    learn_throughput: 1646.267\n",
      "    learn_time_ms: 607.435\n",
      "    load_throughput: 278133.181\n",
      "    load_time_ms: 3.595\n",
      "    sample_throughput: 102.398\n",
      "    sample_time_ms: 9765.828\n",
      "    update_time_ms: 3.065\n",
      "  timestamp: 1631874868\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         209.188</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">    -0.4</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-34-39\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.375\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 16\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7362308316760595\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012558819948976898\n",
      "          policy_loss: -0.13496170205374558\n",
      "          total_loss: -0.15920395793186293\n",
      "          vf_explained_var: -0.5044809579849243\n",
      "          vf_loss: 0.0006082876795618277\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.875\n",
      "    ram_util_percent: 72.875\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045957087619239376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.07550466215285\n",
      "    mean_inference_ms: 1.5592137521537448\n",
      "    mean_raw_obs_processing_ms: 0.14094540791491883\n",
      "  time_since_restore: 220.00272822380066\n",
      "  time_this_iter_s: 10.814385175704956\n",
      "  time_total_s: 220.00272822380066\n",
      "  timers:\n",
      "    learn_throughput: 1654.99\n",
      "    learn_time_ms: 604.233\n",
      "    load_throughput: 275605.612\n",
      "    load_time_ms: 3.628\n",
      "    sample_throughput: 100.816\n",
      "    sample_time_ms: 9919.066\n",
      "    update_time_ms: 2.983\n",
      "  timestamp: 1631874879\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         220.003</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">  -0.375</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 17000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-34-50\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.35294117647058826\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 17\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.767164585325453\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01052333860336061\n",
      "          policy_loss: -0.15111948980225456\n",
      "          total_loss: -0.17646339366005526\n",
      "          vf_explained_var: -0.45454075932502747\n",
      "          vf_loss: 0.00022307237320799483\n",
      "    num_agent_steps_sampled: 17000\n",
      "    num_agent_steps_trained: 17000\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 17000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.02666666666667\n",
      "    ram_util_percent: 72.85333333333331\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04590810191979841\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.61416141263672\n",
      "    mean_inference_ms: 1.5566760033610059\n",
      "    mean_raw_obs_processing_ms: 0.14049975533693987\n",
      "  time_since_restore: 230.8531517982483\n",
      "  time_this_iter_s: 10.850423574447632\n",
      "  time_total_s: 230.8531517982483\n",
      "  timers:\n",
      "    learn_throughput: 1664.161\n",
      "    learn_time_ms: 600.903\n",
      "    load_throughput: 276059.104\n",
      "    load_time_ms: 3.622\n",
      "    sample_throughput: 99.374\n",
      "    sample_time_ms: 10062.964\n",
      "    update_time_ms: 2.569\n",
      "  timestamp: 1631874890\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         230.853</td><td style=\"text-align: right;\">17000</td><td style=\"text-align: right;\">-0.352941</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-35-01\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3333333333333333\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 18\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7476710875829062\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010077119616005594\n",
      "          policy_loss: -0.14747609585109683\n",
      "          total_loss: -0.17274808742933803\n",
      "          vf_explained_var: -0.9603556990623474\n",
      "          vf_loss: 0.0001892943987008443\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.8875\n",
      "    ram_util_percent: 72.98125\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04585856276899083\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.19604655848696\n",
      "    mean_inference_ms: 1.5542395676935723\n",
      "    mean_raw_obs_processing_ms: 0.1400714935281211\n",
      "  time_since_restore: 241.71149039268494\n",
      "  time_this_iter_s: 10.858338594436646\n",
      "  time_total_s: 241.71149039268494\n",
      "  timers:\n",
      "    learn_throughput: 1676.029\n",
      "    learn_time_ms: 596.648\n",
      "    load_throughput: 288919.626\n",
      "    load_time_ms: 3.461\n",
      "    sample_throughput: 98.263\n",
      "    sample_time_ms: 10176.729\n",
      "    update_time_ms: 2.568\n",
      "  timestamp: 1631874901\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         241.711</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">-0.333333</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 19000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-35-12\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3157894736842105\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 19\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.746685838699341\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006519233985550018\n",
      "          policy_loss: 0.012845331006166008\n",
      "          total_loss: -0.013170587038621307\n",
      "          vf_explained_var: -0.62337327003479\n",
      "          vf_loss: 0.000147093769596217\n",
      "    num_agent_steps_sampled: 19000\n",
      "    num_agent_steps_trained: 19000\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 19000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.379999999999995\n",
      "    ram_util_percent: 72.98\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04581180701620766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.814993360165747\n",
      "    mean_inference_ms: 1.5519233359213365\n",
      "    mean_raw_obs_processing_ms: 0.13970680389539786\n",
      "  time_since_restore: 252.54999136924744\n",
      "  time_this_iter_s: 10.8385009765625\n",
      "  time_total_s: 252.54999136924744\n",
      "  timers:\n",
      "    learn_throughput: 1693.773\n",
      "    learn_time_ms: 590.398\n",
      "    load_throughput: 287763.988\n",
      "    load_time_ms: 3.475\n",
      "    sample_throughput: 97.879\n",
      "    sample_time_ms: 10216.687\n",
      "    update_time_ms: 2.562\n",
      "  timestamp: 1631874912\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 19\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">          252.55</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">-0.315789</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-35-23\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 20\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7546134736802843\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01031397364018846\n",
      "          policy_loss: -0.00533918912212054\n",
      "          total_loss: -0.030678332555625173\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0001441958826035261\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.30625\n",
      "    ram_util_percent: 72.975\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04576628263475506\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.46639131328917\n",
      "    mean_inference_ms: 1.5497436140289231\n",
      "    mean_raw_obs_processing_ms: 0.13935687888574028\n",
      "  time_since_restore: 263.48808765411377\n",
      "  time_this_iter_s: 10.938096284866333\n",
      "  time_total_s: 263.48808765411377\n",
      "  timers:\n",
      "    learn_throughput: 1721.323\n",
      "    learn_time_ms: 580.949\n",
      "    load_throughput: 279448.871\n",
      "    load_time_ms: 3.578\n",
      "    sample_throughput: 96.923\n",
      "    sample_time_ms: 10317.494\n",
      "    update_time_ms: 2.461\n",
      "  timestamp: 1631874923\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 20\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         263.488</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">    -0.3</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-35-34\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3333333333333333\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 21\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7693344460593328\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005695995630214319\n",
      "          policy_loss: 0.06475834269076586\n",
      "          total_loss: 0.0701124481856823\n",
      "          vf_explained_var: -0.43163618445396423\n",
      "          vf_loss: 0.031908249699821076\n",
      "    num_agent_steps_sampled: 21000\n",
      "    num_agent_steps_trained: 21000\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.849999999999994\n",
      "    ram_util_percent: 72.88125\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045721221665254975\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.146196651290772\n",
      "    mean_inference_ms: 1.547667810568752\n",
      "    mean_raw_obs_processing_ms: 0.13903377713517087\n",
      "  time_since_restore: 274.4643850326538\n",
      "  time_this_iter_s: 10.976297378540039\n",
      "  time_total_s: 274.4643850326538\n",
      "  timers:\n",
      "    learn_throughput: 1743.951\n",
      "    learn_time_ms: 573.41\n",
      "    load_throughput: 286852.781\n",
      "    load_time_ms: 3.486\n",
      "    sample_throughput: 97.189\n",
      "    sample_time_ms: 10289.28\n",
      "    update_time_ms: 2.141\n",
      "  timestamp: 1631874934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         274.464</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\">-0.333333</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 22000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-35-45\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.3181818181818182\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 22\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7132312297821044\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01120488199058634\n",
      "          policy_loss: -0.14683635292781724\n",
      "          total_loss: -0.17121434915396902\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0005133406058626457\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 22000\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.10000000000001\n",
      "    ram_util_percent: 72.80666666666666\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04568955124932869\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.85075661384892\n",
      "    mean_inference_ms: 1.5456912587998217\n",
      "    mean_raw_obs_processing_ms: 0.13873008135542578\n",
      "  time_since_restore: 285.3461971282959\n",
      "  time_this_iter_s: 10.88181209564209\n",
      "  time_total_s: 285.3461971282959\n",
      "  timers:\n",
      "    learn_throughput: 1750.629\n",
      "    learn_time_ms: 571.223\n",
      "    load_throughput: 287035.346\n",
      "    load_time_ms: 3.484\n",
      "    sample_throughput: 96.796\n",
      "    sample_time_ms: 10330.968\n",
      "    update_time_ms: 2.127\n",
      "  timestamp: 1631874945\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 22\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         285.346</td><td style=\"text-align: right;\">22000</td><td style=\"text-align: right;\">-0.318182</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 23000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-35-56\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.30434782608695654\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 23\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7076077567206487\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02212218843692278\n",
      "          policy_loss: 0.2982171153028806\n",
      "          total_loss: 0.27605080786678526\n",
      "          vf_explained_var: -0.38164883852005005\n",
      "          vf_loss: 0.00048533261975131206\n",
      "    num_agent_steps_sampled: 23000\n",
      "    num_agent_steps_trained: 23000\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 23000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.475\n",
      "    ram_util_percent: 72.7\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045658163573242386\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.577204950146427\n",
      "    mean_inference_ms: 1.54385994941724\n",
      "    mean_raw_obs_processing_ms: 0.13845075076207586\n",
      "  time_since_restore: 296.30650901794434\n",
      "  time_this_iter_s: 10.960311889648438\n",
      "  time_total_s: 296.30650901794434\n",
      "  timers:\n",
      "    learn_throughput: 1741.925\n",
      "    learn_time_ms: 574.077\n",
      "    load_throughput: 288772.427\n",
      "    load_time_ms: 3.463\n",
      "    sample_throughput: 96.843\n",
      "    sample_time_ms: 10325.977\n",
      "    update_time_ms: 2.122\n",
      "  timestamp: 1631874956\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 23\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         296.307</td><td style=\"text-align: right;\">23000</td><td style=\"text-align: right;\">-0.304348</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-36-06\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2916666666666667\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 24\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6993717167112563\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010183728406238539\n",
      "          policy_loss: 0.07369025266832775\n",
      "          total_loss: 0.055539330343405406\n",
      "          vf_explained_var: -0.42849358916282654\n",
      "          vf_loss: 0.005787677632583331\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.26875\n",
      "    ram_util_percent: 72.58125\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04562788578338404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.322909980832545\n",
      "    mean_inference_ms: 1.5421404545915236\n",
      "    mean_raw_obs_processing_ms: 0.13819969999745332\n",
      "  time_since_restore: 307.11032128334045\n",
      "  time_this_iter_s: 10.803812265396118\n",
      "  time_total_s: 307.11032128334045\n",
      "  timers:\n",
      "    learn_throughput: 1737.237\n",
      "    learn_time_ms: 575.627\n",
      "    load_throughput: 285881.062\n",
      "    load_time_ms: 3.498\n",
      "    sample_throughput: 97.328\n",
      "    sample_time_ms: 10274.494\n",
      "    update_time_ms: 2.128\n",
      "  timestamp: 1631874966\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">          307.11</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">-0.291667</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 25000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-36-17\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.28\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 25\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7437744405534534\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008650780012404349\n",
      "          policy_loss: -0.11346637457609177\n",
      "          total_loss: -0.13751179915335443\n",
      "          vf_explained_var: -0.7788160443305969\n",
      "          vf_loss: 0.0007970881264352809\n",
      "    num_agent_steps_sampled: 25000\n",
      "    num_agent_steps_trained: 25000\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 25000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.22000000000001\n",
      "    ram_util_percent: 72.53333333333333\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04560164668495923\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.086021585407067\n",
      "    mean_inference_ms: 1.5405316348434936\n",
      "    mean_raw_obs_processing_ms: 0.13795775086117643\n",
      "  time_since_restore: 318.02914452552795\n",
      "  time_this_iter_s: 10.9188232421875\n",
      "  time_total_s: 318.02914452552795\n",
      "  timers:\n",
      "    learn_throughput: 1732.372\n",
      "    learn_time_ms: 577.243\n",
      "    load_throughput: 285484.11\n",
      "    load_time_ms: 3.503\n",
      "    sample_throughput: 97.108\n",
      "    sample_time_ms: 10297.771\n",
      "    update_time_ms: 2.13\n",
      "  timestamp: 1631874977\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 25\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         318.029</td><td style=\"text-align: right;\">25000</td><td style=\"text-align: right;\">   -0.28</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 26000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-36-28\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.2692307692307692\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 26\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7222952710257635\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00856828213071547\n",
      "          policy_loss: -0.12048743251297209\n",
      "          total_loss: -0.14495802943905195\n",
      "          vf_explained_var: -0.20939777791500092\n",
      "          vf_loss: 0.00018186964589403943\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 26000\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.97333333333333\n",
      "    ram_util_percent: 72.38666666666667\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04557790037995364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.86446997287146\n",
      "    mean_inference_ms: 1.5389865675030483\n",
      "    mean_raw_obs_processing_ms: 0.13772256983963527\n",
      "  time_since_restore: 328.77016615867615\n",
      "  time_this_iter_s: 10.741021633148193\n",
      "  time_total_s: 328.77016615867615\n",
      "  timers:\n",
      "    learn_throughput: 1723.541\n",
      "    learn_time_ms: 580.201\n",
      "    load_throughput: 286742.962\n",
      "    load_time_ms: 3.487\n",
      "    sample_throughput: 97.211\n",
      "    sample_time_ms: 10286.948\n",
      "    update_time_ms: 2.568\n",
      "  timestamp: 1631874988\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 26\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">          328.77</td><td style=\"text-align: right;\">26000</td><td style=\"text-align: right;\">-0.269231</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-36-39\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.2222222222222222\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 27\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7424676126903957\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004435902934521886\n",
      "          policy_loss: -0.005353554400304953\n",
      "          total_loss: 0.051765538503726324\n",
      "          vf_explained_var: -0.32179194688796997\n",
      "          vf_loss: 0.08321300009394893\n",
      "    num_agent_steps_sampled: 27000\n",
      "    num_agent_steps_trained: 27000\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 27000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.129411764705885\n",
      "    ram_util_percent: 72.41764705882353\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04555403665826656\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.657484205413372\n",
      "    mean_inference_ms: 1.5375156137192638\n",
      "    mean_raw_obs_processing_ms: 0.13751430128320552\n",
      "  time_since_restore: 340.00912833213806\n",
      "  time_this_iter_s: 11.238962173461914\n",
      "  time_total_s: 340.00912833213806\n",
      "  timers:\n",
      "    learn_throughput: 1727.881\n",
      "    learn_time_ms: 578.743\n",
      "    load_throughput: 289529.91\n",
      "    load_time_ms: 3.454\n",
      "    sample_throughput: 96.831\n",
      "    sample_time_ms: 10327.248\n",
      "    update_time_ms: 2.561\n",
      "  timestamp: 1631874999\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 27\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         340.009</td><td style=\"text-align: right;\">27000</td><td style=\"text-align: right;\">-0.222222</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-36-50\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.21428571428571427\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 28\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.707646359337701\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016211278401088127\n",
      "          policy_loss: -0.11730006006028917\n",
      "          total_loss: -0.14067160561680794\n",
      "          vf_explained_var: -0.1460801512002945\n",
      "          vf_loss: 0.0012732252446261958\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.01333333333333\n",
      "    ram_util_percent: 72.20000000000002\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045529417335316465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.463023390895188\n",
      "    mean_inference_ms: 1.5360895320827415\n",
      "    mean_raw_obs_processing_ms: 0.13731261831143654\n",
      "  time_since_restore: 350.74098920822144\n",
      "  time_this_iter_s: 10.731860876083374\n",
      "  time_total_s: 350.74098920822144\n",
      "  timers:\n",
      "    learn_throughput: 1726.446\n",
      "    learn_time_ms: 579.224\n",
      "    load_throughput: 291214.486\n",
      "    load_time_ms: 3.434\n",
      "    sample_throughput: 96.954\n",
      "    sample_time_ms: 10314.126\n",
      "    update_time_ms: 2.58\n",
      "  timestamp: 1631875010\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 28\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         350.741</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">-0.214286</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 29000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-37-01\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.20689655172413793\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 29\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.647217321395874\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012527423178069224\n",
      "          policy_loss: -0.03499750913017326\n",
      "          total_loss: -0.05901114799910122\n",
      "          vf_explained_var: -0.32465294003486633\n",
      "          vf_loss: 0.0005794191778275288\n",
      "    num_agent_steps_sampled: 29000\n",
      "    num_agent_steps_trained: 29000\n",
      "    num_steps_sampled: 29000\n",
      "    num_steps_trained: 29000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.5\n",
      "    ram_util_percent: 72.1\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04550429345777201\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.279906532146718\n",
      "    mean_inference_ms: 1.5347114367494668\n",
      "    mean_raw_obs_processing_ms: 0.13712289753025492\n",
      "  time_since_restore: 361.45654249191284\n",
      "  time_this_iter_s: 10.715553283691406\n",
      "  time_total_s: 361.45654249191284\n",
      "  timers:\n",
      "    learn_throughput: 1726.003\n",
      "    learn_time_ms: 579.373\n",
      "    load_throughput: 294136.903\n",
      "    load_time_ms: 3.4\n",
      "    sample_throughput: 97.071\n",
      "    sample_time_ms: 10301.721\n",
      "    update_time_ms: 2.586\n",
      "  timestamp: 1631875021\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 29\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         361.457</td><td style=\"text-align: right;\">29000</td><td style=\"text-align: right;\">-0.206897</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">              1000</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1256)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1256)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-37-29\n",
      "  done: false\n",
      "  episode_len_mean: 996.0666666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.2\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 30\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.62101477517022\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009407828993379014\n",
      "          policy_loss: -0.055977444267935224\n",
      "          total_loss: -0.08014053271876441\n",
      "          vf_explained_var: -0.5508862137794495\n",
      "          vf_loss: 0.0006358831483844875\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.55365853658536\n",
      "    ram_util_percent: 70.63658536585366\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045479148272286406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 15.1073267714268\n",
      "    mean_inference_ms: 1.5333937023551072\n",
      "    mean_raw_obs_processing_ms: 0.156322901052824\n",
      "  time_since_restore: 389.8344130516052\n",
      "  time_this_iter_s: 28.377870559692383\n",
      "  time_total_s: 389.8344130516052\n",
      "  timers:\n",
      "    learn_throughput: 1715.803\n",
      "    learn_time_ms: 582.817\n",
      "    load_throughput: 241594.848\n",
      "    load_time_ms: 4.139\n",
      "    sample_throughput: 83.046\n",
      "    sample_time_ms: 12041.587\n",
      "    update_time_ms: 2.488\n",
      "  timestamp: 1631875049\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 30\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         389.834</td><td style=\"text-align: right;\">30000</td><td style=\"text-align: right;\">    -0.2</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">           996.067</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 31000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-37-42\n",
      "  done: false\n",
      "  episode_len_mean: 996.1935483870968\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.22580645161290322\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 31\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6940680742263794\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010137520556434697\n",
      "          policy_loss: -0.10014041024777624\n",
      "          total_loss: -0.044534631156259115\n",
      "          vf_explained_var: 0.4268380105495453\n",
      "          vf_loss: 0.08102583082185852\n",
      "    num_agent_steps_sampled: 31000\n",
      "    num_agent_steps_trained: 31000\n",
      "    num_steps_sampled: 31000\n",
      "    num_steps_trained: 31000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.74444444444445\n",
      "    ram_util_percent: 71.34444444444445\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04545506447263996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.946060097293818\n",
      "    mean_inference_ms: 1.532174044338203\n",
      "    mean_raw_obs_processing_ms: 0.17368607641376896\n",
      "  time_since_restore: 402.41508388519287\n",
      "  time_this_iter_s: 12.580670833587646\n",
      "  time_total_s: 402.41508388519287\n",
      "  timers:\n",
      "    learn_throughput: 1713.3\n",
      "    learn_time_ms: 583.669\n",
      "    load_throughput: 242631.85\n",
      "    load_time_ms: 4.121\n",
      "    sample_throughput: 81.959\n",
      "    sample_time_ms: 12201.188\n",
      "    update_time_ms: 2.5\n",
      "  timestamp: 1631875062\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 31\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         402.415</td><td style=\"text-align: right;\">31000</td><td style=\"text-align: right;\">-0.225806</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">           996.194</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-37-53\n",
      "  done: false\n",
      "  episode_len_mean: 996.3125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.21875\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 32\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5866889423794217\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014417272954489373\n",
      "          policy_loss: -0.001454330732425054\n",
      "          total_loss: -0.022088467743661667\n",
      "          vf_explained_var: 0.39969635009765625\n",
      "          vf_loss: 0.0030701598876880275\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.96666666666666\n",
      "    ram_util_percent: 72.97333333333336\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04543213064170368\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.793423927453713\n",
      "    mean_inference_ms: 1.5310150338888673\n",
      "    mean_raw_obs_processing_ms: 0.1894131137679278\n",
      "  time_since_restore: 413.3136308193207\n",
      "  time_this_iter_s: 10.898546934127808\n",
      "  time_total_s: 413.3136308193207\n",
      "  timers:\n",
      "    learn_throughput: 1710.883\n",
      "    learn_time_ms: 584.494\n",
      "    load_throughput: 240613.595\n",
      "    load_time_ms: 4.156\n",
      "    sample_throughput: 81.953\n",
      "    sample_time_ms: 12202.043\n",
      "    update_time_ms: 2.51\n",
      "  timestamp: 1631875073\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 32\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         413.314</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">-0.21875</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">           996.312</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 33000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-38-03\n",
      "  done: false\n",
      "  episode_len_mean: 996.4242424242424\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: -0.18181818181818182\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 33\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.629647043016222\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010482558475655858\n",
      "          policy_loss: -0.05330499377515581\n",
      "          total_loss: 0.031146783961190118\n",
      "          vf_explained_var: 0.3829212486743927\n",
      "          vf_loss: 0.10917586208217674\n",
      "    num_agent_steps_sampled: 33000\n",
      "    num_agent_steps_trained: 33000\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 33000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.98666666666666\n",
      "    ram_util_percent: 73.24666666666666\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045409235472509475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.648249298845208\n",
      "    mean_inference_ms: 1.5298984321413969\n",
      "    mean_raw_obs_processing_ms: 0.2037046383180303\n",
      "  time_since_restore: 423.72021436691284\n",
      "  time_this_iter_s: 10.406583547592163\n",
      "  time_total_s: 423.72021436691284\n",
      "  timers:\n",
      "    learn_throughput: 1718.767\n",
      "    learn_time_ms: 581.813\n",
      "    load_throughput: 237325.668\n",
      "    load_time_ms: 4.214\n",
      "    sample_throughput: 82.309\n",
      "    sample_time_ms: 12149.37\n",
      "    update_time_ms: 2.519\n",
      "  timestamp: 1631875083\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 33\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">          423.72</td><td style=\"text-align: right;\">33000</td><td style=\"text-align: right;\">-0.181818</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">           996.424</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 34000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-38-13\n",
      "  done: false\n",
      "  episode_len_mean: 996.5294117647059\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.11764705882352941\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 34\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.641906155480279\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011535019437723362\n",
      "          policy_loss: -0.08092019243372811\n",
      "          total_loss: -0.055771290593677095\n",
      "          vf_explained_var: 0.46582382917404175\n",
      "          vf_loss: 0.04983770885608262\n",
      "    num_agent_steps_sampled: 34000\n",
      "    num_agent_steps_trained: 34000\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 34000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.6\n",
      "    ram_util_percent: 73.08666666666667\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045386391859621227\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.509786460457914\n",
      "    mean_inference_ms: 1.5288176430638318\n",
      "    mean_raw_obs_processing_ms: 0.2166998183434841\n",
      "  time_since_restore: 433.85910415649414\n",
      "  time_this_iter_s: 10.138889789581299\n",
      "  time_total_s: 433.85910415649414\n",
      "  timers:\n",
      "    learn_throughput: 1722.845\n",
      "    learn_time_ms: 580.435\n",
      "    load_throughput: 234710.718\n",
      "    load_time_ms: 4.261\n",
      "    sample_throughput: 82.753\n",
      "    sample_time_ms: 12084.107\n",
      "    update_time_ms: 2.57\n",
      "  timestamp: 1631875093\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 34\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         433.859</td><td style=\"text-align: right;\">34000</td><td style=\"text-align: right;\">-0.117647</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">           996.529</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 35000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-38-24\n",
      "  done: false\n",
      "  episode_len_mean: 996.6285714285714\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.2\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 35\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.629222053951687\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012524586599379096\n",
      "          policy_loss: -0.011046908878617817\n",
      "          total_loss: 0.034887204443415004\n",
      "          vf_explained_var: 0.17581544816493988\n",
      "          vf_loss: 0.07034764254931361\n",
      "    num_agent_steps_sampled: 35000\n",
      "    num_agent_steps_trained: 35000\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 35000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.03999999999999\n",
      "    ram_util_percent: 72.92\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045363888759374214\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.377913140087392\n",
      "    mean_inference_ms: 1.5277832974509322\n",
      "    mean_raw_obs_processing_ms: 0.22853517636688342\n",
      "  time_since_restore: 444.4450330734253\n",
      "  time_this_iter_s: 10.585928916931152\n",
      "  time_total_s: 444.4450330734253\n",
      "  timers:\n",
      "    learn_throughput: 1725.258\n",
      "    learn_time_ms: 579.623\n",
      "    load_throughput: 232638.206\n",
      "    load_time_ms: 4.299\n",
      "    sample_throughput: 82.976\n",
      "    sample_time_ms: 12051.666\n",
      "    update_time_ms: 2.571\n",
      "  timestamp: 1631875104\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 35\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         444.445</td><td style=\"text-align: right;\">35000</td><td style=\"text-align: right;\">    -0.2</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">           996.629</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-38-35\n",
      "  done: false\n",
      "  episode_len_mean: 996.7222222222222\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.3055555555555556\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 36\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5996163421207004\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011854038846491033\n",
      "          policy_loss: -0.04438696660929256\n",
      "          total_loss: 0.009155305557780796\n",
      "          vf_explained_var: -0.01875266246497631\n",
      "          vf_loss: 0.07776032790231208\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.84000000000001\n",
      "    ram_util_percent: 72.85333333333331\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045341761312869294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.252100330556642\n",
      "    mean_inference_ms: 1.5267836478113954\n",
      "    mean_raw_obs_processing_ms: 0.2393307884323239\n",
      "  time_since_restore: 454.9510278701782\n",
      "  time_this_iter_s: 10.50599479675293\n",
      "  time_total_s: 454.9510278701782\n",
      "  timers:\n",
      "    learn_throughput: 1736.318\n",
      "    learn_time_ms: 575.931\n",
      "    load_throughput: 234512.558\n",
      "    load_time_ms: 4.264\n",
      "    sample_throughput: 83.109\n",
      "    sample_time_ms: 12032.415\n",
      "    update_time_ms: 2.186\n",
      "  timestamp: 1631875115\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 36\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         454.951</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">-0.305556</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">           996.722</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 37000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-38-45\n",
      "  done: false\n",
      "  episode_len_mean: 996.8108108108108\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.35135135135135137\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 37\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5639727883868746\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014034765491773439\n",
      "          policy_loss: -0.10970805355658134\n",
      "          total_loss: -0.08114768324626817\n",
      "          vf_explained_var: 0.4222976565361023\n",
      "          vf_loss: 0.052094885157162533\n",
      "    num_agent_steps_sampled: 37000\n",
      "    num_agent_steps_trained: 37000\n",
      "    num_steps_sampled: 37000\n",
      "    num_steps_trained: 37000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.78\n",
      "    ram_util_percent: 72.86666666666665\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04532008652927249\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.132014122447075\n",
      "    mean_inference_ms: 1.5258215317275095\n",
      "    mean_raw_obs_processing_ms: 0.24918901165630764\n",
      "  time_since_restore: 465.5847373008728\n",
      "  time_this_iter_s: 10.63370943069458\n",
      "  time_total_s: 465.5847373008728\n",
      "  timers:\n",
      "    learn_throughput: 1736.942\n",
      "    learn_time_ms: 575.725\n",
      "    load_throughput: 233729.765\n",
      "    load_time_ms: 4.278\n",
      "    sample_throughput: 83.528\n",
      "    sample_time_ms: 11972.031\n",
      "    update_time_ms: 2.27\n",
      "  timestamp: 1631875125\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 37\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         465.585</td><td style=\"text-align: right;\">37000</td><td style=\"text-align: right;\">-0.351351</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">           996.811</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 38000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-38-56\n",
      "  done: false\n",
      "  episode_len_mean: 996.8947368421053\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.5263157894736842\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 38\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5384414010577734\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010245953513802508\n",
      "          policy_loss: -0.01582064438197348\n",
      "          total_loss: 0.04234045859840181\n",
      "          vf_explained_var: 0.5629757642745972\n",
      "          vf_loss: 0.08200862212106586\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 38000\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.95333333333333\n",
      "    ram_util_percent: 72.99333333333334\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04529844063126538\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 14.017030711854728\n",
      "    mean_inference_ms: 1.524880374353623\n",
      "    mean_raw_obs_processing_ms: 0.2582047706093582\n",
      "  time_since_restore: 475.88613057136536\n",
      "  time_this_iter_s: 10.301393270492554\n",
      "  time_total_s: 475.88613057136536\n",
      "  timers:\n",
      "    learn_throughput: 1735.419\n",
      "    learn_time_ms: 576.23\n",
      "    load_throughput: 233576.174\n",
      "    load_time_ms: 4.281\n",
      "    sample_throughput: 83.833\n",
      "    sample_time_ms: 11928.458\n",
      "    update_time_ms: 2.269\n",
      "  timestamp: 1631875136\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 38\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         475.886</td><td style=\"text-align: right;\">38000</td><td style=\"text-align: right;\">-0.526316</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.895</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 39000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-39-06\n",
      "  done: false\n",
      "  episode_len_mean: 996.974358974359\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.5384615384615384\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 39\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.554158565733168\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011162503995865613\n",
      "          policy_loss: -0.14033166848950915\n",
      "          total_loss: -0.07968346464137237\n",
      "          vf_explained_var: 0.2394406497478485\n",
      "          vf_loss: 0.08451541273647713\n",
      "    num_agent_steps_sampled: 39000\n",
      "    num_agent_steps_trained: 39000\n",
      "    num_steps_sampled: 39000\n",
      "    num_steps_trained: 39000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.38\n",
      "    ram_util_percent: 73.15333333333335\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04527696363472077\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.90679578306119\n",
      "    mean_inference_ms: 1.5239638143053358\n",
      "    mean_raw_obs_processing_ms: 0.26646795469961926\n",
      "  time_since_restore: 486.16335797309875\n",
      "  time_this_iter_s: 10.277227401733398\n",
      "  time_total_s: 486.16335797309875\n",
      "  timers:\n",
      "    learn_throughput: 1734.677\n",
      "    learn_time_ms: 576.476\n",
      "    load_throughput: 235755.583\n",
      "    load_time_ms: 4.242\n",
      "    sample_throughput: 84.144\n",
      "    sample_time_ms: 11884.408\n",
      "    update_time_ms: 2.262\n",
      "  timestamp: 1631875146\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 39\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         486.163</td><td style=\"text-align: right;\">39000</td><td style=\"text-align: right;\">-0.538462</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.974</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-39-16\n",
      "  done: false\n",
      "  episode_len_mean: 997.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.525\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 40\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.564166529973348\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01078122720697686\n",
      "          policy_loss: -0.0278818827536371\n",
      "          total_loss: 0.045416447851392955\n",
      "          vf_explained_var: 0.17903614044189453\n",
      "          vf_loss: 0.09732281387680107\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.46428571428571\n",
      "    ram_util_percent: 73.20714285714288\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04525583016016523\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.800897276909879\n",
      "    mean_inference_ms: 1.523066429527792\n",
      "    mean_raw_obs_processing_ms: 0.274039383229051\n",
      "  time_since_restore: 496.26171469688416\n",
      "  time_this_iter_s: 10.0983567237854\n",
      "  time_total_s: 496.26171469688416\n",
      "  timers:\n",
      "    learn_throughput: 1734.947\n",
      "    learn_time_ms: 576.387\n",
      "    load_throughput: 294363.977\n",
      "    load_time_ms: 3.397\n",
      "    sample_throughput: 99.43\n",
      "    sample_time_ms: 10057.34\n",
      "    update_time_ms: 2.268\n",
      "  timestamp: 1631875156\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 40\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         496.262</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  -0.525</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            997.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 41000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-39-26\n",
      "  done: false\n",
      "  episode_len_mean: 997.1219512195122\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.4634146341463415\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 41\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.720531023873223\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006586936021463657\n",
      "          policy_loss: -0.11690084636211395\n",
      "          total_loss: -0.035568858600325055\n",
      "          vf_explained_var: -0.12576617300510406\n",
      "          vf_loss: 0.10754925724403519\n",
      "    num_agent_steps_sampled: 41000\n",
      "    num_agent_steps_trained: 41000\n",
      "    num_steps_sampled: 41000\n",
      "    num_steps_trained: 41000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.75333333333333\n",
      "    ram_util_percent: 73.34666666666666\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04523490317773513\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.699259757085484\n",
      "    mean_inference_ms: 1.5222056408063431\n",
      "    mean_raw_obs_processing_ms: 0.2809807797734518\n",
      "  time_since_restore: 506.6766531467438\n",
      "  time_this_iter_s: 10.41493844985962\n",
      "  time_total_s: 506.6766531467438\n",
      "  timers:\n",
      "    learn_throughput: 1735.067\n",
      "    learn_time_ms: 576.347\n",
      "    load_throughput: 293570.749\n",
      "    load_time_ms: 3.406\n",
      "    sample_throughput: 101.618\n",
      "    sample_time_ms: 9840.783\n",
      "    update_time_ms: 2.267\n",
      "  timestamp: 1631875166\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 41000\n",
      "  training_iteration: 41\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         506.677</td><td style=\"text-align: right;\">41000</td><td style=\"text-align: right;\">-0.463415</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.122</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 42000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-39-37\n",
      "  done: false\n",
      "  episode_len_mean: 997.1904761904761\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.42857142857142855\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 42\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4650899675157336\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014676139627915596\n",
      "          policy_loss: -0.15752836792833275\n",
      "          total_loss: -0.11239002119335863\n",
      "          vf_explained_var: 0.5111238360404968\n",
      "          vf_loss: 0.06758782452800208\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.70666666666667\n",
      "    ram_util_percent: 73.51333333333332\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04521445548523038\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.601579110106693\n",
      "    mean_inference_ms: 1.5213691484140721\n",
      "    mean_raw_obs_processing_ms: 0.28735052750830664\n",
      "  time_since_restore: 517.0004489421844\n",
      "  time_this_iter_s: 10.323795795440674\n",
      "  time_total_s: 517.0004489421844\n",
      "  timers:\n",
      "    learn_throughput: 1736.043\n",
      "    learn_time_ms: 576.023\n",
      "    load_throughput: 296469.624\n",
      "    load_time_ms: 3.373\n",
      "    sample_throughput: 102.213\n",
      "    sample_time_ms: 9783.512\n",
      "    update_time_ms: 2.26\n",
      "  timestamp: 1631875177\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 42\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">             517</td><td style=\"text-align: right;\">42000</td><td style=\"text-align: right;\">-0.428571</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            997.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 43000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-39-47\n",
      "  done: false\n",
      "  episode_len_mean: 997.2558139534884\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.46511627906976744\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 43\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.490919746292962\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013286573941282024\n",
      "          policy_loss: 0.06578570728500684\n",
      "          total_loss: 0.1564805943104956\n",
      "          vf_explained_var: 0.366651713848114\n",
      "          vf_loss: 0.11361110077963935\n",
      "    num_agent_steps_sampled: 43000\n",
      "    num_agent_steps_trained: 43000\n",
      "    num_steps_sampled: 43000\n",
      "    num_steps_trained: 43000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.75\n",
      "    ram_util_percent: 73.52142857142859\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04519432421640228\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.507560882086123\n",
      "    mean_inference_ms: 1.5205551071081007\n",
      "    mean_raw_obs_processing_ms: 0.2932007268839808\n",
      "  time_since_restore: 527.2092518806458\n",
      "  time_this_iter_s: 10.208802938461304\n",
      "  time_total_s: 527.2092518806458\n",
      "  timers:\n",
      "    learn_throughput: 1742.399\n",
      "    learn_time_ms: 573.922\n",
      "    load_throughput: 301876.624\n",
      "    load_time_ms: 3.313\n",
      "    sample_throughput: 102.398\n",
      "    sample_time_ms: 9765.843\n",
      "    update_time_ms: 2.304\n",
      "  timestamp: 1631875187\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 43000\n",
      "  training_iteration: 43\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         527.209</td><td style=\"text-align: right;\">43000</td><td style=\"text-align: right;\">-0.465116</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.256</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-39-57\n",
      "  done: false\n",
      "  episode_len_mean: 997.3181818181819\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.45454545454545453\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 44\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5302426788542007\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011318948511750736\n",
      "          policy_loss: -0.08438720107078553\n",
      "          total_loss: -0.06908097076747152\n",
      "          vf_explained_var: 0.15983164310455322\n",
      "          vf_loss: 0.03891081587514943\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.79999999999999\n",
      "    ram_util_percent: 73.61999999999999\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04517445835462322\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.416893776388491\n",
      "    mean_inference_ms: 1.5197609390947209\n",
      "    mean_raw_obs_processing_ms: 0.29858077403405886\n",
      "  time_since_restore: 537.2364099025726\n",
      "  time_this_iter_s: 10.02715802192688\n",
      "  time_total_s: 537.2364099025726\n",
      "  timers:\n",
      "    learn_throughput: 1741.442\n",
      "    learn_time_ms: 574.237\n",
      "    load_throughput: 310307.622\n",
      "    load_time_ms: 3.223\n",
      "    sample_throughput: 102.517\n",
      "    sample_time_ms: 9754.434\n",
      "    update_time_ms: 2.329\n",
      "  timestamp: 1631875197\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 44\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         537.236</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">-0.454545</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.318</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 45000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-40-07\n",
      "  done: false\n",
      "  episode_len_mean: 997.3777777777777\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.4\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 45\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3687764326731364\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01032283168559292\n",
      "          policy_loss: -0.04681065926949183\n",
      "          total_loss: -0.01192892889181773\n",
      "          vf_explained_var: 0.14233936369419098\n",
      "          vf_loss: 0.057021067845117714\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_agent_steps_trained: 45000\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.01428571428572\n",
      "    ram_util_percent: 73.74285714285715\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04515512980879043\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.329485999592357\n",
      "    mean_inference_ms: 1.5189945175474135\n",
      "    mean_raw_obs_processing_ms: 0.30352663736840635\n",
      "  time_since_restore: 547.496506690979\n",
      "  time_this_iter_s: 10.260096788406372\n",
      "  time_total_s: 547.496506690979\n",
      "  timers:\n",
      "    learn_throughput: 1732.59\n",
      "    learn_time_ms: 577.171\n",
      "    load_throughput: 312958.715\n",
      "    load_time_ms: 3.195\n",
      "    sample_throughput: 102.899\n",
      "    sample_time_ms: 9718.309\n",
      "    update_time_ms: 2.886\n",
      "  timestamp: 1631875207\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 45\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         547.497</td><td style=\"text-align: right;\">45000</td><td style=\"text-align: right;\">    -0.4</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.378</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 46000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-40-18\n",
      "  done: false\n",
      "  episode_len_mean: 997.4347826086956\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.391304347826087\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 46\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4245143360561796\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010038466961559283\n",
      "          policy_loss: -0.15087871270047293\n",
      "          total_loss: -0.15645623654127122\n",
      "          vf_explained_var: 0.3319392800331116\n",
      "          vf_loss: 0.017161850662281115\n",
      "    num_agent_steps_sampled: 46000\n",
      "    num_agent_steps_trained: 46000\n",
      "    num_steps_sampled: 46000\n",
      "    num_steps_trained: 46000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.14666666666667\n",
      "    ram_util_percent: 74.62000000000002\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04513709454442139\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.245201009337359\n",
      "    mean_inference_ms: 1.5182602685046585\n",
      "    mean_raw_obs_processing_ms: 0.30807147581908734\n",
      "  time_since_restore: 557.8141436576843\n",
      "  time_this_iter_s: 10.317636966705322\n",
      "  time_total_s: 557.8141436576843\n",
      "  timers:\n",
      "    learn_throughput: 1732.335\n",
      "    learn_time_ms: 577.256\n",
      "    load_throughput: 312683.41\n",
      "    load_time_ms: 3.198\n",
      "    sample_throughput: 103.099\n",
      "    sample_time_ms: 9699.402\n",
      "    update_time_ms: 2.843\n",
      "  timestamp: 1631875218\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 46\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         557.814</td><td style=\"text-align: right;\">46000</td><td style=\"text-align: right;\">-0.391304</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.435</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 47000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-40-28\n",
      "  done: false\n",
      "  episode_len_mean: 997.4893617021277\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.3829787234042553\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 47\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.49779405064053\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009374480094662112\n",
      "          policy_loss: -0.0769234781463941\n",
      "          total_loss: -0.09596337427695592\n",
      "          vf_explained_var: 0.4722324311733246\n",
      "          vf_loss: 0.004531871881853375\n",
      "    num_agent_steps_sampled: 47000\n",
      "    num_agent_steps_trained: 47000\n",
      "    num_steps_sampled: 47000\n",
      "    num_steps_trained: 47000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.779999999999994\n",
      "    ram_util_percent: 74.54666666666667\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04511942700920891\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.163838743494004\n",
      "    mean_inference_ms: 1.517553474606124\n",
      "    mean_raw_obs_processing_ms: 0.3122492919605829\n",
      "  time_since_restore: 568.0662658214569\n",
      "  time_this_iter_s: 10.252122163772583\n",
      "  time_total_s: 568.0662658214569\n",
      "  timers:\n",
      "    learn_throughput: 1728.505\n",
      "    learn_time_ms: 578.535\n",
      "    load_throughput: 314417.949\n",
      "    load_time_ms: 3.18\n",
      "    sample_throughput: 103.52\n",
      "    sample_time_ms: 9659.969\n",
      "    update_time_ms: 2.851\n",
      "  timestamp: 1631875228\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47000\n",
      "  training_iteration: 47\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         568.066</td><td style=\"text-align: right;\">47000</td><td style=\"text-align: right;\">-0.382979</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.489</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-40-38\n",
      "  done: false\n",
      "  episode_len_mean: 997.5416666666666\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.3333333333333333\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 48\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6545334974924724\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008981364880902573\n",
      "          policy_loss: -0.11860771372707354\n",
      "          total_loss: -0.11210436044881741\n",
      "          vf_explained_var: 0.04177277535200119\n",
      "          vf_loss: 0.031701479360668194\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.578571428571436\n",
      "    ram_util_percent: 74.62142857142858\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045102053060384\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.085212661682368\n",
      "    mean_inference_ms: 1.5168647358329208\n",
      "    mean_raw_obs_processing_ms: 0.3160957042928583\n",
      "  time_since_restore: 578.2807898521423\n",
      "  time_this_iter_s: 10.214524030685425\n",
      "  time_total_s: 578.2807898521423\n",
      "  timers:\n",
      "    learn_throughput: 1716.818\n",
      "    learn_time_ms: 582.473\n",
      "    load_throughput: 314727.016\n",
      "    load_time_ms: 3.177\n",
      "    sample_throughput: 103.658\n",
      "    sample_time_ms: 9647.093\n",
      "    update_time_ms: 2.94\n",
      "  timestamp: 1631875238\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 48\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         578.281</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">-0.333333</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.542</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 49000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-40-49\n",
      "  done: false\n",
      "  episode_len_mean: 997.5918367346939\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.32653061224489793\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 49\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3218179278903537\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010510435926563567\n",
      "          policy_loss: -0.06427380152874523\n",
      "          total_loss: -0.02275232093201743\n",
      "          vf_explained_var: 0.30250316858291626\n",
      "          vf_loss: 0.06316309761928601\n",
      "    num_agent_steps_sampled: 49000\n",
      "    num_agent_steps_trained: 49000\n",
      "    num_steps_sampled: 49000\n",
      "    num_steps_trained: 49000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.42\n",
      "    ram_util_percent: 75.11333333333336\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04508540436285105\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 13.009225330114553\n",
      "    mean_inference_ms: 1.5162061560487576\n",
      "    mean_raw_obs_processing_ms: 0.3196312840238043\n",
      "  time_since_restore: 588.6284973621368\n",
      "  time_this_iter_s: 10.347707509994507\n",
      "  time_total_s: 588.6284973621368\n",
      "  timers:\n",
      "    learn_throughput: 1701.728\n",
      "    learn_time_ms: 587.638\n",
      "    load_throughput: 311030.166\n",
      "    load_time_ms: 3.215\n",
      "    sample_throughput: 103.639\n",
      "    sample_time_ms: 9648.922\n",
      "    update_time_ms: 2.983\n",
      "  timestamp: 1631875249\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49000\n",
      "  training_iteration: 49\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         588.628</td><td style=\"text-align: right;\">49000</td><td style=\"text-align: right;\">-0.326531</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.592</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 50000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-40-59\n",
      "  done: false\n",
      "  episode_len_mean: 997.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.3\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 50\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4779651059044734\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011396711592970576\n",
      "          policy_loss: -0.0006720033784707387\n",
      "          total_loss: 0.04179665330383513\n",
      "          vf_explained_var: 0.15713050961494446\n",
      "          vf_loss: 0.06553879893488354\n",
      "    num_agent_steps_sampled: 50000\n",
      "    num_agent_steps_trained: 50000\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.326666666666675\n",
      "    ram_util_percent: 75.47333333333333\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045069192538904845\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.935664077550783\n",
      "    mean_inference_ms: 1.5155680216779475\n",
      "    mean_raw_obs_processing_ms: 0.3228825456276309\n",
      "  time_since_restore: 598.737566947937\n",
      "  time_this_iter_s: 10.109069585800171\n",
      "  time_total_s: 598.737566947937\n",
      "  timers:\n",
      "    learn_throughput: 1702.6\n",
      "    learn_time_ms: 587.337\n",
      "    load_throughput: 307716.868\n",
      "    load_time_ms: 3.25\n",
      "    sample_throughput: 103.624\n",
      "    sample_time_ms: 9650.241\n",
      "    update_time_ms: 3.044\n",
      "  timestamp: 1631875259\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 50\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         598.738</td><td style=\"text-align: right;\">50000</td><td style=\"text-align: right;\">    -0.3</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            997.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 51000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-41-10\n",
      "  done: false\n",
      "  episode_len_mean: 997.6862745098039\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.3333333333333333\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 51\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5494499550925362\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008899433060046274\n",
      "          policy_loss: 0.12052665998538335\n",
      "          total_loss: 0.1489087101485994\n",
      "          vf_explained_var: 0.10677681863307953\n",
      "          vf_loss: 0.05254163747886196\n",
      "    num_agent_steps_sampled: 51000\n",
      "    num_agent_steps_trained: 51000\n",
      "    num_steps_sampled: 51000\n",
      "    num_steps_trained: 51000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.38\n",
      "    ram_util_percent: 75.47333333333334\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04505402015159379\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.86475822690935\n",
      "    mean_inference_ms: 1.5149845779808964\n",
      "    mean_raw_obs_processing_ms: 0.32588080658165863\n",
      "  time_since_restore: 609.8815832138062\n",
      "  time_this_iter_s: 11.14401626586914\n",
      "  time_total_s: 609.8815832138062\n",
      "  timers:\n",
      "    learn_throughput: 1699.169\n",
      "    learn_time_ms: 588.523\n",
      "    load_throughput: 309120.684\n",
      "    load_time_ms: 3.235\n",
      "    sample_throughput: 102.861\n",
      "    sample_time_ms: 9721.816\n",
      "    update_time_ms: 3.134\n",
      "  timestamp: 1631875270\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 51000\n",
      "  training_iteration: 51\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         609.882</td><td style=\"text-align: right;\">51000</td><td style=\"text-align: right;\">-0.333333</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.686</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-41-20\n",
      "  done: false\n",
      "  episode_len_mean: 997.7307692307693\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.28846153846153844\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 52\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4439065880245634\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01443944264651396\n",
      "          policy_loss: -0.02311795447021723\n",
      "          total_loss: -0.02690473049879074\n",
      "          vf_explained_var: 0.30983540415763855\n",
      "          vf_loss: 0.018486371548432443\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.026666666666664\n",
      "    ram_util_percent: 75.24000000000001\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04503892571273959\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.796033536815683\n",
      "    mean_inference_ms: 1.5144120774874754\n",
      "    mean_raw_obs_processing_ms: 0.32863889678298597\n",
      "  time_since_restore: 619.9786696434021\n",
      "  time_this_iter_s: 10.097086429595947\n",
      "  time_total_s: 619.9786696434021\n",
      "  timers:\n",
      "    learn_throughput: 1700.456\n",
      "    learn_time_ms: 588.077\n",
      "    load_throughput: 305186.781\n",
      "    load_time_ms: 3.277\n",
      "    sample_throughput: 103.096\n",
      "    sample_time_ms: 9699.662\n",
      "    update_time_ms: 3.193\n",
      "  timestamp: 1631875280\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 52\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         619.979</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">-0.288462</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.731</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 53000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-41-31\n",
      "  done: false\n",
      "  episode_len_mean: 997.7735849056604\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.2830188679245283\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 53\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3328451342052885\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011412473811497043\n",
      "          policy_loss: -0.11581057470498814\n",
      "          total_loss: -0.0553324718028307\n",
      "          vf_explained_var: 0.20151910185813904\n",
      "          vf_loss: 0.08209468076109058\n",
      "    num_agent_steps_sampled: 53000\n",
      "    num_agent_steps_trained: 53000\n",
      "    num_steps_sampled: 53000\n",
      "    num_steps_trained: 53000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.373333333333335\n",
      "    ram_util_percent: 75.28\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04502498472229391\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.729617499294394\n",
      "    mean_inference_ms: 1.5138776179189077\n",
      "    mean_raw_obs_processing_ms: 0.3311725591467788\n",
      "  time_since_restore: 630.8045349121094\n",
      "  time_this_iter_s: 10.825865268707275\n",
      "  time_total_s: 630.8045349121094\n",
      "  timers:\n",
      "    learn_throughput: 1697.517\n",
      "    learn_time_ms: 589.096\n",
      "    load_throughput: 302457.851\n",
      "    load_time_ms: 3.306\n",
      "    sample_throughput: 102.455\n",
      "    sample_time_ms: 9760.366\n",
      "    update_time_ms: 3.152\n",
      "  timestamp: 1631875291\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 53000\n",
      "  training_iteration: 53\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         630.805</td><td style=\"text-align: right;\">53000</td><td style=\"text-align: right;\">-0.283019</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.774</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 54000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-41-41\n",
      "  done: false\n",
      "  episode_len_mean: 997.8148148148148\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.24074074074074073\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 54\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3850217872195776\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013800500925801663\n",
      "          policy_loss: -0.1413688284655412\n",
      "          total_loss: -0.13062608987092972\n",
      "          vf_explained_var: -0.07942689955234528\n",
      "          vf_loss: 0.03252288224434273\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 54000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.70666666666667\n",
      "    ram_util_percent: 75.36\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045012184561916496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.66522825719691\n",
      "    mean_inference_ms: 1.5133639187198995\n",
      "    mean_raw_obs_processing_ms: 0.3334996299409029\n",
      "  time_since_restore: 641.1092941761017\n",
      "  time_this_iter_s: 10.30475926399231\n",
      "  time_total_s: 641.1092941761017\n",
      "  timers:\n",
      "    learn_throughput: 1698.202\n",
      "    learn_time_ms: 588.858\n",
      "    load_throughput: 299788.005\n",
      "    load_time_ms: 3.336\n",
      "    sample_throughput: 102.161\n",
      "    sample_time_ms: 9788.449\n",
      "    update_time_ms: 3.085\n",
      "  timestamp: 1631875301\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 54\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         641.109</td><td style=\"text-align: right;\">54000</td><td style=\"text-align: right;\">-0.240741</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.815</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 55000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-41-52\n",
      "  done: false\n",
      "  episode_len_mean: 997.8545454545455\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.2545454545454545\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 55\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3668280204137164\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010598838249482689\n",
      "          policy_loss: -0.056968743768003254\n",
      "          total_loss: -0.04604764497942394\n",
      "          vf_explained_var: 0.2693183124065399\n",
      "          vf_loss: 0.032999553431808534\n",
      "    num_agent_steps_sampled: 55000\n",
      "    num_agent_steps_trained: 55000\n",
      "    num_steps_sampled: 55000\n",
      "    num_steps_trained: 55000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.831250000000004\n",
      "    ram_util_percent: 75.41874999999999\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.045000410286377954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.602960669896449\n",
      "    mean_inference_ms: 1.5128910065234475\n",
      "    mean_raw_obs_processing_ms: 0.3356365928851161\n",
      "  time_since_restore: 652.0560164451599\n",
      "  time_this_iter_s: 10.946722269058228\n",
      "  time_total_s: 652.0560164451599\n",
      "  timers:\n",
      "    learn_throughput: 1708.141\n",
      "    learn_time_ms: 585.432\n",
      "    load_throughput: 295182.276\n",
      "    load_time_ms: 3.388\n",
      "    sample_throughput: 101.408\n",
      "    sample_time_ms: 9861.117\n",
      "    update_time_ms: 2.524\n",
      "  timestamp: 1631875312\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55000\n",
      "  training_iteration: 55\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         652.056</td><td style=\"text-align: right;\">55000</td><td style=\"text-align: right;\">-0.254545</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.855</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-42-03\n",
      "  done: false\n",
      "  episode_len_mean: 997.8928571428571\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.26785714285714285\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 56\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.48392399681939\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012415379348915737\n",
      "          policy_loss: -0.04175745679272546\n",
      "          total_loss: 0.06948029200236003\n",
      "          vf_explained_var: 0.19516532123088837\n",
      "          vf_loss: 0.13421468320820068\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.58666666666667\n",
      "    ram_util_percent: 75.45333333333335\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04498938832280751\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.542696202749088\n",
      "    mean_inference_ms: 1.5124468114228722\n",
      "    mean_raw_obs_processing_ms: 0.3375968101681683\n",
      "  time_since_restore: 662.9742906093597\n",
      "  time_this_iter_s: 10.918274164199829\n",
      "  time_total_s: 662.9742906093597\n",
      "  timers:\n",
      "    learn_throughput: 1694.814\n",
      "    learn_time_ms: 590.035\n",
      "    load_throughput: 294483.848\n",
      "    load_time_ms: 3.396\n",
      "    sample_throughput: 100.842\n",
      "    sample_time_ms: 9916.551\n",
      "    update_time_ms: 2.554\n",
      "  timestamp: 1631875323\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 56\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         662.974</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">-0.267857</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.893</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 57000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-42-13\n",
      "  done: false\n",
      "  episode_len_mean: 997.9298245614035\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.22807017543859648\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 57\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1414334588580664\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010941004071136387\n",
      "          policy_loss: -0.06248415625757641\n",
      "          total_loss: -0.07614764041370815\n",
      "          vf_explained_var: 0.32292431592941284\n",
      "          vf_loss: 0.006109700597719186\n",
      "    num_agent_steps_sampled: 57000\n",
      "    num_agent_steps_trained: 57000\n",
      "    num_steps_sampled: 57000\n",
      "    num_steps_trained: 57000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.486666666666665\n",
      "    ram_util_percent: 75.33999999999999\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04497854157076175\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.484110881183236\n",
      "    mean_inference_ms: 1.5120109995055686\n",
      "    mean_raw_obs_processing_ms: 0.3393928519979217\n",
      "  time_since_restore: 673.0572276115417\n",
      "  time_this_iter_s: 10.082937002182007\n",
      "  time_total_s: 673.0572276115417\n",
      "  timers:\n",
      "    learn_throughput: 1694.164\n",
      "    learn_time_ms: 590.262\n",
      "    load_throughput: 294761.165\n",
      "    load_time_ms: 3.393\n",
      "    sample_throughput: 101.022\n",
      "    sample_time_ms: 9898.86\n",
      "    update_time_ms: 2.498\n",
      "  timestamp: 1631875333\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 57000\n",
      "  training_iteration: 57\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         673.057</td><td style=\"text-align: right;\">57000</td><td style=\"text-align: right;\">-0.22807</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            997.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 58000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-42-23\n",
      "  done: false\n",
      "  episode_len_mean: 997.9655172413793\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.22413793103448276\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 58\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.373874227205912\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012579565048622302\n",
      "          policy_loss: -0.038547158965633974\n",
      "          total_loss: -0.04740183481739627\n",
      "          vf_explained_var: -0.07602822035551071\n",
      "          vf_loss: 0.01299713148166322\n",
      "    num_agent_steps_sampled: 58000\n",
      "    num_agent_steps_trained: 58000\n",
      "    num_steps_sampled: 58000\n",
      "    num_steps_trained: 58000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.92142857142857\n",
      "    ram_util_percent: 75.13571428571429\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04496755530522744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.42708551376275\n",
      "    mean_inference_ms: 1.5115764792667905\n",
      "    mean_raw_obs_processing_ms: 0.34103446022656037\n",
      "  time_since_restore: 682.9485919475555\n",
      "  time_this_iter_s: 9.891364336013794\n",
      "  time_total_s: 682.9485919475555\n",
      "  timers:\n",
      "    learn_throughput: 1702.836\n",
      "    learn_time_ms: 587.256\n",
      "    load_throughput: 293609.795\n",
      "    load_time_ms: 3.406\n",
      "    sample_throughput: 101.321\n",
      "    sample_time_ms: 9869.626\n",
      "    update_time_ms: 2.4\n",
      "  timestamp: 1631875343\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 58000\n",
      "  training_iteration: 58\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         682.949</td><td style=\"text-align: right;\">58000</td><td style=\"text-align: right;\">-0.224138</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.966</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 59000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-42-34\n",
      "  done: false\n",
      "  episode_len_mean: 998.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.1864406779661017\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 59\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3864356888665093\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010687954091639121\n",
      "          policy_loss: 0.005410085287359026\n",
      "          total_loss: 0.03975870112578074\n",
      "          vf_explained_var: 0.05668140947818756\n",
      "          vf_loss: 0.056609778517546755\n",
      "    num_agent_steps_sampled: 59000\n",
      "    num_agent_steps_trained: 59000\n",
      "    num_steps_sampled: 59000\n",
      "    num_steps_trained: 59000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.7\n",
      "    ram_util_percent: 75.58\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044957095965898676\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.371746979612286\n",
      "    mean_inference_ms: 1.511173376846383\n",
      "    mean_raw_obs_processing_ms: 0.34253581223948976\n",
      "  time_since_restore: 693.6266975402832\n",
      "  time_this_iter_s: 10.678105592727661\n",
      "  time_total_s: 693.6266975402832\n",
      "  timers:\n",
      "    learn_throughput: 1714.337\n",
      "    learn_time_ms: 583.316\n",
      "    load_throughput: 285742.782\n",
      "    load_time_ms: 3.5\n",
      "    sample_throughput: 100.944\n",
      "    sample_time_ms: 9906.519\n",
      "    update_time_ms: 2.37\n",
      "  timestamp: 1631875354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 59000\n",
      "  training_iteration: 59\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         693.627</td><td style=\"text-align: right;\">59000</td><td style=\"text-align: right;\">-0.186441</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">               998</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1256)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1256)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-43-02\n",
      "  done: false\n",
      "  episode_len_mean: 995.8166666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.2\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 60\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4168018129136826\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012004057346887266\n",
      "          policy_loss: 0.025600745446152158\n",
      "          total_loss: 0.046089148852560255\n",
      "          vf_explained_var: 0.3261348605155945\n",
      "          vf_loss: 0.042855810412826635\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.3375\n",
      "    ram_util_percent: 75.9875\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044946702399714766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.317916128858421\n",
      "    mean_inference_ms: 1.5107780517429321\n",
      "    mean_raw_obs_processing_ms: 0.3488536974732916\n",
      "  time_since_restore: 721.6717283725739\n",
      "  time_this_iter_s: 28.04503083229065\n",
      "  time_total_s: 721.6717283725739\n",
      "  timers:\n",
      "    learn_throughput: 1720.191\n",
      "    learn_time_ms: 581.331\n",
      "    load_throughput: 199572.905\n",
      "    load_time_ms: 5.011\n",
      "    sample_throughput: 85.465\n",
      "    sample_time_ms: 11700.64\n",
      "    update_time_ms: 2.307\n",
      "  timestamp: 1631875382\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 60\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         721.672</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">    -0.2</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           995.817</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 61000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-43-14\n",
      "  done: false\n",
      "  episode_len_mean: 995.8852459016393\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.19672131147540983\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 61\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2946357594596014\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010469586027075109\n",
      "          policy_loss: -0.03337453847957982\n",
      "          total_loss: -0.004544870720969306\n",
      "          vf_explained_var: 0.3082904815673828\n",
      "          vf_loss: 0.05020558778455274\n",
      "    num_agent_steps_sampled: 61000\n",
      "    num_agent_steps_trained: 61000\n",
      "    num_steps_sampled: 61000\n",
      "    num_steps_trained: 61000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.28823529411765\n",
      "    ram_util_percent: 75.31176470588234\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04493625395055192\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.265961346880127\n",
      "    mean_inference_ms: 1.51038380161757\n",
      "    mean_raw_obs_processing_ms: 0.35480424155669843\n",
      "  time_since_restore: 733.4534394741058\n",
      "  time_this_iter_s: 11.781711101531982\n",
      "  time_total_s: 733.4534394741058\n",
      "  timers:\n",
      "    learn_throughput: 1723.949\n",
      "    learn_time_ms: 580.064\n",
      "    load_throughput: 199364.21\n",
      "    load_time_ms: 5.016\n",
      "    sample_throughput: 84.992\n",
      "    sample_time_ms: 11765.817\n",
      "    update_time_ms: 2.218\n",
      "  timestamp: 1631875394\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 61000\n",
      "  training_iteration: 61\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         733.453</td><td style=\"text-align: right;\">61000</td><td style=\"text-align: right;\">-0.196721</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           995.885</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 62000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-43-24\n",
      "  done: false\n",
      "  episode_len_mean: 995.9516129032259\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.16129032258064516\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 62\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4332562817467585\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010084593687165327\n",
      "          policy_loss: -0.043327590947349864\n",
      "          total_loss: -0.05409569340861506\n",
      "          vf_explained_var: 0.4273887574672699\n",
      "          vf_loss: 0.012051769038144913\n",
      "    num_agent_steps_sampled: 62000\n",
      "    num_agent_steps_trained: 62000\n",
      "    num_steps_sampled: 62000\n",
      "    num_steps_trained: 62000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.021428571428565\n",
      "    ram_util_percent: 75.62857142857145\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04492572985797782\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.215314559898893\n",
      "    mean_inference_ms: 1.5099896105863178\n",
      "    mean_raw_obs_processing_ms: 0.36041479427915285\n",
      "  time_since_restore: 743.4166922569275\n",
      "  time_this_iter_s: 9.963252782821655\n",
      "  time_total_s: 743.4166922569275\n",
      "  timers:\n",
      "    learn_throughput: 1724.575\n",
      "    learn_time_ms: 579.853\n",
      "    load_throughput: 201132.861\n",
      "    load_time_ms: 4.972\n",
      "    sample_throughput: 85.086\n",
      "    sample_time_ms: 11752.747\n",
      "    update_time_ms: 2.16\n",
      "  timestamp: 1631875404\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 62000\n",
      "  training_iteration: 62\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         743.417</td><td style=\"text-align: right;\">62000</td><td style=\"text-align: right;\">-0.16129</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           995.952</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 63000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-43-34\n",
      "  done: false\n",
      "  episode_len_mean: 996.015873015873\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.15873015873015872\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 63\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.439508459303114\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012414529289766147\n",
      "          policy_loss: -0.04254682450555265\n",
      "          total_loss: -0.00567689725301332\n",
      "          vf_explained_var: -0.10028425604104996\n",
      "          vf_loss: 0.05940283093497985\n",
      "    num_agent_steps_sampled: 63000\n",
      "    num_agent_steps_trained: 63000\n",
      "    num_steps_sampled: 63000\n",
      "    num_steps_trained: 63000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.46\n",
      "    ram_util_percent: 75.72666666666667\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04491514775848862\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.165917665398736\n",
      "    mean_inference_ms: 1.5095948711593288\n",
      "    mean_raw_obs_processing_ms: 0.3657029775037471\n",
      "  time_since_restore: 753.3339297771454\n",
      "  time_this_iter_s: 9.917237520217896\n",
      "  time_total_s: 753.3339297771454\n",
      "  timers:\n",
      "    learn_throughput: 1728.828\n",
      "    learn_time_ms: 578.426\n",
      "    load_throughput: 202315.51\n",
      "    load_time_ms: 4.943\n",
      "    sample_throughput: 85.739\n",
      "    sample_time_ms: 11663.285\n",
      "    update_time_ms: 2.154\n",
      "  timestamp: 1631875414\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63000\n",
      "  training_iteration: 63\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         753.334</td><td style=\"text-align: right;\">63000</td><td style=\"text-align: right;\">-0.15873</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.016</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-43-44\n",
      "  done: false\n",
      "  episode_len_mean: 996.078125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.140625\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 64\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3564470026228164\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013166878984405791\n",
      "          policy_loss: -0.07590864702231354\n",
      "          total_loss: -0.0715486420939366\n",
      "          vf_explained_var: 0.05184777081012726\n",
      "          vf_loss: 0.02594944151933305\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.15714285714285\n",
      "    ram_util_percent: 75.71428571428574\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04490456585503313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.117725919612646\n",
      "    mean_inference_ms: 1.5092027327222604\n",
      "    mean_raw_obs_processing_ms: 0.3706892409727167\n",
      "  time_since_restore: 763.2830424308777\n",
      "  time_this_iter_s: 9.9491126537323\n",
      "  time_total_s: 763.2830424308777\n",
      "  timers:\n",
      "    learn_throughput: 1729.059\n",
      "    learn_time_ms: 578.349\n",
      "    load_throughput: 202934.155\n",
      "    load_time_ms: 4.928\n",
      "    sample_throughput: 86.001\n",
      "    sample_time_ms: 11627.776\n",
      "    update_time_ms: 2.171\n",
      "  timestamp: 1631875424\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 64\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         763.283</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">-0.140625</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.078</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 65000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-43-54\n",
      "  done: false\n",
      "  episode_len_mean: 996.1384615384616\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.12307692307692308\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 65\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4249793847401935\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009941728070175859\n",
      "          policy_loss: -0.04708085921075609\n",
      "          total_loss: 0.016345452517271042\n",
      "          vf_explained_var: 0.12372888624668121\n",
      "          vf_loss: 0.08618484565781223\n",
      "    num_agent_steps_sampled: 65000\n",
      "    num_agent_steps_trained: 65000\n",
      "    num_steps_sampled: 65000\n",
      "    num_steps_trained: 65000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.892857142857146\n",
      "    ram_util_percent: 75.8142857142857\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044894025356575634\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.070726229453992\n",
      "    mean_inference_ms: 1.5088162977092856\n",
      "    mean_raw_obs_processing_ms: 0.3753915300592174\n",
      "  time_since_restore: 773.3692691326141\n",
      "  time_this_iter_s: 10.08622670173645\n",
      "  time_total_s: 773.3692691326141\n",
      "  timers:\n",
      "    learn_throughput: 1732.465\n",
      "    learn_time_ms: 577.212\n",
      "    load_throughput: 205438.987\n",
      "    load_time_ms: 4.868\n",
      "    sample_throughput: 86.633\n",
      "    sample_time_ms: 11542.889\n",
      "    update_time_ms: 2.169\n",
      "  timestamp: 1631875434\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 65000\n",
      "  training_iteration: 65\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         773.369</td><td style=\"text-align: right;\">65000</td><td style=\"text-align: right;\">-0.123077</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.138</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 66000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-44-04\n",
      "  done: false\n",
      "  episode_len_mean: 996.1969696969697\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.12121212121212122\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 66\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.367449786927965\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009931535650307759\n",
      "          policy_loss: -0.05255954319404231\n",
      "          total_loss: 0.0062088001933362745\n",
      "          vf_explained_var: -0.17082978785037994\n",
      "          vf_loss: 0.08095310975331813\n",
      "    num_agent_steps_sampled: 66000\n",
      "    num_agent_steps_trained: 66000\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 66000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.186666666666675\n",
      "    ram_util_percent: 75.82000000000001\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04488389562964837\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 12.024898276264468\n",
      "    mean_inference_ms: 1.5084364832483836\n",
      "    mean_raw_obs_processing_ms: 0.37982587890960134\n",
      "  time_since_restore: 783.729297876358\n",
      "  time_this_iter_s: 10.360028743743896\n",
      "  time_total_s: 783.729297876358\n",
      "  timers:\n",
      "    learn_throughput: 1706.843\n",
      "    learn_time_ms: 585.877\n",
      "    load_throughput: 205149.596\n",
      "    load_time_ms: 4.874\n",
      "    sample_throughput: 87.124\n",
      "    sample_time_ms: 11477.946\n",
      "    update_time_ms: 2.46\n",
      "  timestamp: 1631875444\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 66\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         783.729</td><td style=\"text-align: right;\">66000</td><td style=\"text-align: right;\">-0.121212</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.197</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 67000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-44-14\n",
      "  done: false\n",
      "  episode_len_mean: 996.2537313432836\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.08955223880597014\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 67\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.297956715689765\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007875882770354564\n",
      "          policy_loss: -0.06898931949916813\n",
      "          total_loss: -0.035164543406830895\n",
      "          vf_explained_var: 0.32574641704559326\n",
      "          vf_loss: 0.05562295726785022\n",
      "    num_agent_steps_sampled: 67000\n",
      "    num_agent_steps_trained: 67000\n",
      "    num_steps_sampled: 67000\n",
      "    num_steps_trained: 67000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.00000000000001\n",
      "    ram_util_percent: 75.85333333333331\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044874106467381235\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.980201537850116\n",
      "    mean_inference_ms: 1.5080624643818323\n",
      "    mean_raw_obs_processing_ms: 0.38400867225203367\n",
      "  time_since_restore: 793.9845428466797\n",
      "  time_this_iter_s: 10.255244970321655\n",
      "  time_total_s: 793.9845428466797\n",
      "  timers:\n",
      "    learn_throughput: 1705.249\n",
      "    learn_time_ms: 586.425\n",
      "    load_throughput: 204781.002\n",
      "    load_time_ms: 4.883\n",
      "    sample_throughput: 86.993\n",
      "    sample_time_ms: 11495.167\n",
      "    update_time_ms: 2.53\n",
      "  timestamp: 1631875454\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 67000\n",
      "  training_iteration: 67\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         793.985</td><td style=\"text-align: right;\">67000</td><td style=\"text-align: right;\">-0.0895522</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.254</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-44-24\n",
      "  done: false\n",
      "  episode_len_mean: 996.3088235294117\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.07352941176470588\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 68\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.294609785079956\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010243520345651546\n",
      "          policy_loss: 0.021260682286487684\n",
      "          total_loss: 0.020371648255321714\n",
      "          vf_explained_var: 0.17902611196041107\n",
      "          vf_loss: 0.02052053528605029\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.8\n",
      "    ram_util_percent: 75.69285714285715\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04486437145894273\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.936514693092317\n",
      "    mean_inference_ms: 1.507689383757695\n",
      "    mean_raw_obs_processing_ms: 0.3879548646012875\n",
      "  time_since_restore: 803.8307654857635\n",
      "  time_this_iter_s: 9.846222639083862\n",
      "  time_total_s: 803.8307654857635\n",
      "  timers:\n",
      "    learn_throughput: 1709.418\n",
      "    learn_time_ms: 584.994\n",
      "    load_throughput: 205534.626\n",
      "    load_time_ms: 4.865\n",
      "    sample_throughput: 87.015\n",
      "    sample_time_ms: 11492.242\n",
      "    update_time_ms: 2.564\n",
      "  timestamp: 1631875464\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 68\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         803.831</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">-0.0735294</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.309</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 69000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-44-34\n",
      "  done: false\n",
      "  episode_len_mean: 996.3623188405797\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.043478260869565216\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 69\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.357323185602824\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009308814917118881\n",
      "          policy_loss: -0.0743248597615295\n",
      "          total_loss: -0.042877941992547776\n",
      "          vf_explained_var: -0.1221558004617691\n",
      "          vf_loss: 0.053623827453702685\n",
      "    num_agent_steps_sampled: 69000\n",
      "    num_agent_steps_trained: 69000\n",
      "    num_steps_sampled: 69000\n",
      "    num_steps_trained: 69000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.65714285714286\n",
      "    ram_util_percent: 75.69285714285716\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04485467584803438\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.893799216660138\n",
      "    mean_inference_ms: 1.507318223789729\n",
      "    mean_raw_obs_processing_ms: 0.3916771550799185\n",
      "  time_since_restore: 813.6551344394684\n",
      "  time_this_iter_s: 9.824368953704834\n",
      "  time_total_s: 813.6551344394684\n",
      "  timers:\n",
      "    learn_throughput: 1715.67\n",
      "    learn_time_ms: 582.863\n",
      "    load_throughput: 211441.621\n",
      "    load_time_ms: 4.729\n",
      "    sample_throughput: 87.649\n",
      "    sample_time_ms: 11409.154\n",
      "    update_time_ms: 2.553\n",
      "  timestamp: 1631875474\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 69000\n",
      "  training_iteration: 69\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         813.655</td><td style=\"text-align: right;\">69000</td><td style=\"text-align: right;\">-0.0434783</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.362</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 70000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-44-44\n",
      "  done: false\n",
      "  episode_len_mean: 996.4142857142857\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.014285714285714285\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 70\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.410275032785204\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012268806695484047\n",
      "          policy_loss: -0.08731406090988053\n",
      "          total_loss: -0.07874919225772221\n",
      "          vf_explained_var: 0.021750381216406822\n",
      "          vf_loss: 0.03082729946408007\n",
      "    num_agent_steps_sampled: 70000\n",
      "    num_agent_steps_trained: 70000\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.95714285714285\n",
      "    ram_util_percent: 75.77857142857142\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044844975300163036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.852034091765494\n",
      "    mean_inference_ms: 1.506948076544597\n",
      "    mean_raw_obs_processing_ms: 0.39518856436038696\n",
      "  time_since_restore: 823.5291776657104\n",
      "  time_this_iter_s: 9.874043226242065\n",
      "  time_total_s: 823.5291776657104\n",
      "  timers:\n",
      "    learn_throughput: 1719.933\n",
      "    learn_time_ms: 581.418\n",
      "    load_throughput: 312993.747\n",
      "    load_time_ms: 3.195\n",
      "    sample_throughput: 104.221\n",
      "    sample_time_ms: 9595.024\n",
      "    update_time_ms: 2.559\n",
      "  timestamp: 1631875484\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 70\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         823.529</td><td style=\"text-align: right;\">70000</td><td style=\"text-align: right;\">-0.0142857</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.414</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 71000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-44-54\n",
      "  done: false\n",
      "  episode_len_mean: 996.4647887323944\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.014084507042253521\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 71\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.360171733962165\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012015970365299486\n",
      "          policy_loss: 0.007983896860645876\n",
      "          total_loss: 0.03286338159814477\n",
      "          vf_explained_var: -0.08095967769622803\n",
      "          vf_loss: 0.046678806269644865\n",
      "    num_agent_steps_sampled: 71000\n",
      "    num_agent_steps_trained: 71000\n",
      "    num_steps_sampled: 71000\n",
      "    num_steps_trained: 71000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.771428571428565\n",
      "    ram_util_percent: 75.77857142857142\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044835230748293445\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.811174168992324\n",
      "    mean_inference_ms: 1.5065774010300157\n",
      "    mean_raw_obs_processing_ms: 0.398499254645806\n",
      "  time_since_restore: 833.3353831768036\n",
      "  time_this_iter_s: 9.80620551109314\n",
      "  time_total_s: 833.3353831768036\n",
      "  timers:\n",
      "    learn_throughput: 1720.494\n",
      "    learn_time_ms: 581.228\n",
      "    load_throughput: 312385.322\n",
      "    load_time_ms: 3.201\n",
      "    sample_throughput: 106.409\n",
      "    sample_time_ms: 9397.677\n",
      "    update_time_ms: 2.548\n",
      "  timestamp: 1631875494\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 71000\n",
      "  training_iteration: 71\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         833.335</td><td style=\"text-align: right;\">71000</td><td style=\"text-align: right;\">-0.0140845</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.465</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-45-04\n",
      "  done: false\n",
      "  episode_len_mean: 996.5138888888889\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.013888888888888888\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 72\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.420813563134935\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011763631968460588\n",
      "          policy_loss: 0.04392637444867028\n",
      "          total_loss: 0.07818978826204935\n",
      "          vf_explained_var: 0.48844417929649353\n",
      "          vf_loss: 0.0567070041783154\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.19285714285713\n",
      "    ram_util_percent: 75.8785714285714\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044825530992152265\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.771190564684181\n",
      "    mean_inference_ms: 1.5062094375263493\n",
      "    mean_raw_obs_processing_ms: 0.40162045245467276\n",
      "  time_since_restore: 843.1599111557007\n",
      "  time_this_iter_s: 9.824527978897095\n",
      "  time_total_s: 843.1599111557007\n",
      "  timers:\n",
      "    learn_throughput: 1720.77\n",
      "    learn_time_ms: 581.135\n",
      "    load_throughput: 310300.735\n",
      "    load_time_ms: 3.223\n",
      "    sample_throughput: 106.567\n",
      "    sample_time_ms: 9383.769\n",
      "    update_time_ms: 2.553\n",
      "  timestamp: 1631875504\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 72\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">          843.16</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">-0.0138889</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.514</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 73000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-45-14\n",
      "  done: false\n",
      "  episode_len_mean: 996.5616438356165\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: -0.0136986301369863\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 73\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.392137516869439\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011425808907407922\n",
      "          policy_loss: -0.029292654991149903\n",
      "          total_loss: 0.03308379318979052\n",
      "          vf_explained_var: 0.5203356146812439\n",
      "          vf_loss: 0.08458395024968518\n",
      "    num_agent_steps_sampled: 73000\n",
      "    num_agent_steps_trained: 73000\n",
      "    num_steps_sampled: 73000\n",
      "    num_steps_trained: 73000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.09285714285714\n",
      "    ram_util_percent: 76.00714285714285\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04481589872718356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.73209157772109\n",
      "    mean_inference_ms: 1.5058436461092461\n",
      "    mean_raw_obs_processing_ms: 0.40457079478871383\n",
      "  time_since_restore: 853.2234706878662\n",
      "  time_this_iter_s: 10.063559532165527\n",
      "  time_total_s: 853.2234706878662\n",
      "  timers:\n",
      "    learn_throughput: 1719.603\n",
      "    learn_time_ms: 581.529\n",
      "    load_throughput: 310583.357\n",
      "    load_time_ms: 3.22\n",
      "    sample_throughput: 106.405\n",
      "    sample_time_ms: 9398.046\n",
      "    update_time_ms: 2.549\n",
      "  timestamp: 1631875514\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 73000\n",
      "  training_iteration: 73\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">    reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         853.223</td><td style=\"text-align: right;\">73000</td><td style=\"text-align: right;\">-0.0136986</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.562</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 74000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-45-24\n",
      "  done: false\n",
      "  episode_len_mean: 996.6081081081081\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 74\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.270624793900384\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0179000897045071\n",
      "          policy_loss: 0.02533449874156051\n",
      "          total_loss: 0.022161419772439534\n",
      "          vf_explained_var: 0.506225049495697\n",
      "          vf_loss: 0.016848153323452505\n",
      "    num_agent_steps_sampled: 74000\n",
      "    num_agent_steps_trained: 74000\n",
      "    num_steps_sampled: 74000\n",
      "    num_steps_trained: 74000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.64666666666667\n",
      "    ram_util_percent: 76.14666666666668\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044806263402345906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.693899418746145\n",
      "    mean_inference_ms: 1.5054830939057162\n",
      "    mean_raw_obs_processing_ms: 0.40735255195890846\n",
      "  time_since_restore: 863.5587980747223\n",
      "  time_this_iter_s: 10.335327386856079\n",
      "  time_total_s: 863.5587980747223\n",
      "  timers:\n",
      "    learn_throughput: 1718.856\n",
      "    learn_time_ms: 581.782\n",
      "    load_throughput: 311501.396\n",
      "    load_time_ms: 3.21\n",
      "    sample_throughput: 105.972\n",
      "    sample_time_ms: 9436.412\n",
      "    update_time_ms: 2.523\n",
      "  timestamp: 1631875524\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 74000\n",
      "  training_iteration: 74\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         863.559</td><td style=\"text-align: right;\">74000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.608</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 75000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-45-34\n",
      "  done: false\n",
      "  episode_len_mean: 996.6533333333333\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 75\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.215782637066311\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01789628879284181\n",
      "          policy_loss: 0.07499919235706329\n",
      "          total_loss: 0.08006225095854866\n",
      "          vf_explained_var: 0.31050804257392883\n",
      "          vf_loss: 0.024536442151293157\n",
      "    num_agent_steps_sampled: 75000\n",
      "    num_agent_steps_trained: 75000\n",
      "    num_steps_sampled: 75000\n",
      "    num_steps_trained: 75000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.7\n",
      "    ram_util_percent: 76.18000000000002\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04479662647524612\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.6565291933829\n",
      "    mean_inference_ms: 1.5051238552324155\n",
      "    mean_raw_obs_processing_ms: 0.40997505640998766\n",
      "  time_since_restore: 873.5659303665161\n",
      "  time_this_iter_s: 10.007132291793823\n",
      "  time_total_s: 873.5659303665161\n",
      "  timers:\n",
      "    learn_throughput: 1718.341\n",
      "    learn_time_ms: 581.957\n",
      "    load_throughput: 312345.775\n",
      "    load_time_ms: 3.202\n",
      "    sample_throughput: 106.063\n",
      "    sample_time_ms: 9428.357\n",
      "    update_time_ms: 2.528\n",
      "  timestamp: 1631875534\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75000\n",
      "  training_iteration: 75\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         873.566</td><td style=\"text-align: right;\">75000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.653</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-45-45\n",
      "  done: false\n",
      "  episode_len_mean: 996.6973684210526\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 76\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6038427220450506\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01736908584001821\n",
      "          policy_loss: -0.06709800362586975\n",
      "          total_loss: -0.08860138257344564\n",
      "          vf_explained_var: 0.684374213218689\n",
      "          vf_loss: 0.0019296863895659853\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.346666666666664\n",
      "    ram_util_percent: 76.21333333333335\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044787017425617116\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.620044380677799\n",
      "    mean_inference_ms: 1.5047672977797215\n",
      "    mean_raw_obs_processing_ms: 0.412447321846987\n",
      "  time_since_restore: 884.1820273399353\n",
      "  time_this_iter_s: 10.61609697341919\n",
      "  time_total_s: 884.1820273399353\n",
      "  timers:\n",
      "    learn_throughput: 1739.828\n",
      "    learn_time_ms: 574.769\n",
      "    load_throughput: 294694.893\n",
      "    load_time_ms: 3.393\n",
      "    sample_throughput: 105.693\n",
      "    sample_time_ms: 9461.397\n",
      "    update_time_ms: 2.246\n",
      "  timestamp: 1631875545\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 76\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         884.182</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.697</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 77000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-45-56\n",
      "  done: false\n",
      "  episode_len_mean: 996.7402597402597\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 77\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.599806380271912\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01972384091574896\n",
      "          policy_loss: -0.14472035591801008\n",
      "          total_loss: -0.16658620950248507\n",
      "          vf_explained_var: 0.6456493139266968\n",
      "          vf_loss: 0.0011736314109940496\n",
      "    num_agent_steps_sampled: 77000\n",
      "    num_agent_steps_trained: 77000\n",
      "    num_steps_sampled: 77000\n",
      "    num_steps_trained: 77000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.346666666666664\n",
      "    ram_util_percent: 76.51333333333334\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0447786171880986\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.584437845054804\n",
      "    mean_inference_ms: 1.50441813367459\n",
      "    mean_raw_obs_processing_ms: 0.4147798226388856\n",
      "  time_since_restore: 894.9233622550964\n",
      "  time_this_iter_s: 10.741334915161133\n",
      "  time_total_s: 894.9233622550964\n",
      "  timers:\n",
      "    learn_throughput: 1747.163\n",
      "    learn_time_ms: 572.356\n",
      "    load_throughput: 295070.139\n",
      "    load_time_ms: 3.389\n",
      "    sample_throughput: 105.125\n",
      "    sample_time_ms: 9512.478\n",
      "    update_time_ms: 2.147\n",
      "  timestamp: 1631875556\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 77000\n",
      "  training_iteration: 77\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         894.923</td><td style=\"text-align: right;\">77000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 78000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-46-06\n",
      "  done: false\n",
      "  episode_len_mean: 996.7820512820513\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 78\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.542955173386468\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016385880903735935\n",
      "          policy_loss: -0.1033018633723259\n",
      "          total_loss: -0.1245409782561991\n",
      "          vf_explained_var: 0.4286384880542755\n",
      "          vf_loss: 0.0017325526349143022\n",
      "    num_agent_steps_sampled: 78000\n",
      "    num_agent_steps_trained: 78000\n",
      "    num_steps_sampled: 78000\n",
      "    num_steps_trained: 78000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.96666666666667\n",
      "    ram_util_percent: 76.52666666666666\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04477021542337423\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.549674830211021\n",
      "    mean_inference_ms: 1.504071658054288\n",
      "    mean_raw_obs_processing_ms: 0.416977803480697\n",
      "  time_since_restore: 905.5992534160614\n",
      "  time_this_iter_s: 10.675891160964966\n",
      "  time_total_s: 905.5992534160614\n",
      "  timers:\n",
      "    learn_throughput: 1749.752\n",
      "    learn_time_ms: 571.51\n",
      "    load_throughput: 293784.602\n",
      "    load_time_ms: 3.404\n",
      "    sample_throughput: 104.207\n",
      "    sample_time_ms: 9596.292\n",
      "    update_time_ms: 2.107\n",
      "  timestamp: 1631875566\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 78000\n",
      "  training_iteration: 78\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         905.599</td><td style=\"text-align: right;\">78000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.782</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 79000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-46-17\n",
      "  done: false\n",
      "  episode_len_mean: 996.8227848101266\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 79\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5739040427737767\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013090370594560162\n",
      "          policy_loss: -0.18316040829651886\n",
      "          total_loss: -0.20584508627653123\n",
      "          vf_explained_var: 0.46947330236434937\n",
      "          vf_loss: 0.0010908072866085503\n",
      "    num_agent_steps_sampled: 79000\n",
      "    num_agent_steps_trained: 79000\n",
      "    num_steps_sampled: 79000\n",
      "    num_steps_trained: 79000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.88666666666666\n",
      "    ram_util_percent: 76.57333333333332\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04476179014457093\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.515646342444995\n",
      "    mean_inference_ms: 1.5037264426171824\n",
      "    mean_raw_obs_processing_ms: 0.41905014806728474\n",
      "  time_since_restore: 915.7900762557983\n",
      "  time_this_iter_s: 10.190822839736938\n",
      "  time_total_s: 915.7900762557983\n",
      "  timers:\n",
      "    learn_throughput: 1749.938\n",
      "    learn_time_ms: 571.449\n",
      "    load_throughput: 293517.334\n",
      "    load_time_ms: 3.407\n",
      "    sample_throughput: 103.81\n",
      "    sample_time_ms: 9632.995\n",
      "    update_time_ms: 2.106\n",
      "  timestamp: 1631875577\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79000\n",
      "  training_iteration: 79\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">          915.79</td><td style=\"text-align: right;\">79000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.823</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-46-27\n",
      "  done: false\n",
      "  episode_len_mean: 996.8625\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 80\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6398252964019777\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009245974358187655\n",
      "          policy_loss: -0.2893403660919931\n",
      "          total_loss: -0.3138814643025398\n",
      "          vf_explained_var: 0.16715297102928162\n",
      "          vf_loss: 0.0004702585512455294\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.99333333333334\n",
      "    ram_util_percent: 76.49333333333333\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04475339768524707\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.482376825393555\n",
      "    mean_inference_ms: 1.5033863239625531\n",
      "    mean_raw_obs_processing_ms: 0.42100292010182494\n",
      "  time_since_restore: 926.3377802371979\n",
      "  time_this_iter_s: 10.547703981399536\n",
      "  time_total_s: 926.3377802371979\n",
      "  timers:\n",
      "    learn_throughput: 1740.827\n",
      "    learn_time_ms: 574.44\n",
      "    load_throughput: 292875.827\n",
      "    load_time_ms: 3.414\n",
      "    sample_throughput: 103.121\n",
      "    sample_time_ms: 9697.342\n",
      "    update_time_ms: 2.11\n",
      "  timestamp: 1631875587\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 80\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         926.338</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.862</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 81000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-46-40\n",
      "  done: false\n",
      "  episode_len_mean: 996.9012345679013\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 81\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5008334583706326\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012026782716045641\n",
      "          policy_loss: -0.23078171585996946\n",
      "          total_loss: -0.25378804869121974\n",
      "          vf_explained_var: 0.4901493787765503\n",
      "          vf_loss: 0.00019798385263938042\n",
      "    num_agent_steps_sampled: 81000\n",
      "    num_agent_steps_trained: 81000\n",
      "    num_steps_sampled: 81000\n",
      "    num_steps_trained: 81000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.33888888888889\n",
      "    ram_util_percent: 76.85000000000001\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04474543372877808\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.450127473328358\n",
      "    mean_inference_ms: 1.503062535626684\n",
      "    mean_raw_obs_processing_ms: 0.4228427399281517\n",
      "  time_since_restore: 938.866690158844\n",
      "  time_this_iter_s: 12.528909921646118\n",
      "  time_total_s: 938.866690158844\n",
      "  timers:\n",
      "    learn_throughput: 1734.354\n",
      "    learn_time_ms: 576.584\n",
      "    load_throughput: 287082.498\n",
      "    load_time_ms: 3.483\n",
      "    sample_throughput: 100.328\n",
      "    sample_time_ms: 9967.354\n",
      "    update_time_ms: 2.137\n",
      "  timestamp: 1631875600\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 81000\n",
      "  training_iteration: 81\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         938.867</td><td style=\"text-align: right;\">81000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.901</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 82000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-46-51\n",
      "  done: false\n",
      "  episode_len_mean: 996.939024390244\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 82\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4093432346979777\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015973201852165397\n",
      "          policy_loss: -0.15789620905286736\n",
      "          total_loss: -0.17887452704211076\n",
      "          vf_explained_var: 0.27124595642089844\n",
      "          vf_loss: 0.0007191319209394148\n",
      "    num_agent_steps_sampled: 82000\n",
      "    num_agent_steps_trained: 82000\n",
      "    num_steps_sampled: 82000\n",
      "    num_steps_trained: 82000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.125\n",
      "    ram_util_percent: 76.53125\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04473752950065514\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.41870993283912\n",
      "    mean_inference_ms: 1.502745384273536\n",
      "    mean_raw_obs_processing_ms: 0.4245740000731176\n",
      "  time_since_restore: 950.3006236553192\n",
      "  time_this_iter_s: 11.43393349647522\n",
      "  time_total_s: 950.3006236553192\n",
      "  timers:\n",
      "    learn_throughput: 1734.71\n",
      "    learn_time_ms: 576.465\n",
      "    load_throughput: 286741.002\n",
      "    load_time_ms: 3.487\n",
      "    sample_throughput: 98.732\n",
      "    sample_time_ms: 10128.473\n",
      "    update_time_ms: 2.13\n",
      "  timestamp: 1631875611\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 82000\n",
      "  training_iteration: 82\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         950.301</td><td style=\"text-align: right;\">82000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.939</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 83000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-47-02\n",
      "  done: false\n",
      "  episode_len_mean: 996.9759036144578\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 83\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.353852046860589\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019581549888274423\n",
      "          policy_loss: -0.17994934771623877\n",
      "          total_loss: -0.20016179813279045\n",
      "          vf_explained_var: -0.17352399230003357\n",
      "          vf_loss: 0.0003888350137761639\n",
      "    num_agent_steps_sampled: 83000\n",
      "    num_agent_steps_trained: 83000\n",
      "    num_steps_sampled: 83000\n",
      "    num_steps_trained: 83000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.7625\n",
      "    ram_util_percent: 76.1125\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04472967247784097\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.388028851601106\n",
      "    mean_inference_ms: 1.5024303420564726\n",
      "    mean_raw_obs_processing_ms: 0.4262002135623501\n",
      "  time_since_restore: 961.2484631538391\n",
      "  time_this_iter_s: 10.947839498519897\n",
      "  time_total_s: 961.2484631538391\n",
      "  timers:\n",
      "    learn_throughput: 1735.722\n",
      "    learn_time_ms: 576.129\n",
      "    load_throughput: 287092.323\n",
      "    load_time_ms: 3.483\n",
      "    sample_throughput: 97.874\n",
      "    sample_time_ms: 10217.214\n",
      "    update_time_ms: 2.138\n",
      "  timestamp: 1631875622\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 83000\n",
      "  training_iteration: 83\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         961.248</td><td style=\"text-align: right;\">83000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.976</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-47-13\n",
      "  done: false\n",
      "  episode_len_mean: 997.0119047619048\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 84\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3679659909672206\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01973999585921743\n",
      "          policy_loss: -0.07097924712838398\n",
      "          total_loss: -0.09105422426429059\n",
      "          vf_explained_var: -0.12262623012065887\n",
      "          vf_loss: 0.0006436816382726344\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.15625\n",
      "    ram_util_percent: 75.96875\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04472189188279009\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.358047419494104\n",
      "    mean_inference_ms: 1.5021189079642066\n",
      "    mean_raw_obs_processing_ms: 0.427726689585855\n",
      "  time_since_restore: 972.121985912323\n",
      "  time_this_iter_s: 10.873522758483887\n",
      "  time_total_s: 972.121985912323\n",
      "  timers:\n",
      "    learn_throughput: 1740.206\n",
      "    learn_time_ms: 574.645\n",
      "    load_throughput: 282701.715\n",
      "    load_time_ms: 3.537\n",
      "    sample_throughput: 97.347\n",
      "    sample_time_ms: 10272.507\n",
      "    update_time_ms: 2.139\n",
      "  timestamp: 1631875633\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 84\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         972.122</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.012</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 85000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-47-24\n",
      "  done: false\n",
      "  episode_len_mean: 997.0470588235294\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 85\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.367321083280775\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012979646354974535\n",
      "          policy_loss: -0.11263329535722733\n",
      "          total_loss: -0.13408797871735362\n",
      "          vf_explained_var: -0.06647338718175888\n",
      "          vf_loss: 0.00027158174925716593\n",
      "    num_agent_steps_sampled: 85000\n",
      "    num_agent_steps_trained: 85000\n",
      "    num_steps_sampled: 85000\n",
      "    num_steps_trained: 85000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.073333333333345\n",
      "    ram_util_percent: 75.85999999999999\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044714194421522434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.328706525952246\n",
      "    mean_inference_ms: 1.501810428753516\n",
      "    mean_raw_obs_processing_ms: 0.42915982208159253\n",
      "  time_since_restore: 982.7519102096558\n",
      "  time_this_iter_s: 10.629924297332764\n",
      "  time_total_s: 982.7519102096558\n",
      "  timers:\n",
      "    learn_throughput: 1739.66\n",
      "    learn_time_ms: 574.825\n",
      "    load_throughput: 282695.999\n",
      "    load_time_ms: 3.537\n",
      "    sample_throughput: 96.762\n",
      "    sample_time_ms: 10334.606\n",
      "    update_time_ms: 2.143\n",
      "  timestamp: 1631875644\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 85000\n",
      "  training_iteration: 85\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         982.752</td><td style=\"text-align: right;\">85000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.047</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 86000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-47-34\n",
      "  done: false\n",
      "  episode_len_mean: 997.0813953488372\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 86\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.346440749698215\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011979696214635377\n",
      "          policy_loss: -0.05764516956276364\n",
      "          total_loss: -0.07878578085866239\n",
      "          vf_explained_var: -0.014683381654322147\n",
      "          vf_loss: 0.000526839665932736\n",
      "    num_agent_steps_sampled: 86000\n",
      "    num_agent_steps_trained: 86000\n",
      "    num_steps_sampled: 86000\n",
      "    num_steps_trained: 86000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.626666666666665\n",
      "    ram_util_percent: 75.79333333333332\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044706705339607654\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.299984882156968\n",
      "    mean_inference_ms: 1.501504012636729\n",
      "    mean_raw_obs_processing_ms: 0.4305038394783596\n",
      "  time_since_restore: 993.3717348575592\n",
      "  time_this_iter_s: 10.619824647903442\n",
      "  time_total_s: 993.3717348575592\n",
      "  timers:\n",
      "    learn_throughput: 1760.713\n",
      "    learn_time_ms: 567.952\n",
      "    load_throughput: 300686.353\n",
      "    load_time_ms: 3.326\n",
      "    sample_throughput: 96.693\n",
      "    sample_time_ms: 10342.06\n",
      "    update_time_ms: 2.097\n",
      "  timestamp: 1631875654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 86000\n",
      "  training_iteration: 86\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         993.372</td><td style=\"text-align: right;\">86000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.081</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 87000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-47-45\n",
      "  done: false\n",
      "  episode_len_mean: 997.1149425287356\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 87\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1879469288720026\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009357174099006944\n",
      "          policy_loss: -0.08134630978521373\n",
      "          total_loss: -0.10159911302228769\n",
      "          vf_explained_var: -0.15252716839313507\n",
      "          vf_loss: 0.00022308796582769396\n",
      "    num_agent_steps_sampled: 87000\n",
      "    num_agent_steps_trained: 87000\n",
      "    num_steps_sampled: 87000\n",
      "    num_steps_trained: 87000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.980000000000004\n",
      "    ram_util_percent: 75.74666666666667\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044699289717261115\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.271886045176359\n",
      "    mean_inference_ms: 1.5012005841577147\n",
      "    mean_raw_obs_processing_ms: 0.4317636247831464\n",
      "  time_since_restore: 1004.1748607158661\n",
      "  time_this_iter_s: 10.803125858306885\n",
      "  time_total_s: 1004.1748607158661\n",
      "  timers:\n",
      "    learn_throughput: 1761.622\n",
      "    learn_time_ms: 567.659\n",
      "    load_throughput: 299655.214\n",
      "    load_time_ms: 3.337\n",
      "    sample_throughput: 96.632\n",
      "    sample_time_ms: 10348.515\n",
      "    update_time_ms: 2.088\n",
      "  timestamp: 1631875665\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 87000\n",
      "  training_iteration: 87\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         1004.17</td><td style=\"text-align: right;\">87000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.115</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-47-56\n",
      "  done: false\n",
      "  episode_len_mean: 997.1477272727273\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 88\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9190550009409586\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012058210714436551\n",
      "          policy_loss: -0.19082942505677541\n",
      "          total_loss: -0.208095454176267\n",
      "          vf_explained_var: 0.15417936444282532\n",
      "          vf_loss: 0.00011578657823621243\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.193749999999994\n",
      "    ram_util_percent: 75.5875\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04469186546639858\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.244398401619994\n",
      "    mean_inference_ms: 1.500899326691844\n",
      "    mean_raw_obs_processing_ms: 0.4329436252770171\n",
      "  time_since_restore: 1015.0496165752411\n",
      "  time_this_iter_s: 10.874755859375\n",
      "  time_total_s: 1015.0496165752411\n",
      "  timers:\n",
      "    learn_throughput: 1759.746\n",
      "    learn_time_ms: 568.264\n",
      "    load_throughput: 300649.712\n",
      "    load_time_ms: 3.326\n",
      "    sample_throughput: 96.453\n",
      "    sample_time_ms: 10367.776\n",
      "    update_time_ms: 2.091\n",
      "  timestamp: 1631875676\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 88\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         1015.05</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           997.148</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 89000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-48-07\n",
      "  done: false\n",
      "  episode_len_mean: 997.1797752808989\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 89\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.490321816338433\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.022199047076347473\n",
      "          policy_loss: -0.009972866914338536\n",
      "          total_loss: -0.031030077404446074\n",
      "          vf_explained_var: -0.27720338106155396\n",
      "          vf_loss: 0.0005161513394947785\n",
      "    num_agent_steps_sampled: 89000\n",
      "    num_agent_steps_trained: 89000\n",
      "    num_steps_sampled: 89000\n",
      "    num_steps_trained: 89000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.440000000000005\n",
      "    ram_util_percent: 75.5\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04468441797222617\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.21749294458089\n",
      "    mean_inference_ms: 1.5005997394708828\n",
      "    mean_raw_obs_processing_ms: 0.4340462063253147\n",
      "  time_since_restore: 1025.8451058864594\n",
      "  time_this_iter_s: 10.795489311218262\n",
      "  time_total_s: 1025.8451058864594\n",
      "  timers:\n",
      "    learn_throughput: 1759.062\n",
      "    learn_time_ms: 568.485\n",
      "    load_throughput: 300479.558\n",
      "    load_time_ms: 3.328\n",
      "    sample_throughput: 95.896\n",
      "    sample_time_ms: 10427.936\n",
      "    update_time_ms: 2.103\n",
      "  timestamp: 1631875687\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 89000\n",
      "  training_iteration: 89\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         1025.85</td><td style=\"text-align: right;\">89000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            997.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1256)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1256)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 90000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-48-35\n",
      "  done: false\n",
      "  episode_len_mean: 995.7333333333333\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 90\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.206837280591329\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01397693007154825\n",
      "          policy_loss: -0.16006374317738745\n",
      "          total_loss: -0.17765876841213968\n",
      "          vf_explained_var: -0.8684229254722595\n",
      "          vf_loss: 0.0013285378245604484\n",
      "    num_agent_steps_sampled: 90000\n",
      "    num_agent_steps_trained: 90000\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.20000000000001\n",
      "    ram_util_percent: 75.31219512195122\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0446769712892586\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.191161540257792\n",
      "    mean_inference_ms: 1.5003026776653943\n",
      "    mean_raw_obs_processing_ms: 0.43723715989172063\n",
      "  time_since_restore: 1054.2409267425537\n",
      "  time_this_iter_s: 28.39582085609436\n",
      "  time_total_s: 1054.2409267425537\n",
      "  timers:\n",
      "    learn_throughput: 1769.123\n",
      "    learn_time_ms: 565.252\n",
      "    load_throughput: 217938.001\n",
      "    load_time_ms: 4.588\n",
      "    sample_throughput: 81.868\n",
      "    sample_time_ms: 12214.746\n",
      "    update_time_ms: 2.101\n",
      "  timestamp: 1631875715\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 90\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         1054.24</td><td style=\"text-align: right;\">90000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           995.733</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 91000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-48-47\n",
      "  done: false\n",
      "  episode_len_mean: 995.7802197802198\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 91\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4451959901385836\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015053046132017824\n",
      "          policy_loss: 0.016313109464115568\n",
      "          total_loss: -0.0027887991733021207\n",
      "          vf_explained_var: -0.37111493945121765\n",
      "          vf_loss: 0.001963112675649528\n",
      "    num_agent_steps_sampled: 91000\n",
      "    num_agent_steps_trained: 91000\n",
      "    num_steps_sampled: 91000\n",
      "    num_steps_trained: 91000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.38823529411765\n",
      "    ram_util_percent: 74.91176470588233\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04466952039083475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.16553169192691\n",
      "    mean_inference_ms: 1.5000074688931004\n",
      "    mean_raw_obs_processing_ms: 0.440285847938075\n",
      "  time_since_restore: 1066.3401820659637\n",
      "  time_this_iter_s: 12.099255323410034\n",
      "  time_total_s: 1066.3401820659637\n",
      "  timers:\n",
      "    learn_throughput: 1768.906\n",
      "    learn_time_ms: 565.321\n",
      "    load_throughput: 221850.418\n",
      "    load_time_ms: 4.508\n",
      "    sample_throughput: 82.157\n",
      "    sample_time_ms: 12171.813\n",
      "    update_time_ms: 2.07\n",
      "  timestamp: 1631875727\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 91000\n",
      "  training_iteration: 91\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         1066.34</td><td style=\"text-align: right;\">91000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            995.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-48-58\n",
      "  done: false\n",
      "  episode_len_mean: 995.8260869565217\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 92\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1737000624338787\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014493160603834059\n",
      "          policy_loss: 0.01656894056747357\n",
      "          total_loss: 0.002765385475423601\n",
      "          vf_explained_var: -0.8348501324653625\n",
      "          vf_loss: 0.004672482071651353\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.220000000000006\n",
      "    ram_util_percent: 75.49333333333334\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0446620815872818\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.140358914919975\n",
      "    mean_inference_ms: 1.4997158456750475\n",
      "    mean_raw_obs_processing_ms: 0.44319868133872575\n",
      "  time_since_restore: 1076.5752317905426\n",
      "  time_this_iter_s: 10.235049724578857\n",
      "  time_total_s: 1076.5752317905426\n",
      "  timers:\n",
      "    learn_throughput: 1768.306\n",
      "    learn_time_ms: 565.513\n",
      "    load_throughput: 222601.607\n",
      "    load_time_ms: 4.492\n",
      "    sample_throughput: 82.976\n",
      "    sample_time_ms: 12051.717\n",
      "    update_time_ms: 2.126\n",
      "  timestamp: 1631875738\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 92\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         1076.58</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           995.826</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 93000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-49-08\n",
      "  done: false\n",
      "  episode_len_mean: 995.8709677419355\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 93\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5041871706644696\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0156489899415909\n",
      "          policy_loss: -0.017940240270561642\n",
      "          total_loss: -0.03859554926554362\n",
      "          vf_explained_var: 0.22899462282657623\n",
      "          vf_loss: 0.0008655359405868997\n",
      "    num_agent_steps_sampled: 93000\n",
      "    num_agent_steps_trained: 93000\n",
      "    num_steps_sampled: 93000\n",
      "    num_steps_trained: 93000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.77857142857143\n",
      "    ram_util_percent: 75.60000000000001\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044654673444599606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.115603864454553\n",
      "    mean_inference_ms: 1.4994267070007126\n",
      "    mean_raw_obs_processing_ms: 0.44598437634704957\n",
      "  time_since_restore: 1086.5977280139923\n",
      "  time_this_iter_s: 10.022496223449707\n",
      "  time_total_s: 1086.5977280139923\n",
      "  timers:\n",
      "    learn_throughput: 1766.677\n",
      "    learn_time_ms: 566.034\n",
      "    load_throughput: 222521.301\n",
      "    load_time_ms: 4.494\n",
      "    sample_throughput: 83.621\n",
      "    sample_time_ms: 11958.648\n",
      "    update_time_ms: 2.125\n",
      "  timestamp: 1631875748\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 93000\n",
      "  training_iteration: 93\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">          1086.6</td><td style=\"text-align: right;\">93000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           995.871</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 94000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-49-18\n",
      "  done: false\n",
      "  episode_len_mean: 995.9148936170212\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 94\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.47028079562717\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011609878494244013\n",
      "          policy_loss: -0.03845824628240532\n",
      "          total_loss: -0.06015949663188722\n",
      "          vf_explained_var: -0.7552850246429443\n",
      "          vf_loss: 0.000389333875823973\n",
      "    num_agent_steps_sampled: 94000\n",
      "    num_agent_steps_trained: 94000\n",
      "    num_steps_sampled: 94000\n",
      "    num_steps_trained: 94000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.02\n",
      "    ram_util_percent: 75.6\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04464731076575733\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.091261893876341\n",
      "    mean_inference_ms: 1.4991402024763907\n",
      "    mean_raw_obs_processing_ms: 0.44864551551584786\n",
      "  time_since_restore: 1096.647654056549\n",
      "  time_this_iter_s: 10.049926042556763\n",
      "  time_total_s: 1096.647654056549\n",
      "  timers:\n",
      "    learn_throughput: 1764.322\n",
      "    learn_time_ms: 566.79\n",
      "    load_throughput: 222619.329\n",
      "    load_time_ms: 4.492\n",
      "    sample_throughput: 84.207\n",
      "    sample_time_ms: 11875.533\n",
      "    update_time_ms: 2.122\n",
      "  timestamp: 1631875758\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 94000\n",
      "  training_iteration: 94\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         1096.65</td><td style=\"text-align: right;\">94000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           995.915</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 95000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-49-28\n",
      "  done: false\n",
      "  episode_len_mean: 995.9578947368421\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 95\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3567528247833254\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013696210938150008\n",
      "          policy_loss: -0.025338977865046925\n",
      "          total_loss: -0.045410781043271224\n",
      "          vf_explained_var: -0.03521681949496269\n",
      "          vf_loss: 0.00041407672753040163\n",
      "    num_agent_steps_sampled: 95000\n",
      "    num_agent_steps_trained: 95000\n",
      "    num_steps_sampled: 95000\n",
      "    num_steps_trained: 95000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.07857142857142\n",
      "    ram_util_percent: 75.67142857142858\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04463996576217892\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.067313040298766\n",
      "    mean_inference_ms: 1.4988565523167254\n",
      "    mean_raw_obs_processing_ms: 0.4511878251212697\n",
      "  time_since_restore: 1106.615783214569\n",
      "  time_this_iter_s: 9.96812915802002\n",
      "  time_total_s: 1106.615783214569\n",
      "  timers:\n",
      "    learn_throughput: 1764.401\n",
      "    learn_time_ms: 566.765\n",
      "    load_throughput: 222291.332\n",
      "    load_time_ms: 4.499\n",
      "    sample_throughput: 84.678\n",
      "    sample_time_ms: 11809.388\n",
      "    update_time_ms: 2.118\n",
      "  timestamp: 1631875768\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 95000\n",
      "  training_iteration: 95\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         1106.62</td><td style=\"text-align: right;\">95000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           995.958</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-49-38\n",
      "  done: false\n",
      "  episode_len_mean: 996.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 96\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.378947318924798\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012522948391086608\n",
      "          policy_loss: -0.10042374812894397\n",
      "          total_loss: -0.12074660344256295\n",
      "          vf_explained_var: -0.056306347250938416\n",
      "          vf_loss: 0.0006489520932922864\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.52857142857142\n",
      "    ram_util_percent: 75.59285714285714\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044632657062406024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.043737157773785\n",
      "    mean_inference_ms: 1.4985745255087763\n",
      "    mean_raw_obs_processing_ms: 0.45361578702073907\n",
      "  time_since_restore: 1116.4935529232025\n",
      "  time_this_iter_s: 9.877769708633423\n",
      "  time_total_s: 1116.4935529232025\n",
      "  timers:\n",
      "    learn_throughput: 1762.758\n",
      "    learn_time_ms: 567.293\n",
      "    load_throughput: 222719.81\n",
      "    load_time_ms: 4.49\n",
      "    sample_throughput: 85.217\n",
      "    sample_time_ms: 11734.73\n",
      "    update_time_ms: 2.119\n",
      "  timestamp: 1631875778\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 96\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         1116.49</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">               996</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 97000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-49-48\n",
      "  done: false\n",
      "  episode_len_mean: 996.0412371134021\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 97\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3278475761413575\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015337523812663612\n",
      "          policy_loss: -0.08606818922691875\n",
      "          total_loss: -0.10528165093726582\n",
      "          vf_explained_var: -0.3044115900993347\n",
      "          vf_loss: 0.0006140718483948149\n",
      "    num_agent_steps_sampled: 97000\n",
      "    num_agent_steps_trained: 97000\n",
      "    num_steps_sampled: 97000\n",
      "    num_steps_trained: 97000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.457142857142856\n",
      "    ram_util_percent: 75.69285714285716\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044625396263720245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 11.020529098333165\n",
      "    mean_inference_ms: 1.4982968237938195\n",
      "    mean_raw_obs_processing_ms: 0.4559341651207743\n",
      "  time_since_restore: 1126.414778470993\n",
      "  time_this_iter_s: 9.921225547790527\n",
      "  time_total_s: 1126.414778470993\n",
      "  timers:\n",
      "    learn_throughput: 1760.838\n",
      "    learn_time_ms: 567.911\n",
      "    load_throughput: 223399.539\n",
      "    load_time_ms: 4.476\n",
      "    sample_throughput: 85.867\n",
      "    sample_time_ms: 11645.86\n",
      "    update_time_ms: 2.21\n",
      "  timestamp: 1631875788\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 97000\n",
      "  training_iteration: 97\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         1126.41</td><td style=\"text-align: right;\">97000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.041</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 98000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-49-58\n",
      "  done: false\n",
      "  episode_len_mean: 996.0816326530612\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 98\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3533231258392333\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016892694009523333\n",
      "          policy_loss: -0.11206815648410055\n",
      "          total_loss: -0.13129728767606946\n",
      "          vf_explained_var: -0.4673505127429962\n",
      "          vf_loss: 0.0005032444775376158\n",
      "    num_agent_steps_sampled: 98000\n",
      "    num_agent_steps_trained: 98000\n",
      "    num_steps_sampled: 98000\n",
      "    num_steps_trained: 98000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.042857142857144\n",
      "    ram_util_percent: 75.77142857142859\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04461823218567815\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.997687137587452\n",
      "    mean_inference_ms: 1.4980222513893153\n",
      "    mean_raw_obs_processing_ms: 0.45814752781355983\n",
      "  time_since_restore: 1136.4086060523987\n",
      "  time_this_iter_s: 9.99382758140564\n",
      "  time_total_s: 1136.4086060523987\n",
      "  timers:\n",
      "    learn_throughput: 1757.006\n",
      "    learn_time_ms: 569.15\n",
      "    load_throughput: 221946.682\n",
      "    load_time_ms: 4.506\n",
      "    sample_throughput: 86.532\n",
      "    sample_time_ms: 11556.46\n",
      "    update_time_ms: 2.206\n",
      "  timestamp: 1631875798\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 98000\n",
      "  training_iteration: 98\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         1136.41</td><td style=\"text-align: right;\">98000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.082</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 99000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-50-08\n",
      "  done: false\n",
      "  episode_len_mean: 996.1212121212121\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 99\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3851333459218345\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0146051223591065\n",
      "          policy_loss: 0.025760224295987025\n",
      "          total_loss: 0.005714321053690381\n",
      "          vf_explained_var: -0.6313683390617371\n",
      "          vf_loss: 0.0005192784621613101\n",
      "    num_agent_steps_sampled: 99000\n",
      "    num_agent_steps_trained: 99000\n",
      "    num_steps_sampled: 99000\n",
      "    num_steps_trained: 99000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.693333333333335\n",
      "    ram_util_percent: 75.86666666666666\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04461109235756285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.975233161102757\n",
      "    mean_inference_ms: 1.4977495342917713\n",
      "    mean_raw_obs_processing_ms: 0.46026024395554443\n",
      "  time_since_restore: 1146.6719584465027\n",
      "  time_this_iter_s: 10.263352394104004\n",
      "  time_total_s: 1146.6719584465027\n",
      "  timers:\n",
      "    learn_throughput: 1758.804\n",
      "    learn_time_ms: 568.568\n",
      "    load_throughput: 221755.41\n",
      "    load_time_ms: 4.509\n",
      "    sample_throughput: 86.928\n",
      "    sample_time_ms: 11503.82\n",
      "    update_time_ms: 2.288\n",
      "  timestamp: 1631875808\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 99000\n",
      "  training_iteration: 99\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         1146.67</td><td style=\"text-align: right;\">99000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">           996.121</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-50-18\n",
      "  done: false\n",
      "  episode_len_mean: 996.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 100\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.33506817817688\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014819972101785123\n",
      "          policy_loss: -0.019141521263453695\n",
      "          total_loss: -0.03849135459297233\n",
      "          vf_explained_var: -0.4793490469455719\n",
      "          vf_loss: 0.0006663544664560403\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.28666666666666\n",
      "    ram_util_percent: 75.90666666666667\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04460396694567477\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.953136382346184\n",
      "    mean_inference_ms: 1.497478560332088\n",
      "    mean_raw_obs_processing_ms: 0.46227647982284514\n",
      "  time_since_restore: 1156.7434186935425\n",
      "  time_this_iter_s: 10.071460247039795\n",
      "  time_total_s: 1156.7434186935425\n",
      "  timers:\n",
      "    learn_throughput: 1755.232\n",
      "    learn_time_ms: 569.725\n",
      "    load_throughput: 309364.646\n",
      "    load_time_ms: 3.232\n",
      "    sample_throughput: 103.404\n",
      "    sample_time_ms: 9670.826\n",
      "    update_time_ms: 2.834\n",
      "  timestamp: 1631875818\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 100\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         1156.74</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 101000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-50-27\n",
      "  done: false\n",
      "  episode_len_mean: 996.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 101\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6385541094674005\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014032478681066054\n",
      "          policy_loss: -0.03385034576058388\n",
      "          total_loss: -0.054990567887822785\n",
      "          vf_explained_var: -0.562504231929779\n",
      "          vf_loss: 0.0020880094500373364\n",
      "    num_agent_steps_sampled: 101000\n",
      "    num_agent_steps_trained: 101000\n",
      "    num_steps_sampled: 101000\n",
      "    num_steps_trained: 101000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.93076923076924\n",
      "    ram_util_percent: 76.02307692307693\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04454817518174853\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.443702766018767\n",
      "    mean_inference_ms: 1.4954514380096449\n",
      "    mean_raw_obs_processing_ms: 0.46713799406680223\n",
      "  time_since_restore: 1166.1343877315521\n",
      "  time_this_iter_s: 9.390969038009644\n",
      "  time_total_s: 1166.1343877315521\n",
      "  timers:\n",
      "    learn_throughput: 1764.756\n",
      "    learn_time_ms: 566.651\n",
      "    load_throughput: 309451.38\n",
      "    load_time_ms: 3.232\n",
      "    sample_throughput: 106.348\n",
      "    sample_time_ms: 9403.104\n",
      "    update_time_ms: 2.84\n",
      "  timestamp: 1631875827\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 101000\n",
      "  training_iteration: 101\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         1166.13</td><td style=\"text-align: right;\">101000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 102000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-50-37\n",
      "  done: false\n",
      "  episode_len_mean: 996.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.03\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 102\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.228032620747884\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01257099407833187\n",
      "          policy_loss: -0.004417854299147924\n",
      "          total_loss: -0.023405158354176416\n",
      "          vf_explained_var: -0.21762360632419586\n",
      "          vf_loss: 0.000464551295994574\n",
      "    num_agent_steps_sampled: 102000\n",
      "    num_agent_steps_trained: 102000\n",
      "    num_steps_sampled: 102000\n",
      "    num_steps_trained: 102000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.20714285714286\n",
      "    ram_util_percent: 76.16428571428573\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044515788279095324\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.1874790843782\n",
      "    mean_inference_ms: 1.4940537110023262\n",
      "    mean_raw_obs_processing_ms: 0.47211524574622565\n",
      "  time_since_restore: 1176.1221735477448\n",
      "  time_this_iter_s: 9.987785816192627\n",
      "  time_total_s: 1176.1221735477448\n",
      "  timers:\n",
      "    learn_throughput: 1761.148\n",
      "    learn_time_ms: 567.812\n",
      "    load_throughput: 310156.176\n",
      "    load_time_ms: 3.224\n",
      "    sample_throughput: 106.641\n",
      "    sample_time_ms: 9377.233\n",
      "    update_time_ms: 2.812\n",
      "  timestamp: 1631875837\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 102000\n",
      "  training_iteration: 102\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         1176.12</td><td style=\"text-align: right;\">102000</td><td style=\"text-align: right;\">    0.03</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 103000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-50-48\n",
      "  done: false\n",
      "  episode_len_mean: 996.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.03\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 103\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2439244402779472\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013141881351012448\n",
      "          policy_loss: -0.10267029624018405\n",
      "          total_loss: -0.12186826144655545\n",
      "          vf_explained_var: -0.2385166436433792\n",
      "          vf_loss: 0.00028435247950255873\n",
      "    num_agent_steps_sampled: 103000\n",
      "    num_agent_steps_trained: 103000\n",
      "    num_steps_sampled: 103000\n",
      "    num_steps_trained: 103000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.31333333333334\n",
      "    ram_util_percent: 76.22666666666669\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04448013585248198\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 10.01539554139447\n",
      "    mean_inference_ms: 1.492555405785832\n",
      "    mean_raw_obs_processing_ms: 0.4771181553240977\n",
      "  time_since_restore: 1186.1657984256744\n",
      "  time_this_iter_s: 10.043624877929688\n",
      "  time_total_s: 1186.1657984256744\n",
      "  timers:\n",
      "    learn_throughput: 1744.642\n",
      "    learn_time_ms: 573.184\n",
      "    load_throughput: 306978.16\n",
      "    load_time_ms: 3.258\n",
      "    sample_throughput: 106.678\n",
      "    sample_time_ms: 9373.989\n",
      "    update_time_ms: 2.806\n",
      "  timestamp: 1631875848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 103000\n",
      "  training_iteration: 103\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         1186.17</td><td style=\"text-align: right;\">103000</td><td style=\"text-align: right;\">    0.03</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-50-58\n",
      "  done: false\n",
      "  episode_len_mean: 996.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.03\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 104\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.329834474457635\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011376181083482904\n",
      "          policy_loss: -0.0075157697002093\n",
      "          total_loss: -0.027839519249068367\n",
      "          vf_explained_var: -0.2900114357471466\n",
      "          vf_loss: 0.0004149547651953374\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.371428571428574\n",
      "    ram_util_percent: 76.30714285714284\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04445166665827525\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.886515025873328\n",
      "    mean_inference_ms: 1.4912856878030665\n",
      "    mean_raw_obs_processing_ms: 0.4821222434594353\n",
      "  time_since_restore: 1196.2427172660828\n",
      "  time_this_iter_s: 10.076918840408325\n",
      "  time_total_s: 1196.2427172660828\n",
      "  timers:\n",
      "    learn_throughput: 1743.698\n",
      "    learn_time_ms: 573.494\n",
      "    load_throughput: 309047.798\n",
      "    load_time_ms: 3.236\n",
      "    sample_throughput: 106.651\n",
      "    sample_time_ms: 9376.371\n",
      "    update_time_ms: 2.822\n",
      "  timestamp: 1631875858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 104\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         1196.24</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">    0.03</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 105000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-51-08\n",
      "  done: false\n",
      "  episode_len_mean: 996.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.03\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 105\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.234820583131578\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012359977107048791\n",
      "          policy_loss: 0.11091310431559881\n",
      "          total_loss: 0.09158619286285506\n",
      "          vf_explained_var: -0.17708495259284973\n",
      "          vf_loss: 0.00024029945917492215\n",
      "    num_agent_steps_sampled: 105000\n",
      "    num_agent_steps_trained: 105000\n",
      "    num_steps_sampled: 105000\n",
      "    num_steps_trained: 105000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.04666666666666\n",
      "    ram_util_percent: 76.44666666666667\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044430937402692036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.786926631681258\n",
      "    mean_inference_ms: 1.49031291335777\n",
      "    mean_raw_obs_processing_ms: 0.48711848520581674\n",
      "  time_since_restore: 1206.4909501075745\n",
      "  time_this_iter_s: 10.2482328414917\n",
      "  time_total_s: 1206.4909501075745\n",
      "  timers:\n",
      "    learn_throughput: 1742.458\n",
      "    learn_time_ms: 573.902\n",
      "    load_throughput: 308947.636\n",
      "    load_time_ms: 3.237\n",
      "    sample_throughput: 106.339\n",
      "    sample_time_ms: 9403.919\n",
      "    update_time_ms: 2.828\n",
      "  timestamp: 1631875868\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 105000\n",
      "  training_iteration: 105\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         1206.49</td><td style=\"text-align: right;\">105000</td><td style=\"text-align: right;\">    0.03</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 106000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-51-18\n",
      "  done: false\n",
      "  episode_len_mean: 996.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.03\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 106\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4684633678860135\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01291723030767865\n",
      "          policy_loss: 0.04956422108742926\n",
      "          total_loss: 0.028185729185740152\n",
      "          vf_explained_var: -0.044950131326913834\n",
      "          vf_loss: 0.00039976513192717297\n",
      "    num_agent_steps_sampled: 106000\n",
      "    num_agent_steps_trained: 106000\n",
      "    num_steps_sampled: 106000\n",
      "    num_steps_trained: 106000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.028571428571425\n",
      "    ram_util_percent: 76.53571428571429\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04441450330292232\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.70677385827241\n",
      "    mean_inference_ms: 1.4894992521484434\n",
      "    mean_raw_obs_processing_ms: 0.4920225115703818\n",
      "  time_since_restore: 1216.6532933712006\n",
      "  time_this_iter_s: 10.162343263626099\n",
      "  time_total_s: 1216.6532933712006\n",
      "  timers:\n",
      "    learn_throughput: 1743.989\n",
      "    learn_time_ms: 573.398\n",
      "    load_throughput: 301555.407\n",
      "    load_time_ms: 3.316\n",
      "    sample_throughput: 106.014\n",
      "    sample_time_ms: 9432.697\n",
      "    update_time_ms: 2.842\n",
      "  timestamp: 1631875878\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 106000\n",
      "  training_iteration: 106\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         1216.65</td><td style=\"text-align: right;\">106000</td><td style=\"text-align: right;\">    0.03</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 107000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-51-27\n",
      "  done: false\n",
      "  episode_len_mean: 996.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.03\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 107\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.616463836034139\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008815318944531587\n",
      "          policy_loss: -0.08914613330529796\n",
      "          total_loss: -0.11303085654249621\n",
      "          vf_explained_var: 0.04435562714934349\n",
      "          vf_loss: 0.00029646953058646533\n",
      "    num_agent_steps_sampled: 107000\n",
      "    num_agent_steps_trained: 107000\n",
      "    num_steps_sampled: 107000\n",
      "    num_steps_trained: 107000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.07142857142856\n",
      "    ram_util_percent: 76.62142857142858\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044399676078875606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.640206815974594\n",
      "    mean_inference_ms: 1.4887952031193665\n",
      "    mean_raw_obs_processing_ms: 0.496902802254502\n",
      "  time_since_restore: 1226.0565576553345\n",
      "  time_this_iter_s: 9.403264284133911\n",
      "  time_total_s: 1226.0565576553345\n",
      "  timers:\n",
      "    learn_throughput: 1738.532\n",
      "    learn_time_ms: 575.198\n",
      "    load_throughput: 298461.122\n",
      "    load_time_ms: 3.351\n",
      "    sample_throughput: 106.619\n",
      "    sample_time_ms: 9379.17\n",
      "    update_time_ms: 2.755\n",
      "  timestamp: 1631875887\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 107000\n",
      "  training_iteration: 107\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         1226.06</td><td style=\"text-align: right;\">107000</td><td style=\"text-align: right;\">    0.03</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-51-37\n",
      "  done: false\n",
      "  episode_len_mean: 996.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.03\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 108\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6795962466133965\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0065019596031877964\n",
      "          policy_loss: -0.0355247702035639\n",
      "          total_loss: -0.06075565762196978\n",
      "          vf_explained_var: 0.14673630893230438\n",
      "          vf_loss: 0.00010213437617999767\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.66153846153846\n",
      "    ram_util_percent: 76.70000000000002\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04438245881804623\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.583514435621694\n",
      "    mean_inference_ms: 1.4880555599338228\n",
      "    mean_raw_obs_processing_ms: 0.5017437100651532\n",
      "  time_since_restore: 1235.2918767929077\n",
      "  time_this_iter_s: 9.235319137573242\n",
      "  time_total_s: 1235.2918767929077\n",
      "  timers:\n",
      "    learn_throughput: 1744.326\n",
      "    learn_time_ms: 573.287\n",
      "    load_throughput: 301250.018\n",
      "    load_time_ms: 3.32\n",
      "    sample_throughput: 107.465\n",
      "    sample_time_ms: 9305.32\n",
      "    update_time_ms: 2.761\n",
      "  timestamp: 1631875897\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 108\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         1235.29</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">    0.03</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 109000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-51-48\n",
      "  done: false\n",
      "  episode_len_mean: 996.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.03\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 109\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2705948564741347\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013903879027018777\n",
      "          policy_loss: -0.06654291972517967\n",
      "          total_loss: -0.08534948122170237\n",
      "          vf_explained_var: -0.2536120116710663\n",
      "          vf_loss: 0.0007710111809299431\n",
      "    num_agent_steps_sampled: 109000\n",
      "    num_agent_steps_trained: 109000\n",
      "    num_steps_sampled: 109000\n",
      "    num_steps_trained: 109000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.556250000000006\n",
      "    ram_util_percent: 76.42499999999998\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04436666575752429\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.533741918646278\n",
      "    mean_inference_ms: 1.4873378985394203\n",
      "    mean_raw_obs_processing_ms: 0.5065530555182345\n",
      "  time_since_restore: 1246.2024638652802\n",
      "  time_this_iter_s: 10.910587072372437\n",
      "  time_total_s: 1246.2024638652802\n",
      "  timers:\n",
      "    learn_throughput: 1736.009\n",
      "    learn_time_ms: 576.034\n",
      "    load_throughput: 299740.872\n",
      "    load_time_ms: 3.336\n",
      "    sample_throughput: 106.754\n",
      "    sample_time_ms: 9367.367\n",
      "    update_time_ms: 2.671\n",
      "  timestamp: 1631875908\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 109000\n",
      "  training_iteration: 109\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">          1246.2</td><td style=\"text-align: right;\">109000</td><td style=\"text-align: right;\">    0.03</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 110000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-51-58\n",
      "  done: false\n",
      "  episode_len_mean: 996.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.03\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 110\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4908366998036704\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011678733478151947\n",
      "          policy_loss: 0.006224148803287082\n",
      "          total_loss: -0.015637447685003282\n",
      "          vf_explained_var: 0.1255892813205719\n",
      "          vf_loss: 0.0004190534317785932\n",
      "    num_agent_steps_sampled: 110000\n",
      "    num_agent_steps_trained: 110000\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.442857142857136\n",
      "    ram_util_percent: 76.11428571428573\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04435203038296882\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.489931080103853\n",
      "    mean_inference_ms: 1.4866465317771624\n",
      "    mean_raw_obs_processing_ms: 0.5113133507580017\n",
      "  time_since_restore: 1256.3463537693024\n",
      "  time_this_iter_s: 10.143889904022217\n",
      "  time_total_s: 1256.3463537693024\n",
      "  timers:\n",
      "    learn_throughput: 1737.072\n",
      "    learn_time_ms: 575.681\n",
      "    load_throughput: 299389.985\n",
      "    load_time_ms: 3.34\n",
      "    sample_throughput: 106.659\n",
      "    sample_time_ms: 9375.634\n",
      "    update_time_ms: 2.119\n",
      "  timestamp: 1631875918\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 110\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         1256.35</td><td style=\"text-align: right;\">110000</td><td style=\"text-align: right;\">    0.03</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 111000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-52-07\n",
      "  done: false\n",
      "  episode_len_mean: 996.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.03\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 111\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.670605527030097\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012008547240777764\n",
      "          policy_loss: -0.03354993354943064\n",
      "          total_loss: -0.057421627640724185\n",
      "          vf_explained_var: -0.35138005018234253\n",
      "          vf_loss: 0.00013243623487445196\n",
      "    num_agent_steps_sampled: 111000\n",
      "    num_agent_steps_trained: 111000\n",
      "    num_steps_sampled: 111000\n",
      "    num_steps_trained: 111000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.03571428571428\n",
      "    ram_util_percent: 76.07857142857142\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0443359544960348\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.449756265103947\n",
      "    mean_inference_ms: 1.4859456303091374\n",
      "    mean_raw_obs_processing_ms: 0.5160294857437723\n",
      "  time_since_restore: 1265.8459038734436\n",
      "  time_this_iter_s: 9.499550104141235\n",
      "  time_total_s: 1265.8459038734436\n",
      "  timers:\n",
      "    learn_throughput: 1733.63\n",
      "    learn_time_ms: 576.824\n",
      "    load_throughput: 298888.62\n",
      "    load_time_ms: 3.346\n",
      "    sample_throughput: 106.55\n",
      "    sample_time_ms: 9385.279\n",
      "    update_time_ms: 2.123\n",
      "  timestamp: 1631875927\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 111000\n",
      "  training_iteration: 111\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         1265.85</td><td style=\"text-align: right;\">111000</td><td style=\"text-align: right;\">    0.03</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-52-17\n",
      "  done: false\n",
      "  episode_len_mean: 996.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 112\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.598152420255873\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011835312859313676\n",
      "          policy_loss: 0.03562541815141837\n",
      "          total_loss: 0.012424240840805902\n",
      "          vf_explained_var: -0.24544309079647064\n",
      "          vf_loss: 0.00011739909999353889\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.07857142857143\n",
      "    ram_util_percent: 75.92857142857143\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04432077900373099\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.413209791217456\n",
      "    mean_inference_ms: 1.4852998983187728\n",
      "    mean_raw_obs_processing_ms: 0.520707538828675\n",
      "  time_since_restore: 1275.7126739025116\n",
      "  time_this_iter_s: 9.866770029067993\n",
      "  time_total_s: 1275.7126739025116\n",
      "  timers:\n",
      "    learn_throughput: 1736.338\n",
      "    learn_time_ms: 575.925\n",
      "    load_throughput: 298680.035\n",
      "    load_time_ms: 3.348\n",
      "    sample_throughput: 106.677\n",
      "    sample_time_ms: 9374.111\n",
      "    update_time_ms: 2.114\n",
      "  timestamp: 1631875937\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 112\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         1275.71</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 113000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-52-27\n",
      "  done: false\n",
      "  episode_len_mean: 996.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 113\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.511222653918796\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014354732952689315\n",
      "          policy_loss: 0.06534520942303869\n",
      "          total_loss: 0.04440303337242868\n",
      "          vf_explained_var: -0.478215754032135\n",
      "          vf_loss: 0.0009402365784303078\n",
      "    num_agent_steps_sampled: 113000\n",
      "    num_agent_steps_trained: 113000\n",
      "    num_steps_sampled: 113000\n",
      "    num_steps_trained: 113000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.15000000000001\n",
      "    ram_util_percent: 75.6857142857143\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04430615767668653\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.379352262586137\n",
      "    mean_inference_ms: 1.4846807986544581\n",
      "    mean_raw_obs_processing_ms: 0.5253537760208998\n",
      "  time_since_restore: 1285.5491433143616\n",
      "  time_this_iter_s: 9.836469411849976\n",
      "  time_total_s: 1285.5491433143616\n",
      "  timers:\n",
      "    learn_throughput: 1754.412\n",
      "    learn_time_ms: 569.992\n",
      "    load_throughput: 297362.921\n",
      "    load_time_ms: 3.363\n",
      "    sample_throughput: 106.846\n",
      "    sample_time_ms: 9359.287\n",
      "    update_time_ms: 2.132\n",
      "  timestamp: 1631875947\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 113000\n",
      "  training_iteration: 113\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         1285.55</td><td style=\"text-align: right;\">113000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 114000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-52-37\n",
      "  done: false\n",
      "  episode_len_mean: 996.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 114\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.635335593753391\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01704577231767084\n",
      "          policy_loss: -0.03616515166229672\n",
      "          total_loss: -0.05830503288242552\n",
      "          vf_explained_var: 0.2082584798336029\n",
      "          vf_loss: 0.00037817718865779977\n",
      "    num_agent_steps_sampled: 114000\n",
      "    num_agent_steps_trained: 114000\n",
      "    num_steps_sampled: 114000\n",
      "    num_steps_trained: 114000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.34285714285715\n",
      "    ram_util_percent: 75.54285714285713\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04429193240938947\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.34754797904794\n",
      "    mean_inference_ms: 1.4840598328577326\n",
      "    mean_raw_obs_processing_ms: 0.5299664774800852\n",
      "  time_since_restore: 1295.1554069519043\n",
      "  time_this_iter_s: 9.606263637542725\n",
      "  time_total_s: 1295.1554069519043\n",
      "  timers:\n",
      "    learn_throughput: 1754.396\n",
      "    learn_time_ms: 569.997\n",
      "    load_throughput: 299471.215\n",
      "    load_time_ms: 3.339\n",
      "    sample_throughput: 107.386\n",
      "    sample_time_ms: 9312.202\n",
      "    update_time_ms: 2.123\n",
      "  timestamp: 1631875957\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 114000\n",
      "  training_iteration: 114\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         1295.16</td><td style=\"text-align: right;\">114000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 115000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-52-48\n",
      "  done: false\n",
      "  episode_len_mean: 996.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 115\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4307548734876843\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014512968353878808\n",
      "          policy_loss: -0.04303381654123465\n",
      "          total_loss: -0.06314179938700464\n",
      "          vf_explained_var: -0.07781492918729782\n",
      "          vf_loss: 0.000934146051036401\n",
      "    num_agent_steps_sampled: 115000\n",
      "    num_agent_steps_trained: 115000\n",
      "    num_steps_sampled: 115000\n",
      "    num_steps_trained: 115000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.99333333333333\n",
      "    ram_util_percent: 75.53333333333332\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04427930086961213\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.317982532805912\n",
      "    mean_inference_ms: 1.4834994971033195\n",
      "    mean_raw_obs_processing_ms: 0.5345453862171151\n",
      "  time_since_restore: 1306.0987393856049\n",
      "  time_this_iter_s: 10.943332433700562\n",
      "  time_total_s: 1306.0987393856049\n",
      "  timers:\n",
      "    learn_throughput: 1748.609\n",
      "    learn_time_ms: 571.883\n",
      "    load_throughput: 297110.151\n",
      "    load_time_ms: 3.366\n",
      "    sample_throughput: 106.612\n",
      "    sample_time_ms: 9379.794\n",
      "    update_time_ms: 2.114\n",
      "  timestamp: 1631875968\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 115000\n",
      "  training_iteration: 115\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">          1306.1</td><td style=\"text-align: right;\">115000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-52-59\n",
      "  done: false\n",
      "  episode_len_mean: 996.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 116\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.43679100672404\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014152944386818293\n",
      "          policy_loss: 0.07838431662983364\n",
      "          total_loss: 0.05765621612469355\n",
      "          vf_explained_var: -0.9456408023834229\n",
      "          vf_loss: 0.00045539490626348805\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.25\n",
      "    ram_util_percent: 75.44375\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04426489294476696\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.290297018143034\n",
      "    mean_inference_ms: 1.4829814100410084\n",
      "    mean_raw_obs_processing_ms: 0.5390904438336489\n",
      "  time_since_restore: 1316.9294550418854\n",
      "  time_this_iter_s: 10.830715656280518\n",
      "  time_total_s: 1316.9294550418854\n",
      "  timers:\n",
      "    learn_throughput: 1739.489\n",
      "    learn_time_ms: 574.882\n",
      "    load_throughput: 298257.376\n",
      "    load_time_ms: 3.353\n",
      "    sample_throughput: 105.891\n",
      "    sample_time_ms: 9443.714\n",
      "    update_time_ms: 2.124\n",
      "  timestamp: 1631875979\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 116\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         1316.93</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 117000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-53-09\n",
      "  done: false\n",
      "  episode_len_mean: 996.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 117\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.286124246650272\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013638040306789436\n",
      "          policy_loss: -0.03452004517118136\n",
      "          total_loss: -0.03868125304579735\n",
      "          vf_explained_var: -0.538605272769928\n",
      "          vf_loss: 0.015631472340869045\n",
      "    num_agent_steps_sampled: 117000\n",
      "    num_agent_steps_trained: 117000\n",
      "    num_steps_sampled: 117000\n",
      "    num_steps_trained: 117000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.63571428571429\n",
      "    ram_util_percent: 75.47142857142858\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04425164796675188\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.264211900461298\n",
      "    mean_inference_ms: 1.4824966849041326\n",
      "    mean_raw_obs_processing_ms: 0.5436011805090686\n",
      "  time_since_restore: 1327.2659895420074\n",
      "  time_this_iter_s: 10.33653450012207\n",
      "  time_total_s: 1327.2659895420074\n",
      "  timers:\n",
      "    learn_throughput: 1745.116\n",
      "    learn_time_ms: 573.028\n",
      "    load_throughput: 300867.532\n",
      "    load_time_ms: 3.324\n",
      "    sample_throughput: 104.833\n",
      "    sample_time_ms: 9538.939\n",
      "    update_time_ms: 2.121\n",
      "  timestamp: 1631875989\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 117000\n",
      "  training_iteration: 117\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         1327.27</td><td style=\"text-align: right;\">117000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 118000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-53-21\n",
      "  done: false\n",
      "  episode_len_mean: 996.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 118\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5500989225175648\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01781948772492298\n",
      "          policy_loss: -0.052633858016795584\n",
      "          total_loss: -0.07323646197716395\n",
      "          vf_explained_var: -0.5294155478477478\n",
      "          vf_loss: 0.0008890019137955582\n",
      "    num_agent_steps_sampled: 118000\n",
      "    num_agent_steps_trained: 118000\n",
      "    num_steps_sampled: 118000\n",
      "    num_steps_trained: 118000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.91111111111111\n",
      "    ram_util_percent: 75.58888888888887\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044239784328763194\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.239673574451414\n",
      "    mean_inference_ms: 1.482053044495745\n",
      "    mean_raw_obs_processing_ms: 0.5480814227464936\n",
      "  time_since_restore: 1339.502444267273\n",
      "  time_this_iter_s: 12.236454725265503\n",
      "  time_total_s: 1339.502444267273\n",
      "  timers:\n",
      "    learn_throughput: 1735.534\n",
      "    learn_time_ms: 576.191\n",
      "    load_throughput: 276031.853\n",
      "    load_time_ms: 3.623\n",
      "    sample_throughput: 101.672\n",
      "    sample_time_ms: 9835.56\n",
      "    update_time_ms: 2.115\n",
      "  timestamp: 1631876001\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 118000\n",
      "  training_iteration: 118\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">          1339.5</td><td style=\"text-align: right;\">118000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 119000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-53-34\n",
      "  done: false\n",
      "  episode_len_mean: 996.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 119\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.600300563706292\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013747416490008697\n",
      "          policy_loss: -0.032491764922936754\n",
      "          total_loss: -0.05479192706859774\n",
      "          vf_explained_var: -0.7009096741676331\n",
      "          vf_loss: 0.0006096741616299066\n",
      "    num_agent_steps_sampled: 119000\n",
      "    num_agent_steps_trained: 119000\n",
      "    num_steps_sampled: 119000\n",
      "    num_steps_trained: 119000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.38333333333334\n",
      "    ram_util_percent: 75.8611111111111\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044228609514907286\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.21657052022343\n",
      "    mean_inference_ms: 1.481644579797608\n",
      "    mean_raw_obs_processing_ms: 0.552520993786549\n",
      "  time_since_restore: 1351.9565644264221\n",
      "  time_this_iter_s: 12.45412015914917\n",
      "  time_total_s: 1351.9565644264221\n",
      "  timers:\n",
      "    learn_throughput: 1721.273\n",
      "    learn_time_ms: 580.966\n",
      "    load_throughput: 265739.792\n",
      "    load_time_ms: 3.763\n",
      "    sample_throughput: 100.153\n",
      "    sample_time_ms: 9984.711\n",
      "    update_time_ms: 2.224\n",
      "  timestamp: 1631876014\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 119000\n",
      "  training_iteration: 119\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         1351.96</td><td style=\"text-align: right;\">119000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            996.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1256)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1256)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-54-03\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 120\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2697140746646456\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025011073133136483\n",
      "          policy_loss: 0.01547949943277571\n",
      "          total_loss: -0.0005160260531637404\n",
      "          vf_explained_var: -0.07809901237487793\n",
      "          vf_loss: 0.0010741208896635928\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.982926829268294\n",
      "    ram_util_percent: 75.77317073170731\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044218288711936095\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.194631591858657\n",
      "    mean_inference_ms: 1.481258859638642\n",
      "    mean_raw_obs_processing_ms: 0.5583909471419002\n",
      "  time_since_restore: 1380.8900225162506\n",
      "  time_this_iter_s: 28.93345808982849\n",
      "  time_total_s: 1380.8900225162506\n",
      "  timers:\n",
      "    learn_throughput: 1719.323\n",
      "    learn_time_ms: 581.624\n",
      "    load_throughput: 217709.493\n",
      "    load_time_ms: 4.593\n",
      "    sample_throughput: 84.31\n",
      "    sample_time_ms: 11860.967\n",
      "    update_time_ms: 2.892\n",
      "  timestamp: 1631876043\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 120\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">         1380.89</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 121000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-54-13\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.07\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 121\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.344082082642449\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015302747945573852\n",
      "          policy_loss: -0.08017009405626191\n",
      "          total_loss: -0.09768477976322174\n",
      "          vf_explained_var: -0.30596673488616943\n",
      "          vf_loss: 0.0007614588453887134\n",
      "    num_agent_steps_sampled: 121000\n",
      "    num_agent_steps_trained: 121000\n",
      "    num_steps_sampled: 121000\n",
      "    num_steps_trained: 121000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.79333333333333\n",
      "    ram_util_percent: 75.57333333333332\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044208700453399634\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.173674192309843\n",
      "    mean_inference_ms: 1.4808940127053865\n",
      "    mean_raw_obs_processing_ms: 0.5642130937543954\n",
      "  time_since_restore: 1391.4690909385681\n",
      "  time_this_iter_s: 10.579068422317505\n",
      "  time_total_s: 1391.4690909385681\n",
      "  timers:\n",
      "    learn_throughput: 1722.449\n",
      "    learn_time_ms: 580.569\n",
      "    load_throughput: 217879.131\n",
      "    load_time_ms: 4.59\n",
      "    sample_throughput: 83.542\n",
      "    sample_time_ms: 11970.033\n",
      "    update_time_ms: 2.889\n",
      "  timestamp: 1631876053\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 121000\n",
      "  training_iteration: 121\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         1391.47</td><td style=\"text-align: right;\">121000</td><td style=\"text-align: right;\">    0.07</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 122000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-54-24\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.07\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 122\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2481180906295775\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013489565098883311\n",
      "          policy_loss: -0.0032812623100148307\n",
      "          total_loss: -0.020197499502036306\n",
      "          vf_explained_var: 0.14110177755355835\n",
      "          vf_loss: 0.0010122129722731188\n",
      "    num_agent_steps_sampled: 122000\n",
      "    num_agent_steps_trained: 122000\n",
      "    num_steps_sampled: 122000\n",
      "    num_steps_trained: 122000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.960000000000015\n",
      "    ram_util_percent: 75.51333333333334\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04419697016947746\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.153634651018995\n",
      "    mean_inference_ms: 1.4805463049406478\n",
      "    mean_raw_obs_processing_ms: 0.5699891739779975\n",
      "  time_since_restore: 1401.839516878128\n",
      "  time_this_iter_s: 10.370425939559937\n",
      "  time_total_s: 1401.839516878128\n",
      "  timers:\n",
      "    learn_throughput: 1723.689\n",
      "    learn_time_ms: 580.151\n",
      "    load_throughput: 217647.358\n",
      "    load_time_ms: 4.595\n",
      "    sample_throughput: 83.189\n",
      "    sample_time_ms: 12020.824\n",
      "    update_time_ms: 2.876\n",
      "  timestamp: 1631876064\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 122000\n",
      "  training_iteration: 122\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         1401.84</td><td style=\"text-align: right;\">122000</td><td style=\"text-align: right;\">    0.07</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 123000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-54-34\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.07\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 123\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2217010670238073\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013572207300106266\n",
      "          policy_loss: -0.0742927467243539\n",
      "          total_loss: -0.09133234131667349\n",
      "          vf_explained_var: -0.3547532260417938\n",
      "          vf_loss: 0.0005968008525087498\n",
      "    num_agent_steps_sampled: 123000\n",
      "    num_agent_steps_trained: 123000\n",
      "    num_steps_sampled: 123000\n",
      "    num_steps_trained: 123000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.00000000000001\n",
      "    ram_util_percent: 75.50666666666666\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0441857072896599\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.134439363499993\n",
      "    mean_inference_ms: 1.4802022231507865\n",
      "    mean_raw_obs_processing_ms: 0.5757181080736102\n",
      "  time_since_restore: 1412.3241832256317\n",
      "  time_this_iter_s: 10.484666347503662\n",
      "  time_total_s: 1412.3241832256317\n",
      "  timers:\n",
      "    learn_throughput: 1723.442\n",
      "    learn_time_ms: 580.234\n",
      "    load_throughput: 219503.98\n",
      "    load_time_ms: 4.556\n",
      "    sample_throughput: 82.743\n",
      "    sample_time_ms: 12085.588\n",
      "    update_time_ms: 2.859\n",
      "  timestamp: 1631876074\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 123000\n",
      "  training_iteration: 123\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         1412.32</td><td style=\"text-align: right;\">123000</td><td style=\"text-align: right;\">    0.07</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-54-44\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.07\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 124\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5222746239768132\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010562803464457697\n",
      "          policy_loss: -0.02325462336755461\n",
      "          total_loss: -0.044445717562403945\n",
      "          vf_explained_var: -0.8579029440879822\n",
      "          vf_loss: 0.00046670508034165123\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.053333333333335\n",
      "    ram_util_percent: 75.35333333333331\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04417471100205368\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.116037292250924\n",
      "    mean_inference_ms: 1.4798647696676235\n",
      "    mean_raw_obs_processing_ms: 0.5813989856637478\n",
      "  time_since_restore: 1422.4683814048767\n",
      "  time_this_iter_s: 10.144198179244995\n",
      "  time_total_s: 1422.4683814048767\n",
      "  timers:\n",
      "    learn_throughput: 1724.395\n",
      "    learn_time_ms: 579.914\n",
      "    load_throughput: 219408.675\n",
      "    load_time_ms: 4.558\n",
      "    sample_throughput: 82.374\n",
      "    sample_time_ms: 12139.68\n",
      "    update_time_ms: 2.883\n",
      "  timestamp: 1631876084\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 124\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         1422.47</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">    0.07</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 125000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-54-54\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.07\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 125\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6938922127087912\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016572979299734315\n",
      "          policy_loss: 0.035469124217828114\n",
      "          total_loss: 0.024911759462621478\n",
      "          vf_explained_var: -0.07798673212528229\n",
      "          vf_loss: 0.0007881746297546973\n",
      "    num_agent_steps_sampled: 125000\n",
      "    num_agent_steps_trained: 125000\n",
      "    num_steps_sampled: 125000\n",
      "    num_steps_trained: 125000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.15714285714285\n",
      "    ram_util_percent: 75.22857142857143\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044163229341576564\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.098292158500781\n",
      "    mean_inference_ms: 1.479531247670646\n",
      "    mean_raw_obs_processing_ms: 0.5870365271022155\n",
      "  time_since_restore: 1432.3494954109192\n",
      "  time_this_iter_s: 9.88111400604248\n",
      "  time_total_s: 1432.3494954109192\n",
      "  timers:\n",
      "    learn_throughput: 1728.101\n",
      "    learn_time_ms: 578.67\n",
      "    load_throughput: 220792.352\n",
      "    load_time_ms: 4.529\n",
      "    sample_throughput: 83.093\n",
      "    sample_time_ms: 12034.746\n",
      "    update_time_ms: 2.894\n",
      "  timestamp: 1631876094\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 125000\n",
      "  training_iteration: 125\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         1432.35</td><td style=\"text-align: right;\">125000</td><td style=\"text-align: right;\">    0.07</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 126000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-55-04\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.07\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 126\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6369938876893784\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015411741396828778\n",
      "          policy_loss: -0.025435788184404375\n",
      "          total_loss: -0.04578760183519787\n",
      "          vf_explained_var: -0.5933629274368286\n",
      "          vf_loss: 0.0008166565158818332\n",
      "    num_agent_steps_sampled: 126000\n",
      "    num_agent_steps_trained: 126000\n",
      "    num_steps_sampled: 126000\n",
      "    num_steps_trained: 126000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.4\n",
      "    ram_util_percent: 75.23571428571427\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04415153583693014\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.081226545287532\n",
      "    mean_inference_ms: 1.479210819971149\n",
      "    mean_raw_obs_processing_ms: 0.5926328949212746\n",
      "  time_since_restore: 1442.3030862808228\n",
      "  time_this_iter_s: 9.953590869903564\n",
      "  time_total_s: 1442.3030862808228\n",
      "  timers:\n",
      "    learn_throughput: 1737.893\n",
      "    learn_time_ms: 575.409\n",
      "    load_throughput: 223022.981\n",
      "    load_time_ms: 4.484\n",
      "    sample_throughput: 83.68\n",
      "    sample_time_ms: 11950.302\n",
      "    update_time_ms: 2.87\n",
      "  timestamp: 1631876104\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 126000\n",
      "  training_iteration: 126\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">          1442.3</td><td style=\"text-align: right;\">126000</td><td style=\"text-align: right;\">    0.07</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 127000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-55-15\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 127\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3133685933219064\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01090908901768193\n",
      "          policy_loss: -0.023605333185858196\n",
      "          total_loss: -0.04132970207267338\n",
      "          vf_explained_var: -0.9801677465438843\n",
      "          vf_loss: 0.001727500298552008\n",
      "    num_agent_steps_sampled: 127000\n",
      "    num_agent_steps_trained: 127000\n",
      "    num_steps_sampled: 127000\n",
      "    num_steps_trained: 127000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.313333333333325\n",
      "    ram_util_percent: 75.21333333333335\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0441402672801144\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.064612218917562\n",
      "    mean_inference_ms: 1.4788983777597013\n",
      "    mean_raw_obs_processing_ms: 0.5981821273995377\n",
      "  time_since_restore: 1452.5284259319305\n",
      "  time_this_iter_s: 10.225339651107788\n",
      "  time_total_s: 1452.5284259319305\n",
      "  timers:\n",
      "    learn_throughput: 1738.272\n",
      "    learn_time_ms: 575.284\n",
      "    load_throughput: 222363.22\n",
      "    load_time_ms: 4.497\n",
      "    sample_throughput: 83.757\n",
      "    sample_time_ms: 11939.263\n",
      "    update_time_ms: 2.882\n",
      "  timestamp: 1631876115\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 127000\n",
      "  training_iteration: 127\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         1452.53</td><td style=\"text-align: right;\">127000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-55-25\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 128\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2348410447438556\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0128302073715723\n",
      "          policy_loss: -0.046074498775932525\n",
      "          total_loss: -0.06353434328403738\n",
      "          vf_explained_var: -0.8125898838043213\n",
      "          vf_loss: 0.0005583699965629623\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.82142857142857\n",
      "    ram_util_percent: 75.2642857142857\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04412959419765579\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.048566709616424\n",
      "    mean_inference_ms: 1.478600100005233\n",
      "    mean_raw_obs_processing_ms: 0.6036897138434184\n",
      "  time_since_restore: 1462.547839164734\n",
      "  time_this_iter_s: 10.019413232803345\n",
      "  time_total_s: 1462.547839164734\n",
      "  timers:\n",
      "    learn_throughput: 1740.893\n",
      "    learn_time_ms: 574.418\n",
      "    load_throughput: 235901.439\n",
      "    load_time_ms: 4.239\n",
      "    sample_throughput: 85.334\n",
      "    sample_time_ms: 11718.685\n",
      "    update_time_ms: 2.883\n",
      "  timestamp: 1631876125\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 128\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">         1462.55</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 129000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-55-34\n",
      "  done: false\n",
      "  episode_len_mean: 994.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 129\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0613919681972925\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019513855808949435\n",
      "          policy_loss: -0.06278310467799504\n",
      "          total_loss: -0.0761680081486702\n",
      "          vf_explained_var: -0.5198114514350891\n",
      "          vf_loss: 0.0006430921128614702\n",
      "    num_agent_steps_sampled: 129000\n",
      "    num_agent_steps_trained: 129000\n",
      "    num_steps_sampled: 129000\n",
      "    num_steps_trained: 129000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.407142857142865\n",
      "    ram_util_percent: 75.35714285714285\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04411948512676636\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.03303156018812\n",
      "    mean_inference_ms: 1.4783135637614038\n",
      "    mean_raw_obs_processing_ms: 0.6091545511485157\n",
      "  time_since_restore: 1472.2065205574036\n",
      "  time_this_iter_s: 9.658681392669678\n",
      "  time_total_s: 1472.2065205574036\n",
      "  timers:\n",
      "    learn_throughput: 1762.975\n",
      "    learn_time_ms: 567.223\n",
      "    load_throughput: 244689.959\n",
      "    load_time_ms: 4.087\n",
      "    sample_throughput: 87.361\n",
      "    sample_time_ms: 11446.691\n",
      "    update_time_ms: 2.782\n",
      "  timestamp: 1631876134\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 129000\n",
      "  training_iteration: 129\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         1472.21</td><td style=\"text-align: right;\">129000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            994.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 130000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-55-44\n",
      "  done: false\n",
      "  episode_len_mean: 995.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 130\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.489465437995063\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014209094037633966\n",
      "          policy_loss: -0.03858789313170645\n",
      "          total_loss: -0.05829011524717013\n",
      "          vf_explained_var: -0.28310060501098633\n",
      "          vf_loss: 0.0003968606813941733\n",
      "    num_agent_steps_sampled: 130000\n",
      "    num_agent_steps_trained: 130000\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.161538461538456\n",
      "    ram_util_percent: 75.49230769230769\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04410979428659747\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.017898665590973\n",
      "    mean_inference_ms: 1.4780343974703507\n",
      "    mean_raw_obs_processing_ms: 0.6087636610722953\n",
      "  time_since_restore: 1481.7317712306976\n",
      "  time_this_iter_s: 9.525250673294067\n",
      "  time_total_s: 1481.7317712306976\n",
      "  timers:\n",
      "    learn_throughput: 1764.748\n",
      "    learn_time_ms: 566.653\n",
      "    load_throughput: 303315.254\n",
      "    load_time_ms: 3.297\n",
      "    sample_throughput: 105.17\n",
      "    sample_time_ms: 9508.45\n",
      "    update_time_ms: 2.12\n",
      "  timestamp: 1631876144\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 130\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">         1481.73</td><td style=\"text-align: right;\">130000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            995.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 131000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-55-54\n",
      "  done: false\n",
      "  episode_len_mean: 995.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.07\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 131\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.888838373290168\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01682069334351771\n",
      "          policy_loss: -0.05746524574028121\n",
      "          total_loss: -0.07031246829364035\n",
      "          vf_explained_var: -0.7357526421546936\n",
      "          vf_loss: 0.0003641757375185585\n",
      "    num_agent_steps_sampled: 131000\n",
      "    num_agent_steps_trained: 131000\n",
      "    num_steps_sampled: 131000\n",
      "    num_steps_trained: 131000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.96\n",
      "    ram_util_percent: 75.57999999999998\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044100185990337853\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 9.002654719666419\n",
      "    mean_inference_ms: 1.4777490600238616\n",
      "    mean_raw_obs_processing_ms: 0.6085162745266597\n",
      "  time_since_restore: 1491.808930158615\n",
      "  time_this_iter_s: 10.07715892791748\n",
      "  time_total_s: 1491.808930158615\n",
      "  timers:\n",
      "    learn_throughput: 1763.298\n",
      "    learn_time_ms: 567.119\n",
      "    load_throughput: 303017.238\n",
      "    load_time_ms: 3.3\n",
      "    sample_throughput: 105.733\n",
      "    sample_time_ms: 9457.764\n",
      "    update_time_ms: 2.12\n",
      "  timestamp: 1631876154\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 131000\n",
      "  training_iteration: 131\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         1491.81</td><td style=\"text-align: right;\">131000</td><td style=\"text-align: right;\">    0.07</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            995.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-56-04\n",
      "  done: false\n",
      "  episode_len_mean: 995.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.07\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 132\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0405317889319528\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01480096338162728\n",
      "          policy_loss: -0.050550043562220204\n",
      "          total_loss: -0.06548143968813949\n",
      "          vf_explained_var: -0.3912461996078491\n",
      "          vf_loss: 0.0004785990340880946\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.5\n",
      "    ram_util_percent: 75.65714285714287\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04409060864036273\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.987818154600502\n",
      "    mean_inference_ms: 1.477466584474992\n",
      "    mean_raw_obs_processing_ms: 0.6084040027012039\n",
      "  time_since_restore: 1501.8591792583466\n",
      "  time_this_iter_s: 10.050249099731445\n",
      "  time_total_s: 1501.8591792583466\n",
      "  timers:\n",
      "    learn_throughput: 1764.135\n",
      "    learn_time_ms: 566.85\n",
      "    load_throughput: 303831.594\n",
      "    load_time_ms: 3.291\n",
      "    sample_throughput: 106.09\n",
      "    sample_time_ms: 9425.947\n",
      "    update_time_ms: 2.183\n",
      "  timestamp: 1631876164\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 132\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         1501.86</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">    0.07</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            995.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 133000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-56-14\n",
      "  done: false\n",
      "  episode_len_mean: 995.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 133\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.829920188585917\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016601967410808077\n",
      "          policy_loss: -0.03594658474127452\n",
      "          total_loss: -0.04817776812447442\n",
      "          vf_explained_var: 0.12375698983669281\n",
      "          vf_loss: 0.00046485298096538626\n",
      "    num_agent_steps_sampled: 133000\n",
      "    num_agent_steps_trained: 133000\n",
      "    num_steps_sampled: 133000\n",
      "    num_steps_trained: 133000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.18571428571427\n",
      "    ram_util_percent: 75.71428571428571\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044081391590110174\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.973500966926567\n",
      "    mean_inference_ms: 1.477190216138914\n",
      "    mean_raw_obs_processing_ms: 0.6084106290492696\n",
      "  time_since_restore: 1511.6986198425293\n",
      "  time_this_iter_s: 9.83944058418274\n",
      "  time_total_s: 1511.6986198425293\n",
      "  timers:\n",
      "    learn_throughput: 1763.439\n",
      "    learn_time_ms: 567.074\n",
      "    load_throughput: 304420.38\n",
      "    load_time_ms: 3.285\n",
      "    sample_throughput: 106.825\n",
      "    sample_time_ms: 9361.119\n",
      "    update_time_ms: 2.203\n",
      "  timestamp: 1631876174\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 133000\n",
      "  training_iteration: 133\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">          1511.7</td><td style=\"text-align: right;\">133000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            995.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 134000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-56-24\n",
      "  done: false\n",
      "  episode_len_mean: 995.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.04\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 134\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.068759735425313\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01641457246149369\n",
      "          policy_loss: -0.02765360607041253\n",
      "          total_loss: -0.04249037297235595\n",
      "          vf_explained_var: 0.11865590512752533\n",
      "          vf_loss: 0.0003109106653331158\n",
      "    num_agent_steps_sampled: 134000\n",
      "    num_agent_steps_trained: 134000\n",
      "    num_steps_sampled: 134000\n",
      "    num_steps_trained: 134000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.94666666666667\n",
      "    ram_util_percent: 75.81333333333333\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04407255099901371\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.959751686925925\n",
      "    mean_inference_ms: 1.4769222250815686\n",
      "    mean_raw_obs_processing_ms: 0.6085320964868045\n",
      "  time_since_restore: 1521.7699174880981\n",
      "  time_this_iter_s: 10.071297645568848\n",
      "  time_total_s: 1521.7699174880981\n",
      "  timers:\n",
      "    learn_throughput: 1764.979\n",
      "    learn_time_ms: 566.579\n",
      "    load_throughput: 305549.169\n",
      "    load_time_ms: 3.273\n",
      "    sample_throughput: 106.901\n",
      "    sample_time_ms: 9354.408\n",
      "    update_time_ms: 2.173\n",
      "  timestamp: 1631876184\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 134000\n",
      "  training_iteration: 134\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         1521.77</td><td style=\"text-align: right;\">134000</td><td style=\"text-align: right;\">    0.04</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            995.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 135000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-56-34\n",
      "  done: false\n",
      "  episode_len_mean: 995.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.07\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 135\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2414318958918256\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009710176915969834\n",
      "          policy_loss: -0.04991905219439003\n",
      "          total_loss: -0.06860196058534913\n",
      "          vf_explained_var: -0.7741437554359436\n",
      "          vf_loss: 0.0004542246163408789\n",
      "    num_agent_steps_sampled: 135000\n",
      "    num_agent_steps_trained: 135000\n",
      "    num_steps_sampled: 135000\n",
      "    num_steps_trained: 135000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.95714285714286\n",
      "    ram_util_percent: 75.90714285714284\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04406398092934983\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.946400675097387\n",
      "    mean_inference_ms: 1.476657757297605\n",
      "    mean_raw_obs_processing_ms: 0.6087601525928806\n",
      "  time_since_restore: 1531.6900939941406\n",
      "  time_this_iter_s: 9.92017650604248\n",
      "  time_total_s: 1531.6900939941406\n",
      "  timers:\n",
      "    learn_throughput: 1763.343\n",
      "    learn_time_ms: 567.105\n",
      "    load_throughput: 305466.834\n",
      "    load_time_ms: 3.274\n",
      "    sample_throughput: 106.863\n",
      "    sample_time_ms: 9357.807\n",
      "    update_time_ms: 2.17\n",
      "  timestamp: 1631876194\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 135000\n",
      "  training_iteration: 135\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         1531.69</td><td style=\"text-align: right;\">135000</td><td style=\"text-align: right;\">    0.07</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            995.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-56-43\n",
      "  done: false\n",
      "  episode_len_mean: 995.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.11\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 136\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.398867705133226\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013174507352642726\n",
      "          policy_loss: -0.06262925134764777\n",
      "          total_loss: -0.0816917480693923\n",
      "          vf_explained_var: 0.05246450752019882\n",
      "          vf_loss: 0.0004797785493994727\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.66923076923077\n",
      "    ram_util_percent: 75.98461538461538\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04405562763203059\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.933386480727975\n",
      "    mean_inference_ms: 1.4763981673494226\n",
      "    mean_raw_obs_processing_ms: 0.6090867836556948\n",
      "  time_since_restore: 1540.8284344673157\n",
      "  time_this_iter_s: 9.138340473175049\n",
      "  time_total_s: 1540.8284344673157\n",
      "  timers:\n",
      "    learn_throughput: 1760.81\n",
      "    learn_time_ms: 567.92\n",
      "    load_throughput: 304860.7\n",
      "    load_time_ms: 3.28\n",
      "    sample_throughput: 107.811\n",
      "    sample_time_ms: 9275.489\n",
      "    update_time_ms: 2.177\n",
      "  timestamp: 1631876203\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 136\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         1540.83</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">    0.11</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            995.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 137000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-56-53\n",
      "  done: false\n",
      "  episode_len_mean: 995.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.13\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 137\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.298553677399953\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017489685626858473\n",
      "          policy_loss: 0.07495224542087979\n",
      "          total_loss: 0.06706938876046074\n",
      "          vf_explained_var: -0.08836576342582703\n",
      "          vf_loss: 0.009199907712334405\n",
      "    num_agent_steps_sampled: 137000\n",
      "    num_agent_steps_trained: 137000\n",
      "    num_steps_sampled: 137000\n",
      "    num_steps_trained: 137000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.615384615384606\n",
      "    ram_util_percent: 76.07692307692308\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044047455264076225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.920677329548523\n",
      "    mean_inference_ms: 1.4761422688315093\n",
      "    mean_raw_obs_processing_ms: 0.6095064505234009\n",
      "  time_since_restore: 1550.321048259735\n",
      "  time_this_iter_s: 9.492613792419434\n",
      "  time_total_s: 1550.321048259735\n",
      "  timers:\n",
      "    learn_throughput: 1760.174\n",
      "    learn_time_ms: 568.126\n",
      "    load_throughput: 302905.632\n",
      "    load_time_ms: 3.301\n",
      "    sample_throughput: 108.672\n",
      "    sample_time_ms: 9202.011\n",
      "    update_time_ms: 2.172\n",
      "  timestamp: 1631876213\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 137000\n",
      "  training_iteration: 137\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         1550.32</td><td style=\"text-align: right;\">137000</td><td style=\"text-align: right;\">    0.13</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            995.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 138000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-57-03\n",
      "  done: false\n",
      "  episode_len_mean: 995.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.2\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 138\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1039955152405634\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014108709537878333\n",
      "          policy_loss: 0.016607618497477637\n",
      "          total_loss: 0.0013821358895964092\n",
      "          vf_explained_var: -0.10786023736000061\n",
      "          vf_loss: 0.0010527835389237024\n",
      "    num_agent_steps_sampled: 138000\n",
      "    num_agent_steps_trained: 138000\n",
      "    num_steps_sampled: 138000\n",
      "    num_steps_trained: 138000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.30666666666666\n",
      "    ram_util_percent: 76.19333333333336\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044040052161518316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.90839371294812\n",
      "    mean_inference_ms: 1.4758958336955679\n",
      "    mean_raw_obs_processing_ms: 0.6100113199329157\n",
      "  time_since_restore: 1560.5660367012024\n",
      "  time_this_iter_s: 10.244988441467285\n",
      "  time_total_s: 1560.5660367012024\n",
      "  timers:\n",
      "    learn_throughput: 1765.453\n",
      "    learn_time_ms: 566.427\n",
      "    load_throughput: 306679.633\n",
      "    load_time_ms: 3.261\n",
      "    sample_throughput: 108.385\n",
      "    sample_time_ms: 9226.36\n",
      "    update_time_ms: 2.173\n",
      "  timestamp: 1631876223\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 138000\n",
      "  training_iteration: 138\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         1560.57</td><td style=\"text-align: right;\">138000</td><td style=\"text-align: right;\">     0.2</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 139000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-57-13\n",
      "  done: false\n",
      "  episode_len_mean: 995.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.21\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 139\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.083566235171424\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010569022448810432\n",
      "          policy_loss: 0.019055424071848393\n",
      "          total_loss: 0.002337370150619083\n",
      "          vf_explained_var: 0.07382503151893616\n",
      "          vf_loss: 0.0005505620003936606\n",
      "    num_agent_steps_sampled: 139000\n",
      "    num_agent_steps_trained: 139000\n",
      "    num_steps_sampled: 139000\n",
      "    num_steps_trained: 139000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.679999999999986\n",
      "    ram_util_percent: 76.28666666666665\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04403296262220703\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.896522099295812\n",
      "    mean_inference_ms: 1.475656652789558\n",
      "    mean_raw_obs_processing_ms: 0.6105921028826097\n",
      "  time_since_restore: 1570.8090167045593\n",
      "  time_this_iter_s: 10.242980003356934\n",
      "  time_total_s: 1570.8090167045593\n",
      "  timers:\n",
      "    learn_throughput: 1766.408\n",
      "    learn_time_ms: 566.121\n",
      "    load_throughput: 307457.466\n",
      "    load_time_ms: 3.252\n",
      "    sample_throughput: 107.699\n",
      "    sample_time_ms: 9285.148\n",
      "    update_time_ms: 2.17\n",
      "  timestamp: 1631876233\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 139000\n",
      "  training_iteration: 139\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         1570.81</td><td style=\"text-align: right;\">139000</td><td style=\"text-align: right;\">    0.21</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-57-24\n",
      "  done: false\n",
      "  episode_len_mean: 995.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.21\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 140\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8129965133137174\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013237739064524452\n",
      "          policy_loss: -0.0036094206074873607\n",
      "          total_loss: -0.016013983223173354\n",
      "          vf_explained_var: -0.09127794206142426\n",
      "          vf_loss: 0.0012576691288914946\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.886666666666656\n",
      "    ram_util_percent: 76.37333333333332\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044026194442213624\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.885101521206161\n",
      "    mean_inference_ms: 1.4754256957054943\n",
      "    mean_raw_obs_processing_ms: 0.6112477629720776\n",
      "  time_since_restore: 1581.2757012844086\n",
      "  time_this_iter_s: 10.466684579849243\n",
      "  time_total_s: 1581.2757012844086\n",
      "  timers:\n",
      "    learn_throughput: 1767.923\n",
      "    learn_time_ms: 565.636\n",
      "    load_throughput: 311374.208\n",
      "    load_time_ms: 3.212\n",
      "    sample_throughput: 106.612\n",
      "    sample_time_ms: 9379.795\n",
      "    update_time_ms: 2.179\n",
      "  timestamp: 1631876244\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 140\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         1581.28</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">    0.21</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 141000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-57-34\n",
      "  done: false\n",
      "  episode_len_mean: 995.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.19\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 141\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6570304128858777\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013309458051152252\n",
      "          policy_loss: -0.024400141131546763\n",
      "          total_loss: -0.0357762336730957\n",
      "          vf_explained_var: 0.015339531004428864\n",
      "          vf_loss: 0.0007022670575275293\n",
      "    num_agent_steps_sampled: 141000\n",
      "    num_agent_steps_trained: 141000\n",
      "    num_steps_sampled: 141000\n",
      "    num_steps_trained: 141000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.660000000000004\n",
      "    ram_util_percent: 76.32666666666664\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.044019686167939574\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.874026450835162\n",
      "    mean_inference_ms: 1.4751956276779774\n",
      "    mean_raw_obs_processing_ms: 0.6119741297138523\n",
      "  time_since_restore: 1591.6540772914886\n",
      "  time_this_iter_s: 10.378376007080078\n",
      "  time_total_s: 1591.6540772914886\n",
      "  timers:\n",
      "    learn_throughput: 1768.43\n",
      "    learn_time_ms: 565.473\n",
      "    load_throughput: 307518.33\n",
      "    load_time_ms: 3.252\n",
      "    sample_throughput: 106.269\n",
      "    sample_time_ms: 9410.085\n",
      "    update_time_ms: 2.181\n",
      "  timestamp: 1631876254\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 141000\n",
      "  training_iteration: 141\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         1591.65</td><td style=\"text-align: right;\">141000</td><td style=\"text-align: right;\">    0.19</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 142000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-57-44\n",
      "  done: false\n",
      "  episode_len_mean: 995.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.18\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 142\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3810144861539204\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020284860064447387\n",
      "          policy_loss: 0.033172275208764604\n",
      "          total_loss: 0.026862609800365235\n",
      "          vf_explained_var: -0.07156277447938919\n",
      "          vf_loss: 0.0006543386388026799\n",
      "    num_agent_steps_sampled: 142000\n",
      "    num_agent_steps_trained: 142000\n",
      "    num_steps_sampled: 142000\n",
      "    num_steps_trained: 142000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.23571428571428\n",
      "    ram_util_percent: 76.27857142857142\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04401330449614847\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.863288310831987\n",
      "    mean_inference_ms: 1.4749699334830324\n",
      "    mean_raw_obs_processing_ms: 0.6127659412496415\n",
      "  time_since_restore: 1601.8958914279938\n",
      "  time_this_iter_s: 10.241814136505127\n",
      "  time_total_s: 1601.8958914279938\n",
      "  timers:\n",
      "    learn_throughput: 1767.405\n",
      "    learn_time_ms: 565.801\n",
      "    load_throughput: 305164.576\n",
      "    load_time_ms: 3.277\n",
      "    sample_throughput: 106.057\n",
      "    sample_time_ms: 9428.932\n",
      "    update_time_ms: 2.116\n",
      "  timestamp: 1631876264\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 142000\n",
      "  training_iteration: 142\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">          1601.9</td><td style=\"text-align: right;\">142000</td><td style=\"text-align: right;\">    0.18</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 143000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-57-55\n",
      "  done: false\n",
      "  episode_len_mean: 995.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.2\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 143\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3257682204246521\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005324946486998172\n",
      "          policy_loss: 0.02867644735508495\n",
      "          total_loss: 0.018478968056539695\n",
      "          vf_explained_var: -0.2976551055908203\n",
      "          vf_loss: 0.00036444734242751213\n",
      "    num_agent_steps_sampled: 143000\n",
      "    num_agent_steps_trained: 143000\n",
      "    num_steps_sampled: 143000\n",
      "    num_steps_trained: 143000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.2\n",
      "    ram_util_percent: 76.22000000000001\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04400711741879609\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.852899115279028\n",
      "    mean_inference_ms: 1.4747494487635557\n",
      "    mean_raw_obs_processing_ms: 0.6136185426067254\n",
      "  time_since_restore: 1612.199390411377\n",
      "  time_this_iter_s: 10.303498983383179\n",
      "  time_total_s: 1612.199390411377\n",
      "  timers:\n",
      "    learn_throughput: 1769.265\n",
      "    learn_time_ms: 565.207\n",
      "    load_throughput: 276603.445\n",
      "    load_time_ms: 3.615\n",
      "    sample_throughput: 105.533\n",
      "    sample_time_ms: 9475.727\n",
      "    update_time_ms: 2.103\n",
      "  timestamp: 1631876275\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 143000\n",
      "  training_iteration: 143\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">          1612.2</td><td style=\"text-align: right;\">143000</td><td style=\"text-align: right;\">     0.2</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-58-05\n",
      "  done: false\n",
      "  episode_len_mean: 995.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.2\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 144\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.991287773185306\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0163963911244025\n",
      "          policy_loss: -0.02240326851606369\n",
      "          total_loss: -0.033395697962906624\n",
      "          vf_explained_var: -0.5788459777832031\n",
      "          vf_loss: 0.000619775222407447\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.957142857142856\n",
      "    ram_util_percent: 76.20714285714287\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0440011567433144\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.842862152286944\n",
      "    mean_inference_ms: 1.4745345580495017\n",
      "    mean_raw_obs_processing_ms: 0.6145262507926798\n",
      "  time_since_restore: 1622.1616830825806\n",
      "  time_this_iter_s: 9.962292671203613\n",
      "  time_total_s: 1622.1616830825806\n",
      "  timers:\n",
      "    learn_throughput: 1766.134\n",
      "    learn_time_ms: 566.208\n",
      "    load_throughput: 269804.769\n",
      "    load_time_ms: 3.706\n",
      "    sample_throughput: 105.667\n",
      "    sample_time_ms: 9463.71\n",
      "    update_time_ms: 2.11\n",
      "  timestamp: 1631876285\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 144\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         1622.16</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">     0.2</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 145000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-58-15\n",
      "  done: false\n",
      "  episode_len_mean: 995.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.18\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 145\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.788921554883321\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009131930837718648\n",
      "          policy_loss: 0.025283765296141306\n",
      "          total_loss: 0.012777993745274014\n",
      "          vf_explained_var: -0.303287148475647\n",
      "          vf_loss: 0.0007604042220110488\n",
      "    num_agent_steps_sampled: 145000\n",
      "    num_agent_steps_trained: 145000\n",
      "    num_steps_sampled: 145000\n",
      "    num_steps_trained: 145000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.63333333333334\n",
      "    ram_util_percent: 76.18666666666668\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04399526955421809\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.833150302587086\n",
      "    mean_inference_ms: 1.4743209159677173\n",
      "    mean_raw_obs_processing_ms: 0.6154875651528698\n",
      "  time_since_restore: 1632.6044011116028\n",
      "  time_this_iter_s: 10.442718029022217\n",
      "  time_total_s: 1632.6044011116028\n",
      "  timers:\n",
      "    learn_throughput: 1770.8\n",
      "    learn_time_ms: 564.717\n",
      "    load_throughput: 270446.714\n",
      "    load_time_ms: 3.698\n",
      "    sample_throughput: 105.07\n",
      "    sample_time_ms: 9517.492\n",
      "    update_time_ms: 2.103\n",
      "  timestamp: 1631876295\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 145000\n",
      "  training_iteration: 145\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   145</td><td style=\"text-align: right;\">          1632.6</td><td style=\"text-align: right;\">145000</td><td style=\"text-align: right;\">    0.18</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 146000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-58-25\n",
      "  done: false\n",
      "  episode_len_mean: 995.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.18\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 146\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8608945356474982\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011170088970861988\n",
      "          policy_loss: -0.016022323071956633\n",
      "          total_loss: -0.028513415820068784\n",
      "          vf_explained_var: -0.8107267618179321\n",
      "          vf_loss: 0.0004629975854994781\n",
      "    num_agent_steps_sampled: 146000\n",
      "    num_agent_steps_trained: 146000\n",
      "    num_steps_sampled: 146000\n",
      "    num_steps_trained: 146000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.35333333333333\n",
      "    ram_util_percent: 76.17333333333335\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04398910885375089\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.823715236665654\n",
      "    mean_inference_ms: 1.4741057689550456\n",
      "    mean_raw_obs_processing_ms: 0.6165007106699624\n",
      "  time_since_restore: 1642.8295917510986\n",
      "  time_this_iter_s: 10.22519063949585\n",
      "  time_total_s: 1642.8295917510986\n",
      "  timers:\n",
      "    learn_throughput: 1767.515\n",
      "    learn_time_ms: 565.766\n",
      "    load_throughput: 271937.136\n",
      "    load_time_ms: 3.677\n",
      "    sample_throughput: 103.895\n",
      "    sample_time_ms: 9625.105\n",
      "    update_time_ms: 2.098\n",
      "  timestamp: 1631876305\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 146000\n",
      "  training_iteration: 146\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         1642.83</td><td style=\"text-align: right;\">146000</td><td style=\"text-align: right;\">    0.18</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 147000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-58-36\n",
      "  done: false\n",
      "  episode_len_mean: 995.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.18\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 147\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4823205577002632\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005416533537035504\n",
      "          policy_loss: -0.05575202920784553\n",
      "          total_loss: -0.06747174207121134\n",
      "          vf_explained_var: 0.2180517017841339\n",
      "          vf_loss: 0.0003613716885107957\n",
      "    num_agent_steps_sampled: 147000\n",
      "    num_agent_steps_trained: 147000\n",
      "    num_steps_sampled: 147000\n",
      "    num_steps_trained: 147000\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.67333333333333\n",
      "    ram_util_percent: 76.16666666666667\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043983063906052174\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.814565408587601\n",
      "    mean_inference_ms: 1.4738907427467416\n",
      "    mean_raw_obs_processing_ms: 0.6175621665325026\n",
      "  time_since_restore: 1653.1529204845428\n",
      "  time_this_iter_s: 10.323328733444214\n",
      "  time_total_s: 1653.1529204845428\n",
      "  timers:\n",
      "    learn_throughput: 1768.512\n",
      "    learn_time_ms: 565.447\n",
      "    load_throughput: 275287.245\n",
      "    load_time_ms: 3.633\n",
      "    sample_throughput: 103.002\n",
      "    sample_time_ms: 9708.514\n",
      "    update_time_ms: 2.1\n",
      "  timestamp: 1631876316\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 147000\n",
      "  training_iteration: 147\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         1653.15</td><td style=\"text-align: right;\">147000</td><td style=\"text-align: right;\">    0.18</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-58-46\n",
      "  done: false\n",
      "  episode_len_mean: 995.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.16\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 148\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3175871756341722\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006169156112746382\n",
      "          policy_loss: 0.109191133081913\n",
      "          total_loss: 0.09943055361509323\n",
      "          vf_explained_var: 0.015456883236765862\n",
      "          vf_loss: 0.00029215708127594553\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.38666666666667\n",
      "    ram_util_percent: 76.14\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04397719339520784\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.805719900065338\n",
      "    mean_inference_ms: 1.473681318363202\n",
      "    mean_raw_obs_processing_ms: 0.618666353769997\n",
      "  time_since_restore: 1663.8049006462097\n",
      "  time_this_iter_s: 10.65198016166687\n",
      "  time_total_s: 1663.8049006462097\n",
      "  timers:\n",
      "    learn_throughput: 1755.343\n",
      "    learn_time_ms: 569.689\n",
      "    load_throughput: 271783.833\n",
      "    load_time_ms: 3.679\n",
      "    sample_throughput: 102.62\n",
      "    sample_time_ms: 9744.673\n",
      "    update_time_ms: 2.244\n",
      "  timestamp: 1631876326\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 148\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">          1663.8</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">    0.16</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 149000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-58-57\n",
      "  done: false\n",
      "  episode_len_mean: 995.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.16\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 149\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3634496370951334\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01159765072204111\n",
      "          policy_loss: 0.04816485246022542\n",
      "          total_loss: 0.040671227541234756\n",
      "          vf_explained_var: -0.32158103585243225\n",
      "          vf_loss: 0.0002695625781295045\n",
      "    num_agent_steps_sampled: 149000\n",
      "    num_agent_steps_trained: 149000\n",
      "    num_steps_sampled: 149000\n",
      "    num_steps_trained: 149000\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.45333333333333\n",
      "    ram_util_percent: 76.24666666666667\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04397127797181765\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.797134748254496\n",
      "    mean_inference_ms: 1.4734696194763421\n",
      "    mean_raw_obs_processing_ms: 0.6198135657875592\n",
      "  time_since_restore: 1674.2476580142975\n",
      "  time_this_iter_s: 10.442757368087769\n",
      "  time_total_s: 1674.2476580142975\n",
      "  timers:\n",
      "    learn_throughput: 1751.937\n",
      "    learn_time_ms: 570.797\n",
      "    load_throughput: 269992.34\n",
      "    load_time_ms: 3.704\n",
      "    sample_throughput: 102.422\n",
      "    sample_time_ms: 9763.541\n",
      "    update_time_ms: 2.239\n",
      "  timestamp: 1631876337\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 149000\n",
      "  training_iteration: 149\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         1674.25</td><td style=\"text-align: right;\">149000</td><td style=\"text-align: right;\">    0.16</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            995.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1256)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1256)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 150000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-59-26\n",
      "  done: false\n",
      "  episode_len_mean: 994.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.15\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 150\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7038347442944846\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009312254210533558\n",
      "          policy_loss: -0.01893878397014406\n",
      "          total_loss: -0.03068759371009138\n",
      "          vf_explained_var: 0.08207765966653824\n",
      "          vf_loss: 0.0005752068801989986\n",
      "    num_agent_steps_sampled: 150000\n",
      "    num_agent_steps_trained: 150000\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.2547619047619\n",
      "    ram_util_percent: 76.02142857142859\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04396545454790071\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.788842941402503\n",
      "    mean_inference_ms: 1.4732612628669535\n",
      "    mean_raw_obs_processing_ms: 0.6222520415580108\n",
      "  time_since_restore: 1703.5909259319305\n",
      "  time_this_iter_s: 29.343267917633057\n",
      "  time_total_s: 1703.5909259319305\n",
      "  timers:\n",
      "    learn_throughput: 1749.384\n",
      "    learn_time_ms: 571.63\n",
      "    load_throughput: 188014.56\n",
      "    load_time_ms: 5.319\n",
      "    sample_throughput: 85.846\n",
      "    sample_time_ms: 11648.782\n",
      "    update_time_ms: 2.221\n",
      "  timestamp: 1631876366\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 150\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         1703.59</td><td style=\"text-align: right;\">150000</td><td style=\"text-align: right;\">    0.15</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             994.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 151000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-59-37\n",
      "  done: false\n",
      "  episode_len_mean: 994.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.17\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 151\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6779071066114637\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007144074596756174\n",
      "          policy_loss: 0.05993024052845107\n",
      "          total_loss: 0.04753451278019283\n",
      "          vf_explained_var: -0.0720881000161171\n",
      "          vf_loss: 0.0007666540322437261\n",
      "    num_agent_steps_sampled: 151000\n",
      "    num_agent_steps_trained: 151000\n",
      "    num_steps_sampled: 151000\n",
      "    num_steps_trained: 151000\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.59333333333334\n",
      "    ram_util_percent: 75.12000000000002\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04395938219843257\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.780651552573566\n",
      "    mean_inference_ms: 1.4730364369087912\n",
      "    mean_raw_obs_processing_ms: 0.6247146964212706\n",
      "  time_since_restore: 1714.0770120620728\n",
      "  time_this_iter_s: 10.486086130142212\n",
      "  time_total_s: 1714.0770120620728\n",
      "  timers:\n",
      "    learn_throughput: 1744.714\n",
      "    learn_time_ms: 573.16\n",
      "    load_throughput: 187778.872\n",
      "    load_time_ms: 5.325\n",
      "    sample_throughput: 85.778\n",
      "    sample_time_ms: 11657.987\n",
      "    update_time_ms: 2.225\n",
      "  timestamp: 1631876377\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 151000\n",
      "  training_iteration: 151\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         1714.08</td><td style=\"text-align: right;\">151000</td><td style=\"text-align: right;\">    0.17</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-59-47\n",
      "  done: false\n",
      "  episode_len_mean: 994.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.15\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 152\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4990594744682313\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010871369019471663\n",
      "          policy_loss: -0.0012543980239166153\n",
      "          total_loss: -0.007318904892437987\n",
      "          vf_explained_var: 0.06412074714899063\n",
      "          vf_loss: 0.0034224564140054605\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.35714285714287\n",
      "    ram_util_percent: 74.84999999999998\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0439535051056478\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.772720902107904\n",
      "    mean_inference_ms: 1.4728153613670554\n",
      "    mean_raw_obs_processing_ms: 0.6272029473532638\n",
      "  time_since_restore: 1724.4293234348297\n",
      "  time_this_iter_s: 10.352311372756958\n",
      "  time_total_s: 1724.4293234348297\n",
      "  timers:\n",
      "    learn_throughput: 1743.773\n",
      "    learn_time_ms: 573.469\n",
      "    load_throughput: 188586.87\n",
      "    load_time_ms: 5.303\n",
      "    sample_throughput: 85.699\n",
      "    sample_time_ms: 11668.741\n",
      "    update_time_ms: 2.222\n",
      "  timestamp: 1631876387\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 152\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         1724.43</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">    0.15</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 153000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_10-59-57\n",
      "  done: false\n",
      "  episode_len_mean: 994.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.15\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 153\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1729356951183743\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009326090601867499\n",
      "          policy_loss: 0.02371125751071506\n",
      "          total_loss: 0.007033009496000078\n",
      "          vf_explained_var: 0.043826114386320114\n",
      "          vf_loss: 0.00032977803827331326\n",
      "    num_agent_steps_sampled: 153000\n",
      "    num_agent_steps_trained: 153000\n",
      "    num_steps_sampled: 153000\n",
      "    num_steps_trained: 153000\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.77142857142858\n",
      "    ram_util_percent: 74.79285714285712\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043947253950957166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.764858017108388\n",
      "    mean_inference_ms: 1.472583596989519\n",
      "    mean_raw_obs_processing_ms: 0.6297161390682999\n",
      "  time_since_restore: 1733.8831551074982\n",
      "  time_this_iter_s: 9.453831672668457\n",
      "  time_total_s: 1733.8831551074982\n",
      "  timers:\n",
      "    learn_throughput: 1735.837\n",
      "    learn_time_ms: 576.091\n",
      "    load_throughput: 201238.047\n",
      "    load_time_ms: 4.969\n",
      "    sample_throughput: 86.345\n",
      "    sample_time_ms: 11581.416\n",
      "    update_time_ms: 2.269\n",
      "  timestamp: 1631876397\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 153000\n",
      "  training_iteration: 153\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">         1733.88</td><td style=\"text-align: right;\">153000</td><td style=\"text-align: right;\">    0.15</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 154000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-00-06\n",
      "  done: false\n",
      "  episode_len_mean: 994.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.13\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 154\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5769059658050537\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010634295618187945\n",
      "          policy_loss: -0.015039087169700198\n",
      "          total_loss: -0.035015146599875556\n",
      "          vf_explained_var: 0.38117820024490356\n",
      "          vf_loss: 0.00040938705239770774\n",
      "    num_agent_steps_sampled: 154000\n",
      "    num_agent_steps_trained: 154000\n",
      "    num_steps_sampled: 154000\n",
      "    num_steps_trained: 154000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.94615384615385\n",
      "    ram_util_percent: 74.79999999999998\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043940578917706\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.757137246883365\n",
      "    mean_inference_ms: 1.472349074089485\n",
      "    mean_raw_obs_processing_ms: 0.6322518803176794\n",
      "  time_since_restore: 1743.1566987037659\n",
      "  time_this_iter_s: 9.2735435962677\n",
      "  time_total_s: 1743.1566987037659\n",
      "  timers:\n",
      "    learn_throughput: 1738.002\n",
      "    learn_time_ms: 575.373\n",
      "    load_throughput: 205064.341\n",
      "    load_time_ms: 4.877\n",
      "    sample_throughput: 86.856\n",
      "    sample_time_ms: 11513.362\n",
      "    update_time_ms: 2.268\n",
      "  timestamp: 1631876406\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 154000\n",
      "  training_iteration: 154\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         1743.16</td><td style=\"text-align: right;\">154000</td><td style=\"text-align: right;\">    0.13</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 155000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-00-16\n",
      "  done: false\n",
      "  episode_len_mean: 994.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.14\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 155\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0970745272106592\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007698713253763145\n",
      "          policy_loss: -0.11239708862784836\n",
      "          total_loss: -0.1286976926235689\n",
      "          vf_explained_var: -0.10479889810085297\n",
      "          vf_loss: 0.0007726661388915091\n",
      "    num_agent_steps_sampled: 155000\n",
      "    num_agent_steps_trained: 155000\n",
      "    num_steps_sampled: 155000\n",
      "    num_steps_trained: 155000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.364285714285714\n",
      "    ram_util_percent: 74.79285714285712\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04393351844045134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.749483762519846\n",
      "    mean_inference_ms: 1.4721002329259425\n",
      "    mean_raw_obs_processing_ms: 0.6348078827732476\n",
      "  time_since_restore: 1753.006029367447\n",
      "  time_this_iter_s: 9.84933066368103\n",
      "  time_total_s: 1753.006029367447\n",
      "  timers:\n",
      "    learn_throughput: 1738.303\n",
      "    learn_time_ms: 575.274\n",
      "    load_throughput: 202908.63\n",
      "    load_time_ms: 4.928\n",
      "    sample_throughput: 87.306\n",
      "    sample_time_ms: 11453.968\n",
      "    update_time_ms: 2.268\n",
      "  timestamp: 1631876416\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 155000\n",
      "  training_iteration: 155\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         1753.01</td><td style=\"text-align: right;\">155000</td><td style=\"text-align: right;\">    0.14</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-00-26\n",
      "  done: false\n",
      "  episode_len_mean: 994.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.15\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 156\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.670454474290212\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012771876912785135\n",
      "          policy_loss: -0.03391663179629379\n",
      "          total_loss: -0.04312423446940051\n",
      "          vf_explained_var: -0.01618622988462448\n",
      "          vf_loss: 0.0010311803890443924\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.94666666666667\n",
      "    ram_util_percent: 74.93333333333334\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04392622312204178\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.741926710285604\n",
      "    mean_inference_ms: 1.4718434617662048\n",
      "    mean_raw_obs_processing_ms: 0.6373829727544055\n",
      "  time_since_restore: 1763.2706453800201\n",
      "  time_this_iter_s: 10.264616012573242\n",
      "  time_total_s: 1763.2706453800201\n",
      "  timers:\n",
      "    learn_throughput: 1743.399\n",
      "    learn_time_ms: 573.592\n",
      "    load_throughput: 203211.419\n",
      "    load_time_ms: 4.921\n",
      "    sample_throughput: 87.263\n",
      "    sample_time_ms: 11459.652\n",
      "    update_time_ms: 2.277\n",
      "  timestamp: 1631876426\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 156\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">         1763.27</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">    0.15</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 157000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-00-36\n",
      "  done: false\n",
      "  episode_len_mean: 994.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.13\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 157\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3833217951986525\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009533461153151753\n",
      "          policy_loss: 0.11251545051733652\n",
      "          total_loss: 0.10450327814453178\n",
      "          vf_explained_var: -0.01666773110628128\n",
      "          vf_loss: 0.0009947300133515253\n",
      "    num_agent_steps_sampled: 157000\n",
      "    num_agent_steps_trained: 157000\n",
      "    num_steps_sampled: 157000\n",
      "    num_steps_trained: 157000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.65333333333333\n",
      "    ram_util_percent: 75.01333333333332\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04391898995753582\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.734600094419662\n",
      "    mean_inference_ms: 1.4715892490076834\n",
      "    mean_raw_obs_processing_ms: 0.6399756382425745\n",
      "  time_since_restore: 1773.6762130260468\n",
      "  time_this_iter_s: 10.405567646026611\n",
      "  time_total_s: 1773.6762130260468\n",
      "  timers:\n",
      "    learn_throughput: 1742.677\n",
      "    learn_time_ms: 573.83\n",
      "    load_throughput: 199379.373\n",
      "    load_time_ms: 5.016\n",
      "    sample_throughput: 87.203\n",
      "    sample_time_ms: 11467.52\n",
      "    update_time_ms: 2.283\n",
      "  timestamp: 1631876436\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 157000\n",
      "  training_iteration: 157\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">         1773.68</td><td style=\"text-align: right;\">157000</td><td style=\"text-align: right;\">    0.13</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 158000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-00-47\n",
      "  done: false\n",
      "  episode_len_mean: 994.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.13\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 158\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6384264296955533\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010433981701038616\n",
      "          policy_loss: 0.01134790347682105\n",
      "          total_loss: 0.0009731046441528533\n",
      "          vf_explained_var: -0.7176402807235718\n",
      "          vf_loss: 0.000727260601221739\n",
      "    num_agent_steps_sampled: 158000\n",
      "    num_agent_steps_trained: 158000\n",
      "    num_steps_sampled: 158000\n",
      "    num_steps_trained: 158000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.10666666666666\n",
      "    ram_util_percent: 75.2\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04391199146278245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.72751710417751\n",
      "    mean_inference_ms: 1.4713416752361972\n",
      "    mean_raw_obs_processing_ms: 0.6425855262513935\n",
      "  time_since_restore: 1784.003381729126\n",
      "  time_this_iter_s: 10.327168703079224\n",
      "  time_total_s: 1784.003381729126\n",
      "  timers:\n",
      "    learn_throughput: 1757.243\n",
      "    learn_time_ms: 569.073\n",
      "    load_throughput: 201147.329\n",
      "    load_time_ms: 4.971\n",
      "    sample_throughput: 87.424\n",
      "    sample_time_ms: 11438.444\n",
      "    update_time_ms: 2.165\n",
      "  timestamp: 1631876447\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 158000\n",
      "  training_iteration: 158\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">            1784</td><td style=\"text-align: right;\">158000</td><td style=\"text-align: right;\">    0.13</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 159000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-00-57\n",
      "  done: false\n",
      "  episode_len_mean: 994.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.11\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 159\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4106283254093595\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012918988993766018\n",
      "          policy_loss: 0.005773615588744481\n",
      "          total_loss: -0.0013661497582991918\n",
      "          vf_explained_var: -0.621155858039856\n",
      "          vf_loss: 0.0004262766802437707\n",
      "    num_agent_steps_sampled: 159000\n",
      "    num_agent_steps_trained: 159000\n",
      "    num_steps_sampled: 159000\n",
      "    num_steps_trained: 159000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.63333333333333\n",
      "    ram_util_percent: 75.44666666666667\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04390484223753747\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.720556769847104\n",
      "    mean_inference_ms: 1.4710858035945968\n",
      "    mean_raw_obs_processing_ms: 0.6452094604803312\n",
      "  time_since_restore: 1794.3673183918\n",
      "  time_this_iter_s: 10.36393666267395\n",
      "  time_total_s: 1794.3673183918\n",
      "  timers:\n",
      "    learn_throughput: 1759.966\n",
      "    learn_time_ms: 568.193\n",
      "    load_throughput: 201355.909\n",
      "    load_time_ms: 4.966\n",
      "    sample_throughput: 87.478\n",
      "    sample_time_ms: 11431.452\n",
      "    update_time_ms: 2.172\n",
      "  timestamp: 1631876457\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 159000\n",
      "  training_iteration: 159\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">         1794.37</td><td style=\"text-align: right;\">159000</td><td style=\"text-align: right;\">    0.11</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             994.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-01-07\n",
      "  done: false\n",
      "  episode_len_mean: 995.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.12\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 160\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9137677457597522\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012291503713535176\n",
      "          policy_loss: -0.09112357778681648\n",
      "          total_loss: -0.10165562646256553\n",
      "          vf_explained_var: -0.4813947081565857\n",
      "          vf_loss: 0.002383052331182019\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.65714285714285\n",
      "    ram_util_percent: 75.49285714285715\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0438978206443627\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.713752126373459\n",
      "    mean_inference_ms: 1.47083205800124\n",
      "    mean_raw_obs_processing_ms: 0.6448779401831273\n",
      "  time_since_restore: 1804.3038051128387\n",
      "  time_this_iter_s: 9.936486721038818\n",
      "  time_total_s: 1804.3038051128387\n",
      "  timers:\n",
      "    learn_throughput: 1760.916\n",
      "    learn_time_ms: 567.886\n",
      "    load_throughput: 297525.342\n",
      "    load_time_ms: 3.361\n",
      "    sample_throughput: 105.345\n",
      "    sample_time_ms: 9492.618\n",
      "    update_time_ms: 2.171\n",
      "  timestamp: 1631876467\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 160\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">          1804.3</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">    0.12</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 161000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-01-17\n",
      "  done: false\n",
      "  episode_len_mean: 995.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.12\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 161\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4310067839092677\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01565998047539887\n",
      "          policy_loss: -0.017453827129469978\n",
      "          total_loss: -0.03240731341971292\n",
      "          vf_explained_var: 0.13516399264335632\n",
      "          vf_loss: 0.0014287205598925033\n",
      "    num_agent_steps_sampled: 161000\n",
      "    num_agent_steps_trained: 161000\n",
      "    num_steps_sampled: 161000\n",
      "    num_steps_trained: 161000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.72307692307691\n",
      "    ram_util_percent: 75.52307692307691\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04389098104813647\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.70680925658832\n",
      "    mean_inference_ms: 1.4705834879656654\n",
      "    mean_raw_obs_processing_ms: 0.6446091244596311\n",
      "  time_since_restore: 1813.8343353271484\n",
      "  time_this_iter_s: 9.530530214309692\n",
      "  time_total_s: 1813.8343353271484\n",
      "  timers:\n",
      "    learn_throughput: 1765.954\n",
      "    learn_time_ms: 566.266\n",
      "    load_throughput: 301802.77\n",
      "    load_time_ms: 3.313\n",
      "    sample_throughput: 106.398\n",
      "    sample_time_ms: 9398.677\n",
      "    update_time_ms: 2.165\n",
      "  timestamp: 1631876477\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 161000\n",
      "  training_iteration: 161\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         1813.83</td><td style=\"text-align: right;\">161000</td><td style=\"text-align: right;\">    0.12</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 162000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-01-26\n",
      "  done: false\n",
      "  episode_len_mean: 995.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.1\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 162\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.190664225154453\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010824558451883999\n",
      "          policy_loss: -0.12057226912842857\n",
      "          total_loss: -0.13651702536476984\n",
      "          vf_explained_var: -0.2148207277059555\n",
      "          vf_loss: 0.0004819512129971473\n",
      "    num_agent_steps_sampled: 162000\n",
      "    num_agent_steps_trained: 162000\n",
      "    num_steps_sampled: 162000\n",
      "    num_steps_trained: 162000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.32142857142858\n",
      "    ram_util_percent: 75.49285714285713\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0438843297957159\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.700037544502019\n",
      "    mean_inference_ms: 1.4703405010271067\n",
      "    mean_raw_obs_processing_ms: 0.6443974705988538\n",
      "  time_since_restore: 1823.5759077072144\n",
      "  time_this_iter_s: 9.741572380065918\n",
      "  time_total_s: 1823.5759077072144\n",
      "  timers:\n",
      "    learn_throughput: 1762.443\n",
      "    learn_time_ms: 567.394\n",
      "    load_throughput: 301397.221\n",
      "    load_time_ms: 3.318\n",
      "    sample_throughput: 107.107\n",
      "    sample_time_ms: 9336.444\n",
      "    update_time_ms: 2.17\n",
      "  timestamp: 1631876486\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 162000\n",
      "  training_iteration: 162\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         1823.58</td><td style=\"text-align: right;\">162000</td><td style=\"text-align: right;\">     0.1</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 163000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-01-37\n",
      "  done: false\n",
      "  episode_len_mean: 995.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.1\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 163\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8918573604689704\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01175949684140442\n",
      "          policy_loss: -0.0891940256787671\n",
      "          total_loss: -0.10015471730795171\n",
      "          vf_explained_var: -0.35300472378730774\n",
      "          vf_loss: 0.0020046352952097853\n",
      "    num_agent_steps_sampled: 163000\n",
      "    num_agent_steps_trained: 163000\n",
      "    num_steps_sampled: 163000\n",
      "    num_steps_trained: 163000\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.11333333333334\n",
      "    ram_util_percent: 75.58666666666666\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043877907815251666\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.693468654445327\n",
      "    mean_inference_ms: 1.4701045366484005\n",
      "    mean_raw_obs_processing_ms: 0.6442426666568823\n",
      "  time_since_restore: 1833.8764672279358\n",
      "  time_this_iter_s: 10.300559520721436\n",
      "  time_total_s: 1833.8764672279358\n",
      "  timers:\n",
      "    learn_throughput: 1767.588\n",
      "    learn_time_ms: 565.743\n",
      "    load_throughput: 299063.373\n",
      "    load_time_ms: 3.344\n",
      "    sample_throughput: 106.125\n",
      "    sample_time_ms: 9422.808\n",
      "    update_time_ms: 2.112\n",
      "  timestamp: 1631876497\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 163000\n",
      "  training_iteration: 163\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   163</td><td style=\"text-align: right;\">         1833.88</td><td style=\"text-align: right;\">163000</td><td style=\"text-align: right;\">     0.1</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-01-47\n",
      "  done: false\n",
      "  episode_len_mean: 995.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.09\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 164\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1704626970820957\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013939547826684464\n",
      "          policy_loss: -0.0519303867386447\n",
      "          total_loss: -0.06556367029746374\n",
      "          vf_explained_var: -0.40505117177963257\n",
      "          vf_loss: 0.0010144463740289211\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.22666666666666\n",
      "    ram_util_percent: 75.59333333333332\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04387163810511864\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.687096300902402\n",
      "    mean_inference_ms: 1.4698731624139165\n",
      "    mean_raw_obs_processing_ms: 0.64414157493272\n",
      "  time_since_restore: 1844.2117834091187\n",
      "  time_this_iter_s: 10.335316181182861\n",
      "  time_total_s: 1844.2117834091187\n",
      "  timers:\n",
      "    learn_throughput: 1764.472\n",
      "    learn_time_ms: 566.742\n",
      "    load_throughput: 298295.557\n",
      "    load_time_ms: 3.352\n",
      "    sample_throughput: 104.954\n",
      "    sample_time_ms: 9527.97\n",
      "    update_time_ms: 2.107\n",
      "  timestamp: 1631876507\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 164\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         1844.21</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">    0.09</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 165000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-01-58\n",
      "  done: false\n",
      "  episode_len_mean: 995.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.08\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 165\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.178907411628299\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011625942576643143\n",
      "          policy_loss: 0.032379552721977234\n",
      "          total_loss: 0.01699422746896744\n",
      "          vf_explained_var: -0.704806923866272\n",
      "          vf_loss: 0.000518113606732287\n",
      "    num_agent_steps_sampled: 165000\n",
      "    num_agent_steps_trained: 165000\n",
      "    num_steps_sampled: 165000\n",
      "    num_steps_trained: 165000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.9\n",
      "    ram_util_percent: 75.6866666666667\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04386550795754943\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.68089452304632\n",
      "    mean_inference_ms: 1.4696445755739196\n",
      "    mean_raw_obs_processing_ms: 0.6440920105807073\n",
      "  time_since_restore: 1854.5696046352386\n",
      "  time_this_iter_s: 10.357821226119995\n",
      "  time_total_s: 1854.5696046352386\n",
      "  timers:\n",
      "    learn_throughput: 1762.072\n",
      "    learn_time_ms: 567.514\n",
      "    load_throughput: 302829.088\n",
      "    load_time_ms: 3.302\n",
      "    sample_throughput: 104.405\n",
      "    sample_time_ms: 9578.1\n",
      "    update_time_ms: 2.115\n",
      "  timestamp: 1631876518\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 165000\n",
      "  training_iteration: 165\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">         1854.57</td><td style=\"text-align: right;\">165000</td><td style=\"text-align: right;\">    0.08</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 166000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-02-08\n",
      "  done: false\n",
      "  episode_len_mean: 995.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.08\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 166\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2224747472339206\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01317578535290621\n",
      "          policy_loss: -0.001635990043481191\n",
      "          total_loss: -0.016316442812482516\n",
      "          vf_explained_var: -0.9812367558479309\n",
      "          vf_loss: 0.0008740512401952098\n",
      "    num_agent_steps_sampled: 166000\n",
      "    num_agent_steps_trained: 166000\n",
      "    num_steps_sampled: 166000\n",
      "    num_steps_trained: 166000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.09285714285715\n",
      "    ram_util_percent: 75.7785714285714\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04385928847589225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.67483834577644\n",
      "    mean_inference_ms: 1.4694183658051272\n",
      "    mean_raw_obs_processing_ms: 0.6440923197674542\n",
      "  time_since_restore: 1864.8783032894135\n",
      "  time_this_iter_s: 10.308698654174805\n",
      "  time_total_s: 1864.8783032894135\n",
      "  timers:\n",
      "    learn_throughput: 1761.692\n",
      "    learn_time_ms: 567.636\n",
      "    load_throughput: 301494.713\n",
      "    load_time_ms: 3.317\n",
      "    sample_throughput: 104.358\n",
      "    sample_time_ms: 9582.365\n",
      "    update_time_ms: 2.132\n",
      "  timestamp: 1631876528\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 166000\n",
      "  training_iteration: 166\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         1864.88</td><td style=\"text-align: right;\">166000</td><td style=\"text-align: right;\">    0.08</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 167000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-02-18\n",
      "  done: false\n",
      "  episode_len_mean: 995.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.06\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 167\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.335572028160095\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015070919458633201\n",
      "          policy_loss: 0.03638131335998575\n",
      "          total_loss: 0.021366948241160977\n",
      "          vf_explained_var: -0.6098153591156006\n",
      "          vf_loss: 0.0007117013825336471\n",
      "    num_agent_steps_sampled: 167000\n",
      "    num_agent_steps_trained: 167000\n",
      "    num_steps_sampled: 167000\n",
      "    num_steps_trained: 167000\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.17999999999999\n",
      "    ram_util_percent: 75.78666666666665\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043852990267721194\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.66891882526793\n",
      "    mean_inference_ms: 1.4691941128254007\n",
      "    mean_raw_obs_processing_ms: 0.6441399490014067\n",
      "  time_since_restore: 1875.1533377170563\n",
      "  time_this_iter_s: 10.275034427642822\n",
      "  time_total_s: 1875.1533377170563\n",
      "  timers:\n",
      "    learn_throughput: 1762.135\n",
      "    learn_time_ms: 567.493\n",
      "    load_throughput: 308205.279\n",
      "    load_time_ms: 3.245\n",
      "    sample_throughput: 104.498\n",
      "    sample_time_ms: 9569.549\n",
      "    update_time_ms: 2.126\n",
      "  timestamp: 1631876538\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 167000\n",
      "  training_iteration: 167\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         1875.15</td><td style=\"text-align: right;\">167000</td><td style=\"text-align: right;\">    0.06</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-02-28\n",
      "  done: false\n",
      "  episode_len_mean: 995.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.05\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 168\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3011095682779947\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012728784881423814\n",
      "          policy_loss: -0.05787673791249593\n",
      "          total_loss: -0.07370088555746608\n",
      "          vf_explained_var: -0.29079124331474304\n",
      "          vf_loss: 0.0007429998332453478\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.00666666666666\n",
      "    ram_util_percent: 75.85333333333332\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04384681180554293\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.663183143474132\n",
      "    mean_inference_ms: 1.4689757385659896\n",
      "    mean_raw_obs_processing_ms: 0.644232693242259\n",
      "  time_since_restore: 1885.4264106750488\n",
      "  time_this_iter_s: 10.273072957992554\n",
      "  time_total_s: 1885.4264106750488\n",
      "  timers:\n",
      "    learn_throughput: 1758.181\n",
      "    learn_time_ms: 568.77\n",
      "    load_throughput: 301776.713\n",
      "    load_time_ms: 3.314\n",
      "    sample_throughput: 104.554\n",
      "    sample_time_ms: 9564.437\n",
      "    update_time_ms: 2.1\n",
      "  timestamp: 1631876548\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 168\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         1885.43</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">    0.05</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 169000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-02-39\n",
      "  done: false\n",
      "  episode_len_mean: 995.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0\n",
      "  episode_reward_mean: 0.03\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 169\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.274161137474908\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012800890601594507\n",
      "          policy_loss: 0.006575019988748762\n",
      "          total_loss: -0.007906599673959944\n",
      "          vf_explained_var: -0.41175511479377747\n",
      "          vf_loss: 0.0017795432397785286\n",
      "    num_agent_steps_sampled: 169000\n",
      "    num_agent_steps_trained: 169000\n",
      "    num_steps_sampled: 169000\n",
      "    num_steps_trained: 169000\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.733333333333334\n",
      "    ram_util_percent: 75.97333333333333\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04384074645128834\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.657644071267944\n",
      "    mean_inference_ms: 1.4687623157428242\n",
      "    mean_raw_obs_processing_ms: 0.6443692861036618\n",
      "  time_since_restore: 1895.960352897644\n",
      "  time_this_iter_s: 10.533942222595215\n",
      "  time_total_s: 1895.960352897644\n",
      "  timers:\n",
      "    learn_throughput: 1756.814\n",
      "    learn_time_ms: 569.212\n",
      "    load_throughput: 302580.04\n",
      "    load_time_ms: 3.305\n",
      "    sample_throughput: 104.373\n",
      "    sample_time_ms: 9581.016\n",
      "    update_time_ms: 2.088\n",
      "  timestamp: 1631876559\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 169000\n",
      "  training_iteration: 169\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         1895.96</td><td style=\"text-align: right;\">169000</td><td style=\"text-align: right;\">    0.03</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 170000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-02-49\n",
      "  done: false\n",
      "  episode_len_mean: 995.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 170\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2840764893425836\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010646689515057477\n",
      "          policy_loss: -0.029184678776396647\n",
      "          total_loss: -0.04553200455589427\n",
      "          vf_explained_var: -0.3276887536048889\n",
      "          vf_loss: 0.001103551326216095\n",
      "    num_agent_steps_sampled: 170000\n",
      "    num_agent_steps_trained: 170000\n",
      "    num_steps_sampled: 170000\n",
      "    num_steps_trained: 170000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.29333333333334\n",
      "    ram_util_percent: 76.06\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04383481792341376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.652281530869907\n",
      "    mean_inference_ms: 1.4685538350398022\n",
      "    mean_raw_obs_processing_ms: 0.6445476952089745\n",
      "  time_since_restore: 1906.3826642036438\n",
      "  time_this_iter_s: 10.422311305999756\n",
      "  time_total_s: 1906.3826642036438\n",
      "  timers:\n",
      "    learn_throughput: 1758.406\n",
      "    learn_time_ms: 568.697\n",
      "    load_throughput: 303563.317\n",
      "    load_time_ms: 3.294\n",
      "    sample_throughput: 103.84\n",
      "    sample_time_ms: 9630.166\n",
      "    update_time_ms: 2.092\n",
      "  timestamp: 1631876569\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 170000\n",
      "  training_iteration: 170\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">         1906.38</td><td style=\"text-align: right;\">170000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 171000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-03-00\n",
      "  done: false\n",
      "  episode_len_mean: 995.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 171\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.38516137070126\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011094676014775474\n",
      "          policy_loss: -0.08371319671471913\n",
      "          total_loss: -0.10148370448085997\n",
      "          vf_explained_var: -0.6827777624130249\n",
      "          vf_loss: 0.0004644266007946701\n",
      "    num_agent_steps_sampled: 171000\n",
      "    num_agent_steps_trained: 171000\n",
      "    num_steps_sampled: 171000\n",
      "    num_steps_trained: 171000\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.221428571428575\n",
      "    ram_util_percent: 76.13571428571429\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043829064519706876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.647090938020618\n",
      "    mean_inference_ms: 1.468351909258812\n",
      "    mean_raw_obs_processing_ms: 0.6447674148352902\n",
      "  time_since_restore: 1916.6961467266083\n",
      "  time_this_iter_s: 10.313482522964478\n",
      "  time_total_s: 1916.6961467266083\n",
      "  timers:\n",
      "    learn_throughput: 1752.988\n",
      "    learn_time_ms: 570.455\n",
      "    load_throughput: 296201.634\n",
      "    load_time_ms: 3.376\n",
      "    sample_throughput: 103.022\n",
      "    sample_time_ms: 9706.68\n",
      "    update_time_ms: 2.103\n",
      "  timestamp: 1631876580\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 171000\n",
      "  training_iteration: 171\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">          1916.7</td><td style=\"text-align: right;\">171000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-03-10\n",
      "  done: false\n",
      "  episode_len_mean: 995.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 172\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.182523663838704\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01571033091619191\n",
      "          policy_loss: -0.07426611404452059\n",
      "          total_loss: -0.08720737256937557\n",
      "          vf_explained_var: 0.06249856948852539\n",
      "          vf_loss: 0.0009306250545907662\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.82666666666667\n",
      "    ram_util_percent: 76.19333333333336\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043823435518338894\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.642052944545208\n",
      "    mean_inference_ms: 1.4681536137951863\n",
      "    mean_raw_obs_processing_ms: 0.6450266402706668\n",
      "  time_since_restore: 1926.749804019928\n",
      "  time_this_iter_s: 10.053657293319702\n",
      "  time_total_s: 1926.749804019928\n",
      "  timers:\n",
      "    learn_throughput: 1758.144\n",
      "    learn_time_ms: 568.782\n",
      "    load_throughput: 288534.045\n",
      "    load_time_ms: 3.466\n",
      "    sample_throughput: 102.675\n",
      "    sample_time_ms: 9739.493\n",
      "    update_time_ms: 2.152\n",
      "  timestamp: 1631876590\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 172\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         1926.75</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 173000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-03-20\n",
      "  done: false\n",
      "  episode_len_mean: 995.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.02\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 173\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.224958732393053\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010371352499100803\n",
      "          policy_loss: -0.11046441404355897\n",
      "          total_loss: -0.12078305847114987\n",
      "          vf_explained_var: 0.5977604985237122\n",
      "          vf_loss: 0.0066804433642472655\n",
      "    num_agent_steps_sampled: 173000\n",
      "    num_agent_steps_trained: 173000\n",
      "    num_steps_sampled: 173000\n",
      "    num_steps_trained: 173000\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.26666666666667\n",
      "    ram_util_percent: 76.22666666666669\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04381790312756882\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.63716264249421\n",
      "    mean_inference_ms: 1.4679593997783664\n",
      "    mean_raw_obs_processing_ms: 0.6453187254262832\n",
      "  time_since_restore: 1937.306592464447\n",
      "  time_this_iter_s: 10.556788444519043\n",
      "  time_total_s: 1937.306592464447\n",
      "  timers:\n",
      "    learn_throughput: 1754.571\n",
      "    learn_time_ms: 569.94\n",
      "    load_throughput: 290152.814\n",
      "    load_time_ms: 3.446\n",
      "    sample_throughput: 102.418\n",
      "    sample_time_ms: 9763.884\n",
      "    update_time_ms: 2.153\n",
      "  timestamp: 1631876600\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 173000\n",
      "  training_iteration: 173\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         1937.31</td><td style=\"text-align: right;\">173000</td><td style=\"text-align: right;\">    0.02</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 174000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-03-31\n",
      "  done: false\n",
      "  episode_len_mean: 995.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 174\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.282733792728848\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017467400343491334\n",
      "          policy_loss: -0.11323611707323128\n",
      "          total_loss: -0.125669711165958\n",
      "          vf_explained_var: -0.27573341131210327\n",
      "          vf_loss: 0.0015508715056865993\n",
      "    num_agent_steps_sampled: 174000\n",
      "    num_agent_steps_trained: 174000\n",
      "    num_steps_sampled: 174000\n",
      "    num_steps_trained: 174000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.78125\n",
      "    ram_util_percent: 76.39375\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04381268039901808\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.632395276263884\n",
      "    mean_inference_ms: 1.467770585862283\n",
      "    mean_raw_obs_processing_ms: 0.6456472210282358\n",
      "  time_since_restore: 1948.2796421051025\n",
      "  time_this_iter_s: 10.973049640655518\n",
      "  time_total_s: 1948.2796421051025\n",
      "  timers:\n",
      "    learn_throughput: 1743.18\n",
      "    learn_time_ms: 573.664\n",
      "    load_throughput: 278236.504\n",
      "    load_time_ms: 3.594\n",
      "    sample_throughput: 101.794\n",
      "    sample_time_ms: 9823.797\n",
      "    update_time_ms: 2.15\n",
      "  timestamp: 1631876611\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 174000\n",
      "  training_iteration: 174\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         1948.28</td><td style=\"text-align: right;\">174000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 175000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-03-43\n",
      "  done: false\n",
      "  episode_len_mean: 995.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 175\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3679453796810574\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012870424175674897\n",
      "          policy_loss: -0.06831992051253716\n",
      "          total_loss: -0.06857431355553369\n",
      "          vf_explained_var: -0.3477434515953064\n",
      "          vf_loss: 0.01690940961578033\n",
      "    num_agent_steps_sampled: 175000\n",
      "    num_agent_steps_trained: 175000\n",
      "    num_steps_sampled: 175000\n",
      "    num_steps_trained: 175000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.84375\n",
      "    ram_util_percent: 76.48750000000001\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043807677513896165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.627808268783271\n",
      "    mean_inference_ms: 1.4675894344755194\n",
      "    mean_raw_obs_processing_ms: 0.6460099355673347\n",
      "  time_since_restore: 1959.564971446991\n",
      "  time_this_iter_s: 11.285329341888428\n",
      "  time_total_s: 1959.564971446991\n",
      "  timers:\n",
      "    learn_throughput: 1744.311\n",
      "    learn_time_ms: 573.292\n",
      "    load_throughput: 275551.293\n",
      "    load_time_ms: 3.629\n",
      "    sample_throughput: 100.837\n",
      "    sample_time_ms: 9916.964\n",
      "    update_time_ms: 2.146\n",
      "  timestamp: 1631876623\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 175000\n",
      "  training_iteration: 175\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         1959.56</td><td style=\"text-align: right;\">175000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-03-53\n",
      "  done: false\n",
      "  episode_len_mean: 995.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 176\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2835155619515315\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013064539191614037\n",
      "          policy_loss: 0.0297146574076679\n",
      "          total_loss: 0.014081344629327456\n",
      "          vf_explained_var: -0.16117353737354279\n",
      "          vf_loss: 0.0005879176376361606\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.73571428571429\n",
      "    ram_util_percent: 76.35000000000001\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043802806486509666\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.623274498698992\n",
      "    mean_inference_ms: 1.4674117765126709\n",
      "    mean_raw_obs_processing_ms: 0.646405028924608\n",
      "  time_since_restore: 1969.8295221328735\n",
      "  time_this_iter_s: 10.264550685882568\n",
      "  time_total_s: 1969.8295221328735\n",
      "  timers:\n",
      "    learn_throughput: 1743.018\n",
      "    learn_time_ms: 573.718\n",
      "    load_throughput: 274455.023\n",
      "    load_time_ms: 3.644\n",
      "    sample_throughput: 100.886\n",
      "    sample_time_ms: 9912.136\n",
      "    update_time_ms: 2.117\n",
      "  timestamp: 1631876633\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 176\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">         1969.83</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 177000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-04-04\n",
      "  done: false\n",
      "  episode_len_mean: 995.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 177\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.227232805887858\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011936475693456532\n",
      "          policy_loss: -0.025215549446228478\n",
      "          total_loss: -0.040716562968575294\n",
      "          vf_explained_var: -0.14029870927333832\n",
      "          vf_loss: 0.0007284702222225153\n",
      "    num_agent_steps_sampled: 177000\n",
      "    num_agent_steps_trained: 177000\n",
      "    num_steps_sampled: 177000\n",
      "    num_steps_trained: 177000\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.8125\n",
      "    ram_util_percent: 76.25\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043797154228433374\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.61878568690999\n",
      "    mean_inference_ms: 1.4672343455395491\n",
      "    mean_raw_obs_processing_ms: 0.6468293952025749\n",
      "  time_since_restore: 1980.3407654762268\n",
      "  time_this_iter_s: 10.511243343353271\n",
      "  time_total_s: 1980.3407654762268\n",
      "  timers:\n",
      "    learn_throughput: 1742.791\n",
      "    learn_time_ms: 573.792\n",
      "    load_throughput: 275186.101\n",
      "    load_time_ms: 3.634\n",
      "    sample_throughput: 100.647\n",
      "    sample_time_ms: 9935.721\n",
      "    update_time_ms: 2.108\n",
      "  timestamp: 1631876644\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 177000\n",
      "  training_iteration: 177\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         1980.34</td><td style=\"text-align: right;\">177000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 178000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-04-14\n",
      "  done: false\n",
      "  episode_len_mean: 995.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 178\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1923691670099896\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011254348768065527\n",
      "          policy_loss: -0.010531577467918395\n",
      "          total_loss: -0.026149318284458583\n",
      "          vf_explained_var: -0.40956616401672363\n",
      "          vf_loss: 0.0006084373831981793\n",
      "    num_agent_steps_sampled: 178000\n",
      "    num_agent_steps_trained: 178000\n",
      "    num_steps_sampled: 178000\n",
      "    num_steps_trained: 178000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.021428571428565\n",
      "    ram_util_percent: 76.25\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043791644623190296\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.614316937797378\n",
      "    mean_inference_ms: 1.4670607080218927\n",
      "    mean_raw_obs_processing_ms: 0.6472834006733923\n",
      "  time_since_restore: 1990.375663280487\n",
      "  time_this_iter_s: 10.034897804260254\n",
      "  time_total_s: 1990.375663280487\n",
      "  timers:\n",
      "    learn_throughput: 1746.048\n",
      "    learn_time_ms: 572.722\n",
      "    load_throughput: 279832.94\n",
      "    load_time_ms: 3.574\n",
      "    sample_throughput: 100.878\n",
      "    sample_time_ms: 9913.007\n",
      "    update_time_ms: 2.105\n",
      "  timestamp: 1631876654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 178000\n",
      "  training_iteration: 178\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   178</td><td style=\"text-align: right;\">         1990.38</td><td style=\"text-align: right;\">178000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 179000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-04-24\n",
      "  done: false\n",
      "  episode_len_mean: 995.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 179\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.180376594596439\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0156926360711396\n",
      "          policy_loss: -0.13126194303234417\n",
      "          total_loss: -0.14087089598178865\n",
      "          vf_explained_var: -0.8716051578521729\n",
      "          vf_loss: 0.00425041631808401\n",
      "    num_agent_steps_sampled: 179000\n",
      "    num_agent_steps_trained: 179000\n",
      "    num_steps_sampled: 179000\n",
      "    num_steps_trained: 179000\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.742857142857154\n",
      "    ram_util_percent: 76.20000000000002\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04378627265990083\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.60993237121444\n",
      "    mean_inference_ms: 1.4668914566290012\n",
      "    mean_raw_obs_processing_ms: 0.6477643558422524\n",
      "  time_since_restore: 2000.4597551822662\n",
      "  time_this_iter_s: 10.084091901779175\n",
      "  time_total_s: 2000.4597551822662\n",
      "  timers:\n",
      "    learn_throughput: 1745.271\n",
      "    learn_time_ms: 572.977\n",
      "    load_throughput: 276976.068\n",
      "    load_time_ms: 3.61\n",
      "    sample_throughput: 101.342\n",
      "    sample_time_ms: 9867.611\n",
      "    update_time_ms: 2.202\n",
      "  timestamp: 1631876664\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 179000\n",
      "  training_iteration: 179\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         2000.46</td><td style=\"text-align: right;\">179000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            995.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1256)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=1256)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-04-53\n",
      "  done: false\n",
      "  episode_len_mean: 994.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 180\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1687488238016766\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011667559376757808\n",
      "          policy_loss: -0.020664643889500036\n",
      "          total_loss: -0.03556946383582221\n",
      "          vf_explained_var: -0.17514975368976593\n",
      "          vf_loss: 0.0008759678661590442\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.87619047619047\n",
      "    ram_util_percent: 76.09047619047621\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043781018508667696\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.605618075107305\n",
      "    mean_inference_ms: 1.4667261715486846\n",
      "    mean_raw_obs_processing_ms: 0.6492921665816812\n",
      "  time_since_restore: 2029.4828264713287\n",
      "  time_this_iter_s: 29.0230712890625\n",
      "  time_total_s: 2029.4828264713287\n",
      "  timers:\n",
      "    learn_throughput: 1739.235\n",
      "    learn_time_ms: 574.965\n",
      "    load_throughput: 191158.49\n",
      "    load_time_ms: 5.231\n",
      "    sample_throughput: 85.295\n",
      "    sample_time_ms: 11724.085\n",
      "    update_time_ms: 2.206\n",
      "  timestamp: 1631876693\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 180\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         2029.48</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 181000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-05-04\n",
      "  done: false\n",
      "  episode_len_mean: 994.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 181\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.309056300587124\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01201441664914621\n",
      "          policy_loss: -0.0564750418273939\n",
      "          total_loss: -0.07263309222956499\n",
      "          vf_explained_var: -0.24261228740215302\n",
      "          vf_loss: 0.000850212015858334\n",
      "    num_agent_steps_sampled: 181000\n",
      "    num_agent_steps_trained: 181000\n",
      "    num_steps_sampled: 181000\n",
      "    num_steps_trained: 181000\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.1\n",
      "    ram_util_percent: 75.85999999999999\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04377554457832841\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.601144211605744\n",
      "    mean_inference_ms: 1.4665541752568003\n",
      "    mean_raw_obs_processing_ms: 0.6508390178584221\n",
      "  time_since_restore: 2040.1610527038574\n",
      "  time_this_iter_s: 10.678226232528687\n",
      "  time_total_s: 2040.1610527038574\n",
      "  timers:\n",
      "    learn_throughput: 1743.351\n",
      "    learn_time_ms: 573.608\n",
      "    load_throughput: 194285.979\n",
      "    load_time_ms: 5.147\n",
      "    sample_throughput: 85.02\n",
      "    sample_time_ms: 11761.971\n",
      "    update_time_ms: 2.198\n",
      "  timestamp: 1631876704\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 181000\n",
      "  training_iteration: 181\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">         2040.16</td><td style=\"text-align: right;\">181000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 182000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-05-14\n",
      "  done: false\n",
      "  episode_len_mean: 994.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 182\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.225244069099426\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01680151791036187\n",
      "          policy_loss: -0.1111113323105706\n",
      "          total_loss: -0.12377512339088652\n",
      "          vf_explained_var: -0.10492980480194092\n",
      "          vf_loss: 0.0010828832436042526\n",
      "    num_agent_steps_sampled: 182000\n",
      "    num_agent_steps_trained: 182000\n",
      "    num_steps_sampled: 182000\n",
      "    num_steps_trained: 182000\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.449999999999996\n",
      "    ram_util_percent: 76.02142857142857\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04377013717333953\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.596603542806518\n",
      "    mean_inference_ms: 1.466382409340527\n",
      "    mean_raw_obs_processing_ms: 0.6524050052351873\n",
      "  time_since_restore: 2050.2882113456726\n",
      "  time_this_iter_s: 10.127158641815186\n",
      "  time_total_s: 2050.2882113456726\n",
      "  timers:\n",
      "    learn_throughput: 1738.914\n",
      "    learn_time_ms: 575.072\n",
      "    load_throughput: 196808.499\n",
      "    load_time_ms: 5.081\n",
      "    sample_throughput: 84.977\n",
      "    sample_time_ms: 11767.879\n",
      "    update_time_ms: 2.15\n",
      "  timestamp: 1631876714\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 182000\n",
      "  training_iteration: 182\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         2050.29</td><td style=\"text-align: right;\">182000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 183000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-05-24\n",
      "  done: false\n",
      "  episode_len_mean: 994.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 183\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.265346254242791\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011703601084769252\n",
      "          policy_loss: -0.040201109668446915\n",
      "          total_loss: -0.05631825142643518\n",
      "          vf_explained_var: -0.7376633286476135\n",
      "          vf_loss: 0.000611370137873261\n",
      "    num_agent_steps_sampled: 183000\n",
      "    num_agent_steps_trained: 183000\n",
      "    num_steps_sampled: 183000\n",
      "    num_steps_trained: 183000\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.88666666666666\n",
      "    ram_util_percent: 76.01333333333334\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04376481255297593\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.592065958874338\n",
      "    mean_inference_ms: 1.4662142514061074\n",
      "    mean_raw_obs_processing_ms: 0.6539921172055879\n",
      "  time_since_restore: 2060.6871824264526\n",
      "  time_this_iter_s: 10.39897108078003\n",
      "  time_total_s: 2060.6871824264526\n",
      "  timers:\n",
      "    learn_throughput: 1744.165\n",
      "    learn_time_ms: 573.34\n",
      "    load_throughput: 196942.495\n",
      "    load_time_ms: 5.078\n",
      "    sample_throughput: 85.078\n",
      "    sample_time_ms: 11753.895\n",
      "    update_time_ms: 2.155\n",
      "  timestamp: 1631876724\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 183000\n",
      "  training_iteration: 183\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">         2060.69</td><td style=\"text-align: right;\">183000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-05-34\n",
      "  done: false\n",
      "  episode_len_mean: 994.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 184\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.21157714260949\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012727961205346134\n",
      "          policy_loss: -0.10105713274743822\n",
      "          total_loss: -0.11610578108165\n",
      "          vf_explained_var: -0.34089672565460205\n",
      "          vf_loss: 0.000623591627421168\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.09333333333334\n",
      "    ram_util_percent: 76.0\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04375955739161871\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.587535078204056\n",
      "    mean_inference_ms: 1.4660482505275545\n",
      "    mean_raw_obs_processing_ms: 0.6555983221627133\n",
      "  time_since_restore: 2070.9698774814606\n",
      "  time_this_iter_s: 10.282695055007935\n",
      "  time_total_s: 2070.9698774814606\n",
      "  timers:\n",
      "    learn_throughput: 1757.178\n",
      "    learn_time_ms: 569.094\n",
      "    load_throughput: 203013.717\n",
      "    load_time_ms: 4.926\n",
      "    sample_throughput: 85.549\n",
      "    sample_time_ms: 11689.207\n",
      "    update_time_ms: 2.156\n",
      "  timestamp: 1631876734\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 184\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">         2070.97</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 185000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-05-44\n",
      "  done: false\n",
      "  episode_len_mean: 994.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 185\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.174842890103658\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016109971798889372\n",
      "          policy_loss: -0.10752680874947045\n",
      "          total_loss: -0.12055228675405184\n",
      "          vf_explained_var: -0.8872271180152893\n",
      "          vf_loss: 0.0005672761510747174\n",
      "    num_agent_steps_sampled: 185000\n",
      "    num_agent_steps_trained: 185000\n",
      "    num_steps_sampled: 185000\n",
      "    num_steps_trained: 185000\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.03999999999999\n",
      "    ram_util_percent: 75.92\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043754352506147544\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.583030404745777\n",
      "    mean_inference_ms: 1.46588486379095\n",
      "    mean_raw_obs_processing_ms: 0.6572214985458076\n",
      "  time_since_restore: 2081.0609662532806\n",
      "  time_this_iter_s: 10.091088771820068\n",
      "  time_total_s: 2081.0609662532806\n",
      "  timers:\n",
      "    learn_throughput: 1757.897\n",
      "    learn_time_ms: 568.861\n",
      "    load_throughput: 204314.155\n",
      "    load_time_ms: 4.894\n",
      "    sample_throughput: 86.43\n",
      "    sample_time_ms: 11570.057\n",
      "    update_time_ms: 2.155\n",
      "  timestamp: 1631876744\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 185000\n",
      "  training_iteration: 185\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">         2081.06</td><td style=\"text-align: right;\">185000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 186000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-05-55\n",
      "  done: false\n",
      "  episode_len_mean: 994.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 186\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2400975200865005\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01211819023545478\n",
      "          policy_loss: -0.055414970964193344\n",
      "          total_loss: -0.07126228403713968\n",
      "          vf_explained_var: -0.8475503921508789\n",
      "          vf_loss: 0.0004188264197889819\n",
      "    num_agent_steps_sampled: 186000\n",
      "    num_agent_steps_trained: 186000\n",
      "    num_steps_sampled: 186000\n",
      "    num_steps_trained: 186000\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.385714285714286\n",
      "    ram_util_percent: 75.88571428571427\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.043749053275761084\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.57856009838327\n",
      "    mean_inference_ms: 1.4657245536833352\n",
      "    mean_raw_obs_processing_ms: 0.6588611625595117\n",
      "  time_since_restore: 2091.306697845459\n",
      "  time_this_iter_s: 10.245731592178345\n",
      "  time_total_s: 2091.306697845459\n",
      "  timers:\n",
      "    learn_throughput: 1761.266\n",
      "    learn_time_ms: 567.773\n",
      "    load_throughput: 205566.861\n",
      "    load_time_ms: 4.865\n",
      "    sample_throughput: 86.436\n",
      "    sample_time_ms: 11569.197\n",
      "    update_time_ms: 2.164\n",
      "  timestamp: 1631876755\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 186000\n",
      "  training_iteration: 186\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">         2091.31</td><td style=\"text-align: right;\">186000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 187000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-06-05\n",
      "  done: false\n",
      "  episode_len_mean: 994.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 187\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.228593791855706\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012164522411974271\n",
      "          policy_loss: -0.09036179184913636\n",
      "          total_loss: -0.10591664765444067\n",
      "          vf_explained_var: -0.7796304225921631\n",
      "          vf_loss: 0.0005727878351333654\n",
      "    num_agent_steps_sampled: 187000\n",
      "    num_agent_steps_trained: 187000\n",
      "    num_steps_sampled: 187000\n",
      "    num_steps_trained: 187000\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.2\n",
      "    ram_util_percent: 75.89333333333335\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04374379815108733\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.57409117680183\n",
      "    mean_inference_ms: 1.4655668715613124\n",
      "    mean_raw_obs_processing_ms: 0.6605160131060382\n",
      "  time_since_restore: 2101.3497800827026\n",
      "  time_this_iter_s: 10.043082237243652\n",
      "  time_total_s: 2101.3497800827026\n",
      "  timers:\n",
      "    learn_throughput: 1760.504\n",
      "    learn_time_ms: 568.019\n",
      "    load_throughput: 203520.05\n",
      "    load_time_ms: 4.914\n",
      "    sample_throughput: 86.79\n",
      "    sample_time_ms: 11522.069\n",
      "    update_time_ms: 2.179\n",
      "  timestamp: 1631876765\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 187000\n",
      "  training_iteration: 187\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">         2101.35</td><td style=\"text-align: right;\">187000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-06-15\n",
      "  done: false\n",
      "  episode_len_mean: 994.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 188\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.217365324497223\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012953798971556334\n",
      "          policy_loss: -0.023773056993054018\n",
      "          total_loss: -0.033955124786330594\n",
      "          vf_explained_var: -0.955640435218811\n",
      "          vf_loss: 0.005433721238902459\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.21428571428572\n",
      "    ram_util_percent: 75.87142857142855\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04373865970266362\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.56963116778479\n",
      "    mean_inference_ms: 1.4654121778621514\n",
      "    mean_raw_obs_processing_ms: 0.6621849740168722\n",
      "  time_since_restore: 2111.693195581436\n",
      "  time_this_iter_s: 10.34341549873352\n",
      "  time_total_s: 2111.693195581436\n",
      "  timers:\n",
      "    learn_throughput: 1761.301\n",
      "    learn_time_ms: 567.762\n",
      "    load_throughput: 204079.543\n",
      "    load_time_ms: 4.9\n",
      "    sample_throughput: 86.556\n",
      "    sample_time_ms: 11553.205\n",
      "    update_time_ms: 2.196\n",
      "  timestamp: 1631876775\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 188\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   188</td><td style=\"text-align: right;\">         2111.69</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 189000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-06-25\n",
      "  done: false\n",
      "  episode_len_mean: 994.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 189\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.267400532298618\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01370651001896124\n",
      "          policy_loss: -0.04136600012166632\n",
      "          total_loss: -0.05643868429793252\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0006624003523029387\n",
      "    num_agent_steps_sampled: 189000\n",
      "    num_agent_steps_trained: 189000\n",
      "    num_steps_sampled: 189000\n",
      "    num_steps_trained: 189000\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.25333333333334\n",
      "    ram_util_percent: 75.94666666666667\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04373363299237731\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.565168719438313\n",
      "    mean_inference_ms: 1.46526041816228\n",
      "    mean_raw_obs_processing_ms: 0.6638686408616046\n",
      "  time_since_restore: 2121.654836177826\n",
      "  time_this_iter_s: 9.96164059638977\n",
      "  time_total_s: 2121.654836177826\n",
      "  timers:\n",
      "    learn_throughput: 1763.172\n",
      "    learn_time_ms: 567.16\n",
      "    load_throughput: 206100.202\n",
      "    load_time_ms: 4.852\n",
      "    sample_throughput: 86.642\n",
      "    sample_time_ms: 11541.714\n",
      "    update_time_ms: 2.115\n",
      "  timestamp: 1631876785\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 189000\n",
      "  training_iteration: 189\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">         2121.65</td><td style=\"text-align: right;\">189000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            994.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 190000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-06-36\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 190\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8242662787437438\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013770649587550447\n",
      "          policy_loss: -0.08026177552011278\n",
      "          total_loss: -0.08465497112936443\n",
      "          vf_explained_var: -0.5383815765380859\n",
      "          vf_loss: 0.006878074133419431\n",
      "    num_agent_steps_sampled: 190000\n",
      "    num_agent_steps_trained: 190000\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.35714285714287\n",
      "    ram_util_percent: 76.14285714285714\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04372872511611256\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.560716321402039\n",
      "    mean_inference_ms: 1.4651123098157681\n",
      "    mean_raw_obs_processing_ms: 0.6636208118038485\n",
      "  time_since_restore: 2132.0831570625305\n",
      "  time_this_iter_s: 10.42832088470459\n",
      "  time_total_s: 2132.0831570625305\n",
      "  timers:\n",
      "    learn_throughput: 1766.724\n",
      "    learn_time_ms: 566.019\n",
      "    load_throughput: 307171.503\n",
      "    load_time_ms: 3.256\n",
      "    sample_throughput: 103.253\n",
      "    sample_time_ms: 9684.908\n",
      "    update_time_ms: 2.113\n",
      "  timestamp: 1631876796\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 190\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   190</td><td style=\"text-align: right;\">         2132.08</td><td style=\"text-align: right;\">190000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 191000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-06-46\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 191\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0665960788726805\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010598838887983872\n",
      "          policy_loss: -0.052793215851609905\n",
      "          total_loss: -0.06752159482695991\n",
      "          vf_explained_var: -0.6836352348327637\n",
      "          vf_loss: 0.0005719186663756975\n",
      "    num_agent_steps_sampled: 191000\n",
      "    num_agent_steps_trained: 191000\n",
      "    num_steps_sampled: 191000\n",
      "    num_steps_trained: 191000\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.51333333333333\n",
      "    ram_util_percent: 76.21333333333335\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04372391957379856\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.556128564099833\n",
      "    mean_inference_ms: 1.4649671980975862\n",
      "    mean_raw_obs_processing_ms: 0.6634087555717211\n",
      "  time_since_restore: 2142.254143714905\n",
      "  time_this_iter_s: 10.170986652374268\n",
      "  time_total_s: 2142.254143714905\n",
      "  timers:\n",
      "    learn_throughput: 1765.145\n",
      "    learn_time_ms: 566.526\n",
      "    load_throughput: 306944.463\n",
      "    load_time_ms: 3.258\n",
      "    sample_throughput: 103.803\n",
      "    sample_time_ms: 9633.646\n",
      "    update_time_ms: 2.142\n",
      "  timestamp: 1631876806\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 191000\n",
      "  training_iteration: 191\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">         2142.25</td><td style=\"text-align: right;\">191000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_52b7f_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-09-17_11-06-56\n",
      "  done: false\n",
      "  episode_len_mean: 995.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 1.0\n",
      "  episode_reward_mean: 0.01\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 192\n",
      "  experiment_id: 47168954e393400b9c4f2abc75550571\n",
      "  hostname: linar-B360M-D2V\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1529735565185546\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012683575204726181\n",
      "          policy_loss: -0.027272874825737543\n",
      "          total_loss: -0.041187533599117566\n",
      "          vf_explained_var: -0.4902389645576477\n",
      "          vf_loss: 0.0011940196690071994\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 192.168.1.100\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.70666666666667\n",
      "    ram_util_percent: 76.19333333333336\n",
      "  pid: 1260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.04371921957021139\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 8.55161598817834\n",
      "    mean_inference_ms: 1.4648242691324904\n",
      "    mean_raw_obs_processing_ms: 0.6632311427128512\n",
      "  time_since_restore: 2152.5745429992676\n",
      "  time_this_iter_s: 10.320399284362793\n",
      "  time_total_s: 2152.5745429992676\n",
      "  timers:\n",
      "    learn_throughput: 1763.375\n",
      "    learn_time_ms: 567.094\n",
      "    load_throughput: 310137.829\n",
      "    load_time_ms: 3.224\n",
      "    sample_throughput: 103.601\n",
      "    sample_time_ms: 9652.438\n",
      "    update_time_ms: 2.139\n",
      "  timestamp: 1631876816\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 192\n",
      "  trial_id: 52b7f_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/6 CPUs, 1.0/1 GPUs, 0.0/8.57 GiB heap, 0.0/4.28 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-09-17_10-30-48<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_52b7f_00000</td><td>RUNNING </td><td>192.168.1.100:1260</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">         2152.57</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">    0.01</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             995.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 1,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO C22 not pretrained\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger])#callbacks=[\n",
    "        #    CustomLoggerCallback(),\n",
    "        #])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2c1f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
