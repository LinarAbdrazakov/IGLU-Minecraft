{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb870666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caa5d9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4, padding=0),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(64, 512, kernel_size=2, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a746e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'self.encoder = VisualEncoder()\\n        self.encoder.load_state_dict(\\n            torch.load(\"/IGLU-Minecraft/models/AnnaCNN/encoder_weigths.pth\", map_location=torch.device(\\'cpu\\'))\\n        )'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"self.encoder = VisualEncoder()\n",
    "        self.encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AnnaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7442b65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "env.update_taskset(TaskSet(preset=['C17']))\n",
    "env = PovOnlyWrapper(env)\n",
    "env = IgluActionWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ffed01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2f10f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space.sample()\n",
    "obs, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63453f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 11, 11)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info['target_grid'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d505edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import one_hot\n",
    "\n",
    "target = info['target_grid']\n",
    "target = one_hot(torch.tensor(target, dtype=torch.long), num_classes=7).permute(3, 0, 1, 2).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dfef2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 9, 11, 11])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5103b338",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.tensor(obs.copy(), dtype=torch.float).permute(2, 0, 1) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3eefe574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a493e9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d6b06f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_encoder = VisualEncoder()\n",
    "img_features = visual_encoder(img[None,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1570421c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "168157ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_1 = nn.Linear(512, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45c9548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = linear_1(img_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80ab4131",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_1 = nn.Conv3d(7, 7, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f74c00f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 9, 11, 11])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = conv_1(target[None,:])\n",
    "V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf92a21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7]), torch.Size([7, 9, 11, 11]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q[0].shape, V[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6afacbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention proposed in \"Attention Is All You Need\"\n",
    "    Compute the dot products of the query with all keys, divide each by sqrt(dim),\n",
    "    and apply a softmax function to obtain the weights on the values\n",
    "    Args: dim, mask\n",
    "        dim (int): dimention of attention\n",
    "        mask (torch.Tensor): tensor containing indices to be masked\n",
    "    Inputs: query, key, value, mask\n",
    "        - **query** (batch, q_len, d_model): tensor containing projection vector for decoder.\n",
    "        - **key** (batch, k_len, d_model): tensor containing projection vector for encoder.\n",
    "        - **value** (batch, v_len, d_model): tensor containing features of the encoded input sequence.\n",
    "        - **mask** (-): tensor containing indices to be masked\n",
    "    Returns: context, attn\n",
    "        - **context**: tensor containing the context vector from attention mechanism.\n",
    "        - **attn**: tensor containing the attention (alignment) from the encoder outputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.sqrt_dim = np.sqrt(dim)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor, mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:\n",
    "        score = torch.bmm(query, key.transpose(1, 2)) / self.sqrt_dim\n",
    "\n",
    "        if mask is not None:\n",
    "            score.masked_fill_(mask.view(score.size()), -float('Inf'))\n",
    "\n",
    "        attn = F.softmax(score, -1)\n",
    "        context = torch.bmm(attn, value)\n",
    "        return context, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f9eb231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1089, 7])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = value = V.reshape(-1, 7, 9*11*11).permute(0, 2, 1)\n",
    "value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d08b40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 7])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = Q.reshape(-1, 1, 7)\n",
    "query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "978aa9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = ScaledDotProductAttention(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c66f8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1089])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention(query, key, value)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d101bf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea4a9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3d72b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7ed148f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (693x11 and 7x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_91/1842398965.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (693x11 and 7x1)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Q: (bs, q_length=ch)\n",
    "    K, V: (bs, ch, heigth, width, depth)\n",
    "\"\"\"\n",
    "\n",
    "Q[0] @ V.permute(0, 2, 3, 4, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "185b956d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 11, 11, 7])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.permute(0, 2, 3, 4, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5b6b8fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "693"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7 * 9 * 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9298d042",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (693x11 and 7x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_91/1185961937.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcross_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_91/950179151.py\u001b[0m in \u001b[0;36mcross_attention\u001b[0;34m(Q, K, V)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheigth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mK\u001b[0m \u001b[0;31m# (bs, 1, height, width, depth)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (693x11 and 7x1)"
     ]
    }
   ],
   "source": [
    "cross_attention(Q, V, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "22e97ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5262ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7bbae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1737d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Q: (bs, q_length=ch)\n",
    "    K, V: (bs, ch, heigth, width, depth)\n",
    "    \"\"\"\n",
    "    scores = Q @ K # (bs, 1, height, width, depth)\n",
    "    scores = nn.functional.softmax(scores, dim=1)\n",
    "    result = torch.matmul(scores.view(0, 2, 3, 4, 1), V.view(0, 2, 3, 4, 1)).view(0, 4, 1, 2, 3)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d239376",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        self.visual_encoder = VisualEncoder()\n",
    "        self.visual_dim = 512\n",
    "        \n",
    "        self.conv_1 = nn.Conv3d(7, 7, 1)\n",
    "        self.linear_1 = nn.Linear(self.visual_dim, 7)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb19c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(self, Q, K, V):\n",
    "    batch_size = Q.size(0)\n",
    "    k_length = K.size(-2)\n",
    "\n",
    "    # Scaling by d_k so that the soft(arg)max doesnt saturate\n",
    "    Q = Q / np.sqrt(self.d_k)  # (bs, n_heads, q_length, dim_per_head)\n",
    "    scores = torch.matmul(Q, K.transpose(2,3))  # (bs, n_heads, q_length, k_length)\n",
    "\n",
    "    A = nn_Softargmax(dim=-1)(scores)  # (bs, n_heads, q_length, k_length)\n",
    "\n",
    "    # Get the weighted average of the values\n",
    "    H = torch.matmul(A, V)  # (bs, n_heads, q_length, dim_per_head)\n",
    "\n",
    "    return H, A"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
