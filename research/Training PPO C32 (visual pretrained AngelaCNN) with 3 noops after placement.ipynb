{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *\n",
    "from wrappers_2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9fd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=2, stride=2, padding=0),  \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(), \n",
    "            nn.Conv2d(128, 256, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        features_dim = 512\n",
    "        self.encoder = VisualEncoder()\n",
    "        self.encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AngelaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.action_head = nn.Linear(features_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(features_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.encoder.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        if self.use_cuda:\n",
    "            obs.cuda()\n",
    "            \n",
    "        features = self.encoder(obs)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C32']))\n",
    "    env = PovOnlyWrapper(env)\n",
    "    env = SelectAndPlace(env)\n",
    "    env = Discretization(env, flat_action_space('human-level'))\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 3.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-08 15:09:44,567\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-10-08 15:09:44,578\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=213)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=213)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO C32 pretrained (AngelaCNN) (3 noops after placement)</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/c4eb6_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/c4eb6_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20211008_150945-c4eb6_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=213)\u001b[0m 2021-10-08 15:09:48,089\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=213)\u001b[0m 2021-10-08 15:09:48,090\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=213)\u001b[0m 2021-10-08 15:09:56,791\tINFO trainable.py:109 -- Trainable.setup took 11.199 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=213)\u001b[0m 2021-10-08 15:09:56,792\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-11-02\n",
      "  done: false\n",
      "  episode_len_mean: 423.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.75391415754954\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013047522511651602\n",
      "          policy_loss: -0.07453308064076636\n",
      "          total_loss: -0.09316770566834344\n",
      "          vf_explained_var: 0.47365933656692505\n",
      "          vf_loss: 0.0062950101241262425\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.061052631578946\n",
      "    ram_util_percent: 49.15157894736841\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03987330418604833\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 63.05911228968785\n",
      "    mean_inference_ms: 1.642445584277173\n",
      "    mean_raw_obs_processing_ms: 0.167414620444253\n",
      "  time_since_restore: 66.10909414291382\n",
      "  time_this_iter_s: 66.10909414291382\n",
      "  time_total_s: 66.10909414291382\n",
      "  timers:\n",
      "    learn_throughput: 942.682\n",
      "    learn_time_ms: 1060.804\n",
      "    load_throughput: 57161.018\n",
      "    load_time_ms: 17.494\n",
      "    sample_throughput: 15.379\n",
      "    sample_time_ms: 65024.61\n",
      "    update_time_ms: 3.127\n",
      "  timestamp: 1633705862\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         66.1091</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               423</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-11-22\n",
      "  done: false\n",
      "  episode_len_mean: 421.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 4\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.722431707382202\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012597103227755133\n",
      "          policy_loss: -0.1291792506352067\n",
      "          total_loss: -0.14846383455312914\n",
      "          vf_explained_var: 0.09331028163433075\n",
      "          vf_loss: 0.005420312057766649\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.32857142857143\n",
      "    ram_util_percent: 56.53928571428571\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03975166667054376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 51.552163363878456\n",
      "    mean_inference_ms: 1.6319145913195623\n",
      "    mean_raw_obs_processing_ms: 0.1634744697179401\n",
      "  time_since_restore: 86.03031134605408\n",
      "  time_this_iter_s: 19.92121720314026\n",
      "  time_total_s: 86.03031134605408\n",
      "  timers:\n",
      "    learn_throughput: 940.438\n",
      "    learn_time_ms: 1063.334\n",
      "    load_throughput: 57352.121\n",
      "    load_time_ms: 17.436\n",
      "    sample_throughput: 23.85\n",
      "    sample_time_ms: 41928.31\n",
      "    update_time_ms: 3.159\n",
      "  timestamp: 1633705882\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         86.0303</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            421.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-11-41\n",
      "  done: false\n",
      "  episode_len_mean: 421.85714285714283\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 7\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7138678948084514\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013443993380699442\n",
      "          policy_loss: -0.07384034018549654\n",
      "          total_loss: -0.09373285575873322\n",
      "          vf_explained_var: 0.030553756281733513\n",
      "          vf_loss: 0.004557364018789182\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.22222222222223\n",
      "    ram_util_percent: 56.04444444444444\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0394690831416011\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 43.17888608690489\n",
      "    mean_inference_ms: 1.6175953995837278\n",
      "    mean_raw_obs_processing_ms: 0.16398521249056014\n",
      "  time_since_restore: 104.83790135383606\n",
      "  time_this_iter_s: 18.807590007781982\n",
      "  time_total_s: 104.83790135383606\n",
      "  timers:\n",
      "    learn_throughput: 945.275\n",
      "    learn_time_ms: 1057.893\n",
      "    load_throughput: 57104.725\n",
      "    load_time_ms: 17.512\n",
      "    sample_throughput: 29.529\n",
      "    sample_time_ms: 33864.847\n",
      "    update_time_ms: 2.844\n",
      "  timestamp: 1633705901\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         104.838</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           421.857</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-12-01\n",
      "  done: false\n",
      "  episode_len_mean: 421.8888888888889\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 9\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6943951580259533\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010635788737023042\n",
      "          policy_loss: -0.04061764763254258\n",
      "          total_loss: -0.0622682001027796\n",
      "          vf_explained_var: -0.05338457599282265\n",
      "          vf_loss: 0.0031662413358895317\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.59285714285715\n",
      "    ram_util_percent: 55.86428571428572\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03932486431950893\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 39.84077309524078\n",
      "    mean_inference_ms: 1.6112955700877531\n",
      "    mean_raw_obs_processing_ms: 0.16289665168821246\n",
      "  time_since_restore: 124.36448740959167\n",
      "  time_this_iter_s: 19.526586055755615\n",
      "  time_total_s: 124.36448740959167\n",
      "  timers:\n",
      "    learn_throughput: 925.186\n",
      "    learn_time_ms: 1080.864\n",
      "    load_throughput: 56624.116\n",
      "    load_time_ms: 17.66\n",
      "    sample_throughput: 33.348\n",
      "    sample_time_ms: 29987.016\n",
      "    update_time_ms: 2.731\n",
      "  timestamp: 1633705921\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         124.364</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           421.889</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-12-21\n",
      "  done: false\n",
      "  episode_len_mean: 419.90909090909093\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 11\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.688299865192837\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01250895821218485\n",
      "          policy_loss: -0.14078282540043194\n",
      "          total_loss: -0.16361454207864073\n",
      "          vf_explained_var: 0.19691674411296844\n",
      "          vf_loss: 0.0015494911666286903\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.5344827586207\n",
      "    ram_util_percent: 56.217241379310344\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03927571554135178\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 37.318210557367124\n",
      "    mean_inference_ms: 1.6080884192578226\n",
      "    mean_raw_obs_processing_ms: 0.1620172656479409\n",
      "  time_since_restore: 144.54406476020813\n",
      "  time_this_iter_s: 20.179577350616455\n",
      "  time_total_s: 144.54406476020813\n",
      "  timers:\n",
      "    learn_throughput: 917.183\n",
      "    learn_time_ms: 1090.295\n",
      "    load_throughput: 60737.896\n",
      "    load_time_ms: 16.464\n",
      "    sample_throughput: 35.976\n",
      "    sample_time_ms: 27796.516\n",
      "    update_time_ms: 2.653\n",
      "  timestamp: 1633705941\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         144.544</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           419.909</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-12-42\n",
      "  done: false\n",
      "  episode_len_mean: 418.2857142857143\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 14\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6581474171744452\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012614581941380099\n",
      "          policy_loss: -0.06924359343118137\n",
      "          total_loss: -0.09107043080859714\n",
      "          vf_explained_var: -0.1405741423368454\n",
      "          vf_loss: 0.0022317237738105986\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.12\n",
      "    ram_util_percent: 56.42666666666666\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03926550373983429\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 34.6009508374901\n",
      "    mean_inference_ms: 1.6055060105520174\n",
      "    mean_raw_obs_processing_ms: 0.16239710070355381\n",
      "  time_since_restore: 165.46687078475952\n",
      "  time_this_iter_s: 20.92280602455139\n",
      "  time_total_s: 165.46687078475952\n",
      "  timers:\n",
      "    learn_throughput: 924.16\n",
      "    learn_time_ms: 1082.063\n",
      "    load_throughput: 60463.375\n",
      "    load_time_ms: 16.539\n",
      "    sample_throughput: 37.773\n",
      "    sample_time_ms: 26473.728\n",
      "    update_time_ms: 2.597\n",
      "  timestamp: 1633705962\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         165.467</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           418.286</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-13-02\n",
      "  done: false\n",
      "  episode_len_mean: 416.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 16\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.671984084447225\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013677547312679117\n",
      "          policy_loss: -0.14313411919607055\n",
      "          total_loss: -0.16445394007282124\n",
      "          vf_explained_var: -0.8282077312469482\n",
      "          vf_loss: 0.0026645082217227256\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 7000\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 7000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.165517241379305\n",
      "    ram_util_percent: 56.39310344827588\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039241823186321856\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.229197029923846\n",
      "    mean_inference_ms: 1.6039330517457777\n",
      "    mean_raw_obs_processing_ms: 0.162491127195452\n",
      "  time_since_restore: 185.9252414703369\n",
      "  time_this_iter_s: 20.458370685577393\n",
      "  time_total_s: 185.9252414703369\n",
      "  timers:\n",
      "    learn_throughput: 925.34\n",
      "    learn_time_ms: 1080.684\n",
      "    load_throughput: 58332.578\n",
      "    load_time_ms: 17.143\n",
      "    sample_throughput: 39.281\n",
      "    sample_time_ms: 25457.497\n",
      "    update_time_ms: 2.561\n",
      "  timestamp: 1633705982\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         185.925</td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            416.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-13-23\n",
      "  done: false\n",
      "  episode_len_mean: 416.8421052631579\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 19\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6481262975268893\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010112771823538867\n",
      "          policy_loss: -0.16511741479237874\n",
      "          total_loss: -0.1880350376168887\n",
      "          vf_explained_var: 0.5416609644889832\n",
      "          vf_loss: 0.0015410831476199544\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.46999999999999\n",
      "    ram_util_percent: 56.516666666666666\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039197347933504725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.60226638287861\n",
      "    mean_inference_ms: 1.6017817565795425\n",
      "    mean_raw_obs_processing_ms: 0.16287734047324096\n",
      "  time_since_restore: 206.95577263832092\n",
      "  time_this_iter_s: 21.03053116798401\n",
      "  time_total_s: 206.95577263832092\n",
      "  timers:\n",
      "    learn_throughput: 914.941\n",
      "    learn_time_ms: 1092.966\n",
      "    load_throughput: 58670.723\n",
      "    load_time_ms: 17.044\n",
      "    sample_throughput: 40.398\n",
      "    sample_time_ms: 24753.904\n",
      "    update_time_ms: 2.525\n",
      "  timestamp: 1633706003\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         206.956</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           416.842</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-13-44\n",
      "  done: false\n",
      "  episode_len_mean: 414.6190476190476\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 21\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.580951380729675\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012326405196749985\n",
      "          policy_loss: -0.0644924667560392\n",
      "          total_loss: -0.08591784851418602\n",
      "          vf_explained_var: -0.17501646280288696\n",
      "          vf_loss: 0.0019188482676529223\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 9000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.85666666666667\n",
      "    ram_util_percent: 56.63333333333334\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03916611795243834\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 30.721025199647237\n",
      "    mean_inference_ms: 1.6005059501737153\n",
      "    mean_raw_obs_processing_ms: 0.16305883661377646\n",
      "  time_since_restore: 227.5608983039856\n",
      "  time_this_iter_s: 20.605125665664673\n",
      "  time_total_s: 227.5608983039856\n",
      "  timers:\n",
      "    learn_throughput: 919.916\n",
      "    learn_time_ms: 1087.056\n",
      "    load_throughput: 58300.557\n",
      "    load_time_ms: 17.152\n",
      "    sample_throughput: 41.365\n",
      "    sample_time_ms: 24174.808\n",
      "    update_time_ms: 2.508\n",
      "  timestamp: 1633706024\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         227.561</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           414.619</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-14-05\n",
      "  done: false\n",
      "  episode_len_mean: 415.125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 24\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.550762078497145\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013565588826011535\n",
      "          policy_loss: -0.09027705776194732\n",
      "          total_loss: -0.1118733826196856\n",
      "          vf_explained_var: 0.02386361174285412\n",
      "          vf_loss: 0.001198175043423867\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.09310344827586\n",
      "    ram_util_percent: 56.724137931034484\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03911934691142261\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.619569122620955\n",
      "    mean_inference_ms: 1.5987952590327854\n",
      "    mean_raw_obs_processing_ms: 0.16341391894876633\n",
      "  time_since_restore: 248.3941421508789\n",
      "  time_this_iter_s: 20.83324384689331\n",
      "  time_total_s: 248.3941421508789\n",
      "  timers:\n",
      "    learn_throughput: 923.372\n",
      "    learn_time_ms: 1082.987\n",
      "    load_throughput: 58328.023\n",
      "    load_time_ms: 17.144\n",
      "    sample_throughput: 42.134\n",
      "    sample_time_ms: 23733.806\n",
      "    update_time_ms: 2.479\n",
      "  timestamp: 1633706045\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         248.394</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           415.125</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 11000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-14-25\n",
      "  done: false\n",
      "  episode_len_mean: 416.5769230769231\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 26\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5972038480970596\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015441008787174247\n",
      "          policy_loss: -0.05289385300129652\n",
      "          total_loss: -0.07453854090223709\n",
      "          vf_explained_var: 0.13316361606121063\n",
      "          vf_loss: 0.001239146909210831\n",
      "    num_agent_steps_sampled: 11000\n",
      "    num_agent_steps_trained: 11000\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 11000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.89642857142858\n",
      "    ram_util_percent: 56.67142857142858\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.039084424525432364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.99143170846749\n",
      "    mean_inference_ms: 1.5976291219173826\n",
      "    mean_raw_obs_processing_ms: 0.1634437392333086\n",
      "  time_since_restore: 268.10664772987366\n",
      "  time_this_iter_s: 19.71250557899475\n",
      "  time_total_s: 268.10664772987366\n",
      "  timers:\n",
      "    learn_throughput: 925.955\n",
      "    learn_time_ms: 1079.966\n",
      "    load_throughput: 58175.524\n",
      "    load_time_ms: 17.189\n",
      "    sample_throughput: 52.364\n",
      "    sample_time_ms: 19097.236\n",
      "    update_time_ms: 2.382\n",
      "  timestamp: 1633706065\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         268.107</td><td style=\"text-align: right;\">11000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           416.577</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-14-45\n",
      "  done: false\n",
      "  episode_len_mean: 417.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 28\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.573284363746643\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013552451747802912\n",
      "          policy_loss: -0.08916212798406681\n",
      "          total_loss: -0.1101001417885224\n",
      "          vf_explained_var: -0.07213204354047775\n",
      "          vf_loss: 0.002084337231159831\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.96206896551725\n",
      "    ram_util_percent: 56.762068965517244\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03905040952828298\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 28.429145660668336\n",
      "    mean_inference_ms: 1.596485261422107\n",
      "    mean_raw_obs_processing_ms: 0.16335100456973592\n",
      "  time_since_restore: 288.4145483970642\n",
      "  time_this_iter_s: 20.30790066719055\n",
      "  time_total_s: 288.4145483970642\n",
      "  timers:\n",
      "    learn_throughput: 925.167\n",
      "    learn_time_ms: 1080.886\n",
      "    load_throughput: 57924.855\n",
      "    load_time_ms: 17.264\n",
      "    sample_throughput: 52.26\n",
      "    sample_time_ms: 19134.987\n",
      "    update_time_ms: 2.292\n",
      "  timestamp: 1633706085\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         288.415</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               417</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 13000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-15-24\n",
      "  done: false\n",
      "  episode_len_mean: 413.5806451612903\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 31\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5825312190585668\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015120872491253915\n",
      "          policy_loss: -0.07540992788142628\n",
      "          total_loss: -0.09704839413364728\n",
      "          vf_explained_var: 0.25896155834198\n",
      "          vf_loss: 0.0011626726934789782\n",
      "    num_agent_steps_sampled: 13000\n",
      "    num_agent_steps_trained: 13000\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 13000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.95892857142857\n",
      "    ram_util_percent: 55.78928571428571\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03900664111674458\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.70408832609697\n",
      "    mean_inference_ms: 1.5949874875778534\n",
      "    mean_raw_obs_processing_ms: 0.2903352668806567\n",
      "  time_since_restore: 327.1007215976715\n",
      "  time_this_iter_s: 38.6861732006073\n",
      "  time_total_s: 327.1007215976715\n",
      "  timers:\n",
      "    learn_throughput: 925.284\n",
      "    learn_time_ms: 1080.75\n",
      "    load_throughput: 60217.134\n",
      "    load_time_ms: 16.607\n",
      "    sample_throughput: 47.34\n",
      "    sample_time_ms: 21123.606\n",
      "    update_time_ms: 2.296\n",
      "  timestamp: 1633706124\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         327.101</td><td style=\"text-align: right;\">13000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           413.581</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 14000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-15-43\n",
      "  done: false\n",
      "  episode_len_mean: 413.75757575757575\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 33\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5822434504826863\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016389730797699117\n",
      "          policy_loss: -0.04583663543065389\n",
      "          total_loss: -0.06711434804730945\n",
      "          vf_explained_var: 0.47074148058891296\n",
      "          vf_loss: 0.0012667745849790257\n",
      "    num_agent_steps_sampled: 14000\n",
      "    num_agent_steps_trained: 14000\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.875\n",
      "    ram_util_percent: 57.02857142857142\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03898008611730997\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.275826977028153\n",
      "    mean_inference_ms: 1.5941366753259645\n",
      "    mean_raw_obs_processing_ms: 0.3564061120476026\n",
      "  time_since_restore: 346.6983461380005\n",
      "  time_this_iter_s: 19.59762454032898\n",
      "  time_total_s: 346.6983461380005\n",
      "  timers:\n",
      "    learn_throughput: 935.215\n",
      "    learn_time_ms: 1069.273\n",
      "    load_throughput: 60025.903\n",
      "    load_time_ms: 16.659\n",
      "    sample_throughput: 47.299\n",
      "    sample_time_ms: 21142.14\n",
      "    update_time_ms: 2.277\n",
      "  timestamp: 1633706143\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         346.698</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           413.758</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-16-04\n",
      "  done: false\n",
      "  episode_len_mean: 412.69444444444446\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 36\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4996821085611978\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01412253563520184\n",
      "          policy_loss: -0.05924334828224447\n",
      "          total_loss: -0.08066859489513768\n",
      "          vf_explained_var: -0.050842709839344025\n",
      "          vf_loss: 0.000747067631325788\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_agent_steps_trained: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.36896551724137\n",
      "    ram_util_percent: 57.05862068965517\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038943085409713835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.70471723826536\n",
      "    mean_inference_ms: 1.5929888593694765\n",
      "    mean_raw_obs_processing_ms: 0.43503680552465956\n",
      "  time_since_restore: 366.9673812389374\n",
      "  time_this_iter_s: 20.26903510093689\n",
      "  time_total_s: 366.9673812389374\n",
      "  timers:\n",
      "    learn_throughput: 943.444\n",
      "    learn_time_ms: 1059.946\n",
      "    load_throughput: 60374.89\n",
      "    load_time_ms: 16.563\n",
      "    sample_throughput: 47.258\n",
      "    sample_time_ms: 21160.525\n",
      "    update_time_ms: 2.265\n",
      "  timestamp: 1633706164\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         366.967</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           412.694</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-16-25\n",
      "  done: false\n",
      "  episode_len_mean: 411.10526315789474\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 38\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6022141853968304\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01320600625556361\n",
      "          policy_loss: -0.0674698702370127\n",
      "          total_loss: -0.08965001023477978\n",
      "          vf_explained_var: -0.44220301508903503\n",
      "          vf_loss: 0.001200799625237576\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.743333333333325\n",
      "    ram_util_percent: 56.960000000000015\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03892028918591637\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.366981146765635\n",
      "    mean_inference_ms: 1.592265330713582\n",
      "    mean_raw_obs_processing_ms: 0.4768143458602296\n",
      "  time_since_restore: 388.1575961112976\n",
      "  time_this_iter_s: 21.19021487236023\n",
      "  time_total_s: 388.1575961112976\n",
      "  timers:\n",
      "    learn_throughput: 936.955\n",
      "    learn_time_ms: 1067.287\n",
      "    load_throughput: 60906.179\n",
      "    load_time_ms: 16.419\n",
      "    sample_throughput: 47.214\n",
      "    sample_time_ms: 21180.059\n",
      "    update_time_ms: 2.258\n",
      "  timestamp: 1633706185\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         388.158</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           411.105</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 17000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-16-46\n",
      "  done: false\n",
      "  episode_len_mean: 410.3414634146341\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 41\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.531606266233656\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01709730324160669\n",
      "          policy_loss: -0.03537093504435486\n",
      "          total_loss: -0.0560199450287554\n",
      "          vf_explained_var: 0.5201541185379028\n",
      "          vf_loss: 0.0012475915000929186\n",
      "    num_agent_steps_sampled: 17000\n",
      "    num_agent_steps_trained: 17000\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 17000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.586666666666666\n",
      "    ram_util_percent: 56.973333333333336\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0388882870561712\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.912517902238434\n",
      "    mean_inference_ms: 1.5912809116281028\n",
      "    mean_raw_obs_processing_ms: 0.5273408325083514\n",
      "  time_since_restore: 409.13727498054504\n",
      "  time_this_iter_s: 20.979678869247437\n",
      "  time_total_s: 409.13727498054504\n",
      "  timers:\n",
      "    learn_throughput: 938.481\n",
      "    learn_time_ms: 1065.552\n",
      "    load_throughput: 63070.249\n",
      "    load_time_ms: 15.855\n",
      "    sample_throughput: 47.093\n",
      "    sample_time_ms: 21234.472\n",
      "    update_time_ms: 2.247\n",
      "  timestamp: 1633706206\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         409.137</td><td style=\"text-align: right;\">17000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           410.341</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-17-08\n",
      "  done: false\n",
      "  episode_len_mean: 409.02272727272725\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 44\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5008445262908934\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020221017541326484\n",
      "          policy_loss: -0.05088463106917011\n",
      "          total_loss: -0.07046633486946424\n",
      "          vf_explained_var: -0.19587230682373047\n",
      "          vf_loss: 0.0013825382190083878\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.82903225806452\n",
      "    ram_util_percent: 57.09999999999998\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03886133960233888\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.515961211462482\n",
      "    mean_inference_ms: 1.5903850481498822\n",
      "    mean_raw_obs_processing_ms: 0.5672641098221711\n",
      "  time_since_restore: 431.09699273109436\n",
      "  time_this_iter_s: 21.959717750549316\n",
      "  time_total_s: 431.09699273109436\n",
      "  timers:\n",
      "    learn_throughput: 951.241\n",
      "    learn_time_ms: 1051.259\n",
      "    load_throughput: 63001.281\n",
      "    load_time_ms: 15.873\n",
      "    sample_throughput: 46.856\n",
      "    sample_time_ms: 21341.794\n",
      "    update_time_ms: 2.244\n",
      "  timestamp: 1633706228\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         431.097</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           409.023</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 19000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-17-29\n",
      "  done: false\n",
      "  episode_len_mean: 408.60869565217394\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 46\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.527618079715305\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012347411758027308\n",
      "          policy_loss: -0.10336225032806397\n",
      "          total_loss: -0.12389992165068785\n",
      "          vf_explained_var: -0.44667497277259827\n",
      "          vf_loss: 0.0010342858696175325\n",
      "    num_agent_steps_sampled: 19000\n",
      "    num_agent_steps_trained: 19000\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 19000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.84193548387096\n",
      "    ram_util_percent: 57.23548387096774\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038845731826714386\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.276764929526067\n",
      "    mean_inference_ms: 1.5898403326105481\n",
      "    mean_raw_obs_processing_ms: 0.588785957543884\n",
      "  time_since_restore: 452.49595308303833\n",
      "  time_this_iter_s: 21.39896035194397\n",
      "  time_total_s: 452.49595308303833\n",
      "  timers:\n",
      "    learn_throughput: 950.512\n",
      "    learn_time_ms: 1052.065\n",
      "    load_throughput: 63535.043\n",
      "    load_time_ms: 15.739\n",
      "    sample_throughput: 46.684\n",
      "    sample_time_ms: 21420.528\n",
      "    update_time_ms: 2.229\n",
      "  timestamp: 1633706249\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 19\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         452.496</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           408.609</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-17-50\n",
      "  done: false\n",
      "  episode_len_mean: 408.9375\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 48\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.526723692152235\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01514803273777775\n",
      "          policy_loss: -0.13824496418237686\n",
      "          total_loss: -0.15782277348140875\n",
      "          vf_explained_var: -0.1366717368364334\n",
      "          vf_loss: 0.001145014902835505\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.58965517241379\n",
      "    ram_util_percent: 57.34137931034485\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0388312441031549\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.0522397465782\n",
      "    mean_inference_ms: 1.5893113352530381\n",
      "    mean_raw_obs_processing_ms: 0.6065979088362273\n",
      "  time_since_restore: 472.8201701641083\n",
      "  time_this_iter_s: 20.324217081069946\n",
      "  time_total_s: 472.8201701641083\n",
      "  timers:\n",
      "    learn_throughput: 950.869\n",
      "    learn_time_ms: 1051.67\n",
      "    load_throughput: 63613.095\n",
      "    load_time_ms: 15.72\n",
      "    sample_throughput: 46.795\n",
      "    sample_time_ms: 21370.02\n",
      "    update_time_ms: 2.227\n",
      "  timestamp: 1633706270\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 20\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">          472.82</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           408.938</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-18-10\n",
      "  done: false\n",
      "  episode_len_mean: 409.3529411764706\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 51\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5117613236109415\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014137051701775663\n",
      "          policy_loss: -0.10581533573567867\n",
      "          total_loss: -0.12564332907398543\n",
      "          vf_explained_var: -0.48422786593437195\n",
      "          vf_loss: 0.0010485045459871698\n",
      "    num_agent_steps_sampled: 21000\n",
      "    num_agent_steps_trained: 21000\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.77931034482759\n",
      "    ram_util_percent: 57.489655172413784\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038809769581023314\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.74142371070643\n",
      "    mean_inference_ms: 1.5885764755235545\n",
      "    mean_raw_obs_processing_ms: 0.6283427468003348\n",
      "  time_since_restore: 493.0654516220093\n",
      "  time_this_iter_s: 20.245281457901\n",
      "  time_total_s: 493.0654516220093\n",
      "  timers:\n",
      "    learn_throughput: 950.09\n",
      "    learn_time_ms: 1052.532\n",
      "    load_throughput: 63788.397\n",
      "    load_time_ms: 15.677\n",
      "    sample_throughput: 46.68\n",
      "    sample_time_ms: 21422.44\n",
      "    update_time_ms: 2.242\n",
      "  timestamp: 1633706290\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         493.065</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           409.353</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 22000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-18-31\n",
      "  done: false\n",
      "  episode_len_mean: 408.9433962264151\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 53\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.499843732515971\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015970859881195665\n",
      "          policy_loss: -0.05271011789639791\n",
      "          total_loss: -0.07166555143064923\n",
      "          vf_explained_var: -0.44425562024116516\n",
      "          vf_loss: 0.0012517452750924146\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 22000\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.51333333333333\n",
      "    ram_util_percent: 57.476666666666674\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038796255927060895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.551177302919065\n",
      "    mean_inference_ms: 1.5881166002314728\n",
      "    mean_raw_obs_processing_ms: 0.6400453080122921\n",
      "  time_since_restore: 514.1842679977417\n",
      "  time_this_iter_s: 21.118816375732422\n",
      "  time_total_s: 514.1842679977417\n",
      "  timers:\n",
      "    learn_throughput: 951.623\n",
      "    learn_time_ms: 1050.836\n",
      "    load_throughput: 64221.27\n",
      "    load_time_ms: 15.571\n",
      "    sample_throughput: 46.5\n",
      "    sample_time_ms: 21505.343\n",
      "    update_time_ms: 2.234\n",
      "  timestamp: 1633706311\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 22\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         514.184</td><td style=\"text-align: right;\">22000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           408.943</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 23000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-18-51\n",
      "  done: false\n",
      "  episode_len_mean: 409.55357142857144\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 56\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.496037377251519\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016305639645904424\n",
      "          policy_loss: -0.05138727186454667\n",
      "          total_loss: -0.07069598121775521\n",
      "          vf_explained_var: 0.24479635059833527\n",
      "          vf_loss: 0.0007599728744632254\n",
      "    num_agent_steps_sampled: 23000\n",
      "    num_agent_steps_trained: 23000\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 23000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.74285714285714\n",
      "    ram_util_percent: 57.49285714285715\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038776747494221234\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.285006360232423\n",
      "    mean_inference_ms: 1.5874587303618415\n",
      "    mean_raw_obs_processing_ms: 0.6542898972936096\n",
      "  time_since_restore: 534.0921218395233\n",
      "  time_this_iter_s: 19.907853841781616\n",
      "  time_total_s: 534.0921218395233\n",
      "  timers:\n",
      "    learn_throughput: 950.261\n",
      "    learn_time_ms: 1052.343\n",
      "    load_throughput: 61365.451\n",
      "    load_time_ms: 16.296\n",
      "    sample_throughput: 50.955\n",
      "    sample_time_ms: 19625.311\n",
      "    update_time_ms: 2.219\n",
      "  timestamp: 1633706331\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 23\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         534.092</td><td style=\"text-align: right;\">23000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           409.554</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-19-11\n",
      "  done: false\n",
      "  episode_len_mean: 409.44827586206895\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 58\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.490700374709235\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017111341490425057\n",
      "          policy_loss: -0.11612036546899213\n",
      "          total_loss: -0.13448448210126823\n",
      "          vf_explained_var: -0.8529549241065979\n",
      "          vf_loss: 0.0014094861993928336\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.55862068965517\n",
      "    ram_util_percent: 57.496551724137916\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03876406427335449\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.119940992350955\n",
      "    mean_inference_ms: 1.5870435600843267\n",
      "    mean_raw_obs_processing_ms: 0.6618718803978108\n",
      "  time_since_restore: 554.4974095821381\n",
      "  time_this_iter_s: 20.405287742614746\n",
      "  time_total_s: 554.4974095821381\n",
      "  timers:\n",
      "    learn_throughput: 948.493\n",
      "    learn_time_ms: 1054.304\n",
      "    load_throughput: 61440.24\n",
      "    load_time_ms: 16.276\n",
      "    sample_throughput: 50.751\n",
      "    sample_time_ms: 19704.139\n",
      "    update_time_ms: 2.21\n",
      "  timestamp: 1633706351\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         554.497</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           409.448</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 25000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-19-49\n",
      "  done: false\n",
      "  episode_len_mean: 409.2295081967213\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 61\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5051515950096976\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014181628162442787\n",
      "          policy_loss: -0.06198268733504746\n",
      "          total_loss: -0.08180220321648651\n",
      "          vf_explained_var: -0.7264434099197388\n",
      "          vf_loss: 0.0009775098400293953\n",
      "    num_agent_steps_sampled: 25000\n",
      "    num_agent_steps_trained: 25000\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 25000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.64629629629629\n",
      "    ram_util_percent: 57.49814814814816\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03874622002050842\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.8901310734875\n",
      "    mean_inference_ms: 1.5864500902447018\n",
      "    mean_raw_obs_processing_ms: 0.7034184933186906\n",
      "  time_since_restore: 592.0918922424316\n",
      "  time_this_iter_s: 37.59448266029358\n",
      "  time_total_s: 592.0918922424316\n",
      "  timers:\n",
      "    learn_throughput: 948.898\n",
      "    learn_time_ms: 1053.854\n",
      "    load_throughput: 58592.103\n",
      "    load_time_ms: 17.067\n",
      "    sample_throughput: 46.65\n",
      "    sample_time_ms: 21436.366\n",
      "    update_time_ms: 2.199\n",
      "  timestamp: 1633706389\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 25\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         592.092</td><td style=\"text-align: right;\">25000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            409.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 26000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-20-09\n",
      "  done: false\n",
      "  episode_len_mean: 409.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 63\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5070088121626113\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013922682369666046\n",
      "          policy_loss: -0.03200952232711845\n",
      "          total_loss: -0.05182940618445476\n",
      "          vf_explained_var: -0.6755536198616028\n",
      "          vf_loss: 0.0010733980316823969\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 26000\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.010344827586195\n",
      "    ram_util_percent: 57.46896551724138\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03873501431848212\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.74685579357221\n",
      "    mean_inference_ms: 1.586071657080826\n",
      "    mean_raw_obs_processing_ms: 0.7272535650443785\n",
      "  time_since_restore: 612.541647195816\n",
      "  time_this_iter_s: 20.4497549533844\n",
      "  time_total_s: 612.541647195816\n",
      "  timers:\n",
      "    learn_throughput: 956.358\n",
      "    learn_time_ms: 1045.633\n",
      "    load_throughput: 57622.456\n",
      "    load_time_ms: 17.354\n",
      "    sample_throughput: 46.794\n",
      "    sample_time_ms: 21370.242\n",
      "    update_time_ms: 2.196\n",
      "  timestamp: 1633706409\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 26\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         612.542</td><td style=\"text-align: right;\">26000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               409</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-20-27\n",
      "  done: false\n",
      "  episode_len_mean: 409.44615384615383\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 65\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.5491653150982327\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015705198745140456\n",
      "          policy_loss: -0.14624242871585819\n",
      "          total_loss: -0.16599797594050567\n",
      "          vf_explained_var: 0.1819779872894287\n",
      "          vf_loss: 0.0010245468367227457\n",
      "    num_agent_steps_sampled: 27000\n",
      "    num_agent_steps_trained: 27000\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 27000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.857692307692304\n",
      "    ram_util_percent: 57.45384615384616\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038724541146701846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.60739780785493\n",
      "    mean_inference_ms: 1.5856935752865648\n",
      "    mean_raw_obs_processing_ms: 0.7481271197429644\n",
      "  time_since_restore: 630.3418781757355\n",
      "  time_this_iter_s: 17.800230979919434\n",
      "  time_total_s: 630.3418781757355\n",
      "  timers:\n",
      "    learn_throughput: 958.317\n",
      "    learn_time_ms: 1043.496\n",
      "    load_throughput: 60379.236\n",
      "    load_time_ms: 16.562\n",
      "    sample_throughput: 47.495\n",
      "    sample_time_ms: 21055.007\n",
      "    update_time_ms: 2.395\n",
      "  timestamp: 1633706427\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 27\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         630.342</td><td style=\"text-align: right;\">27000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           409.446</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-20-49\n",
      "  done: false\n",
      "  episode_len_mean: 409.80882352941177\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 68\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4335214720831977\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01435554945148121\n",
      "          policy_loss: -0.08900729285346137\n",
      "          total_loss: -0.10770770894984404\n",
      "          vf_explained_var: -0.29649966955184937\n",
      "          vf_loss: 0.0013281334136586844\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.96774193548388\n",
      "    ram_util_percent: 57.4516129032258\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03871106433097777\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.4127562736843\n",
      "    mean_inference_ms: 1.5851806841515248\n",
      "    mean_raw_obs_processing_ms: 0.7752079887945742\n",
      "  time_since_restore: 651.9983651638031\n",
      "  time_this_iter_s: 21.656486988067627\n",
      "  time_total_s: 651.9983651638031\n",
      "  timers:\n",
      "    learn_throughput: 950.689\n",
      "    learn_time_ms: 1051.869\n",
      "    load_throughput: 61713.341\n",
      "    load_time_ms: 16.204\n",
      "    sample_throughput: 47.582\n",
      "    sample_time_ms: 21016.328\n",
      "    update_time_ms: 2.644\n",
      "  timestamp: 1633706449\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 28\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         651.998</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           409.809</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 29000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-21-12\n",
      "  done: false\n",
      "  episode_len_mean: 409.2857142857143\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 70\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.320324198404948\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016052671316030134\n",
      "          policy_loss: -0.09594849869608879\n",
      "          total_loss: -0.11318505133191745\n",
      "          vf_explained_var: -0.3297210931777954\n",
      "          vf_loss: 0.0011508864240669127\n",
      "    num_agent_steps_sampled: 29000\n",
      "    num_agent_steps_trained: 29000\n",
      "    num_steps_sampled: 29000\n",
      "    num_steps_trained: 29000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.118181818181824\n",
      "    ram_util_percent: 57.65151515151515\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03870408335788362\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.29363802315588\n",
      "    mean_inference_ms: 1.5849013276781048\n",
      "    mean_raw_obs_processing_ms: 0.7907814700081544\n",
      "  time_since_restore: 675.5083692073822\n",
      "  time_this_iter_s: 23.5100040435791\n",
      "  time_total_s: 675.5083692073822\n",
      "  timers:\n",
      "    learn_throughput: 949.377\n",
      "    learn_time_ms: 1053.323\n",
      "    load_throughput: 61249.403\n",
      "    load_time_ms: 16.327\n",
      "    sample_throughput: 47.112\n",
      "    sample_time_ms: 21225.828\n",
      "    update_time_ms: 2.646\n",
      "  timestamp: 1633706472\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 29\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         675.508</td><td style=\"text-align: right;\">29000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           409.286</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-21-35\n",
      "  done: false\n",
      "  episode_len_mean: 408.35616438356163\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 73\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.406226944923401\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017120386939594615\n",
      "          policy_loss: -0.06451144990407759\n",
      "          total_loss: -0.08204127616352505\n",
      "          vf_explained_var: 0.08716471493244171\n",
      "          vf_loss: 0.0013963246769789193\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.053124999999994\n",
      "    ram_util_percent: 57.640625\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03869481895389936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.127614224307873\n",
      "    mean_inference_ms: 1.584533896427441\n",
      "    mean_raw_obs_processing_ms: 0.810998839468465\n",
      "  time_since_restore: 698.014270067215\n",
      "  time_this_iter_s: 22.505900859832764\n",
      "  time_total_s: 698.014270067215\n",
      "  timers:\n",
      "    learn_throughput: 945.601\n",
      "    learn_time_ms: 1057.529\n",
      "    load_throughput: 61459.866\n",
      "    load_time_ms: 16.271\n",
      "    sample_throughput: 46.642\n",
      "    sample_time_ms: 21439.805\n",
      "    update_time_ms: 2.669\n",
      "  timestamp: 1633706495\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 30\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         698.014</td><td style=\"text-align: right;\">30000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           408.356</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 31000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-21-55\n",
      "  done: false\n",
      "  episode_len_mean: 408.81333333333333\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 75\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4161370780732896\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018420034195982326\n",
      "          policy_loss: -0.06447459695239861\n",
      "          total_loss: -0.08209984882010354\n",
      "          vf_explained_var: -0.20420069992542267\n",
      "          vf_loss: 0.001010109418227027\n",
      "    num_agent_steps_sampled: 31000\n",
      "    num_agent_steps_trained: 31000\n",
      "    num_steps_sampled: 31000\n",
      "    num_steps_trained: 31000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.18965517241379\n",
      "    ram_util_percent: 57.73448275862068\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03868892089426042\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.022786040612225\n",
      "    mean_inference_ms: 1.5842958227649258\n",
      "    mean_raw_obs_processing_ms: 0.8226015364328051\n",
      "  time_since_restore: 718.3344020843506\n",
      "  time_this_iter_s: 20.32013201713562\n",
      "  time_total_s: 718.3344020843506\n",
      "  timers:\n",
      "    learn_throughput: 946.334\n",
      "    learn_time_ms: 1056.709\n",
      "    load_throughput: 61865.72\n",
      "    load_time_ms: 16.164\n",
      "    sample_throughput: 46.625\n",
      "    sample_time_ms: 21447.546\n",
      "    update_time_ms: 3.321\n",
      "  timestamp: 1633706515\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 31\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         718.334</td><td style=\"text-align: right;\">31000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           408.813</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-22-16\n",
      "  done: false\n",
      "  episode_len_mean: 408.84615384615387\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 78\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3939179182052612\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019601894289268767\n",
      "          policy_loss: -0.0728805270873838\n",
      "          total_loss: -0.08990642817484008\n",
      "          vf_explained_var: -0.27063682675361633\n",
      "          vf_loss: 0.0010327076461787025\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.370000000000005\n",
      "    ram_util_percent: 57.686666666666675\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03868024802055547\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.874384087432798\n",
      "    mean_inference_ms: 1.5839444331785728\n",
      "    mean_raw_obs_processing_ms: 0.8376097546796724\n",
      "  time_since_restore: 739.3629426956177\n",
      "  time_this_iter_s: 21.02854061126709\n",
      "  time_total_s: 739.3629426956177\n",
      "  timers:\n",
      "    learn_throughput: 948.527\n",
      "    learn_time_ms: 1054.266\n",
      "    load_throughput: 61953.628\n",
      "    load_time_ms: 16.141\n",
      "    sample_throughput: 46.64\n",
      "    sample_time_ms: 21440.948\n",
      "    update_time_ms: 3.315\n",
      "  timestamp: 1633706536\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 32\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         739.363</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           408.846</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 33000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-22-36\n",
      "  done: false\n",
      "  episode_len_mean: 409.8625\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 80\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4101211494869657\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02066497120656514\n",
      "          policy_loss: -0.0503890002767245\n",
      "          total_loss: -0.06774364846448104\n",
      "          vf_explained_var: -0.2327914834022522\n",
      "          vf_loss: 0.000547071866134906\n",
      "    num_agent_steps_sampled: 33000\n",
      "    num_agent_steps_trained: 33000\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 33000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.12413793103449\n",
      "    ram_util_percent: 57.772413793103446\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03867467534234933\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.78005060331808\n",
      "    mean_inference_ms: 1.5837111689597934\n",
      "    mean_raw_obs_processing_ms: 0.8461864714255704\n",
      "  time_since_restore: 759.2991511821747\n",
      "  time_this_iter_s: 19.936208486557007\n",
      "  time_total_s: 759.2991511821747\n",
      "  timers:\n",
      "    learn_throughput: 949.057\n",
      "    learn_time_ms: 1053.677\n",
      "    load_throughput: 62354.276\n",
      "    load_time_ms: 16.037\n",
      "    sample_throughput: 46.634\n",
      "    sample_time_ms: 21443.626\n",
      "    update_time_ms: 4.017\n",
      "  timestamp: 1633706556\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 33\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         759.299</td><td style=\"text-align: right;\">33000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           409.863</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 34000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-22-59\n",
      "  done: false\n",
      "  episode_len_mean: 408.43373493975906\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 83\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.369122791290283\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016928257507236293\n",
      "          policy_loss: -0.03816678722699483\n",
      "          total_loss: -0.053189370367262095\n",
      "          vf_explained_var: -0.4777269661426544\n",
      "          vf_loss: 0.0010509298257400386\n",
      "    num_agent_steps_sampled: 34000\n",
      "    num_agent_steps_trained: 34000\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 34000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.8375\n",
      "    ram_util_percent: 58.05\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866738411513369\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.64746385659735\n",
      "    mean_inference_ms: 1.5834037430767718\n",
      "    mean_raw_obs_processing_ms: 0.8572166145682956\n",
      "  time_since_restore: 781.7896919250488\n",
      "  time_this_iter_s: 22.490540742874146\n",
      "  time_total_s: 781.7896919250488\n",
      "  timers:\n",
      "    learn_throughput: 940.749\n",
      "    learn_time_ms: 1062.982\n",
      "    load_throughput: 61455.544\n",
      "    load_time_ms: 16.272\n",
      "    sample_throughput: 46.206\n",
      "    sample_time_ms: 21642.247\n",
      "    update_time_ms: 4.357\n",
      "  timestamp: 1633706579\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 34\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">          781.79</td><td style=\"text-align: right;\">34000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           408.434</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 35000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-23-24\n",
      "  done: false\n",
      "  episode_len_mean: 406.8953488372093\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 86\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.330386514133877\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015965958276609862\n",
      "          policy_loss: -0.053087541295422445\n",
      "          total_loss: -0.06090371939871046\n",
      "          vf_explained_var: -0.6407071352005005\n",
      "          vf_loss: 0.008303000228220805\n",
      "    num_agent_steps_sampled: 35000\n",
      "    num_agent_steps_trained: 35000\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 35000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.03333333333333\n",
      "    ram_util_percent: 58.29722222222223\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866200632553249\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.527365307421782\n",
      "    mean_inference_ms: 1.5831618119702773\n",
      "    mean_raw_obs_processing_ms: 0.8665182052207688\n",
      "  time_since_restore: 807.1192996501923\n",
      "  time_this_iter_s: 25.329607725143433\n",
      "  time_total_s: 807.1192996501923\n",
      "  timers:\n",
      "    learn_throughput: 936.957\n",
      "    learn_time_ms: 1067.284\n",
      "    load_throughput: 63428.202\n",
      "    load_time_ms: 15.766\n",
      "    sample_throughput: 48.991\n",
      "    sample_time_ms: 20411.817\n",
      "    update_time_ms: 4.464\n",
      "  timestamp: 1633706604\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 35\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         807.119</td><td style=\"text-align: right;\">35000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           406.895</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-23-47\n",
      "  done: false\n",
      "  episode_len_mean: 406.1363636363636\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 88\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.390363510449727\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013219266703856805\n",
      "          policy_loss: -0.04326977365546757\n",
      "          total_loss: -0.060575917777087956\n",
      "          vf_explained_var: 0.41719529032707214\n",
      "          vf_loss: 0.0006488206634660148\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.52727272727273\n",
      "    ram_util_percent: 58.048484848484854\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865819720682683\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.452540318914647\n",
      "    mean_inference_ms: 1.5829995659505898\n",
      "    mean_raw_obs_processing_ms: 0.8717555356329225\n",
      "  time_since_restore: 830.2625560760498\n",
      "  time_this_iter_s: 23.143256425857544\n",
      "  time_total_s: 830.2625560760498\n",
      "  timers:\n",
      "    learn_throughput: 937.421\n",
      "    learn_time_ms: 1066.756\n",
      "    load_throughput: 64117.697\n",
      "    load_time_ms: 15.596\n",
      "    sample_throughput: 48.355\n",
      "    sample_time_ms: 20680.441\n",
      "    update_time_ms: 5.866\n",
      "  timestamp: 1633706627\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 36\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         830.263</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           406.136</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 37000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-24-27\n",
      "  done: false\n",
      "  episode_len_mean: 404.5054945054945\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 91\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3100219700071545\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013502258608008372\n",
      "          policy_loss: -0.033584527050455414\n",
      "          total_loss: -0.0495913174831205\n",
      "          vf_explained_var: -0.09738624095916748\n",
      "          vf_loss: 0.0010174130011970798\n",
      "    num_agent_steps_sampled: 37000\n",
      "    num_agent_steps_trained: 37000\n",
      "    num_steps_sampled: 37000\n",
      "    num_steps_trained: 37000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.54821428571429\n",
      "    ram_util_percent: 57.80357142857143\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386523407020507\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.34686741723339\n",
      "    mean_inference_ms: 1.5827542006057191\n",
      "    mean_raw_obs_processing_ms: 0.8932506274630992\n",
      "  time_since_restore: 869.510452747345\n",
      "  time_this_iter_s: 39.247896671295166\n",
      "  time_total_s: 869.510452747345\n",
      "  timers:\n",
      "    learn_throughput: 937.336\n",
      "    learn_time_ms: 1066.853\n",
      "    load_throughput: 62481.346\n",
      "    load_time_ms: 16.005\n",
      "    sample_throughput: 43.812\n",
      "    sample_time_ms: 22824.901\n",
      "    update_time_ms: 5.669\n",
      "  timestamp: 1633706667\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 37\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">          869.51</td><td style=\"text-align: right;\">37000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           404.505</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 38000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-24-48\n",
      "  done: false\n",
      "  episode_len_mean: 403.6595744680851\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 94\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3939082675509984\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011228461155507278\n",
      "          policy_loss: -0.07347914286785656\n",
      "          total_loss: -0.09115239278309875\n",
      "          vf_explained_var: -0.4560646414756775\n",
      "          vf_loss: 0.0012130250779187513\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 38000\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.683870967741946\n",
      "    ram_util_percent: 57.880645161290346\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864678710888786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.247528713598\n",
      "    mean_inference_ms: 1.5825131509503874\n",
      "    mean_raw_obs_processing_ms: 0.9122471884999638\n",
      "  time_since_restore: 891.1391224861145\n",
      "  time_this_iter_s: 21.62866973876953\n",
      "  time_total_s: 891.1391224861145\n",
      "  timers:\n",
      "    learn_throughput: 944.665\n",
      "    learn_time_ms: 1058.576\n",
      "    load_throughput: 60332.51\n",
      "    load_time_ms: 16.575\n",
      "    sample_throughput: 43.802\n",
      "    sample_time_ms: 22830.14\n",
      "    update_time_ms: 5.44\n",
      "  timestamp: 1633706688\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 38\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         891.139</td><td style=\"text-align: right;\">38000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            403.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 39000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-25-10\n",
      "  done: false\n",
      "  episode_len_mean: 403.375\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 96\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.372447458902995\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014343710573770175\n",
      "          policy_loss: -0.01425115644103951\n",
      "          total_loss: -0.030627796136670642\n",
      "          vf_explained_var: -0.35565096139907837\n",
      "          vf_loss: 0.0008931635891915196\n",
      "    num_agent_steps_sampled: 39000\n",
      "    num_agent_steps_trained: 39000\n",
      "    num_steps_sampled: 39000\n",
      "    num_steps_trained: 39000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.41935483870968\n",
      "    ram_util_percent: 57.85806451612902\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864308156875271\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.184602768299033\n",
      "    mean_inference_ms: 1.5823560469589426\n",
      "    mean_raw_obs_processing_ms: 0.9235439856521515\n",
      "  time_since_restore: 912.9529254436493\n",
      "  time_this_iter_s: 21.81380295753479\n",
      "  time_total_s: 912.9529254436493\n",
      "  timers:\n",
      "    learn_throughput: 946.422\n",
      "    learn_time_ms: 1056.611\n",
      "    load_throughput: 60783.05\n",
      "    load_time_ms: 16.452\n",
      "    sample_throughput: 44.126\n",
      "    sample_time_ms: 22662.595\n",
      "    update_time_ms: 5.44\n",
      "  timestamp: 1633706710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 39\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         912.953</td><td style=\"text-align: right;\">39000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           403.375</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-25-32\n",
      "  done: false\n",
      "  episode_len_mean: 402.2828282828283\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 99\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3204543537563747\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011664979172819933\n",
      "          policy_loss: -0.14282531914197735\n",
      "          total_loss: -0.15993298296299246\n",
      "          vf_explained_var: -0.4970002770423889\n",
      "          vf_loss: 0.0008476391373228075\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.253125\n",
      "    ram_util_percent: 57.6625\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038637797957758166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.09496040603567\n",
      "    mean_inference_ms: 1.5821277035219754\n",
      "    mean_raw_obs_processing_ms: 0.9386749570223649\n",
      "  time_since_restore: 935.0320131778717\n",
      "  time_this_iter_s: 22.079087734222412\n",
      "  time_total_s: 935.0320131778717\n",
      "  timers:\n",
      "    learn_throughput: 951.104\n",
      "    learn_time_ms: 1051.409\n",
      "    load_throughput: 60769.84\n",
      "    load_time_ms: 16.456\n",
      "    sample_throughput: 44.199\n",
      "    sample_time_ms: 22625.136\n",
      "    update_time_ms: 5.418\n",
      "  timestamp: 1633706732\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 40\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         935.032</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">           402.283</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 41000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-25-53\n",
      "  done: false\n",
      "  episode_len_mean: 402.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 101\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.333419911066691\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011425954459008262\n",
      "          policy_loss: -0.11984586368004481\n",
      "          total_loss: -0.13744529452588822\n",
      "          vf_explained_var: -0.7860745787620544\n",
      "          vf_loss: 0.0005930860944014664\n",
      "    num_agent_steps_sampled: 41000\n",
      "    num_agent_steps_trained: 41000\n",
      "    num_steps_sampled: 41000\n",
      "    num_steps_trained: 41000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.32413793103448\n",
      "    ram_util_percent: 57.64827586206898\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038622059463615487\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.62712369923374\n",
      "    mean_inference_ms: 1.5813733665216845\n",
      "    mean_raw_obs_processing_ms: 0.9554512394454906\n",
      "  time_since_restore: 955.4196910858154\n",
      "  time_this_iter_s: 20.387677907943726\n",
      "  time_total_s: 955.4196910858154\n",
      "  timers:\n",
      "    learn_throughput: 949.939\n",
      "    learn_time_ms: 1052.699\n",
      "    load_throughput: 60090.401\n",
      "    load_time_ms: 16.642\n",
      "    sample_throughput: 44.187\n",
      "    sample_time_ms: 22631.076\n",
      "    update_time_ms: 4.775\n",
      "  timestamp: 1633706753\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 41000\n",
      "  training_iteration: 41\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">          955.42</td><td style=\"text-align: right;\">41000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            402.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 42000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-26-14\n",
      "  done: false\n",
      "  episode_len_mean: 401.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 104\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3768936157226563\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014107737200525172\n",
      "          policy_loss: -0.054850105237629676\n",
      "          total_loss: -0.07126599852409628\n",
      "          vf_explained_var: -0.6091209053993225\n",
      "          vf_loss: 0.001004562159262908\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.27096774193548\n",
      "    ram_util_percent: 57.658064516129045\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03858464513873165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.770885299609017\n",
      "    mean_inference_ms: 1.5797454675321803\n",
      "    mean_raw_obs_processing_ms: 0.9914709067402044\n",
      "  time_since_restore: 977.049350976944\n",
      "  time_this_iter_s: 21.62965989112854\n",
      "  time_total_s: 977.049350976944\n",
      "  timers:\n",
      "    learn_throughput: 948.816\n",
      "    learn_time_ms: 1053.945\n",
      "    load_throughput: 59949.374\n",
      "    load_time_ms: 16.681\n",
      "    sample_throughput: 44.072\n",
      "    sample_time_ms: 22689.907\n",
      "    update_time_ms: 4.789\n",
      "  timestamp: 1633706774\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 42\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         977.049</td><td style=\"text-align: right;\">42000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            401.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 43000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-26-33\n",
      "  done: false\n",
      "  episode_len_mean: 400.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 106\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.4671180221769546\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013185844145071495\n",
      "          policy_loss: -0.051812904493676294\n",
      "          total_loss: -0.06979021289282375\n",
      "          vf_explained_var: -0.3220367729663849\n",
      "          vf_loss: 0.0007602410045769324\n",
      "    num_agent_steps_sampled: 43000\n",
      "    num_agent_steps_trained: 43000\n",
      "    num_steps_sampled: 43000\n",
      "    num_steps_trained: 43000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.46666666666666\n",
      "    ram_util_percent: 57.7925925925926\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03857180870743723\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.512653446722698\n",
      "    mean_inference_ms: 1.579243655419317\n",
      "    mean_raw_obs_processing_ms: 1.0148685364591603\n",
      "  time_since_restore: 995.9327566623688\n",
      "  time_this_iter_s: 18.883405685424805\n",
      "  time_total_s: 995.9327566623688\n",
      "  timers:\n",
      "    learn_throughput: 950.241\n",
      "    learn_time_ms: 1052.365\n",
      "    load_throughput: 59834.348\n",
      "    load_time_ms: 16.713\n",
      "    sample_throughput: 44.273\n",
      "    sample_time_ms: 22586.983\n",
      "    update_time_ms: 4.097\n",
      "  timestamp: 1633706793\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 43000\n",
      "  training_iteration: 43\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         995.933</td><td style=\"text-align: right;\">43000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            400.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-26-58\n",
      "  done: false\n",
      "  episode_len_mean: 399.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 109\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2697643041610718\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012655906928676448\n",
      "          policy_loss: -0.07646236109236876\n",
      "          total_loss: -0.09249729530678855\n",
      "          vf_explained_var: -0.6756010055541992\n",
      "          vf_loss: 0.0009675479775372272\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.32857142857142\n",
      "    ram_util_percent: 57.900000000000006\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03855788001389492\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.204440782244195\n",
      "    mean_inference_ms: 1.5786694301844977\n",
      "    mean_raw_obs_processing_ms: 1.049284833281126\n",
      "  time_since_restore: 1020.8149695396423\n",
      "  time_this_iter_s: 24.88221287727356\n",
      "  time_total_s: 1020.8149695396423\n",
      "  timers:\n",
      "    learn_throughput: 960.758\n",
      "    learn_time_ms: 1040.844\n",
      "    load_throughput: 61197.57\n",
      "    load_time_ms: 16.341\n",
      "    sample_throughput: 43.786\n",
      "    sample_time_ms: 22838.388\n",
      "    update_time_ms: 3.763\n",
      "  timestamp: 1633706818\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 44\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         1020.81</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            399.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 45000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-27-20\n",
      "  done: false\n",
      "  episode_len_mean: 397.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 112\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3382974174287585\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01641288227445303\n",
      "          policy_loss: -0.034804716871844395\n",
      "          total_loss: -0.04993203754226367\n",
      "          vf_explained_var: -0.33705219626426697\n",
      "          vf_loss: 0.0008698588696360174\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_agent_steps_trained: 45000\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.090625\n",
      "    ram_util_percent: 58.0\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03853764518362101\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.01381944598526\n",
      "    mean_inference_ms: 1.5780255269690775\n",
      "    mean_raw_obs_processing_ms: 1.0829817048073584\n",
      "  time_since_restore: 1042.8674805164337\n",
      "  time_this_iter_s: 22.052510976791382\n",
      "  time_total_s: 1042.8674805164337\n",
      "  timers:\n",
      "    learn_throughput: 964.412\n",
      "    learn_time_ms: 1036.901\n",
      "    load_throughput: 59854.584\n",
      "    load_time_ms: 16.707\n",
      "    sample_throughput: 44.416\n",
      "    sample_time_ms: 22514.365\n",
      "    update_time_ms: 3.671\n",
      "  timestamp: 1633706840\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 45\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         1042.87</td><td style=\"text-align: right;\">45000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            397.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 46000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-27-40\n",
      "  done: false\n",
      "  episode_len_mean: 397.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 115\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3409555938508775\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01166413812646053\n",
      "          policy_loss: 0.012006347253918647\n",
      "          total_loss: -0.005436339974403381\n",
      "          vf_explained_var: 0.16586747765541077\n",
      "          vf_loss: 0.0007180061514696313\n",
      "    num_agent_steps_sampled: 46000\n",
      "    num_agent_steps_trained: 46000\n",
      "    num_steps_sampled: 46000\n",
      "    num_steps_trained: 46000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.48571428571428\n",
      "    ram_util_percent: 58.09999999999999\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03851494009749033\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.85857184275477\n",
      "    mean_inference_ms: 1.5773494894079065\n",
      "    mean_raw_obs_processing_ms: 1.1158551608826703\n",
      "  time_since_restore: 1062.879314661026\n",
      "  time_this_iter_s: 20.011834144592285\n",
      "  time_total_s: 1062.879314661026\n",
      "  timers:\n",
      "    learn_throughput: 963.787\n",
      "    learn_time_ms: 1037.574\n",
      "    load_throughput: 59090.517\n",
      "    load_time_ms: 16.923\n",
      "    sample_throughput: 45.041\n",
      "    sample_time_ms: 22201.788\n",
      "    update_time_ms: 2.257\n",
      "  timestamp: 1633706860\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 46\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         1062.88</td><td style=\"text-align: right;\">46000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            397.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 47000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-28-02\n",
      "  done: false\n",
      "  episode_len_mean: 396.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 117\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3432159105936687\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012896143475172532\n",
      "          policy_loss: -0.09237645674082968\n",
      "          total_loss: -0.10923393910957707\n",
      "          vf_explained_var: 0.07146064937114716\n",
      "          vf_loss: 0.0007714100460159696\n",
      "    num_agent_steps_sampled: 47000\n",
      "    num_agent_steps_trained: 47000\n",
      "    num_steps_sampled: 47000\n",
      "    num_steps_trained: 47000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.090322580645164\n",
      "    ram_util_percent: 58.09999999999998\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03850288331733052\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.775395953472927\n",
      "    mean_inference_ms: 1.5769625434975563\n",
      "    mean_raw_obs_processing_ms: 1.1372868756513566\n",
      "  time_since_restore: 1084.4549715518951\n",
      "  time_this_iter_s: 21.57565689086914\n",
      "  time_total_s: 1084.4549715518951\n",
      "  timers:\n",
      "    learn_throughput: 961.803\n",
      "    learn_time_ms: 1039.714\n",
      "    load_throughput: 57363.181\n",
      "    load_time_ms: 17.433\n",
      "    sample_throughput: 48.943\n",
      "    sample_time_ms: 20431.916\n",
      "    update_time_ms: 2.26\n",
      "  timestamp: 1633706882\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47000\n",
      "  training_iteration: 47\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         1084.45</td><td style=\"text-align: right;\">47000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            396.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-28-40\n",
      "  done: false\n",
      "  episode_len_mean: 395.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 120\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.323434395260281\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011133614691469281\n",
      "          policy_loss: -0.06544215058286985\n",
      "          total_loss: -0.08320697744687398\n",
      "          vf_explained_var: -0.7088407278060913\n",
      "          vf_loss: 0.00045939003919354745\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.9290909090909\n",
      "    ram_util_percent: 58.092727272727274\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038487232038431934\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.666564124194668\n",
      "    mean_inference_ms: 1.5764324309025688\n",
      "    mean_raw_obs_processing_ms: 1.1792381193957764\n",
      "  time_since_restore: 1122.654201745987\n",
      "  time_this_iter_s: 38.1992301940918\n",
      "  time_total_s: 1122.654201745987\n",
      "  timers:\n",
      "    learn_throughput: 962.1\n",
      "    learn_time_ms: 1039.393\n",
      "    load_throughput: 56845.095\n",
      "    load_time_ms: 17.592\n",
      "    sample_throughput: 45.271\n",
      "    sample_time_ms: 22089.157\n",
      "    update_time_ms: 2.23\n",
      "  timestamp: 1633706920\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 48\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         1122.65</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            395.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 49000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-29-02\n",
      "  done: false\n",
      "  episode_len_mean: 395.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 122\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.263989456494649\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013627313707734463\n",
      "          policy_loss: -0.0974124585174852\n",
      "          total_loss: -0.113113080834349\n",
      "          vf_explained_var: -0.5337077975273132\n",
      "          vf_loss: 0.000806982902253771\n",
      "    num_agent_steps_sampled: 49000\n",
      "    num_agent_steps_trained: 49000\n",
      "    num_steps_sampled: 49000\n",
      "    num_steps_trained: 49000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.61935483870966\n",
      "    ram_util_percent: 58.04838709677419\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03847843852792086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.605945050844028\n",
      "    mean_inference_ms: 1.5761135822836814\n",
      "    mean_raw_obs_processing_ms: 1.2066218297347233\n",
      "  time_since_restore: 1144.2515478134155\n",
      "  time_this_iter_s: 21.59734606742859\n",
      "  time_total_s: 1144.2515478134155\n",
      "  timers:\n",
      "    learn_throughput: 962.092\n",
      "    learn_time_ms: 1039.402\n",
      "    load_throughput: 56698.939\n",
      "    load_time_ms: 17.637\n",
      "    sample_throughput: 45.316\n",
      "    sample_time_ms: 22067.056\n",
      "    update_time_ms: 2.641\n",
      "  timestamp: 1633706942\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49000\n",
      "  training_iteration: 49\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         1144.25</td><td style=\"text-align: right;\">49000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            395.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 50000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-29-25\n",
      "  done: false\n",
      "  episode_len_mean: 393.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 125\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.398398616578844\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013027490163962756\n",
      "          policy_loss: -0.13691056836396456\n",
      "          total_loss: -0.15450175625996457\n",
      "          vf_explained_var: -0.6091822385787964\n",
      "          vf_loss: 0.0005304284241800714\n",
      "    num_agent_steps_sampled: 50000\n",
      "    num_agent_steps_trained: 50000\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.021212121212116\n",
      "    ram_util_percent: 57.924242424242415\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03846731173740717\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.52684401127194\n",
      "    mean_inference_ms: 1.5756853864981355\n",
      "    mean_raw_obs_processing_ms: 1.2468890176010146\n",
      "  time_since_restore: 1167.3346436023712\n",
      "  time_this_iter_s: 23.08309578895569\n",
      "  time_total_s: 1167.3346436023712\n",
      "  timers:\n",
      "    learn_throughput: 961.875\n",
      "    learn_time_ms: 1039.636\n",
      "    load_throughput: 57849.517\n",
      "    load_time_ms: 17.286\n",
      "    sample_throughput: 45.111\n",
      "    sample_time_ms: 22167.551\n",
      "    update_time_ms: 2.641\n",
      "  timestamp: 1633706965\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 50\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         1167.33</td><td style=\"text-align: right;\">50000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            393.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 51000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-29-45\n",
      "  done: false\n",
      "  episode_len_mean: 394.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 127\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2100488397810194\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01185597930332533\n",
      "          policy_loss: -0.039194484593139754\n",
      "          total_loss: -0.0553234760546022\n",
      "          vf_explained_var: -0.7969089150428772\n",
      "          vf_loss: 0.0006363059584853343\n",
      "    num_agent_steps_sampled: 51000\n",
      "    num_agent_steps_trained: 51000\n",
      "    num_steps_sampled: 51000\n",
      "    num_steps_trained: 51000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.29310344827586\n",
      "    ram_util_percent: 57.67931034482756\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038461914923695315\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.482983807381196\n",
      "    mean_inference_ms: 1.575457129234935\n",
      "    mean_raw_obs_processing_ms: 1.2732504639423416\n",
      "  time_since_restore: 1187.912449836731\n",
      "  time_this_iter_s: 20.57780623435974\n",
      "  time_total_s: 1187.912449836731\n",
      "  timers:\n",
      "    learn_throughput: 962.385\n",
      "    learn_time_ms: 1039.085\n",
      "    load_throughput: 57820.568\n",
      "    load_time_ms: 17.295\n",
      "    sample_throughput: 45.071\n",
      "    sample_time_ms: 22187.126\n",
      "    update_time_ms: 2.623\n",
      "  timestamp: 1633706985\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 51000\n",
      "  training_iteration: 51\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         1187.91</td><td style=\"text-align: right;\">51000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            394.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-30-09\n",
      "  done: false\n",
      "  episode_len_mean: 393.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 130\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3939916928609213\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011826667609547157\n",
      "          policy_loss: -0.10348692271444533\n",
      "          total_loss: -0.12101281310121219\n",
      "          vf_explained_var: -0.4846968650817871\n",
      "          vf_loss: 0.0010920226389619832\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.07058823529411\n",
      "    ram_util_percent: 57.55\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038454661260564946\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.42676365166075\n",
      "    mean_inference_ms: 1.5751509836569522\n",
      "    mean_raw_obs_processing_ms: 1.2858086534990811\n",
      "  time_since_restore: 1211.4509539604187\n",
      "  time_this_iter_s: 23.538504123687744\n",
      "  time_total_s: 1211.4509539604187\n",
      "  timers:\n",
      "    learn_throughput: 963.715\n",
      "    learn_time_ms: 1037.651\n",
      "    load_throughput: 57471.335\n",
      "    load_time_ms: 17.4\n",
      "    sample_throughput: 44.684\n",
      "    sample_time_ms: 22379.343\n",
      "    update_time_ms: 2.637\n",
      "  timestamp: 1633707009\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 52\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         1211.45</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            393.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 53000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-30-32\n",
      "  done: false\n",
      "  episode_len_mean: 392.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 133\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.39494534863366\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011723350913281156\n",
      "          policy_loss: -0.1803598926299148\n",
      "          total_loss: -0.1981760621070862\n",
      "          vf_explained_var: -0.3764055371284485\n",
      "          vf_loss: 0.0008577745921987419\n",
      "    num_agent_steps_sampled: 53000\n",
      "    num_agent_steps_trained: 53000\n",
      "    num_steps_sampled: 53000\n",
      "    num_steps_trained: 53000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.0625\n",
      "    ram_util_percent: 57.5625\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03844795928958888\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.378763424501233\n",
      "    mean_inference_ms: 1.574845659551597\n",
      "    mean_raw_obs_processing_ms: 1.2864142855103646\n",
      "  time_since_restore: 1234.1869022846222\n",
      "  time_this_iter_s: 22.73594832420349\n",
      "  time_total_s: 1234.1869022846222\n",
      "  timers:\n",
      "    learn_throughput: 962.93\n",
      "    learn_time_ms: 1038.497\n",
      "    load_throughput: 56888.966\n",
      "    load_time_ms: 17.578\n",
      "    sample_throughput: 43.93\n",
      "    sample_time_ms: 22763.578\n",
      "    update_time_ms: 2.635\n",
      "  timestamp: 1633707032\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 53000\n",
      "  training_iteration: 53\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         1234.19</td><td style=\"text-align: right;\">53000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             392.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 54000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-30-54\n",
      "  done: false\n",
      "  episode_len_mean: 392.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 135\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3107956409454347\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014237399555558186\n",
      "          policy_loss: -0.04601718431545628\n",
      "          total_loss: -0.06170841790735722\n",
      "          vf_explained_var: -0.7568357586860657\n",
      "          vf_loss: 0.0010098924161866308\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 54000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.0375\n",
      "    ram_util_percent: 57.653125\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03844437354438221\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.35309076548452\n",
      "    mean_inference_ms: 1.5746510143214636\n",
      "    mean_raw_obs_processing_ms: 1.288585240584325\n",
      "  time_since_restore: 1256.2408754825592\n",
      "  time_this_iter_s: 22.05397319793701\n",
      "  time_total_s: 1256.2408754825592\n",
      "  timers:\n",
      "    learn_throughput: 961.911\n",
      "    learn_time_ms: 1039.597\n",
      "    load_throughput: 56127.985\n",
      "    load_time_ms: 17.816\n",
      "    sample_throughput: 44.485\n",
      "    sample_time_ms: 22479.424\n",
      "    update_time_ms: 2.641\n",
      "  timestamp: 1633707054\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 54\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         1256.24</td><td style=\"text-align: right;\">54000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            392.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 55000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-31-16\n",
      "  done: false\n",
      "  episode_len_mean: 392.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 138\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3984121057722305\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011818739374484149\n",
      "          policy_loss: -0.06210493902779288\n",
      "          total_loss: -0.08015384199097753\n",
      "          vf_explained_var: -0.86616051197052\n",
      "          vf_loss: 0.0006167854787135083\n",
      "    num_agent_steps_sampled: 55000\n",
      "    num_agent_steps_trained: 55000\n",
      "    num_steps_sampled: 55000\n",
      "    num_steps_trained: 55000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.170967741935485\n",
      "    ram_util_percent: 57.761290322580635\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03843917869591575\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.317418429135863\n",
      "    mean_inference_ms: 1.5743785299888142\n",
      "    mean_raw_obs_processing_ms: 1.2925966811560123\n",
      "  time_since_restore: 1278.5133121013641\n",
      "  time_this_iter_s: 22.27243661880493\n",
      "  time_total_s: 1278.5133121013641\n",
      "  timers:\n",
      "    learn_throughput: 961.645\n",
      "    learn_time_ms: 1039.885\n",
      "    load_throughput: 55485.643\n",
      "    load_time_ms: 18.023\n",
      "    sample_throughput: 44.443\n",
      "    sample_time_ms: 22500.933\n",
      "    update_time_ms: 2.64\n",
      "  timestamp: 1633707076\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55000\n",
      "  training_iteration: 55\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         1278.51</td><td style=\"text-align: right;\">55000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            392.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-31-38\n",
      "  done: false\n",
      "  episode_len_mean: 392.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 140\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.34898964299096\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011370198856243895\n",
      "          policy_loss: -0.052779334359284905\n",
      "          total_loss: -0.07059429738049706\n",
      "          vf_explained_var: -0.9554060101509094\n",
      "          vf_loss: 0.0005583458602713007\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.9875\n",
      "    ram_util_percent: 57.896874999999994\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038436386297997886\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.29709245881104\n",
      "    mean_inference_ms: 1.5742113301032254\n",
      "    mean_raw_obs_processing_ms: 1.2965337118226616\n",
      "  time_since_restore: 1300.3028752803802\n",
      "  time_this_iter_s: 21.789563179016113\n",
      "  time_total_s: 1300.3028752803802\n",
      "  timers:\n",
      "    learn_throughput: 960.262\n",
      "    learn_time_ms: 1041.383\n",
      "    load_throughput: 55620.219\n",
      "    load_time_ms: 17.979\n",
      "    sample_throughput: 44.099\n",
      "    sample_time_ms: 22676.287\n",
      "    update_time_ms: 3.605\n",
      "  timestamp: 1633707098\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 56\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">          1300.3</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            392.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 57000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-31-59\n",
      "  done: false\n",
      "  episode_len_mean: 393.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 143\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2980007621977063\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011802948346666989\n",
      "          policy_loss: -0.02411598474201229\n",
      "          total_loss: -0.0404235754472514\n",
      "          vf_explained_var: -0.9746467471122742\n",
      "          vf_loss: 0.0013610894717405446\n",
      "    num_agent_steps_sampled: 57000\n",
      "    num_agent_steps_trained: 57000\n",
      "    num_steps_sampled: 57000\n",
      "    num_steps_trained: 57000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.95333333333333\n",
      "    ram_util_percent: 58.00999999999999\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03843181846834646\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.267584506705628\n",
      "    mean_inference_ms: 1.573968705879674\n",
      "    mean_raw_obs_processing_ms: 1.3029015730333173\n",
      "  time_since_restore: 1321.9193019866943\n",
      "  time_this_iter_s: 21.616426706314087\n",
      "  time_total_s: 1321.9193019866943\n",
      "  timers:\n",
      "    learn_throughput: 960.413\n",
      "    learn_time_ms: 1041.219\n",
      "    load_throughput: 54407.534\n",
      "    load_time_ms: 18.38\n",
      "    sample_throughput: 44.091\n",
      "    sample_time_ms: 22680.132\n",
      "    update_time_ms: 3.603\n",
      "  timestamp: 1633707119\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 57000\n",
      "  training_iteration: 57\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         1321.92</td><td style=\"text-align: right;\">57000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            393.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 58000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-32-22\n",
      "  done: false\n",
      "  episode_len_mean: 392.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 145\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3279359367158676\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01214146637849846\n",
      "          policy_loss: -0.07595320112175412\n",
      "          total_loss: -0.09271491091284487\n",
      "          vf_explained_var: -0.8897081017494202\n",
      "          vf_loss: 0.00105398857461599\n",
      "    num_agent_steps_sampled: 58000\n",
      "    num_agent_steps_trained: 58000\n",
      "    num_steps_sampled: 58000\n",
      "    num_steps_trained: 58000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.06969696969696\n",
      "    ram_util_percent: 58.10909090909092\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03842853845805196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.2495165049108\n",
      "    mean_inference_ms: 1.5738120973775227\n",
      "    mean_raw_obs_processing_ms: 1.3076023752741641\n",
      "  time_since_restore: 1345.012761592865\n",
      "  time_this_iter_s: 23.093459606170654\n",
      "  time_total_s: 1345.012761592865\n",
      "  timers:\n",
      "    learn_throughput: 959.659\n",
      "    learn_time_ms: 1042.036\n",
      "    load_throughput: 55115.98\n",
      "    load_time_ms: 18.144\n",
      "    sample_throughput: 47.239\n",
      "    sample_time_ms: 21168.955\n",
      "    update_time_ms: 3.611\n",
      "  timestamp: 1633707142\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 58000\n",
      "  training_iteration: 58\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         1345.01</td><td style=\"text-align: right;\">58000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            392.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 59000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-32-44\n",
      "  done: false\n",
      "  episode_len_mean: 392.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 148\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3881861554251778\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011877552812318587\n",
      "          policy_loss: -0.0007701544505026606\n",
      "          total_loss: -0.018629600604375202\n",
      "          vf_explained_var: -0.5250116586685181\n",
      "          vf_loss: 0.0006775164110068646\n",
      "    num_agent_steps_sampled: 59000\n",
      "    num_agent_steps_trained: 59000\n",
      "    num_steps_sampled: 59000\n",
      "    num_steps_trained: 59000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.21612903225807\n",
      "    ram_util_percent: 58.1516129032258\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03842344881326877\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.225959685562025\n",
      "    mean_inference_ms: 1.573592704835825\n",
      "    mean_raw_obs_processing_ms: 1.3157510077789507\n",
      "  time_since_restore: 1366.6271283626556\n",
      "  time_this_iter_s: 21.61436676979065\n",
      "  time_total_s: 1366.6271283626556\n",
      "  timers:\n",
      "    learn_throughput: 959.808\n",
      "    learn_time_ms: 1041.875\n",
      "    load_throughput: 54638.874\n",
      "    load_time_ms: 18.302\n",
      "    sample_throughput: 47.234\n",
      "    sample_time_ms: 21171.09\n",
      "    update_time_ms: 3.193\n",
      "  timestamp: 1633707164\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 59000\n",
      "  training_iteration: 59\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         1366.63</td><td style=\"text-align: right;\">59000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            392.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-33-25\n",
      "  done: false\n",
      "  episode_len_mean: 392.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 150\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3332065025965374\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013630282921526377\n",
      "          policy_loss: -0.10816716606625253\n",
      "          total_loss: -0.12444439504502548\n",
      "          vf_explained_var: -0.5750746130943298\n",
      "          vf_loss: 0.0009212113399472502\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.991379310344826\n",
      "    ram_util_percent: 58.048275862068955\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038420746240722385\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.214075647464302\n",
      "    mean_inference_ms: 1.5734547710596671\n",
      "    mean_raw_obs_processing_ms: 1.3275582473047356\n",
      "  time_since_restore: 1407.3253951072693\n",
      "  time_this_iter_s: 40.69826674461365\n",
      "  time_total_s: 1407.3253951072693\n",
      "  timers:\n",
      "    learn_throughput: 958.431\n",
      "    learn_time_ms: 1043.372\n",
      "    load_throughput: 53558.209\n",
      "    load_time_ms: 18.671\n",
      "    sample_throughput: 43.61\n",
      "    sample_time_ms: 22930.764\n",
      "    update_time_ms: 3.203\n",
      "  timestamp: 1633707205\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 60\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         1407.33</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            392.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 61000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-33-50\n",
      "  done: false\n",
      "  episode_len_mean: 390.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 153\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.378480648994446\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0131592678317066\n",
      "          policy_loss: -0.09685716595914629\n",
      "          total_loss: -0.1138841567767991\n",
      "          vf_explained_var: -0.3870663642883301\n",
      "          vf_loss: 0.0008361448443287776\n",
      "    num_agent_steps_sampled: 61000\n",
      "    num_agent_steps_trained: 61000\n",
      "    num_steps_sampled: 61000\n",
      "    num_steps_trained: 61000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.83888888888888\n",
      "    ram_util_percent: 58.18055555555556\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03841777914632724\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.19888956507375\n",
      "    mean_inference_ms: 1.5732727123987544\n",
      "    mean_raw_obs_processing_ms: 1.3453412715570476\n",
      "  time_since_restore: 1432.0126414299011\n",
      "  time_this_iter_s: 24.687246322631836\n",
      "  time_total_s: 1432.0126414299011\n",
      "  timers:\n",
      "    learn_throughput: 956.011\n",
      "    learn_time_ms: 1046.013\n",
      "    load_throughput: 55253.642\n",
      "    load_time_ms: 18.098\n",
      "    sample_throughput: 42.846\n",
      "    sample_time_ms: 23339.636\n",
      "    update_time_ms: 3.212\n",
      "  timestamp: 1633707230\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 61000\n",
      "  training_iteration: 61\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         1432.01</td><td style=\"text-align: right;\">61000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            390.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 62000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-34-13\n",
      "  done: false\n",
      "  episode_len_mean: 388.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 156\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2022084765964083\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01202232066572513\n",
      "          policy_loss: -0.07082159970369604\n",
      "          total_loss: -0.08617038586073451\n",
      "          vf_explained_var: -0.8894367814064026\n",
      "          vf_loss: 0.001263255022543793\n",
      "    num_agent_steps_sampled: 62000\n",
      "    num_agent_steps_trained: 62000\n",
      "    num_steps_sampled: 62000\n",
      "    num_steps_trained: 62000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.43030303030302\n",
      "    ram_util_percent: 57.833333333333336\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038416111557182255\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.188650063451735\n",
      "    mean_inference_ms: 1.5731230485173773\n",
      "    mean_raw_obs_processing_ms: 1.3638191544796776\n",
      "  time_since_restore: 1455.6826910972595\n",
      "  time_this_iter_s: 23.6700496673584\n",
      "  time_total_s: 1455.6826910972595\n",
      "  timers:\n",
      "    learn_throughput: 954.518\n",
      "    learn_time_ms: 1047.649\n",
      "    load_throughput: 55820.153\n",
      "    load_time_ms: 17.915\n",
      "    sample_throughput: 42.824\n",
      "    sample_time_ms: 23351.358\n",
      "    update_time_ms: 3.184\n",
      "  timestamp: 1633707253\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 62000\n",
      "  training_iteration: 62\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         1455.68</td><td style=\"text-align: right;\">62000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            388.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 63000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-34-36\n",
      "  done: false\n",
      "  episode_len_mean: 387.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 159\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.311572480201721\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011759842562029811\n",
      "          policy_loss: -0.07458106830923093\n",
      "          total_loss: -0.09120743368855781\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0011974291785413192\n",
      "    num_agent_steps_sampled: 63000\n",
      "    num_agent_steps_trained: 63000\n",
      "    num_steps_sampled: 63000\n",
      "    num_steps_trained: 63000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.478787878787884\n",
      "    ram_util_percent: 57.481818181818184\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03841581176767973\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.1816573478998\n",
      "    mean_inference_ms: 1.5730006663021388\n",
      "    mean_raw_obs_processing_ms: 1.3762869234197532\n",
      "  time_since_restore: 1478.2705571651459\n",
      "  time_this_iter_s: 22.587866067886353\n",
      "  time_total_s: 1478.2705571651459\n",
      "  timers:\n",
      "    learn_throughput: 953.271\n",
      "    learn_time_ms: 1049.02\n",
      "    load_throughput: 56445.687\n",
      "    load_time_ms: 17.716\n",
      "    sample_throughput: 42.853\n",
      "    sample_time_ms: 23335.373\n",
      "    update_time_ms: 3.19\n",
      "  timestamp: 1633707276\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63000\n",
      "  training_iteration: 63\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         1478.27</td><td style=\"text-align: right;\">63000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             387.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-34-59\n",
      "  done: false\n",
      "  episode_len_mean: 387.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 161\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2852359188927545\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013150578461375101\n",
      "          policy_loss: -0.11313517118493716\n",
      "          total_loss: -0.12878392247690096\n",
      "          vf_explained_var: -0.7102711200714111\n",
      "          vf_loss: 0.0012858461582153622\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.68181818181818\n",
      "    ram_util_percent: 57.40909090909091\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0384158060266234\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.17802363295414\n",
      "    mean_inference_ms: 1.572925295622082\n",
      "    mean_raw_obs_processing_ms: 1.3757441335547695\n",
      "  time_since_restore: 1501.5328607559204\n",
      "  time_this_iter_s: 23.262303590774536\n",
      "  time_total_s: 1501.5328607559204\n",
      "  timers:\n",
      "    learn_throughput: 949.389\n",
      "    learn_time_ms: 1053.309\n",
      "    load_throughput: 57301.192\n",
      "    load_time_ms: 17.452\n",
      "    sample_throughput: 42.64\n",
      "    sample_time_ms: 23452.152\n",
      "    update_time_ms: 3.203\n",
      "  timestamp: 1633707299\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 64\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         1501.53</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            387.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 65000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-35-25\n",
      "  done: false\n",
      "  episode_len_mean: 385.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 164\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.118957993719313\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012172124469002357\n",
      "          policy_loss: -0.029587843517462412\n",
      "          total_loss: -0.04451901134517458\n",
      "          vf_explained_var: -0.7644549012184143\n",
      "          vf_loss: 0.0007809563730714014\n",
      "    num_agent_steps_sampled: 65000\n",
      "    num_agent_steps_trained: 65000\n",
      "    num_steps_sampled: 65000\n",
      "    num_steps_trained: 65000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.64054054054054\n",
      "    ram_util_percent: 57.5135135135135\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03841609445723375\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.177915209067912\n",
      "    mean_inference_ms: 1.572834812958426\n",
      "    mean_raw_obs_processing_ms: 1.3763916313346487\n",
      "  time_since_restore: 1527.196707725525\n",
      "  time_this_iter_s: 25.663846969604492\n",
      "  time_total_s: 1527.196707725525\n",
      "  timers:\n",
      "    learn_throughput: 948.256\n",
      "    learn_time_ms: 1054.567\n",
      "    load_throughput: 58498.29\n",
      "    load_time_ms: 17.095\n",
      "    sample_throughput: 42.034\n",
      "    sample_time_ms: 23790.378\n",
      "    update_time_ms: 3.201\n",
      "  timestamp: 1633707325\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 65000\n",
      "  training_iteration: 65\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">          1527.2</td><td style=\"text-align: right;\">65000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            385.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 66000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-35-48\n",
      "  done: false\n",
      "  episode_len_mean: 383.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 167\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.031539367304908\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015185017893906898\n",
      "          policy_loss: -0.04911345371769534\n",
      "          total_loss: -0.060745519834260145\n",
      "          vf_explained_var: -0.6623243093490601\n",
      "          vf_loss: 0.0018500694086671704\n",
      "    num_agent_steps_sampled: 66000\n",
      "    num_agent_steps_trained: 66000\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 66000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.29375\n",
      "    ram_util_percent: 57.60625\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03841583664778944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.181819985265523\n",
      "    mean_inference_ms: 1.5727533343958324\n",
      "    mean_raw_obs_processing_ms: 1.3782997866310185\n",
      "  time_since_restore: 1550.2474126815796\n",
      "  time_this_iter_s: 23.050704956054688\n",
      "  time_total_s: 1550.2474126815796\n",
      "  timers:\n",
      "    learn_throughput: 947.959\n",
      "    learn_time_ms: 1054.898\n",
      "    load_throughput: 58704.288\n",
      "    load_time_ms: 17.035\n",
      "    sample_throughput: 41.811\n",
      "    sample_time_ms: 23917.151\n",
      "    update_time_ms: 2.251\n",
      "  timestamp: 1633707348\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 66\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         1550.25</td><td style=\"text-align: right;\">66000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             383.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 67000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-36-12\n",
      "  done: false\n",
      "  episode_len_mean: 382.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 170\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2521740436553954\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013183085766644669\n",
      "          policy_loss: -0.08257977523737484\n",
      "          total_loss: -0.09819849125213093\n",
      "          vf_explained_var: -0.31043893098831177\n",
      "          vf_loss: 0.0009706358477059337\n",
      "    num_agent_steps_sampled: 67000\n",
      "    num_agent_steps_trained: 67000\n",
      "    num_steps_sampled: 67000\n",
      "    num_steps_trained: 67000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.13055555555556\n",
      "    ram_util_percent: 57.71111111111112\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03841431688502421\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.186023326585495\n",
      "    mean_inference_ms: 1.5726462238228263\n",
      "    mean_raw_obs_processing_ms: 1.3809113094412158\n",
      "  time_since_restore: 1574.864665031433\n",
      "  time_this_iter_s: 24.617252349853516\n",
      "  time_total_s: 1574.864665031433\n",
      "  timers:\n",
      "    learn_throughput: 948.256\n",
      "    learn_time_ms: 1054.568\n",
      "    load_throughput: 59739.155\n",
      "    load_time_ms: 16.739\n",
      "    sample_throughput: 41.292\n",
      "    sample_time_ms: 24217.791\n",
      "    update_time_ms: 2.338\n",
      "  timestamp: 1633707372\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 67000\n",
      "  training_iteration: 67\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         1574.86</td><td style=\"text-align: right;\">67000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               382</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-36-39\n",
      "  done: false\n",
      "  episode_len_mean: 380.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 173\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.129625732368893\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011651506078355921\n",
      "          policy_loss: -0.13640111097031168\n",
      "          total_loss: -0.15172625912560356\n",
      "          vf_explained_var: -0.6280807852745056\n",
      "          vf_loss: 0.0007279316910878858\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.71351351351352\n",
      "    ram_util_percent: 58.286486486486496\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038412352186570645\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.191290283629932\n",
      "    mean_inference_ms: 1.572517014522215\n",
      "    mean_raw_obs_processing_ms: 1.3845157397912673\n",
      "  time_since_restore: 1601.3435170650482\n",
      "  time_this_iter_s: 26.478852033615112\n",
      "  time_total_s: 1601.3435170650482\n",
      "  timers:\n",
      "    learn_throughput: 945.741\n",
      "    learn_time_ms: 1057.372\n",
      "    load_throughput: 59988.043\n",
      "    load_time_ms: 16.67\n",
      "    sample_throughput: 40.727\n",
      "    sample_time_ms: 24553.594\n",
      "    update_time_ms: 2.346\n",
      "  timestamp: 1633707399\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 68\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         1601.34</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            380.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 69000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-37-07\n",
      "  done: false\n",
      "  episode_len_mean: 377.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 176\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1706648561689588\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012982020398925856\n",
      "          policy_loss: -0.09711193396813339\n",
      "          total_loss: -0.11230510688490337\n",
      "          vf_explained_var: -0.7268186211585999\n",
      "          vf_loss: 0.0006715627565022765\n",
      "    num_agent_steps_sampled: 69000\n",
      "    num_agent_steps_trained: 69000\n",
      "    num_steps_sampled: 69000\n",
      "    num_steps_trained: 69000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.8780487804878\n",
      "    ram_util_percent: 59.914634146341456\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038411885392747716\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.201033927454226\n",
      "    mean_inference_ms: 1.572437632039461\n",
      "    mean_raw_obs_processing_ms: 1.3890229114668304\n",
      "  time_since_restore: 1629.4261825084686\n",
      "  time_this_iter_s: 28.08266544342041\n",
      "  time_total_s: 1629.4261825084686\n",
      "  timers:\n",
      "    learn_throughput: 938.895\n",
      "    learn_time_ms: 1065.081\n",
      "    load_throughput: 62403.166\n",
      "    load_time_ms: 16.025\n",
      "    sample_throughput: 39.693\n",
      "    sample_time_ms: 25193.304\n",
      "    update_time_ms: 2.387\n",
      "  timestamp: 1633707427\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 69000\n",
      "  training_iteration: 69\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         1629.43</td><td style=\"text-align: right;\">69000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            377.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 70000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-37-35\n",
      "  done: false\n",
      "  episode_len_mean: 374.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 179\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.126349014706082\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013943297450059452\n",
      "          policy_loss: -0.015715707176261477\n",
      "          total_loss: -0.029476052108738156\n",
      "          vf_explained_var: -0.08452221751213074\n",
      "          vf_loss: 0.001228659035405144\n",
      "    num_agent_steps_sampled: 70000\n",
      "    num_agent_steps_trained: 70000\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.40512820512821\n",
      "    ram_util_percent: 60.369230769230796\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03841238075865395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.2142869613123\n",
      "    mean_inference_ms: 1.5723957052206707\n",
      "    mean_raw_obs_processing_ms: 1.3940013949254877\n",
      "  time_since_restore: 1657.138222694397\n",
      "  time_this_iter_s: 27.712040185928345\n",
      "  time_total_s: 1657.138222694397\n",
      "  timers:\n",
      "    learn_throughput: 934.618\n",
      "    learn_time_ms: 1069.956\n",
      "    load_throughput: 63635.969\n",
      "    load_time_ms: 15.714\n",
      "    sample_throughput: 41.858\n",
      "    sample_time_ms: 23890.109\n",
      "    update_time_ms: 2.397\n",
      "  timestamp: 1633707455\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 70\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         1657.14</td><td style=\"text-align: right;\">70000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            374.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 71000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-38-17\n",
      "  done: false\n",
      "  episode_len_mean: 371.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 182\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1340115626653033\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010940129662024727\n",
      "          policy_loss: 0.012379792808658547\n",
      "          total_loss: -0.003041872237291601\n",
      "          vf_explained_var: -0.19989672303199768\n",
      "          vf_loss: 0.0009953926058693064\n",
      "    num_agent_steps_sampled: 71000\n",
      "    num_agent_steps_trained: 71000\n",
      "    num_steps_sampled: 71000\n",
      "    num_steps_trained: 71000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.52333333333333\n",
      "    ram_util_percent: 60.385\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03841338846992095\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.229358340456336\n",
      "    mean_inference_ms: 1.5723747314354448\n",
      "    mean_raw_obs_processing_ms: 1.4072467910174524\n",
      "  time_since_restore: 1699.3801732063293\n",
      "  time_this_iter_s: 42.24195051193237\n",
      "  time_total_s: 1699.3801732063293\n",
      "  timers:\n",
      "    learn_throughput: 931.713\n",
      "    learn_time_ms: 1073.292\n",
      "    load_throughput: 61325.793\n",
      "    load_time_ms: 16.306\n",
      "    sample_throughput: 38.999\n",
      "    sample_time_ms: 25641.628\n",
      "    update_time_ms: 2.409\n",
      "  timestamp: 1633707497\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 71000\n",
      "  training_iteration: 71\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         1699.38</td><td style=\"text-align: right;\">71000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            371.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-38-42\n",
      "  done: false\n",
      "  episode_len_mean: 371.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 185\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.151399318377177\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009824011260514463\n",
      "          policy_loss: -0.15485681220889091\n",
      "          total_loss: -0.1712997228735023\n",
      "          vf_explained_var: -0.07219359278678894\n",
      "          vf_loss: 0.0006502786263202627\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.62222222222222\n",
      "    ram_util_percent: 60.3111111111111\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038414089106150254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.24336973670943\n",
      "    mean_inference_ms: 1.572345766532077\n",
      "    mean_raw_obs_processing_ms: 1.420745873275696\n",
      "  time_since_restore: 1724.1881620883942\n",
      "  time_this_iter_s: 24.80798888206482\n",
      "  time_total_s: 1724.1881620883942\n",
      "  timers:\n",
      "    learn_throughput: 925.739\n",
      "    learn_time_ms: 1080.218\n",
      "    load_throughput: 61163.213\n",
      "    load_time_ms: 16.35\n",
      "    sample_throughput: 38.837\n",
      "    sample_time_ms: 25748.397\n",
      "    update_time_ms: 2.435\n",
      "  timestamp: 1633707522\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 72\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         1724.19</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            371.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 73000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-39-09\n",
      "  done: false\n",
      "  episode_len_mean: 370.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 188\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1012557877434626\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01506225215230423\n",
      "          policy_loss: -0.11601667354504268\n",
      "          total_loss: -0.12921479274001385\n",
      "          vf_explained_var: -0.3720015585422516\n",
      "          vf_loss: 0.0010364265531987056\n",
      "    num_agent_steps_sampled: 73000\n",
      "    num_agent_steps_trained: 73000\n",
      "    num_steps_sampled: 73000\n",
      "    num_steps_trained: 73000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.064102564102555\n",
      "    ram_util_percent: 59.95897435897437\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03841546686930491\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.257688532666677\n",
      "    mean_inference_ms: 1.5723385399298817\n",
      "    mean_raw_obs_processing_ms: 1.4344809861868297\n",
      "  time_since_restore: 1751.331175327301\n",
      "  time_this_iter_s: 27.14301323890686\n",
      "  time_total_s: 1751.331175327301\n",
      "  timers:\n",
      "    learn_throughput: 921.639\n",
      "    learn_time_ms: 1085.024\n",
      "    load_throughput: 61762.868\n",
      "    load_time_ms: 16.191\n",
      "    sample_throughput: 38.169\n",
      "    sample_time_ms: 26199.28\n",
      "    update_time_ms: 2.437\n",
      "  timestamp: 1633707549\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 73000\n",
      "  training_iteration: 73\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         1751.33</td><td style=\"text-align: right;\">73000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            370.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 74000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-39-35\n",
      "  done: false\n",
      "  episode_len_mean: 370.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 191\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.213610304726495\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013198804036008996\n",
      "          policy_loss: -0.06424454036686156\n",
      "          total_loss: -0.0792712953976459\n",
      "          vf_explained_var: -0.8533276319503784\n",
      "          vf_loss: 0.0011698841780243027\n",
      "    num_agent_steps_sampled: 74000\n",
      "    num_agent_steps_trained: 74000\n",
      "    num_steps_sampled: 74000\n",
      "    num_steps_trained: 74000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.03243243243242\n",
      "    ram_util_percent: 59.78108108108107\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038418047786343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.272828342701505\n",
      "    mean_inference_ms: 1.5723706670788473\n",
      "    mean_raw_obs_processing_ms: 1.4351199791179807\n",
      "  time_since_restore: 1777.69948387146\n",
      "  time_this_iter_s: 26.368308544158936\n",
      "  time_total_s: 1777.69948387146\n",
      "  timers:\n",
      "    learn_throughput: 920.905\n",
      "    learn_time_ms: 1085.888\n",
      "    load_throughput: 60227.25\n",
      "    load_time_ms: 16.604\n",
      "    sample_throughput: 37.724\n",
      "    sample_time_ms: 26508.589\n",
      "    update_time_ms: 2.447\n",
      "  timestamp: 1633707575\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 74000\n",
      "  training_iteration: 74\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">          1777.7</td><td style=\"text-align: right;\">74000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            370.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 75000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-40-03\n",
      "  done: false\n",
      "  episode_len_mean: 368.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 194\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.099246528413561\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009285908846312729\n",
      "          policy_loss: 0.0030442372585336368\n",
      "          total_loss: -0.013296854371825853\n",
      "          vf_explained_var: -0.5408296585083008\n",
      "          vf_loss: 0.00047271388094587667\n",
      "    num_agent_steps_sampled: 75000\n",
      "    num_agent_steps_trained: 75000\n",
      "    num_steps_sampled: 75000\n",
      "    num_steps_trained: 75000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.246153846153845\n",
      "    ram_util_percent: 59.84615384615386\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03842125897431415\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.29009898041784\n",
      "    mean_inference_ms: 1.5724341975384606\n",
      "    mean_raw_obs_processing_ms: 1.4362714696002143\n",
      "  time_since_restore: 1804.80775308609\n",
      "  time_this_iter_s: 27.108269214630127\n",
      "  time_total_s: 1804.80775308609\n",
      "  timers:\n",
      "    learn_throughput: 917.261\n",
      "    learn_time_ms: 1090.202\n",
      "    load_throughput: 58289.196\n",
      "    load_time_ms: 17.156\n",
      "    sample_throughput: 37.526\n",
      "    sample_time_ms: 26648.162\n",
      "    update_time_ms: 2.46\n",
      "  timestamp: 1633707603\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75000\n",
      "  training_iteration: 75\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         1804.81</td><td style=\"text-align: right;\">75000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            368.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-40-31\n",
      "  done: false\n",
      "  episode_len_mean: 366.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 197\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0675296836429173\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00985552864715016\n",
      "          policy_loss: 0.011157469492819575\n",
      "          total_loss: -0.004349848400387499\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0007329931049348993\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.8525\n",
      "    ram_util_percent: 59.95\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03842529768532463\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.309624723697024\n",
      "    mean_inference_ms: 1.572524084535817\n",
      "    mean_raw_obs_processing_ms: 1.4382276406281007\n",
      "  time_since_restore: 1832.7514288425446\n",
      "  time_this_iter_s: 27.943675756454468\n",
      "  time_total_s: 1832.7514288425446\n",
      "  timers:\n",
      "    learn_throughput: 914.275\n",
      "    learn_time_ms: 1093.763\n",
      "    load_throughput: 58353.261\n",
      "    load_time_ms: 17.137\n",
      "    sample_throughput: 36.854\n",
      "    sample_time_ms: 27133.916\n",
      "    update_time_ms: 2.465\n",
      "  timestamp: 1633707631\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 76\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         1832.75</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            366.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 77000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-40-57\n",
      "  done: false\n",
      "  episode_len_mean: 364.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 200\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9615869283676148\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010607724636208321\n",
      "          policy_loss: -0.07386157578892177\n",
      "          total_loss: -0.0880402713186211\n",
      "          vf_explained_var: -0.39094078540802\n",
      "          vf_loss: 0.0006636973852033002\n",
      "    num_agent_steps_sampled: 77000\n",
      "    num_agent_steps_trained: 77000\n",
      "    num_steps_sampled: 77000\n",
      "    num_steps_trained: 77000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.06315789473683\n",
      "    ram_util_percent: 60.16578947368423\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038429588071418726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.331085536233232\n",
      "    mean_inference_ms: 1.5726360519584943\n",
      "    mean_raw_obs_processing_ms: 1.4406094900734312\n",
      "  time_since_restore: 1859.4874205589294\n",
      "  time_this_iter_s: 26.735991716384888\n",
      "  time_total_s: 1859.4874205589294\n",
      "  timers:\n",
      "    learn_throughput: 910.783\n",
      "    learn_time_ms: 1097.957\n",
      "    load_throughput: 59256.563\n",
      "    load_time_ms: 16.876\n",
      "    sample_throughput: 36.574\n",
      "    sample_time_ms: 27341.9\n",
      "    update_time_ms: 2.394\n",
      "  timestamp: 1633707657\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 77000\n",
      "  training_iteration: 77\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         1859.49</td><td style=\"text-align: right;\">77000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            364.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 78000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-41-25\n",
      "  done: false\n",
      "  episode_len_mean: 361.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 204\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0268542329470316\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012452718928832516\n",
      "          policy_loss: -0.0543788036538495\n",
      "          total_loss: -0.0682596823407544\n",
      "          vf_explained_var: -0.5218468308448792\n",
      "          vf_loss: 0.0007839406342504339\n",
      "    num_agent_steps_sampled: 78000\n",
      "    num_agent_steps_trained: 78000\n",
      "    num_steps_sampled: 78000\n",
      "    num_steps_trained: 78000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.01282051282052\n",
      "    ram_util_percent: 60.34871794871795\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03843608601903367\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.363336941976524\n",
      "    mean_inference_ms: 1.572827161383073\n",
      "    mean_raw_obs_processing_ms: 1.4448256221796187\n",
      "  time_since_restore: 1886.795598268509\n",
      "  time_this_iter_s: 27.308177709579468\n",
      "  time_total_s: 1886.795598268509\n",
      "  timers:\n",
      "    learn_throughput: 908.645\n",
      "    learn_time_ms: 1100.54\n",
      "    load_throughput: 60788.865\n",
      "    load_time_ms: 16.45\n",
      "    sample_throughput: 36.466\n",
      "    sample_time_ms: 27422.692\n",
      "    update_time_ms: 2.384\n",
      "  timestamp: 1633707685\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 78000\n",
      "  training_iteration: 78\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">          1886.8</td><td style=\"text-align: right;\">78000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            361.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 79000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-41-53\n",
      "  done: false\n",
      "  episode_len_mean: 359.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 207\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.02669517993927\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012865922394928238\n",
      "          policy_loss: -0.06604932463831371\n",
      "          total_loss: -0.08010640541712442\n",
      "          vf_explained_var: -0.9640890955924988\n",
      "          vf_loss: 0.000420206713089202\n",
      "    num_agent_steps_sampled: 79000\n",
      "    num_agent_steps_trained: 79000\n",
      "    num_steps_sampled: 79000\n",
      "    num_steps_trained: 79000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.59024390243903\n",
      "    ram_util_percent: 60.631707317073165\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03844223944882014\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.39122819753082\n",
      "    mean_inference_ms: 1.5730201322035828\n",
      "    mean_raw_obs_processing_ms: 1.4488263215518342\n",
      "  time_since_restore: 1915.3918480873108\n",
      "  time_this_iter_s: 28.59624981880188\n",
      "  time_total_s: 1915.3918480873108\n",
      "  timers:\n",
      "    learn_throughput: 909.443\n",
      "    learn_time_ms: 1099.574\n",
      "    load_throughput: 58873.043\n",
      "    load_time_ms: 16.986\n",
      "    sample_throughput: 36.397\n",
      "    sample_time_ms: 27474.511\n",
      "    update_time_ms: 2.374\n",
      "  timestamp: 1633707713\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79000\n",
      "  training_iteration: 79\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         1915.39</td><td style=\"text-align: right;\">79000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            359.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-42-34\n",
      "  done: false\n",
      "  episode_len_mean: 359.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 210\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1018105996979606\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01181402277181932\n",
      "          policy_loss: -0.042529111955728796\n",
      "          total_loss: -0.057512490451335906\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0007184158671104039\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.83793103448276\n",
      "    ram_util_percent: 60.53275862068965\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03844902957249132\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.418180235495264\n",
      "    mean_inference_ms: 1.5732369696330792\n",
      "    mean_raw_obs_processing_ms: 1.4592483487610537\n",
      "  time_since_restore: 1955.8329741954803\n",
      "  time_this_iter_s: 40.441126108169556\n",
      "  time_total_s: 1955.8329741954803\n",
      "  timers:\n",
      "    learn_throughput: 908.198\n",
      "    learn_time_ms: 1101.081\n",
      "    load_throughput: 57955.59\n",
      "    load_time_ms: 17.255\n",
      "    sample_throughput: 34.788\n",
      "    sample_time_ms: 28745.594\n",
      "    update_time_ms: 2.41\n",
      "  timestamp: 1633707754\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 80\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         1955.83</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            359.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 81000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-43-01\n",
      "  done: false\n",
      "  episode_len_mean: 357.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 213\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9882981671227349\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013809579846798082\n",
      "          policy_loss: -0.02044807822547025\n",
      "          total_loss: -0.03360983516193099\n",
      "          vf_explained_var: -0.9985558390617371\n",
      "          vf_loss: 0.0005069129544103311\n",
      "    num_agent_steps_sampled: 81000\n",
      "    num_agent_steps_trained: 81000\n",
      "    num_steps_sampled: 81000\n",
      "    num_steps_trained: 81000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.63333333333333\n",
      "    ram_util_percent: 59.88717948717951\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038456859658962776\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.447238749232103\n",
      "    mean_inference_ms: 1.5734822836838505\n",
      "    mean_raw_obs_processing_ms: 1.4698665614402517\n",
      "  time_since_restore: 1983.4403734207153\n",
      "  time_this_iter_s: 27.607399225234985\n",
      "  time_total_s: 1983.4403734207153\n",
      "  timers:\n",
      "    learn_throughput: 906.817\n",
      "    learn_time_ms: 1102.758\n",
      "    load_throughput: 60375.151\n",
      "    load_time_ms: 16.563\n",
      "    sample_throughput: 36.656\n",
      "    sample_time_ms: 27281.01\n",
      "    update_time_ms: 2.468\n",
      "  timestamp: 1633707781\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 81000\n",
      "  training_iteration: 81\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         1983.44</td><td style=\"text-align: right;\">81000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            357.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 82000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-43-28\n",
      "  done: false\n",
      "  episode_len_mean: 355.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 216\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.033764640490214\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008090480968220782\n",
      "          policy_loss: -0.028593622893095017\n",
      "          total_loss: -0.04483887387129168\n",
      "          vf_explained_var: -0.8418025374412537\n",
      "          vf_loss: 0.0004516796748955838\n",
      "    num_agent_steps_sampled: 82000\n",
      "    num_agent_steps_trained: 82000\n",
      "    num_steps_sampled: 82000\n",
      "    num_steps_trained: 82000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.82368421052632\n",
      "    ram_util_percent: 59.931578947368436\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03846543040865862\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.47843164540157\n",
      "    mean_inference_ms: 1.573758437020401\n",
      "    mean_raw_obs_processing_ms: 1.4806689136470355\n",
      "  time_since_restore: 2009.7642221450806\n",
      "  time_this_iter_s: 26.323848724365234\n",
      "  time_total_s: 2009.7642221450806\n",
      "  timers:\n",
      "    learn_throughput: 908.686\n",
      "    learn_time_ms: 1100.49\n",
      "    load_throughput: 62194.319\n",
      "    load_time_ms: 16.079\n",
      "    sample_throughput: 36.449\n",
      "    sample_time_ms: 27435.363\n",
      "    update_time_ms: 2.478\n",
      "  timestamp: 1633707808\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 82000\n",
      "  training_iteration: 82\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         2009.76</td><td style=\"text-align: right;\">82000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            355.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 83000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-43-55\n",
      "  done: false\n",
      "  episode_len_mean: 354.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 219\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9859650943014358\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013037975465780624\n",
      "          policy_loss: -0.07730996968845526\n",
      "          total_loss: -0.09073164011869166\n",
      "          vf_explained_var: 0.0054681915789842606\n",
      "          vf_loss: 0.0005708917723192523\n",
      "    num_agent_steps_sampled: 83000\n",
      "    num_agent_steps_trained: 83000\n",
      "    num_steps_sampled: 83000\n",
      "    num_steps_trained: 83000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.65526315789474\n",
      "    ram_util_percent: 59.83157894736842\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03847516095349932\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.511383145376286\n",
      "    mean_inference_ms: 1.574067772740059\n",
      "    mean_raw_obs_processing_ms: 1.4848775684986728\n",
      "  time_since_restore: 2036.7233040332794\n",
      "  time_this_iter_s: 26.959081888198853\n",
      "  time_total_s: 2036.7233040332794\n",
      "  timers:\n",
      "    learn_throughput: 907.509\n",
      "    learn_time_ms: 1101.918\n",
      "    load_throughput: 61084.825\n",
      "    load_time_ms: 16.371\n",
      "    sample_throughput: 36.476\n",
      "    sample_time_ms: 27415.2\n",
      "    update_time_ms: 2.494\n",
      "  timestamp: 1633707835\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 83000\n",
      "  training_iteration: 83\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         2036.72</td><td style=\"text-align: right;\">83000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            354.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-44-20\n",
      "  done: false\n",
      "  episode_len_mean: 353.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 221\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0201540059513516\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01396494270624925\n",
      "          policy_loss: -0.04355205897655752\n",
      "          total_loss: -0.056481967597372\n",
      "          vf_explained_var: -0.5501288771629333\n",
      "          vf_loss: 0.0009874071072166165\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.44722222222222\n",
      "    ram_util_percent: 59.861111111111114\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03848223640535743\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.53402897162831\n",
      "    mean_inference_ms: 1.5742908486555514\n",
      "    mean_raw_obs_processing_ms: 1.4854443733065272\n",
      "  time_since_restore: 2062.136037826538\n",
      "  time_this_iter_s: 25.412733793258667\n",
      "  time_total_s: 2062.136037826538\n",
      "  timers:\n",
      "    learn_throughput: 906.385\n",
      "    learn_time_ms: 1103.284\n",
      "    load_throughput: 62496.241\n",
      "    load_time_ms: 16.001\n",
      "    sample_throughput: 36.605\n",
      "    sample_time_ms: 27318.66\n",
      "    update_time_ms: 2.488\n",
      "  timestamp: 1633707860\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 84\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         2062.14</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            353.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 85000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-44-46\n",
      "  done: false\n",
      "  episode_len_mean: 352.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 224\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9060606784290737\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012026417949332293\n",
      "          policy_loss: -0.04012857228517532\n",
      "          total_loss: -0.052760984417465\n",
      "          vf_explained_var: -0.15935508906841278\n",
      "          vf_loss: 0.001016307576921665\n",
      "    num_agent_steps_sampled: 85000\n",
      "    num_agent_steps_trained: 85000\n",
      "    num_steps_sampled: 85000\n",
      "    num_steps_trained: 85000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.694736842105264\n",
      "    ram_util_percent: 59.797368421052646\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03849393904848281\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.568701723643937\n",
      "    mean_inference_ms: 1.5746535266670474\n",
      "    mean_raw_obs_processing_ms: 1.4867768358022284\n",
      "  time_since_restore: 2088.364321947098\n",
      "  time_this_iter_s: 26.228284120559692\n",
      "  time_total_s: 2088.364321947098\n",
      "  timers:\n",
      "    learn_throughput: 904.035\n",
      "    learn_time_ms: 1106.151\n",
      "    load_throughput: 66379.749\n",
      "    load_time_ms: 15.065\n",
      "    sample_throughput: 36.726\n",
      "    sample_time_ms: 27228.71\n",
      "    update_time_ms: 2.484\n",
      "  timestamp: 1633707886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 85000\n",
      "  training_iteration: 85\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         2088.36</td><td style=\"text-align: right;\">85000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            352.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 86000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-45-11\n",
      "  done: false\n",
      "  episode_len_mean: 351.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 227\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.050778365135193\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01308138441834128\n",
      "          policy_loss: -0.06633173755059639\n",
      "          total_loss: -0.07936335330208143\n",
      "          vf_explained_var: -0.7519276142120361\n",
      "          vf_loss: 0.0015895457944781002\n",
      "    num_agent_steps_sampled: 86000\n",
      "    num_agent_steps_trained: 86000\n",
      "    num_steps_sampled: 86000\n",
      "    num_steps_trained: 86000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.87647058823529\n",
      "    ram_util_percent: 59.832352941176474\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03850640559901203\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.604000088623266\n",
      "    mean_inference_ms: 1.575037625769457\n",
      "    mean_raw_obs_processing_ms: 1.4884268822105478\n",
      "  time_since_restore: 2112.523748397827\n",
      "  time_this_iter_s: 24.15942645072937\n",
      "  time_total_s: 2112.523748397827\n",
      "  timers:\n",
      "    learn_throughput: 901.84\n",
      "    learn_time_ms: 1108.845\n",
      "    load_throughput: 65592.471\n",
      "    load_time_ms: 15.246\n",
      "    sample_throughput: 37.248\n",
      "    sample_time_ms: 26847.382\n",
      "    update_time_ms: 2.511\n",
      "  timestamp: 1633707911\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 86000\n",
      "  training_iteration: 86\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         2112.52</td><td style=\"text-align: right;\">86000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            351.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 87000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-45-36\n",
      "  done: false\n",
      "  episode_len_mean: 350.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 230\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9918261766433716\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01479128576435027\n",
      "          policy_loss: -0.11718869391414855\n",
      "          total_loss: -0.1293157728181945\n",
      "          vf_explained_var: -0.1595516800880432\n",
      "          vf_loss: 0.001135103910928592\n",
      "    num_agent_steps_sampled: 87000\n",
      "    num_agent_steps_trained: 87000\n",
      "    num_steps_sampled: 87000\n",
      "    num_steps_trained: 87000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.19166666666667\n",
      "    ram_util_percent: 60.016666666666666\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03851954366859058\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.639285836385444\n",
      "    mean_inference_ms: 1.5754504549926267\n",
      "    mean_raw_obs_processing_ms: 1.4906173614340719\n",
      "  time_since_restore: 2137.64354801178\n",
      "  time_this_iter_s: 25.119799613952637\n",
      "  time_total_s: 2137.64354801178\n",
      "  timers:\n",
      "    learn_throughput: 900.353\n",
      "    learn_time_ms: 1110.676\n",
      "    load_throughput: 64163.11\n",
      "    load_time_ms: 15.585\n",
      "    sample_throughput: 37.476\n",
      "    sample_time_ms: 26683.599\n",
      "    update_time_ms: 2.516\n",
      "  timestamp: 1633707936\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 87000\n",
      "  training_iteration: 87\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         2137.64</td><td style=\"text-align: right;\">87000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            350.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-46-02\n",
      "  done: false\n",
      "  episode_len_mean: 349.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 233\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4500000000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9652743524975247\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020461162715758215\n",
      "          policy_loss: -0.022813485769761933\n",
      "          total_loss: -0.03223744552168581\n",
      "          vf_explained_var: -0.4247286021709442\n",
      "          vf_loss: 0.0010212618569817602\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.70789473684211\n",
      "    ram_util_percent: 60.17105263157894\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03853324115186709\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.675179694868884\n",
      "    mean_inference_ms: 1.575884055545564\n",
      "    mean_raw_obs_processing_ms: 1.4930671708538596\n",
      "  time_since_restore: 2163.947735786438\n",
      "  time_this_iter_s: 26.304187774658203\n",
      "  time_total_s: 2163.947735786438\n",
      "  timers:\n",
      "    learn_throughput: 899.719\n",
      "    learn_time_ms: 1111.458\n",
      "    load_throughput: 61671.872\n",
      "    load_time_ms: 16.215\n",
      "    sample_throughput: 37.621\n",
      "    sample_time_ms: 26581.24\n",
      "    update_time_ms: 3.003\n",
      "  timestamp: 1633707962\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 88\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         2163.95</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            349.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 89000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-46-26\n",
      "  done: false\n",
      "  episode_len_mean: 348.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 236\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.017232420709398\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01186949327067638\n",
      "          policy_loss: -0.08363336380571126\n",
      "          total_loss: -0.09432551998438107\n",
      "          vf_explained_var: -0.6284500956535339\n",
      "          vf_loss: 0.001468260105725171\n",
      "    num_agent_steps_sampled: 89000\n",
      "    num_agent_steps_trained: 89000\n",
      "    num_steps_sampled: 89000\n",
      "    num_steps_trained: 89000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.90882352941177\n",
      "    ram_util_percent: 60.20882352941176\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03854746696406195\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.71113696248397\n",
      "    mean_inference_ms: 1.576337957760921\n",
      "    mean_raw_obs_processing_ms: 1.4960094858785076\n",
      "  time_since_restore: 2187.757711172104\n",
      "  time_this_iter_s: 23.809975385665894\n",
      "  time_total_s: 2187.757711172104\n",
      "  timers:\n",
      "    learn_throughput: 899.175\n",
      "    learn_time_ms: 1112.13\n",
      "    load_throughput: 63413.05\n",
      "    load_time_ms: 15.77\n",
      "    sample_throughput: 38.311\n",
      "    sample_time_ms: 26102.387\n",
      "    update_time_ms: 2.984\n",
      "  timestamp: 1633707986\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 89000\n",
      "  training_iteration: 89\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         2187.76</td><td style=\"text-align: right;\">89000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            348.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 90000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-46-51\n",
      "  done: false\n",
      "  episode_len_mean: 347.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 238\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.041628642876943\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008345708387975314\n",
      "          policy_loss: -0.06612817908947667\n",
      "          total_loss: -0.07994146180442638\n",
      "          vf_explained_var: -0.7527444362640381\n",
      "          vf_loss: 0.0009696526181263228\n",
      "    num_agent_steps_sampled: 90000\n",
      "    num_agent_steps_trained: 90000\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.05833333333334\n",
      "    ram_util_percent: 60.25\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0385573448916388\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.735425339773272\n",
      "    mean_inference_ms: 1.5766525531679207\n",
      "    mean_raw_obs_processing_ms: 1.4979683182561245\n",
      "  time_since_restore: 2212.8015620708466\n",
      "  time_this_iter_s: 25.043850898742676\n",
      "  time_total_s: 2212.8015620708466\n",
      "  timers:\n",
      "    learn_throughput: 898.082\n",
      "    learn_time_ms: 1113.484\n",
      "    load_throughput: 65399.581\n",
      "    load_time_ms: 15.291\n",
      "    sample_throughput: 40.714\n",
      "    sample_time_ms: 24561.781\n",
      "    update_time_ms: 2.979\n",
      "  timestamp: 1633708011\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 90\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">          2212.8</td><td style=\"text-align: right;\">90000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            347.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 91000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-47-35\n",
      "  done: false\n",
      "  episode_len_mean: 346.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 241\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0229256934589808\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011525887214964373\n",
      "          policy_loss: -0.08429101417875952\n",
      "          total_loss: -0.09567944024586016\n",
      "          vf_explained_var: -0.9713106155395508\n",
      "          vf_loss: 0.0010608567843317157\n",
      "    num_agent_steps_sampled: 91000\n",
      "    num_agent_steps_trained: 91000\n",
      "    num_steps_sampled: 91000\n",
      "    num_steps_trained: 91000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.867741935483885\n",
      "    ram_util_percent: 60.47096774193549\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0385732346358889\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.77283207789208\n",
      "    mean_inference_ms: 1.5771506207476864\n",
      "    mean_raw_obs_processing_ms: 1.507255938348254\n",
      "  time_since_restore: 2256.491817712784\n",
      "  time_this_iter_s: 43.690255641937256\n",
      "  time_total_s: 2256.491817712784\n",
      "  timers:\n",
      "    learn_throughput: 899.813\n",
      "    learn_time_ms: 1111.342\n",
      "    load_throughput: 62375.696\n",
      "    load_time_ms: 16.032\n",
      "    sample_throughput: 38.209\n",
      "    sample_time_ms: 26171.636\n",
      "    update_time_ms: 2.907\n",
      "  timestamp: 1633708055\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 91000\n",
      "  training_iteration: 91\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         2256.49</td><td style=\"text-align: right;\">91000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            346.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-47-58\n",
      "  done: false\n",
      "  episode_len_mean: 344.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 244\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.050669099224938\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009913413536455825\n",
      "          policy_loss: -0.07488674236875441\n",
      "          total_loss: -0.08749773076011075\n",
      "          vf_explained_var: -0.5838000178337097\n",
      "          vf_loss: 0.001204145629890263\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.18787878787878\n",
      "    ram_util_percent: 60.43939393939394\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038589642624788936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.81002774664644\n",
      "    mean_inference_ms: 1.5776687359260841\n",
      "    mean_raw_obs_processing_ms: 1.5166827157168004\n",
      "  time_since_restore: 2279.395083665848\n",
      "  time_this_iter_s: 22.903265953063965\n",
      "  time_total_s: 2279.395083665848\n",
      "  timers:\n",
      "    learn_throughput: 899.413\n",
      "    learn_time_ms: 1111.836\n",
      "    load_throughput: 59689.506\n",
      "    load_time_ms: 16.753\n",
      "    sample_throughput: 38.717\n",
      "    sample_time_ms: 25828.378\n",
      "    update_time_ms: 2.884\n",
      "  timestamp: 1633708078\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 92\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">          2279.4</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            344.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 93000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-48-20\n",
      "  done: false\n",
      "  episode_len_mean: 345.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 246\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0781247284677296\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010999357993609552\n",
      "          policy_loss: -0.12834735607935321\n",
      "          total_loss: -0.13997093683315648\n",
      "          vf_explained_var: -0.8668122887611389\n",
      "          vf_loss: 0.0017331011160018129\n",
      "    num_agent_steps_sampled: 93000\n",
      "    num_agent_steps_trained: 93000\n",
      "    num_steps_sampled: 93000\n",
      "    num_steps_trained: 93000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.132258064516144\n",
      "    ram_util_percent: 60.20645161290321\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038600836855259814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.834405260963905\n",
      "    mean_inference_ms: 1.578024009213128\n",
      "    mean_raw_obs_processing_ms: 1.5231177639692455\n",
      "  time_since_restore: 2301.6971576213837\n",
      "  time_this_iter_s: 22.30207395553589\n",
      "  time_total_s: 2301.6971576213837\n",
      "  timers:\n",
      "    learn_throughput: 899.652\n",
      "    learn_time_ms: 1111.541\n",
      "    load_throughput: 61384.4\n",
      "    load_time_ms: 16.291\n",
      "    sample_throughput: 39.427\n",
      "    sample_time_ms: 25363.383\n",
      "    update_time_ms: 2.953\n",
      "  timestamp: 1633708100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 93000\n",
      "  training_iteration: 93\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">          2301.7</td><td style=\"text-align: right;\">93000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            345.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 94000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-48-41\n",
      "  done: false\n",
      "  episode_len_mean: 344.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 249\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0872020522753396\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00970698391650077\n",
      "          policy_loss: -0.09223619256582526\n",
      "          total_loss: -0.10553994327783585\n",
      "          vf_explained_var: -0.6143307685852051\n",
      "          vf_loss: 0.0010160543244435555\n",
      "    num_agent_steps_sampled: 94000\n",
      "    num_agent_steps_trained: 94000\n",
      "    num_steps_sampled: 94000\n",
      "    num_steps_trained: 94000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.932258064516134\n",
      "    ram_util_percent: 60.06451612903224\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038618054816645334\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.870221280780214\n",
      "    mean_inference_ms: 1.5785723017273952\n",
      "    mean_raw_obs_processing_ms: 1.5299734556883766\n",
      "  time_since_restore: 2323.330420732498\n",
      "  time_this_iter_s: 21.633263111114502\n",
      "  time_total_s: 2323.330420732498\n",
      "  timers:\n",
      "    learn_throughput: 900.045\n",
      "    learn_time_ms: 1111.056\n",
      "    load_throughput: 63776.564\n",
      "    load_time_ms: 15.68\n",
      "    sample_throughput: 40.022\n",
      "    sample_time_ms: 24986.522\n",
      "    update_time_ms: 2.955\n",
      "  timestamp: 1633708121\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 94000\n",
      "  training_iteration: 94\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         2323.33</td><td style=\"text-align: right;\">94000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            344.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 95000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-49-04\n",
      "  done: false\n",
      "  episode_len_mean: 346.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 251\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.094086084100935\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009609946921691944\n",
      "          policy_loss: -0.059412944648000926\n",
      "          total_loss: -0.07224447590609391\n",
      "          vf_explained_var: -0.8031301498413086\n",
      "          vf_loss: 0.0016226121735396898\n",
      "    num_agent_steps_sampled: 95000\n",
      "    num_agent_steps_trained: 95000\n",
      "    num_steps_sampled: 95000\n",
      "    num_steps_trained: 95000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.765625\n",
      "    ram_util_percent: 60.15\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862955820463425\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.893088306366813\n",
      "    mean_inference_ms: 1.578942512005054\n",
      "    mean_raw_obs_processing_ms: 1.5309503886917946\n",
      "  time_since_restore: 2345.715507030487\n",
      "  time_this_iter_s: 22.38508629798889\n",
      "  time_total_s: 2345.715507030487\n",
      "  timers:\n",
      "    learn_throughput: 901.405\n",
      "    learn_time_ms: 1109.38\n",
      "    load_throughput: 61111.882\n",
      "    load_time_ms: 16.363\n",
      "    sample_throughput: 40.646\n",
      "    sample_time_ms: 24602.655\n",
      "    update_time_ms: 3.513\n",
      "  timestamp: 1633708144\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 95000\n",
      "  training_iteration: 95\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         2345.72</td><td style=\"text-align: right;\">95000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            346.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-49-29\n",
      "  done: false\n",
      "  episode_len_mean: 346.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 254\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.020629886786143\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009385025334161265\n",
      "          policy_loss: -0.047706222906708715\n",
      "          total_loss: -0.060200561065640715\n",
      "          vf_explained_var: -0.9624892473220825\n",
      "          vf_loss: 0.001377066948528712\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.2\n",
      "    ram_util_percent: 60.20285714285714\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864656870586868\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.92688005130985\n",
      "    mean_inference_ms: 1.5795016762022143\n",
      "    mean_raw_obs_processing_ms: 1.5325342622947447\n",
      "  time_since_restore: 2370.3237721920013\n",
      "  time_this_iter_s: 24.608265161514282\n",
      "  time_total_s: 2370.3237721920013\n",
      "  timers:\n",
      "    learn_throughput: 898.519\n",
      "    learn_time_ms: 1112.942\n",
      "    load_throughput: 61284.843\n",
      "    load_time_ms: 16.317\n",
      "    sample_throughput: 40.578\n",
      "    sample_time_ms: 24644.047\n",
      "    update_time_ms: 3.489\n",
      "  timestamp: 1633708169\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 96\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         2370.32</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            346.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 97000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-49-53\n",
      "  done: false\n",
      "  episode_len_mean: 347.32\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 257\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9920831282933553\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010149464302460645\n",
      "          policy_loss: -0.025297072571184902\n",
      "          total_loss: -0.03735468251009782\n",
      "          vf_explained_var: -0.8952181339263916\n",
      "          vf_loss: 0.0010123315412783995\n",
      "    num_agent_steps_sampled: 97000\n",
      "    num_agent_steps_trained: 97000\n",
      "    num_steps_sampled: 97000\n",
      "    num_steps_trained: 97000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 48.68055555555556\n",
      "    ram_util_percent: 60.64444444444445\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866356926871875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.960484493023955\n",
      "    mean_inference_ms: 1.5800664440671428\n",
      "    mean_raw_obs_processing_ms: 1.534348465033922\n",
      "  time_since_restore: 2394.8964688777924\n",
      "  time_this_iter_s: 24.572696685791016\n",
      "  time_total_s: 2394.8964688777924\n",
      "  timers:\n",
      "    learn_throughput: 904.47\n",
      "    learn_time_ms: 1105.619\n",
      "    load_throughput: 59732.094\n",
      "    load_time_ms: 16.741\n",
      "    sample_throughput: 40.657\n",
      "    sample_time_ms: 24596.228\n",
      "    update_time_ms: 3.495\n",
      "  timestamp: 1633708193\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 97000\n",
      "  training_iteration: 97\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">          2394.9</td><td style=\"text-align: right;\">97000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            347.32</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 98000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-50-18\n",
      "  done: false\n",
      "  episode_len_mean: 346.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 260\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.129740741517809\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009037086154373913\n",
      "          policy_loss: -0.0695960679401954\n",
      "          total_loss: -0.08417844788895713\n",
      "          vf_explained_var: -0.9462442994117737\n",
      "          vf_loss: 0.0006149933453545802\n",
      "    num_agent_steps_sampled: 98000\n",
      "    num_agent_steps_trained: 98000\n",
      "    num_steps_sampled: 98000\n",
      "    num_steps_trained: 98000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.102857142857154\n",
      "    ram_util_percent: 60.508571428571436\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03868003644502493\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.99435382131297\n",
      "    mean_inference_ms: 1.5806228074469555\n",
      "    mean_raw_obs_processing_ms: 1.536370945176064\n",
      "  time_since_restore: 2420.0371034145355\n",
      "  time_this_iter_s: 25.140634536743164\n",
      "  time_total_s: 2420.0371034145355\n",
      "  timers:\n",
      "    learn_throughput: 908.381\n",
      "    learn_time_ms: 1100.86\n",
      "    load_throughput: 60740.799\n",
      "    load_time_ms: 16.463\n",
      "    sample_throughput: 40.841\n",
      "    sample_time_ms: 24485.462\n",
      "    update_time_ms: 3.004\n",
      "  timestamp: 1633708218\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 98000\n",
      "  training_iteration: 98\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         2420.04</td><td style=\"text-align: right;\">98000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            346.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 99000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-50-44\n",
      "  done: false\n",
      "  episode_len_mean: 345.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 263\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9996025376849704\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00680825070301904\n",
      "          policy_loss: -0.011809198434154193\n",
      "          total_loss: -0.026696950404180422\n",
      "          vf_explained_var: -0.8610448837280273\n",
      "          vf_loss: 0.0005127041894916652\n",
      "    num_agent_steps_sampled: 99000\n",
      "    num_agent_steps_trained: 99000\n",
      "    num_steps_sampled: 99000\n",
      "    num_steps_trained: 99000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.11621621621621\n",
      "    ram_util_percent: 60.37567567567569\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038696080488119386\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.02758634414424\n",
      "    mean_inference_ms: 1.581169410721075\n",
      "    mean_raw_obs_processing_ms: 1.5387927588966799\n",
      "  time_since_restore: 2445.8908417224884\n",
      "  time_this_iter_s: 25.85373830795288\n",
      "  time_total_s: 2445.8908417224884\n",
      "  timers:\n",
      "    learn_throughput: 913.704\n",
      "    learn_time_ms: 1094.447\n",
      "    load_throughput: 59499.582\n",
      "    load_time_ms: 16.807\n",
      "    sample_throughput: 40.493\n",
      "    sample_time_ms: 24695.903\n",
      "    update_time_ms: 3.005\n",
      "  timestamp: 1633708244\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 99000\n",
      "  training_iteration: 99\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         2445.89</td><td style=\"text-align: right;\">99000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            345.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-51-08\n",
      "  done: false\n",
      "  episode_len_mean: 345.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 265\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.078396893872155\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008050139956411724\n",
      "          policy_loss: -0.09165032915771007\n",
      "          total_loss: -0.10635116948849625\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0006492841568413294\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.19428571428572\n",
      "    ram_util_percent: 60.251428571428576\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03870662138144673\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.04946766969139\n",
      "    mean_inference_ms: 1.5815280908810598\n",
      "    mean_raw_obs_processing_ms: 1.5404555706505938\n",
      "  time_since_restore: 2470.199040412903\n",
      "  time_this_iter_s: 24.30819869041443\n",
      "  time_total_s: 2470.199040412903\n",
      "  timers:\n",
      "    learn_throughput: 921.344\n",
      "    learn_time_ms: 1085.371\n",
      "    load_throughput: 57660.162\n",
      "    load_time_ms: 17.343\n",
      "    sample_throughput: 40.599\n",
      "    sample_time_ms: 24630.943\n",
      "    update_time_ms: 2.956\n",
      "  timestamp: 1633708268\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 100\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">          2470.2</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            345.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 101000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-51-32\n",
      "  done: false\n",
      "  episode_len_mean: 344.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 268\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.073288622167375\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009792654079922553\n",
      "          policy_loss: -0.08113576401438978\n",
      "          total_loss: -0.0946448977622721\n",
      "          vf_explained_var: -0.659798264503479\n",
      "          vf_loss: 0.0006137120883472057\n",
      "    num_agent_steps_sampled: 101000\n",
      "    num_agent_steps_trained: 101000\n",
      "    num_steps_sampled: 101000\n",
      "    num_steps_trained: 101000\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.205882352941174\n",
      "    ram_util_percent: 60.211764705882345\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03872229293750173\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.08187069116892\n",
      "    mean_inference_ms: 1.5820558613386964\n",
      "    mean_raw_obs_processing_ms: 1.5430269727723487\n",
      "  time_since_restore: 2493.9322245121\n",
      "  time_this_iter_s: 23.733184099197388\n",
      "  time_total_s: 2493.9322245121\n",
      "  timers:\n",
      "    learn_throughput: 924.802\n",
      "    learn_time_ms: 1081.313\n",
      "    load_throughput: 58238.935\n",
      "    load_time_ms: 17.171\n",
      "    sample_throughput: 44.171\n",
      "    sample_time_ms: 22639.147\n",
      "    update_time_ms: 3.266\n",
      "  timestamp: 1633708292\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 101000\n",
      "  training_iteration: 101\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         2493.93</td><td style=\"text-align: right;\">101000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            344.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 102000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-52-15\n",
      "  done: false\n",
      "  episode_len_mean: 345.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 271\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.075579411453671\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010977888335242388\n",
      "          policy_loss: -0.03702631988045242\n",
      "          total_loss: -0.04934470194081465\n",
      "          vf_explained_var: -0.9972339868545532\n",
      "          vf_loss: 0.0010273358597057975\n",
      "    num_agent_steps_sampled: 102000\n",
      "    num_agent_steps_trained: 102000\n",
      "    num_steps_sampled: 102000\n",
      "    num_steps_trained: 102000\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.64833333333333\n",
      "    ram_util_percent: 59.99500000000001\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038737596735547324\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.11357502419614\n",
      "    mean_inference_ms: 1.5825708709933242\n",
      "    mean_raw_obs_processing_ms: 1.5507274554463013\n",
      "  time_since_restore: 2536.2531859874725\n",
      "  time_this_iter_s: 42.320961475372314\n",
      "  time_total_s: 2536.2531859874725\n",
      "  timers:\n",
      "    learn_throughput: 928.959\n",
      "    learn_time_ms: 1076.474\n",
      "    load_throughput: 59274.065\n",
      "    load_time_ms: 16.871\n",
      "    sample_throughput: 40.673\n",
      "    sample_time_ms: 24586.059\n",
      "    update_time_ms: 3.258\n",
      "  timestamp: 1633708335\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 102000\n",
      "  training_iteration: 102\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         2536.25</td><td style=\"text-align: right;\">102000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            345.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 103000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-52-39\n",
      "  done: false\n",
      "  episode_len_mean: 347.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 274\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0663206418355307\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01276951198249263\n",
      "          policy_loss: -0.051002918328675956\n",
      "          total_loss: -0.06195124698181947\n",
      "          vf_explained_var: -0.6825546622276306\n",
      "          vf_loss: 0.0010954564857658827\n",
      "    num_agent_steps_sampled: 103000\n",
      "    num_agent_steps_trained: 103000\n",
      "    num_steps_sampled: 103000\n",
      "    num_steps_trained: 103000\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.497142857142855\n",
      "    ram_util_percent: 59.917142857142856\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03875207426521071\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.143549288238095\n",
      "    mean_inference_ms: 1.583065233826183\n",
      "    mean_raw_obs_processing_ms: 1.5585411131043299\n",
      "  time_since_restore: 2560.686374425888\n",
      "  time_this_iter_s: 24.433188438415527\n",
      "  time_total_s: 2560.686374425888\n",
      "  timers:\n",
      "    learn_throughput: 935.381\n",
      "    learn_time_ms: 1069.083\n",
      "    load_throughput: 58094.784\n",
      "    load_time_ms: 17.213\n",
      "    sample_throughput: 40.312\n",
      "    sample_time_ms: 24806.332\n",
      "    update_time_ms: 3.163\n",
      "  timestamp: 1633708359\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 103000\n",
      "  training_iteration: 103\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         2560.69</td><td style=\"text-align: right;\">103000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            347.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-53-04\n",
      "  done: false\n",
      "  episode_len_mean: 347.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 277\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0119917816585966\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013171500203250246\n",
      "          policy_loss: -0.10276408221365677\n",
      "          total_loss: -0.11231639278638694\n",
      "          vf_explained_var: -0.6054362058639526\n",
      "          vf_loss: 0.0016768408654671575\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.955555555555556\n",
      "    ram_util_percent: 59.87777777777779\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03876536480246925\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.17153918197892\n",
      "    mean_inference_ms: 1.583522557140976\n",
      "    mean_raw_obs_processing_ms: 1.5664521484827538\n",
      "  time_since_restore: 2585.359036922455\n",
      "  time_this_iter_s: 24.672662496566772\n",
      "  time_total_s: 2585.359036922455\n",
      "  timers:\n",
      "    learn_throughput: 940.312\n",
      "    learn_time_ms: 1063.477\n",
      "    load_throughput: 55976.896\n",
      "    load_time_ms: 17.865\n",
      "    sample_throughput: 39.816\n",
      "    sample_time_ms: 25115.265\n",
      "    update_time_ms: 3.139\n",
      "  timestamp: 1633708384\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 104\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         2585.36</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            347.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 105000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-53-30\n",
      "  done: false\n",
      "  episode_len_mean: 349.25\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 280\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.012426393561893\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008313928166831167\n",
      "          policy_loss: -0.06250982876453134\n",
      "          total_loss: -0.07629839554429055\n",
      "          vf_explained_var: -0.6415514945983887\n",
      "          vf_loss: 0.0007237941841594875\n",
      "    num_agent_steps_sampled: 105000\n",
      "    num_agent_steps_trained: 105000\n",
      "    num_steps_sampled: 105000\n",
      "    num_steps_trained: 105000\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.13243243243244\n",
      "    ram_util_percent: 59.86486486486488\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03877784418431706\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.19852459010289\n",
      "    mean_inference_ms: 1.5839519260409327\n",
      "    mean_raw_obs_processing_ms: 1.5719469237677333\n",
      "  time_since_restore: 2611.324012517929\n",
      "  time_this_iter_s: 25.964975595474243\n",
      "  time_total_s: 2611.324012517929\n",
      "  timers:\n",
      "    learn_throughput: 944.358\n",
      "    learn_time_ms: 1058.921\n",
      "    load_throughput: 57180.188\n",
      "    load_time_ms: 17.489\n",
      "    sample_throughput: 39.248\n",
      "    sample_time_ms: 25478.747\n",
      "    update_time_ms: 2.583\n",
      "  timestamp: 1633708410\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 105000\n",
      "  training_iteration: 105\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         2611.32</td><td style=\"text-align: right;\">105000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            349.25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 106000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-53-56\n",
      "  done: false\n",
      "  episode_len_mean: 348.51\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 283\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.008609566423628\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014346447413302361\n",
      "          policy_loss: -0.04667208832171228\n",
      "          total_loss: -0.056092965851227444\n",
      "          vf_explained_var: -0.22296996414661407\n",
      "          vf_loss: 0.0009813672267935342\n",
      "    num_agent_steps_sampled: 106000\n",
      "    num_agent_steps_trained: 106000\n",
      "    num_steps_sampled: 106000\n",
      "    num_steps_trained: 106000\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.25945945945946\n",
      "    ram_util_percent: 59.84594594594595\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03878942218096221\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.22552290718137\n",
      "    mean_inference_ms: 1.584353066556662\n",
      "    mean_raw_obs_processing_ms: 1.572537381891346\n",
      "  time_since_restore: 2637.6210010051727\n",
      "  time_this_iter_s: 26.296988487243652\n",
      "  time_total_s: 2637.6210010051727\n",
      "  timers:\n",
      "    learn_throughput: 953.825\n",
      "    learn_time_ms: 1048.41\n",
      "    load_throughput: 57584.562\n",
      "    load_time_ms: 17.366\n",
      "    sample_throughput: 38.974\n",
      "    sample_time_ms: 25658.273\n",
      "    update_time_ms: 2.58\n",
      "  timestamp: 1633708436\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 106000\n",
      "  training_iteration: 106\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         2637.62</td><td style=\"text-align: right;\">106000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            348.51</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 107000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-54-19\n",
      "  done: false\n",
      "  episode_len_mean: 349.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 285\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.972996007071601\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0101312307871711\n",
      "          policy_loss: -0.06151305432948801\n",
      "          total_loss: -0.07281744041376643\n",
      "          vf_explained_var: -0.5964726209640503\n",
      "          vf_loss: 0.0015869922140457978\n",
      "    num_agent_steps_sampled: 107000\n",
      "    num_agent_steps_trained: 107000\n",
      "    num_steps_sampled: 107000\n",
      "    num_steps_trained: 107000\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.9\n",
      "    ram_util_percent: 59.98235294117647\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03879673401384129\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.243127774242424\n",
      "    mean_inference_ms: 1.584606765592519\n",
      "    mean_raw_obs_processing_ms: 1.5729207011282624\n",
      "  time_since_restore: 2661.047387123108\n",
      "  time_this_iter_s: 23.42638611793518\n",
      "  time_total_s: 2661.047387123108\n",
      "  timers:\n",
      "    learn_throughput: 952.905\n",
      "    learn_time_ms: 1049.422\n",
      "    load_throughput: 59500.172\n",
      "    load_time_ms: 16.807\n",
      "    sample_throughput: 39.149\n",
      "    sample_time_ms: 25543.227\n",
      "    update_time_ms: 2.55\n",
      "  timestamp: 1633708459\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 107000\n",
      "  training_iteration: 107\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         2661.05</td><td style=\"text-align: right;\">107000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            349.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-54-44\n",
      "  done: false\n",
      "  episode_len_mean: 349.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 288\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0376879215240478\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007498500616671164\n",
      "          policy_loss: -0.07745809165967835\n",
      "          total_loss: -0.0921314523037937\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0006420291300552587\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.08285714285714\n",
      "    ram_util_percent: 60.06285714285714\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03880688625257638\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.26811411308733\n",
      "    mean_inference_ms: 1.5849578098104544\n",
      "    mean_raw_obs_processing_ms: 1.5736773214852238\n",
      "  time_since_restore: 2685.9608359336853\n",
      "  time_this_iter_s: 24.913448810577393\n",
      "  time_total_s: 2685.9608359336853\n",
      "  timers:\n",
      "    learn_throughput: 955.516\n",
      "    learn_time_ms: 1046.555\n",
      "    load_throughput: 59373.327\n",
      "    load_time_ms: 16.843\n",
      "    sample_throughput: 39.18\n",
      "    sample_time_ms: 25523.329\n",
      "    update_time_ms: 2.559\n",
      "  timestamp: 1633708484\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 108\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         2685.96</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             349.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 109000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-55-08\n",
      "  done: false\n",
      "  episode_len_mean: 351.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 291\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0178037193086413\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01210818581436405\n",
      "          policy_loss: -0.0841218434067236\n",
      "          total_loss: -0.09416904925472207\n",
      "          vf_explained_var: -0.9831727743148804\n",
      "          vf_loss: 0.0019578077118947274\n",
      "    num_agent_steps_sampled: 109000\n",
      "    num_agent_steps_trained: 109000\n",
      "    num_steps_sampled: 109000\n",
      "    num_steps_trained: 109000\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.029411764705884\n",
      "    ram_util_percent: 60.17352941176471\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03881621135014457\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.29175434504994\n",
      "    mean_inference_ms: 1.5852826185921416\n",
      "    mean_raw_obs_processing_ms: 1.5746089996277388\n",
      "  time_since_restore: 2709.9174721240997\n",
      "  time_this_iter_s: 23.95663619041443\n",
      "  time_total_s: 2709.9174721240997\n",
      "  timers:\n",
      "    learn_throughput: 954.014\n",
      "    learn_time_ms: 1048.202\n",
      "    load_throughput: 59909.043\n",
      "    load_time_ms: 16.692\n",
      "    sample_throughput: 39.476\n",
      "    sample_time_ms: 25332.12\n",
      "    update_time_ms: 2.553\n",
      "  timestamp: 1633708508\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 109000\n",
      "  training_iteration: 109\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">         2709.92</td><td style=\"text-align: right;\">109000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            351.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 110000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-55-35\n",
      "  done: false\n",
      "  episode_len_mean: 352.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 294\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8939390738805135\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008450745476634955\n",
      "          policy_loss: -0.05876888549990124\n",
      "          total_loss: -0.07121734027233388\n",
      "          vf_explained_var: -0.8683445453643799\n",
      "          vf_loss: 0.0007866824880087127\n",
      "    num_agent_steps_sampled: 110000\n",
      "    num_agent_steps_trained: 110000\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.263157894736835\n",
      "    ram_util_percent: 60.28684210526318\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03882473129010351\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.31447476680319\n",
      "    mean_inference_ms: 1.5855808807928715\n",
      "    mean_raw_obs_processing_ms: 1.5757052860533427\n",
      "  time_since_restore: 2736.3652350902557\n",
      "  time_this_iter_s: 26.447762966156006\n",
      "  time_total_s: 2736.3652350902557\n",
      "  timers:\n",
      "    learn_throughput: 952.515\n",
      "    learn_time_ms: 1049.852\n",
      "    load_throughput: 59378.118\n",
      "    load_time_ms: 16.841\n",
      "    sample_throughput: 39.148\n",
      "    sample_time_ms: 25544.251\n",
      "    update_time_ms: 2.573\n",
      "  timestamp: 1633708535\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 110\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         2736.37</td><td style=\"text-align: right;\">110000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            352.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 111000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-55-59\n",
      "  done: false\n",
      "  episode_len_mean: 353.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 296\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0010560340351526\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009784181589822591\n",
      "          policy_loss: -0.07893535981679128\n",
      "          total_loss: -0.09078714970706238\n",
      "          vf_explained_var: -0.6935277581214905\n",
      "          vf_loss: 0.0015544474913945629\n",
      "    num_agent_steps_sampled: 111000\n",
      "    num_agent_steps_trained: 111000\n",
      "    num_steps_sampled: 111000\n",
      "    num_steps_trained: 111000\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.11142857142857\n",
      "    ram_util_percent: 60.245714285714286\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03882992638980053\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.328461257755453\n",
      "    mean_inference_ms: 1.5857642930862494\n",
      "    mean_raw_obs_processing_ms: 1.5765371292366166\n",
      "  time_since_restore: 2760.8721356391907\n",
      "  time_this_iter_s: 24.506900548934937\n",
      "  time_total_s: 2760.8721356391907\n",
      "  timers:\n",
      "    learn_throughput: 953.534\n",
      "    learn_time_ms: 1048.731\n",
      "    load_throughput: 58905.62\n",
      "    load_time_ms: 16.976\n",
      "    sample_throughput: 39.028\n",
      "    sample_time_ms: 25622.913\n",
      "    update_time_ms: 2.265\n",
      "  timestamp: 1633708559\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 111000\n",
      "  training_iteration: 111\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         2760.87</td><td style=\"text-align: right;\">111000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            353.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-56-22\n",
      "  done: false\n",
      "  episode_len_mean: 354.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 299\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.058118516869015\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011498614316449959\n",
      "          policy_loss: -0.06587399099436071\n",
      "          total_loss: -0.07722117317219575\n",
      "          vf_explained_var: -0.999880313873291\n",
      "          vf_loss: 0.001472439045836735\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.99696969696969\n",
      "    ram_util_percent: 60.21515151515151\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038837315562861685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.348249333356296\n",
      "    mean_inference_ms: 1.5860221984068639\n",
      "    mean_raw_obs_processing_ms: 1.5777675058742295\n",
      "  time_since_restore: 2783.6645002365112\n",
      "  time_this_iter_s: 22.792364597320557\n",
      "  time_total_s: 2783.6645002365112\n",
      "  timers:\n",
      "    learn_throughput: 953.338\n",
      "    learn_time_ms: 1048.946\n",
      "    load_throughput: 57843.693\n",
      "    load_time_ms: 17.288\n",
      "    sample_throughput: 42.248\n",
      "    sample_time_ms: 23669.512\n",
      "    update_time_ms: 2.278\n",
      "  timestamp: 1633708582\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 112\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         2783.66</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            354.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 113000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-57-03\n",
      "  done: false\n",
      "  episode_len_mean: 355.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 302\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.049188959598541\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013460702035278634\n",
      "          policy_loss: -0.07279273335718446\n",
      "          total_loss: -0.08247647987057766\n",
      "          vf_explained_var: -0.5438706278800964\n",
      "          vf_loss: 0.0017221707061657475\n",
      "    num_agent_steps_sampled: 113000\n",
      "    num_agent_steps_trained: 113000\n",
      "    num_steps_sampled: 113000\n",
      "    num_steps_trained: 113000\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.45084745762712\n",
      "    ram_util_percent: 59.828813559322036\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038844136569677126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.366648492313264\n",
      "    mean_inference_ms: 1.5862583625264788\n",
      "    mean_raw_obs_processing_ms: 1.5836336849444128\n",
      "  time_since_restore: 2824.944699048996\n",
      "  time_this_iter_s: 41.28019881248474\n",
      "  time_total_s: 2824.944699048996\n",
      "  timers:\n",
      "    learn_throughput: 952.339\n",
      "    learn_time_ms: 1050.047\n",
      "    load_throughput: 57524.539\n",
      "    load_time_ms: 17.384\n",
      "    sample_throughput: 39.443\n",
      "    sample_time_ms: 25352.998\n",
      "    update_time_ms: 2.292\n",
      "  timestamp: 1633708623\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 113000\n",
      "  training_iteration: 113\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         2824.94</td><td style=\"text-align: right;\">113000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            355.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 114000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-57-26\n",
      "  done: false\n",
      "  episode_len_mean: 356.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 304\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9892760939068264\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008535076372776314\n",
      "          policy_loss: -0.03085453738975856\n",
      "          total_loss: -0.04399068702219261\n",
      "          vf_explained_var: -0.6341675519943237\n",
      "          vf_loss: 0.0009954362774604103\n",
      "    num_agent_steps_sampled: 114000\n",
      "    num_agent_steps_trained: 114000\n",
      "    num_steps_sampled: 114000\n",
      "    num_steps_trained: 114000\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.359375\n",
      "    ram_util_percent: 59.662499999999994\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03884846661236975\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.37840485905632\n",
      "    mean_inference_ms: 1.5864074535148205\n",
      "    mean_raw_obs_processing_ms: 1.587385966674072\n",
      "  time_since_restore: 2847.5914256572723\n",
      "  time_this_iter_s: 22.646726608276367\n",
      "  time_total_s: 2847.5914256572723\n",
      "  timers:\n",
      "    learn_throughput: 952.331\n",
      "    learn_time_ms: 1050.055\n",
      "    load_throughput: 57308.395\n",
      "    load_time_ms: 17.449\n",
      "    sample_throughput: 39.761\n",
      "    sample_time_ms: 25150.238\n",
      "    update_time_ms: 2.375\n",
      "  timestamp: 1633708646\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 114000\n",
      "  training_iteration: 114\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         2847.59</td><td style=\"text-align: right;\">114000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            356.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 115000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-57-50\n",
      "  done: false\n",
      "  episode_len_mean: 358.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 307\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9168065494961208\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011181660895349112\n",
      "          policy_loss: -0.11578376044829687\n",
      "          total_loss: -0.12648277257879575\n",
      "          vf_explained_var: -0.9923496842384338\n",
      "          vf_loss: 0.0009214311378956255\n",
      "    num_agent_steps_sampled: 115000\n",
      "    num_agent_steps_trained: 115000\n",
      "    num_steps_sampled: 115000\n",
      "    num_steps_trained: 115000\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.01764705882353\n",
      "    ram_util_percent: 59.61764705882352\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038854036816268436\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.394089588910816\n",
      "    mean_inference_ms: 1.5866022012247885\n",
      "    mean_raw_obs_processing_ms: 1.593116669592738\n",
      "  time_since_restore: 2871.685876607895\n",
      "  time_this_iter_s: 24.09445095062256\n",
      "  time_total_s: 2871.685876607895\n",
      "  timers:\n",
      "    learn_throughput: 954.398\n",
      "    learn_time_ms: 1047.78\n",
      "    load_throughput: 56130.99\n",
      "    load_time_ms: 17.815\n",
      "    sample_throughput: 40.056\n",
      "    sample_time_ms: 24965.083\n",
      "    update_time_ms: 2.375\n",
      "  timestamp: 1633708670\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 115000\n",
      "  training_iteration: 115\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         2871.69</td><td style=\"text-align: right;\">115000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             358.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-58-13\n",
      "  done: false\n",
      "  episode_len_mean: 359.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 310\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9351397236188252\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010919393980921496\n",
      "          policy_loss: -0.06476453385419316\n",
      "          total_loss: -0.07564275846299198\n",
      "          vf_explained_var: -0.8159981369972229\n",
      "          vf_loss: 0.0011025828629499302\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.009090909090915\n",
      "    ram_util_percent: 59.54848484848485\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0388589425026455\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.40929494302939\n",
      "    mean_inference_ms: 1.586773324660819\n",
      "    mean_raw_obs_processing_ms: 1.5928287174193614\n",
      "  time_since_restore: 2894.829529762268\n",
      "  time_this_iter_s: 23.14365315437317\n",
      "  time_total_s: 2894.829529762268\n",
      "  timers:\n",
      "    learn_throughput: 953.754\n",
      "    learn_time_ms: 1048.488\n",
      "    load_throughput: 56075.757\n",
      "    load_time_ms: 17.833\n",
      "    sample_throughput: 40.57\n",
      "    sample_time_ms: 24649.017\n",
      "    update_time_ms: 2.374\n",
      "  timestamp: 1633708693\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 116\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         2894.83</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            359.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 117000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-58-38\n",
      "  done: false\n",
      "  episode_len_mean: 360.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 313\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9613182968563503\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011075575705391492\n",
      "          policy_loss: -0.0016202765206495922\n",
      "          total_loss: -0.012058978734744919\n",
      "          vf_explained_var: -0.879878580570221\n",
      "          vf_loss: 0.0016984680855077587\n",
      "    num_agent_steps_sampled: 117000\n",
      "    num_agent_steps_trained: 117000\n",
      "    num_steps_sampled: 117000\n",
      "    num_steps_trained: 117000\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.1\n",
      "    ram_util_percent: 59.57222222222222\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03886296014172001\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.42311721543289\n",
      "    mean_inference_ms: 1.5869194719466384\n",
      "    mean_raw_obs_processing_ms: 1.592707055987252\n",
      "  time_since_restore: 2919.3998486995697\n",
      "  time_this_iter_s: 24.570318937301636\n",
      "  time_total_s: 2919.3998486995697\n",
      "  timers:\n",
      "    learn_throughput: 955.198\n",
      "    learn_time_ms: 1046.903\n",
      "    load_throughput: 55571.729\n",
      "    load_time_ms: 17.995\n",
      "    sample_throughput: 40.38\n",
      "    sample_time_ms: 24764.822\n",
      "    update_time_ms: 2.375\n",
      "  timestamp: 1633708718\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 117000\n",
      "  training_iteration: 117\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">          2919.4</td><td style=\"text-align: right;\">117000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            360.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 118000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-59-02\n",
      "  done: false\n",
      "  episode_len_mean: 360.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 315\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6830743551254272\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008989625375978161\n",
      "          policy_loss: -0.047179419671495755\n",
      "          total_loss: -0.057039399517493115\n",
      "          vf_explained_var: -0.9175442457199097\n",
      "          vf_loss: 0.0009027670525635282\n",
      "    num_agent_steps_sampled: 118000\n",
      "    num_agent_steps_trained: 118000\n",
      "    num_steps_sampled: 118000\n",
      "    num_steps_trained: 118000\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.061764705882354\n",
      "    ram_util_percent: 59.70294117647058\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03886532214421602\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.431695489312634\n",
      "    mean_inference_ms: 1.587000948957648\n",
      "    mean_raw_obs_processing_ms: 1.5927329336132958\n",
      "  time_since_restore: 2943.7214500904083\n",
      "  time_this_iter_s: 24.321601390838623\n",
      "  time_total_s: 2943.7214500904083\n",
      "  timers:\n",
      "    learn_throughput: 952.868\n",
      "    learn_time_ms: 1049.463\n",
      "    load_throughput: 55083.552\n",
      "    load_time_ms: 18.154\n",
      "    sample_throughput: 40.481\n",
      "    sample_time_ms: 24702.926\n",
      "    update_time_ms: 2.367\n",
      "  timestamp: 1633708742\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 118000\n",
      "  training_iteration: 118\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">         2943.72</td><td style=\"text-align: right;\">118000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            360.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 119000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-59-26\n",
      "  done: false\n",
      "  episode_len_mean: 361.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 318\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8000671638382806\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010833274107888811\n",
      "          policy_loss: -0.09799516436954339\n",
      "          total_loss: -0.10767935013605488\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0010040273243147465\n",
      "    num_agent_steps_sampled: 119000\n",
      "    num_agent_steps_trained: 119000\n",
      "    num_steps_sampled: 119000\n",
      "    num_steps_trained: 119000\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.16764705882353\n",
      "    ram_util_percent: 59.86176470588235\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03886809018678502\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.443754348824413\n",
      "    mean_inference_ms: 1.5871015684437402\n",
      "    mean_raw_obs_processing_ms: 1.5927558826156272\n",
      "  time_since_restore: 2967.5704686641693\n",
      "  time_this_iter_s: 23.849018573760986\n",
      "  time_total_s: 2967.5704686641693\n",
      "  timers:\n",
      "    learn_throughput: 954.72\n",
      "    learn_time_ms: 1047.428\n",
      "    load_throughput: 53949.918\n",
      "    load_time_ms: 18.536\n",
      "    sample_throughput: 40.496\n",
      "    sample_time_ms: 24693.832\n",
      "    update_time_ms: 2.363\n",
      "  timestamp: 1633708766\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 119000\n",
      "  training_iteration: 119\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         2967.57</td><td style=\"text-align: right;\">119000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            361.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_15-59-52\n",
      "  done: false\n",
      "  episode_len_mean: 361.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 321\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6981039841969807\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010117847118348363\n",
      "          policy_loss: -0.05913523469741146\n",
      "          total_loss: -0.06827213043967882\n",
      "          vf_explained_var: -0.2223641574382782\n",
      "          vf_loss: 0.0010145969433425408\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.13513513513514\n",
      "    ram_util_percent: 60.018918918918914\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03887003709939045\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.455335643706793\n",
      "    mean_inference_ms: 1.5871772699184785\n",
      "    mean_raw_obs_processing_ms: 1.5929430119275185\n",
      "  time_since_restore: 2993.074220895767\n",
      "  time_this_iter_s: 25.5037522315979\n",
      "  time_total_s: 2993.074220895767\n",
      "  timers:\n",
      "    learn_throughput: 955.174\n",
      "    learn_time_ms: 1046.93\n",
      "    load_throughput: 53300.056\n",
      "    load_time_ms: 18.762\n",
      "    sample_throughput: 40.651\n",
      "    sample_time_ms: 24599.731\n",
      "    update_time_ms: 2.337\n",
      "  timestamp: 1633708792\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 120\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">         2993.07</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            361.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 121000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-00-17\n",
      "  done: false\n",
      "  episode_len_mean: 362.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 324\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8635096033414205\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012784977804102813\n",
      "          policy_loss: -0.014217534102499485\n",
      "          total_loss: -0.023072230960759853\n",
      "          vf_explained_var: -0.999064564704895\n",
      "          vf_loss: 0.001150537516999369\n",
      "    num_agent_steps_sampled: 121000\n",
      "    num_agent_steps_trained: 121000\n",
      "    num_steps_sampled: 121000\n",
      "    num_steps_trained: 121000\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.08\n",
      "    ram_util_percent: 60.151428571428575\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03887101168101922\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.465962082329984\n",
      "    mean_inference_ms: 1.587222149590908\n",
      "    mean_raw_obs_processing_ms: 1.5934438293833295\n",
      "  time_since_restore: 3017.9795422554016\n",
      "  time_this_iter_s: 24.9053213596344\n",
      "  time_total_s: 3017.9795422554016\n",
      "  timers:\n",
      "    learn_throughput: 956.057\n",
      "    learn_time_ms: 1045.963\n",
      "    load_throughput: 54904.513\n",
      "    load_time_ms: 18.213\n",
      "    sample_throughput: 40.583\n",
      "    sample_time_ms: 24641.075\n",
      "    update_time_ms: 2.332\n",
      "  timestamp: 1633708817\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 121000\n",
      "  training_iteration: 121\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         3017.98</td><td style=\"text-align: right;\">121000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            362.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 122000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-00-42\n",
      "  done: false\n",
      "  episode_len_mean: 360.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 327\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8487186551094055\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010831478209350365\n",
      "          policy_loss: -0.04441004635559188\n",
      "          total_loss: -0.05502970880932278\n",
      "          vf_explained_var: -0.9581928849220276\n",
      "          vf_loss: 0.000556275351846125\n",
      "    num_agent_steps_sampled: 122000\n",
      "    num_agent_steps_trained: 122000\n",
      "    num_steps_sampled: 122000\n",
      "    num_steps_trained: 122000\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.01891891891892\n",
      "    ram_util_percent: 60.248648648648654\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03887141395014703\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.476774986771446\n",
      "    mean_inference_ms: 1.5872466997630545\n",
      "    mean_raw_obs_processing_ms: 1.594080558333077\n",
      "  time_since_restore: 3043.4596145153046\n",
      "  time_this_iter_s: 25.480072259902954\n",
      "  time_total_s: 3043.4596145153046\n",
      "  timers:\n",
      "    learn_throughput: 958.133\n",
      "    learn_time_ms: 1043.697\n",
      "    load_throughput: 54894.237\n",
      "    load_time_ms: 18.217\n",
      "    sample_throughput: 40.141\n",
      "    sample_time_ms: 24912.128\n",
      "    update_time_ms: 2.316\n",
      "  timestamp: 1633708842\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 122000\n",
      "  training_iteration: 122\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         3043.46</td><td style=\"text-align: right;\">122000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            360.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 123000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-01-24\n",
      "  done: false\n",
      "  episode_len_mean: 360.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 330\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8757915205425686\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010500778927692838\n",
      "          policy_loss: -0.06633069614569347\n",
      "          total_loss: -0.07725923409064611\n",
      "          vf_explained_var: -0.4676779806613922\n",
      "          vf_loss: 0.0007413488651056671\n",
      "    num_agent_steps_sampled: 123000\n",
      "    num_agent_steps_trained: 123000\n",
      "    num_steps_sampled: 123000\n",
      "    num_steps_trained: 123000\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.73220338983052\n",
      "    ram_util_percent: 60.16101694915254\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03887133673593877\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.487171980755566\n",
      "    mean_inference_ms: 1.5872456382534015\n",
      "    mean_raw_obs_processing_ms: 1.5990493846782081\n",
      "  time_since_restore: 3085.050807952881\n",
      "  time_this_iter_s: 41.591193437576294\n",
      "  time_total_s: 3085.050807952881\n",
      "  timers:\n",
      "    learn_throughput: 959.16\n",
      "    learn_time_ms: 1042.579\n",
      "    load_throughput: 54996.374\n",
      "    load_time_ms: 18.183\n",
      "    sample_throughput: 40.089\n",
      "    sample_time_ms: 24944.375\n",
      "    update_time_ms: 2.31\n",
      "  timestamp: 1633708884\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 123000\n",
      "  training_iteration: 123\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         3085.05</td><td style=\"text-align: right;\">123000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            360.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-01-48\n",
      "  done: false\n",
      "  episode_len_mean: 360.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 333\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.774088900619083\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012740356955934907\n",
      "          policy_loss: -0.04226596237470706\n",
      "          total_loss: -0.050638385075661874\n",
      "          vf_explained_var: -0.834460437297821\n",
      "          vf_loss: 0.0007687258886613159\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.79705882352942\n",
      "    ram_util_percent: 60.13823529411765\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03887071215881389\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.49663151341602\n",
      "    mean_inference_ms: 1.5872250675069315\n",
      "    mean_raw_obs_processing_ms: 1.604114037928716\n",
      "  time_since_restore: 3108.920311689377\n",
      "  time_this_iter_s: 23.86950373649597\n",
      "  time_total_s: 3108.920311689377\n",
      "  timers:\n",
      "    learn_throughput: 960.073\n",
      "    learn_time_ms: 1041.587\n",
      "    load_throughput: 54996.014\n",
      "    load_time_ms: 18.183\n",
      "    sample_throughput: 39.892\n",
      "    sample_time_ms: 25067.724\n",
      "    update_time_ms: 2.229\n",
      "  timestamp: 1633708908\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 124\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         3108.92</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            360.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 125000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-02-10\n",
      "  done: false\n",
      "  episode_len_mean: 361.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 335\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9699006610446506\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011327438008526454\n",
      "          policy_loss: -0.06469607231103712\n",
      "          total_loss: -0.07517063099270066\n",
      "          vf_explained_var: -0.5465576648712158\n",
      "          vf_loss: 0.0015784270556954045\n",
      "    num_agent_steps_sampled: 125000\n",
      "    num_agent_steps_trained: 125000\n",
      "    num_steps_sampled: 125000\n",
      "    num_steps_trained: 125000\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.04375\n",
      "    ram_util_percent: 60.0125\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03886992404382641\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.50260933051995\n",
      "    mean_inference_ms: 1.5871979454324798\n",
      "    mean_raw_obs_processing_ms: 1.6075456583236882\n",
      "  time_since_restore: 3131.003669500351\n",
      "  time_this_iter_s: 22.08335781097412\n",
      "  time_total_s: 3131.003669500351\n",
      "  timers:\n",
      "    learn_throughput: 960.632\n",
      "    learn_time_ms: 1040.981\n",
      "    load_throughput: 55044.515\n",
      "    load_time_ms: 18.167\n",
      "    sample_throughput: 40.214\n",
      "    sample_time_ms: 24867.24\n",
      "    update_time_ms: 2.232\n",
      "  timestamp: 1633708930\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 125000\n",
      "  training_iteration: 125\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">            3131</td><td style=\"text-align: right;\">125000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            361.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 126000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-02-35\n",
      "  done: false\n",
      "  episode_len_mean: 360.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 338\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9214632127020095\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00916837326338883\n",
      "          policy_loss: -0.06431519018693102\n",
      "          total_loss: -0.07680365633633401\n",
      "          vf_explained_var: -0.6095467209815979\n",
      "          vf_loss: 0.0005375132912174902\n",
      "    num_agent_steps_sampled: 126000\n",
      "    num_agent_steps_trained: 126000\n",
      "    num_steps_sampled: 126000\n",
      "    num_steps_trained: 126000\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.11944444444444\n",
      "    ram_util_percent: 59.75555555555555\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03886843026917268\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.511569415315744\n",
      "    mean_inference_ms: 1.587142250217771\n",
      "    mean_raw_obs_processing_ms: 1.6126388324329115\n",
      "  time_since_restore: 3156.0785393714905\n",
      "  time_this_iter_s: 25.074869871139526\n",
      "  time_total_s: 3156.0785393714905\n",
      "  timers:\n",
      "    learn_throughput: 962.559\n",
      "    learn_time_ms: 1038.898\n",
      "    load_throughput: 55052.101\n",
      "    load_time_ms: 18.165\n",
      "    sample_throughput: 39.9\n",
      "    sample_time_ms: 25062.462\n",
      "    update_time_ms: 2.214\n",
      "  timestamp: 1633708955\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 126000\n",
      "  training_iteration: 126\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         3156.08</td><td style=\"text-align: right;\">126000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            360.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 127000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-02-57\n",
      "  done: false\n",
      "  episode_len_mean: 361.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 341\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9666611207856073\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010766756651733782\n",
      "          policy_loss: -0.10381818239887555\n",
      "          total_loss: -0.11524161468777391\n",
      "          vf_explained_var: -0.7461637854576111\n",
      "          vf_loss: 0.0009756154037960287\n",
      "    num_agent_steps_sampled: 127000\n",
      "    num_agent_steps_trained: 127000\n",
      "    num_steps_sampled: 127000\n",
      "    num_steps_trained: 127000\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.18387096774193\n",
      "    ram_util_percent: 59.70000000000002\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0388656816325961\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.519248205673804\n",
      "    mean_inference_ms: 1.5870550932687837\n",
      "    mean_raw_obs_processing_ms: 1.6120554771829427\n",
      "  time_since_restore: 3178.3151128292084\n",
      "  time_this_iter_s: 22.236573457717896\n",
      "  time_total_s: 3178.3151128292084\n",
      "  timers:\n",
      "    learn_throughput: 962.58\n",
      "    learn_time_ms: 1038.875\n",
      "    load_throughput: 54987.867\n",
      "    load_time_ms: 18.186\n",
      "    sample_throughput: 40.275\n",
      "    sample_time_ms: 24829.102\n",
      "    update_time_ms: 2.217\n",
      "  timestamp: 1633708977\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 127000\n",
      "  training_iteration: 127\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         3178.32</td><td style=\"text-align: right;\">127000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            361.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-03-21\n",
      "  done: false\n",
      "  episode_len_mean: 362.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 343\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9473359333144293\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012057715370748134\n",
      "          policy_loss: -0.04358921224872271\n",
      "          total_loss: -0.05367896196742852\n",
      "          vf_explained_var: -0.906208872795105\n",
      "          vf_loss: 0.0012446509544841117\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.05588235294118\n",
      "    ram_util_percent: 59.70882352941176\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03886352180517697\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.524465527617878\n",
      "    mean_inference_ms: 1.586982903485128\n",
      "    mean_raw_obs_processing_ms: 1.611755644866417\n",
      "  time_since_restore: 3201.7416813373566\n",
      "  time_this_iter_s: 23.426568508148193\n",
      "  time_total_s: 3201.7416813373566\n",
      "  timers:\n",
      "    learn_throughput: 963.926\n",
      "    learn_time_ms: 1037.424\n",
      "    load_throughput: 54737.062\n",
      "    load_time_ms: 18.269\n",
      "    sample_throughput: 40.419\n",
      "    sample_time_ms: 24740.942\n",
      "    update_time_ms: 2.227\n",
      "  timestamp: 1633709001\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 128\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">         3201.74</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               362</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 129000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-03-44\n",
      "  done: false\n",
      "  episode_len_mean: 361.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 346\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8147942145665488\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012781206781403768\n",
      "          policy_loss: -0.10424014967348841\n",
      "          total_loss: -0.1126998061935107\n",
      "          vf_explained_var: -0.9418931603431702\n",
      "          vf_loss: 0.001060970801497913\n",
      "    num_agent_steps_sampled: 129000\n",
      "    num_agent_steps_trained: 129000\n",
      "    num_steps_sampled: 129000\n",
      "    num_steps_trained: 129000\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.236363636363635\n",
      "    ram_util_percent: 59.85151515151516\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03886010959247333\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.53240987241802\n",
      "    mean_inference_ms: 1.586862905694518\n",
      "    mean_raw_obs_processing_ms: 1.6112884617630354\n",
      "  time_since_restore: 3224.791731119156\n",
      "  time_this_iter_s: 23.050049781799316\n",
      "  time_total_s: 3224.791731119156\n",
      "  timers:\n",
      "    learn_throughput: 963.517\n",
      "    learn_time_ms: 1037.864\n",
      "    load_throughput: 54881.452\n",
      "    load_time_ms: 18.221\n",
      "    sample_throughput: 40.55\n",
      "    sample_time_ms: 24660.65\n",
      "    update_time_ms: 2.229\n",
      "  timestamp: 1633709024\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 129000\n",
      "  training_iteration: 129\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         3224.79</td><td style=\"text-align: right;\">129000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            361.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 130000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-04-08\n",
      "  done: false\n",
      "  episode_len_mean: 359.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 349\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9814336127705043\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007711762137010089\n",
      "          policy_loss: -0.024486540464891328\n",
      "          total_loss: -0.03834887974792057\n",
      "          vf_explained_var: -0.6833171248435974\n",
      "          vf_loss: 0.000746556765322263\n",
      "    num_agent_steps_sampled: 130000\n",
      "    num_agent_steps_trained: 130000\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.06470588235294\n",
      "    ram_util_percent: 59.932352941176475\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03885617014069859\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.541163324348194\n",
      "    mean_inference_ms: 1.5867217730195093\n",
      "    mean_raw_obs_processing_ms: 1.6110995619537019\n",
      "  time_since_restore: 3248.871666431427\n",
      "  time_this_iter_s: 24.079935312271118\n",
      "  time_total_s: 3248.871666431427\n",
      "  timers:\n",
      "    learn_throughput: 962.872\n",
      "    learn_time_ms: 1038.559\n",
      "    load_throughput: 55279.422\n",
      "    load_time_ms: 18.09\n",
      "    sample_throughput: 40.787\n",
      "    sample_time_ms: 24517.696\n",
      "    update_time_ms: 2.234\n",
      "  timestamp: 1633709048\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 130\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">         3248.87</td><td style=\"text-align: right;\">130000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            359.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 131000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-04-32\n",
      "  done: false\n",
      "  episode_len_mean: 357.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 352\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7244645516077677\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012212374456340866\n",
      "          policy_loss: -0.027500499246848956\n",
      "          total_loss: -0.03527086331612534\n",
      "          vf_explained_var: -0.49338990449905396\n",
      "          vf_loss: 0.0012309277171476019\n",
      "    num_agent_steps_sampled: 131000\n",
      "    num_agent_steps_trained: 131000\n",
      "    num_steps_sampled: 131000\n",
      "    num_steps_trained: 131000\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.10588235294118\n",
      "    ram_util_percent: 60.041176470588226\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03885156403368862\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.550176454758333\n",
      "    mean_inference_ms: 1.5865615422231127\n",
      "    mean_raw_obs_processing_ms: 1.6111922441866449\n",
      "  time_since_restore: 3272.7544269561768\n",
      "  time_this_iter_s: 23.882760524749756\n",
      "  time_total_s: 3272.7544269561768\n",
      "  timers:\n",
      "    learn_throughput: 962.958\n",
      "    learn_time_ms: 1038.467\n",
      "    load_throughput: 55481.092\n",
      "    load_time_ms: 18.024\n",
      "    sample_throughput: 40.957\n",
      "    sample_time_ms: 24415.623\n",
      "    update_time_ms: 2.23\n",
      "  timestamp: 1633709072\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 131000\n",
      "  training_iteration: 131\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         3272.75</td><td style=\"text-align: right;\">131000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            357.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-04-54\n",
      "  done: false\n",
      "  episode_len_mean: 358.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 354\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9026631368531122\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007535913636654495\n",
      "          policy_loss: -0.05992359363784393\n",
      "          total_loss: -0.07303492232329316\n",
      "          vf_explained_var: -0.5456580519676208\n",
      "          vf_loss: 0.00082856077933684\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.040625\n",
      "    ram_util_percent: 60.171875\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03884827129424584\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.555812489833205\n",
      "    mean_inference_ms: 1.5864439472054448\n",
      "    mean_raw_obs_processing_ms: 1.6112318831791117\n",
      "  time_since_restore: 3294.8941311836243\n",
      "  time_this_iter_s: 22.13970422744751\n",
      "  time_total_s: 3294.8941311836243\n",
      "  timers:\n",
      "    learn_throughput: 960.698\n",
      "    learn_time_ms: 1040.909\n",
      "    load_throughput: 56333.031\n",
      "    load_time_ms: 17.752\n",
      "    sample_throughput: 41.529\n",
      "    sample_time_ms: 24079.42\n",
      "    update_time_ms: 2.233\n",
      "  timestamp: 1633709094\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 132\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         3294.89</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            358.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 133000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-05-16\n",
      "  done: false\n",
      "  episode_len_mean: 359.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 357\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.831049789322747\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010792789668387944\n",
      "          policy_loss: 0.0059099497894446055\n",
      "          total_loss: -0.0043239878283606635\n",
      "          vf_explained_var: -0.48175960779190063\n",
      "          vf_loss: 0.0007914247340522707\n",
      "    num_agent_steps_sampled: 133000\n",
      "    num_agent_steps_trained: 133000\n",
      "    num_steps_sampled: 133000\n",
      "    num_steps_trained: 133000\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.183870967741946\n",
      "    ram_util_percent: 60.23548387096775\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038842658375352494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.563574970732585\n",
      "    mean_inference_ms: 1.5862453120697244\n",
      "    mean_raw_obs_processing_ms: 1.6113956233970566\n",
      "  time_since_restore: 3317.0013580322266\n",
      "  time_this_iter_s: 22.107226848602295\n",
      "  time_total_s: 3317.0013580322266\n",
      "  timers:\n",
      "    learn_throughput: 961.262\n",
      "    learn_time_ms: 1040.3\n",
      "    load_throughput: 55998.569\n",
      "    load_time_ms: 17.858\n",
      "    sample_throughput: 45.184\n",
      "    sample_time_ms: 22131.534\n",
      "    update_time_ms: 2.232\n",
      "  timestamp: 1633709116\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 133000\n",
      "  training_iteration: 133\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">            3317</td><td style=\"text-align: right;\">133000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            359.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 134000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-05-54\n",
      "  done: false\n",
      "  episode_len_mean: 360.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 360\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7706930094295077\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010938650046164927\n",
      "          policy_loss: -0.035592003208067685\n",
      "          total_loss: -0.04454112557901276\n",
      "          vf_explained_var: -0.1507227122783661\n",
      "          vf_loss: 0.0013742175690519313\n",
      "    num_agent_steps_sampled: 134000\n",
      "    num_agent_steps_trained: 134000\n",
      "    num_steps_sampled: 134000\n",
      "    num_steps_trained: 134000\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.760000000000005\n",
      "    ram_util_percent: 60.212727272727264\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038836996884545436\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.570333169272775\n",
      "    mean_inference_ms: 1.5860416644166335\n",
      "    mean_raw_obs_processing_ms: 1.6154059355347707\n",
      "  time_since_restore: 3355.248531103134\n",
      "  time_this_iter_s: 38.24717307090759\n",
      "  time_total_s: 3355.248531103134\n",
      "  timers:\n",
      "    learn_throughput: 959.52\n",
      "    learn_time_ms: 1042.188\n",
      "    load_throughput: 55972.712\n",
      "    load_time_ms: 17.866\n",
      "    sample_throughput: 42.432\n",
      "    sample_time_ms: 23567.378\n",
      "    update_time_ms: 2.246\n",
      "  timestamp: 1633709154\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 134000\n",
      "  training_iteration: 134\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         3355.25</td><td style=\"text-align: right;\">134000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            360.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 135000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-06-17\n",
      "  done: false\n",
      "  episode_len_mean: 361.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 362\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9597249137030708\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011700025086939172\n",
      "          policy_loss: -0.11245742729968494\n",
      "          total_loss: -0.12298682940502961\n",
      "          vf_explained_var: -0.013885941356420517\n",
      "          vf_loss: 0.00117032554699108\n",
      "    num_agent_steps_sampled: 135000\n",
      "    num_agent_steps_trained: 135000\n",
      "    num_steps_sampled: 135000\n",
      "    num_steps_trained: 135000\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.693749999999994\n",
      "    ram_util_percent: 59.925000000000004\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03883328659316504\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.57421186423725\n",
      "    mean_inference_ms: 1.5859086908566666\n",
      "    mean_raw_obs_processing_ms: 1.618129469451195\n",
      "  time_since_restore: 3377.988789319992\n",
      "  time_this_iter_s: 22.74025821685791\n",
      "  time_total_s: 3377.988789319992\n",
      "  timers:\n",
      "    learn_throughput: 959.561\n",
      "    learn_time_ms: 1042.144\n",
      "    load_throughput: 55739.296\n",
      "    load_time_ms: 17.941\n",
      "    sample_throughput: 42.314\n",
      "    sample_time_ms: 23633.045\n",
      "    update_time_ms: 2.243\n",
      "  timestamp: 1633709177\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 135000\n",
      "  training_iteration: 135\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         3377.99</td><td style=\"text-align: right;\">135000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            361.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-06-37\n",
      "  done: false\n",
      "  episode_len_mean: 363.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 364\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.902323681778378\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010008494824757488\n",
      "          policy_loss: -0.09583292487594816\n",
      "          total_loss: -0.10643246231807603\n",
      "          vf_explained_var: -0.879984438419342\n",
      "          vf_loss: 0.00166796484328289\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.21666666666667\n",
      "    ram_util_percent: 59.97\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03882951812877629\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.577530643172977\n",
      "    mean_inference_ms: 1.585775356424515\n",
      "    mean_raw_obs_processing_ms: 1.620767823143682\n",
      "  time_since_restore: 3398.358631372452\n",
      "  time_this_iter_s: 20.369842052459717\n",
      "  time_total_s: 3398.358631372452\n",
      "  timers:\n",
      "    learn_throughput: 958.251\n",
      "    learn_time_ms: 1043.568\n",
      "    load_throughput: 55569.299\n",
      "    load_time_ms: 17.996\n",
      "    sample_throughput: 43.176\n",
      "    sample_time_ms: 23161.037\n",
      "    update_time_ms: 2.252\n",
      "  timestamp: 1633709197\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 136\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         3398.36</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            363.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 137000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-06-58\n",
      "  done: false\n",
      "  episode_len_mean: 365.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 367\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.066269866625468\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011201860967018758\n",
      "          policy_loss: -0.025597623756362334\n",
      "          total_loss: -0.03747038128268387\n",
      "          vf_explained_var: -0.6434616446495056\n",
      "          vf_loss: 0.001228685598471202\n",
      "    num_agent_steps_sampled: 137000\n",
      "    num_agent_steps_trained: 137000\n",
      "    num_steps_sampled: 137000\n",
      "    num_steps_trained: 137000\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.210344827586205\n",
      "    ram_util_percent: 59.77241379310346\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038823860316269394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.581679416674014\n",
      "    mean_inference_ms: 1.5855801126760531\n",
      "    mean_raw_obs_processing_ms: 1.6248761375401235\n",
      "  time_since_restore: 3419.147915840149\n",
      "  time_this_iter_s: 20.789284467697144\n",
      "  time_total_s: 3419.147915840149\n",
      "  timers:\n",
      "    learn_throughput: 956.539\n",
      "    learn_time_ms: 1045.436\n",
      "    load_throughput: 55766.494\n",
      "    load_time_ms: 17.932\n",
      "    sample_throughput: 43.451\n",
      "    sample_time_ms: 23014.442\n",
      "    update_time_ms: 2.261\n",
      "  timestamp: 1633709218\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 137000\n",
      "  training_iteration: 137\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         3419.15</td><td style=\"text-align: right;\">137000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            365.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 138000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-07-20\n",
      "  done: false\n",
      "  episode_len_mean: 365.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 369\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9777209454112583\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009833630594754282\n",
      "          policy_loss: -0.09427741907743944\n",
      "          total_loss: -0.106684484001663\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0007324426738907479\n",
      "    num_agent_steps_sampled: 138000\n",
      "    num_agent_steps_trained: 138000\n",
      "    num_steps_sampled: 138000\n",
      "    num_steps_trained: 138000\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.95625\n",
      "    ram_util_percent: 59.634375\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03882009860941821\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.58399560155986\n",
      "    mean_inference_ms: 1.58545154352287\n",
      "    mean_raw_obs_processing_ms: 1.6259625332188221\n",
      "  time_since_restore: 3441.088681459427\n",
      "  time_this_iter_s: 21.940765619277954\n",
      "  time_total_s: 3441.088681459427\n",
      "  timers:\n",
      "    learn_throughput: 956.924\n",
      "    learn_time_ms: 1045.015\n",
      "    load_throughput: 55571.729\n",
      "    load_time_ms: 17.995\n",
      "    sample_throughput: 43.733\n",
      "    sample_time_ms: 22866.249\n",
      "    update_time_ms: 2.256\n",
      "  timestamp: 1633709240\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 138000\n",
      "  training_iteration: 138\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         3441.09</td><td style=\"text-align: right;\">138000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             365.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 139000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-07-44\n",
      "  done: false\n",
      "  episode_len_mean: 366.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 372\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8075074619717069\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010239368061413344\n",
      "          policy_loss: -0.06519385452071826\n",
      "          total_loss: -0.07527185045182705\n",
      "          vf_explained_var: -0.47556135058403015\n",
      "          vf_loss: 0.0010855048519766166\n",
      "    num_agent_steps_sampled: 139000\n",
      "    num_agent_steps_trained: 139000\n",
      "    num_steps_sampled: 139000\n",
      "    num_steps_trained: 139000\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.22727272727273\n",
      "    ram_util_percent: 59.630303030303025\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038814545377140616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.587055980221866\n",
      "    mean_inference_ms: 1.585258903128029\n",
      "    mean_raw_obs_processing_ms: 1.625132967160853\n",
      "  time_since_restore: 3464.524097442627\n",
      "  time_this_iter_s: 23.435415983200073\n",
      "  time_total_s: 3464.524097442627\n",
      "  timers:\n",
      "    learn_throughput: 958.762\n",
      "    learn_time_ms: 1043.012\n",
      "    load_throughput: 55738.481\n",
      "    load_time_ms: 17.941\n",
      "    sample_throughput: 43.655\n",
      "    sample_time_ms: 22906.857\n",
      "    update_time_ms: 2.251\n",
      "  timestamp: 1633709264\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 139000\n",
      "  training_iteration: 139\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         3464.52</td><td style=\"text-align: right;\">139000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            366.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-08-07\n",
      "  done: false\n",
      "  episode_len_mean: 366.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 375\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8695955740080938\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013816794985918198\n",
      "          policy_loss: -0.04665684344040023\n",
      "          total_loss: -0.054384432112177215\n",
      "          vf_explained_var: -0.18758919835090637\n",
      "          vf_loss: 0.0016420287746263462\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.130303030303025\n",
      "    ram_util_percent: 59.654545454545456\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03880897092782451\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.589729085421244\n",
      "    mean_inference_ms: 1.5850666391002266\n",
      "    mean_raw_obs_processing_ms: 1.624418470080341\n",
      "  time_since_restore: 3487.722716331482\n",
      "  time_this_iter_s: 23.19861888885498\n",
      "  time_total_s: 3487.722716331482\n",
      "  timers:\n",
      "    learn_throughput: 959.227\n",
      "    learn_time_ms: 1042.507\n",
      "    load_throughput: 56058.744\n",
      "    load_time_ms: 17.838\n",
      "    sample_throughput: 43.822\n",
      "    sample_time_ms: 22819.343\n",
      "    update_time_ms: 2.247\n",
      "  timestamp: 1633709287\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 140\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         3487.72</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            366.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 141000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-08-30\n",
      "  done: false\n",
      "  episode_len_mean: 366.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 378\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8701025168100993\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011967055117687605\n",
      "          policy_loss: -0.09688983923859067\n",
      "          total_loss: -0.10588724915352132\n",
      "          vf_explained_var: -0.35182538628578186\n",
      "          vf_loss: 0.0016258550348639902\n",
      "    num_agent_steps_sampled: 141000\n",
      "    num_agent_steps_trained: 141000\n",
      "    num_steps_sampled: 141000\n",
      "    num_steps_trained: 141000\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.03333333333334\n",
      "    ram_util_percent: 59.75757575757576\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03880338859949044\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.591861515124382\n",
      "    mean_inference_ms: 1.584875532445617\n",
      "    mean_raw_obs_processing_ms: 1.6238170873800002\n",
      "  time_since_restore: 3510.965253353119\n",
      "  time_this_iter_s: 23.242537021636963\n",
      "  time_total_s: 3510.965253353119\n",
      "  timers:\n",
      "    learn_throughput: 957.898\n",
      "    learn_time_ms: 1043.952\n",
      "    load_throughput: 54143.945\n",
      "    load_time_ms: 18.469\n",
      "    sample_throughput: 43.95\n",
      "    sample_time_ms: 22753.248\n",
      "    update_time_ms: 2.245\n",
      "  timestamp: 1633709310\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 141000\n",
      "  training_iteration: 141\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         3510.97</td><td style=\"text-align: right;\">141000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            366.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 142000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-08-55\n",
      "  done: false\n",
      "  episode_len_mean: 366.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 380\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6749999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8604018264346653\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0018391375467830232\n",
      "          policy_loss: -0.2044549802939097\n",
      "          total_loss: -0.22108006825049717\n",
      "          vf_explained_var: -0.4950316846370697\n",
      "          vf_loss: 0.0007375112737968771\n",
      "    num_agent_steps_sampled: 142000\n",
      "    num_agent_steps_trained: 142000\n",
      "    num_steps_sampled: 142000\n",
      "    num_steps_trained: 142000\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.65555555555555\n",
      "    ram_util_percent: 59.90277777777778\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038799602324885406\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.593163015984114\n",
      "    mean_inference_ms: 1.584748023024136\n",
      "    mean_raw_obs_processing_ms: 1.6233922737496598\n",
      "  time_since_restore: 3535.837475299835\n",
      "  time_this_iter_s: 24.87222194671631\n",
      "  time_total_s: 3535.837475299835\n",
      "  timers:\n",
      "    learn_throughput: 958.446\n",
      "    learn_time_ms: 1043.356\n",
      "    load_throughput: 53699.051\n",
      "    load_time_ms: 18.622\n",
      "    sample_throughput: 43.427\n",
      "    sample_time_ms: 23026.922\n",
      "    update_time_ms: 2.259\n",
      "  timestamp: 1633709335\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 142000\n",
      "  training_iteration: 142\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         3535.84</td><td style=\"text-align: right;\">142000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">               366</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 143000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-09-22\n",
      "  done: false\n",
      "  episode_len_mean: 365.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 384\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8115908092922635\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016539904220593905\n",
      "          policy_loss: -0.09874152346617646\n",
      "          total_loss: -0.11040772108568085\n",
      "          vf_explained_var: -0.7954832911491394\n",
      "          vf_loss: 0.0008674934419104829\n",
      "    num_agent_steps_sampled: 143000\n",
      "    num_agent_steps_trained: 143000\n",
      "    num_steps_sampled: 143000\n",
      "    num_steps_trained: 143000\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.59487179487179\n",
      "    ram_util_percent: 60.07692307692305\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03879217303304055\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.595672761709046\n",
      "    mean_inference_ms: 1.5844993437181203\n",
      "    mean_raw_obs_processing_ms: 1.6228367056470352\n",
      "  time_since_restore: 3562.933938026428\n",
      "  time_this_iter_s: 27.096462726593018\n",
      "  time_total_s: 3562.933938026428\n",
      "  timers:\n",
      "    learn_throughput: 958.873\n",
      "    learn_time_ms: 1042.891\n",
      "    load_throughput: 53758.93\n",
      "    load_time_ms: 18.602\n",
      "    sample_throughput: 42.506\n",
      "    sample_time_ms: 23526.328\n",
      "    update_time_ms: 2.262\n",
      "  timestamp: 1633709362\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 143000\n",
      "  training_iteration: 143\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">         3562.93</td><td style=\"text-align: right;\">143000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            365.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-09-44\n",
      "  done: false\n",
      "  episode_len_mean: 365.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 386\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0070921752187942\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012666876704923865\n",
      "          policy_loss: -0.04110445251895322\n",
      "          total_loss: -0.0561793031791846\n",
      "          vf_explained_var: -0.7453349828720093\n",
      "          vf_loss: 0.0007209990040994145\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.25161290322581\n",
      "    ram_util_percent: 60.20000000000002\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03878846768829689\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.596588033806302\n",
      "    mean_inference_ms: 1.584375230189633\n",
      "    mean_raw_obs_processing_ms: 1.6226927374012325\n",
      "  time_since_restore: 3585.0138018131256\n",
      "  time_this_iter_s: 22.079863786697388\n",
      "  time_total_s: 3585.0138018131256\n",
      "  timers:\n",
      "    learn_throughput: 955.068\n",
      "    learn_time_ms: 1047.046\n",
      "    load_throughput: 53974.981\n",
      "    load_time_ms: 18.527\n",
      "    sample_throughput: 45.651\n",
      "    sample_time_ms: 21905.555\n",
      "    update_time_ms: 2.244\n",
      "  timestamp: 1633709384\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 144\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         3585.01</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            365.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 145000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-10-27\n",
      "  done: false\n",
      "  episode_len_mean: 363.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 390\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8891740825441148\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01162280161270017\n",
      "          policy_loss: -0.08345173050959905\n",
      "          total_loss: -0.09780054808490807\n",
      "          vf_explained_var: -0.5611055493354797\n",
      "          vf_loss: 0.000620228833415442\n",
      "    num_agent_steps_sampled: 145000\n",
      "    num_agent_steps_trained: 145000\n",
      "    num_steps_sampled: 145000\n",
      "    num_steps_trained: 145000\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.39677419354839\n",
      "    ram_util_percent: 60.17258064516129\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038781103632643046\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.598931399097356\n",
      "    mean_inference_ms: 1.5841294388660725\n",
      "    mean_raw_obs_processing_ms: 1.6271010281254883\n",
      "  time_since_restore: 3628.237538576126\n",
      "  time_this_iter_s: 43.22373676300049\n",
      "  time_total_s: 3628.237538576126\n",
      "  timers:\n",
      "    learn_throughput: 953.565\n",
      "    learn_time_ms: 1048.696\n",
      "    load_throughput: 54207.483\n",
      "    load_time_ms: 18.448\n",
      "    sample_throughput: 41.75\n",
      "    sample_time_ms: 23952.332\n",
      "    update_time_ms: 2.237\n",
      "  timestamp: 1633709427\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 145000\n",
      "  training_iteration: 145\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   145</td><td style=\"text-align: right;\">         3628.24</td><td style=\"text-align: right;\">145000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            363.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 146000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-10-51\n",
      "  done: false\n",
      "  episode_len_mean: 364.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 392\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8684935225380792\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016045459941130207\n",
      "          policy_loss: -0.06502268698273433\n",
      "          total_loss: -0.07656607294662131\n",
      "          vf_explained_var: -0.5392154455184937\n",
      "          vf_loss: 0.0017262049892451613\n",
      "    num_agent_steps_sampled: 146000\n",
      "    num_agent_steps_trained: 146000\n",
      "    num_steps_sampled: 146000\n",
      "    num_steps_trained: 146000\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.903030303030306\n",
      "    ram_util_percent: 60.13636363636363\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03877750775289459\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.59974446777908\n",
      "    mean_inference_ms: 1.5840083435328092\n",
      "    mean_raw_obs_processing_ms: 1.629349582124801\n",
      "  time_since_restore: 3651.4203009605408\n",
      "  time_this_iter_s: 23.182762384414673\n",
      "  time_total_s: 3651.4203009605408\n",
      "  timers:\n",
      "    learn_throughput: 953.405\n",
      "    learn_time_ms: 1048.872\n",
      "    load_throughput: 54227.948\n",
      "    load_time_ms: 18.441\n",
      "    sample_throughput: 41.265\n",
      "    sample_time_ms: 24233.405\n",
      "    update_time_ms: 2.291\n",
      "  timestamp: 1633709451\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 146000\n",
      "  training_iteration: 146\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         3651.42</td><td style=\"text-align: right;\">146000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            364.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 147000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-11-16\n",
      "  done: false\n",
      "  episode_len_mean: 365.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.08\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 395\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7339710778660244\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020919437733518877\n",
      "          policy_loss: -0.04381223618984222\n",
      "          total_loss: -0.05220995143883758\n",
      "          vf_explained_var: -0.3191450238227844\n",
      "          vf_loss: 0.0018816863742863966\n",
      "    num_agent_steps_sampled: 147000\n",
      "    num_agent_steps_trained: 147000\n",
      "    num_steps_sampled: 147000\n",
      "    num_steps_trained: 147000\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.85277777777778\n",
      "    ram_util_percent: 59.997222222222234\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03877219454093676\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.60075457994129\n",
      "    mean_inference_ms: 1.5838286048218726\n",
      "    mean_raw_obs_processing_ms: 1.6327337235367594\n",
      "  time_since_restore: 3676.361545562744\n",
      "  time_this_iter_s: 24.94124460220337\n",
      "  time_total_s: 3676.361545562744\n",
      "  timers:\n",
      "    learn_throughput: 954.973\n",
      "    learn_time_ms: 1047.15\n",
      "    load_throughput: 53530.047\n",
      "    load_time_ms: 18.681\n",
      "    sample_throughput: 40.568\n",
      "    sample_time_ms: 24650.129\n",
      "    update_time_ms: 2.287\n",
      "  timestamp: 1633709476\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 147000\n",
      "  training_iteration: 147\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         3676.36</td><td style=\"text-align: right;\">147000</td><td style=\"text-align: right;\">   -0.08</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            365.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-11-38\n",
      "  done: false\n",
      "  episode_len_mean: 366.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 397\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7905659291479323\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009189111143494896\n",
      "          policy_loss: 0.0057351806097560454\n",
      "          total_loss: 0.01137965288427141\n",
      "          vf_explained_var: -0.17967963218688965\n",
      "          vf_loss: 0.018898147587767904\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.059375\n",
      "    ram_util_percent: 59.809374999999996\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03876875278360294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.60128516279881\n",
      "    mean_inference_ms: 1.5837111915034279\n",
      "    mean_raw_obs_processing_ms: 1.635074188147724\n",
      "  time_since_restore: 3699.268256664276\n",
      "  time_this_iter_s: 22.906711101531982\n",
      "  time_total_s: 3699.268256664276\n",
      "  timers:\n",
      "    learn_throughput: 954.152\n",
      "    learn_time_ms: 1048.051\n",
      "    load_throughput: 54002.014\n",
      "    load_time_ms: 18.518\n",
      "    sample_throughput: 40.411\n",
      "    sample_time_ms: 24745.973\n",
      "    update_time_ms: 2.283\n",
      "  timestamp: 1633709498\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 148\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">         3699.27</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            366.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 149000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-12-03\n",
      "  done: false\n",
      "  episode_len_mean: 367.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 400\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.907503158516354\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014438992169845804\n",
      "          policy_loss: -0.15468100735710727\n",
      "          total_loss: -0.16390277637789646\n",
      "          vf_explained_var: -0.05775044858455658\n",
      "          vf_loss: 0.002543522083821396\n",
      "    num_agent_steps_sampled: 149000\n",
      "    num_agent_steps_trained: 149000\n",
      "    num_steps_sampled: 149000\n",
      "    num_steps_trained: 149000\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.66285714285715\n",
      "    ram_util_percent: 59.731428571428566\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03876365906573023\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.602242633564465\n",
      "    mean_inference_ms: 1.583537015468383\n",
      "    mean_raw_obs_processing_ms: 1.6370897952398806\n",
      "  time_since_restore: 3723.4632012844086\n",
      "  time_this_iter_s: 24.194944620132446\n",
      "  time_total_s: 3723.4632012844086\n",
      "  timers:\n",
      "    learn_throughput: 954.608\n",
      "    learn_time_ms: 1047.55\n",
      "    load_throughput: 54003.404\n",
      "    load_time_ms: 18.517\n",
      "    sample_throughput: 40.286\n",
      "    sample_time_ms: 24822.4\n",
      "    update_time_ms: 2.286\n",
      "  timestamp: 1633709523\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 149000\n",
      "  training_iteration: 149\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         3723.46</td><td style=\"text-align: right;\">149000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            367.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 150000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-12-31\n",
      "  done: false\n",
      "  episode_len_mean: 365.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 403\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.74394787285063\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0093380619131728\n",
      "          policy_loss: -0.05601600477885869\n",
      "          total_loss: -0.06806549094617367\n",
      "          vf_explained_var: -0.28943273425102234\n",
      "          vf_loss: 0.0006625964474450383\n",
      "    num_agent_steps_sampled: 150000\n",
      "    num_agent_steps_trained: 150000\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.826829268292684\n",
      "    ram_util_percent: 59.707317073170735\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03875871823645542\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.604126890488445\n",
      "    mean_inference_ms: 1.583367128867259\n",
      "    mean_raw_obs_processing_ms: 1.6361887916599214\n",
      "  time_since_restore: 3752.0318734645844\n",
      "  time_this_iter_s: 28.56867218017578\n",
      "  time_total_s: 3752.0318734645844\n",
      "  timers:\n",
      "    learn_throughput: 954.938\n",
      "    learn_time_ms: 1047.188\n",
      "    load_throughput: 53801.684\n",
      "    load_time_ms: 18.587\n",
      "    sample_throughput: 39.433\n",
      "    sample_time_ms: 25359.69\n",
      "    update_time_ms: 2.286\n",
      "  timestamp: 1633709551\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 150\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         3752.03</td><td style=\"text-align: right;\">150000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            365.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 151000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-12-56\n",
      "  done: false\n",
      "  episode_len_mean: 365.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 405\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9255290389060975\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013358317233935659\n",
      "          policy_loss: -0.05954233000261916\n",
      "          total_loss: -0.07083589531895187\n",
      "          vf_explained_var: -0.5105516314506531\n",
      "          vf_loss: 0.0011990749134889078\n",
      "    num_agent_steps_sampled: 151000\n",
      "    num_agent_steps_trained: 151000\n",
      "    num_steps_sampled: 151000\n",
      "    num_steps_trained: 151000\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.811428571428564\n",
      "    ram_util_percent: 59.80857142857143\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03875546630089009\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.60558934805368\n",
      "    mean_inference_ms: 1.583257430287043\n",
      "    mean_raw_obs_processing_ms: 1.6356986882309192\n",
      "  time_since_restore: 3776.8345263004303\n",
      "  time_this_iter_s: 24.802652835845947\n",
      "  time_total_s: 3776.8345263004303\n",
      "  timers:\n",
      "    learn_throughput: 954.244\n",
      "    learn_time_ms: 1047.95\n",
      "    load_throughput: 53878.329\n",
      "    load_time_ms: 18.56\n",
      "    sample_throughput: 39.193\n",
      "    sample_time_ms: 25514.965\n",
      "    update_time_ms: 2.29\n",
      "  timestamp: 1633709576\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 151000\n",
      "  training_iteration: 151\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         3776.83</td><td style=\"text-align: right;\">151000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            365.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-13-19\n",
      "  done: false\n",
      "  episode_len_mean: 366.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 408\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.017670879099104\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014030465654633773\n",
      "          policy_loss: -0.042708634336789446\n",
      "          total_loss: -0.05482237959901492\n",
      "          vf_explained_var: -0.7114428877830505\n",
      "          vf_loss: 0.0009600389069722345\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.8969696969697\n",
      "    ram_util_percent: 59.9\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03875060355134708\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.607678797822008\n",
      "    mean_inference_ms: 1.5830957373492498\n",
      "    mean_raw_obs_processing_ms: 1.6349964778382735\n",
      "  time_since_restore: 3800.1259548664093\n",
      "  time_this_iter_s: 23.291428565979004\n",
      "  time_total_s: 3800.1259548664093\n",
      "  timers:\n",
      "    learn_throughput: 954.634\n",
      "    learn_time_ms: 1047.521\n",
      "    load_throughput: 54109.09\n",
      "    load_time_ms: 18.481\n",
      "    sample_throughput: 39.436\n",
      "    sample_time_ms: 25357.395\n",
      "    update_time_ms: 2.292\n",
      "  timestamp: 1633709599\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 152\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         3800.13</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            366.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 153000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-13-45\n",
      "  done: false\n",
      "  episode_len_mean: 366.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 411\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8816739877065023\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01287306546072722\n",
      "          policy_loss: -0.11628696587350633\n",
      "          total_loss: -0.1276578575372696\n",
      "          vf_explained_var: -0.6382432579994202\n",
      "          vf_loss: 0.0009288584153788785\n",
      "    num_agent_steps_sampled: 153000\n",
      "    num_agent_steps_trained: 153000\n",
      "    num_steps_sampled: 153000\n",
      "    num_steps_trained: 153000\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.943243243243245\n",
      "    ram_util_percent: 60.062162162162146\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03874581904134202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.610110677261655\n",
      "    mean_inference_ms: 1.5829381975070516\n",
      "    mean_raw_obs_processing_ms: 1.6343862766280461\n",
      "  time_since_restore: 3825.5934269428253\n",
      "  time_this_iter_s: 25.467472076416016\n",
      "  time_total_s: 3825.5934269428253\n",
      "  timers:\n",
      "    learn_throughput: 954.537\n",
      "    learn_time_ms: 1047.628\n",
      "    load_throughput: 54458.043\n",
      "    load_time_ms: 18.363\n",
      "    sample_throughput: 39.691\n",
      "    sample_time_ms: 25194.516\n",
      "    update_time_ms: 2.285\n",
      "  timestamp: 1633709625\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 153000\n",
      "  training_iteration: 153\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">         3825.59</td><td style=\"text-align: right;\">153000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            366.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 154000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-14-09\n",
      "  done: false\n",
      "  episode_len_mean: 367.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 413\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8700995220078362\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013347397021383782\n",
      "          policy_loss: -0.07466699245075385\n",
      "          total_loss: -0.08545459157062901\n",
      "          vf_explained_var: -0.8352000713348389\n",
      "          vf_loss: 0.0011562760427801144\n",
      "    num_agent_steps_sampled: 154000\n",
      "    num_agent_steps_trained: 154000\n",
      "    num_steps_sampled: 154000\n",
      "    num_steps_trained: 154000\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.79411764705882\n",
      "    ram_util_percent: 60.13235294117648\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03874263476225022\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.611704432555793\n",
      "    mean_inference_ms: 1.5828340364940883\n",
      "    mean_raw_obs_processing_ms: 1.6339560373758613\n",
      "  time_since_restore: 3849.875981092453\n",
      "  time_this_iter_s: 24.282554149627686\n",
      "  time_total_s: 3849.875981092453\n",
      "  timers:\n",
      "    learn_throughput: 960.542\n",
      "    learn_time_ms: 1041.079\n",
      "    load_throughput: 53934.725\n",
      "    load_time_ms: 18.541\n",
      "    sample_throughput: 39.337\n",
      "    sample_time_ms: 25421.151\n",
      "    update_time_ms: 2.287\n",
      "  timestamp: 1633709649\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 154000\n",
      "  training_iteration: 154\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         3849.88</td><td style=\"text-align: right;\">154000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">             367.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 155000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-14-33\n",
      "  done: false\n",
      "  episode_len_mean: 368.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 416\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7693500558535258\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012544218292133083\n",
      "          policy_loss: -0.13035258534881805\n",
      "          total_loss: -0.13992878111700216\n",
      "          vf_explained_var: -0.5697557330131531\n",
      "          vf_loss: 0.0017667983865572346\n",
      "    num_agent_steps_sampled: 155000\n",
      "    num_agent_steps_trained: 155000\n",
      "    num_steps_sampled: 155000\n",
      "    num_steps_trained: 155000\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.83529411764707\n",
      "    ram_util_percent: 60.226470588235294\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03873796533838854\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.613836808046088\n",
      "    mean_inference_ms: 1.5826828412456881\n",
      "    mean_raw_obs_processing_ms: 1.6335270662760746\n",
      "  time_since_restore: 3873.399625301361\n",
      "  time_this_iter_s: 23.52364420890808\n",
      "  time_total_s: 3873.399625301361\n",
      "  timers:\n",
      "    learn_throughput: 959.651\n",
      "    learn_time_ms: 1042.045\n",
      "    load_throughput: 54408.028\n",
      "    load_time_ms: 18.38\n",
      "    sample_throughput: 42.643\n",
      "    sample_time_ms: 23450.332\n",
      "    update_time_ms: 2.29\n",
      "  timestamp: 1633709673\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 155000\n",
      "  training_iteration: 155\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">          3873.4</td><td style=\"text-align: right;\">155000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            368.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-14-57\n",
      "  done: false\n",
      "  episode_len_mean: 369.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 418\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6889174766010708\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018175510453822173\n",
      "          policy_loss: -0.020147471378246944\n",
      "          total_loss: -0.02636060035891003\n",
      "          vf_explained_var: -0.04526154696941376\n",
      "          vf_loss: 0.0014746951741269893\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.985294117647065\n",
      "    ram_util_percent: 60.25882352941176\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0387348497707892\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.615297269356383\n",
      "    mean_inference_ms: 1.5825828613104085\n",
      "    mean_raw_obs_processing_ms: 1.6332155865587117\n",
      "  time_since_restore: 3897.5340995788574\n",
      "  time_this_iter_s: 24.134474277496338\n",
      "  time_total_s: 3897.5340995788574\n",
      "  timers:\n",
      "    learn_throughput: 960.664\n",
      "    learn_time_ms: 1040.947\n",
      "    load_throughput: 53830.064\n",
      "    load_time_ms: 18.577\n",
      "    sample_throughput: 42.469\n",
      "    sample_time_ms: 23546.484\n",
      "    update_time_ms: 2.228\n",
      "  timestamp: 1633709697\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 156\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">         3897.53</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            369.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 157000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-15-37\n",
      "  done: false\n",
      "  episode_len_mean: 370.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 421\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7976036455896165\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012571985017857478\n",
      "          policy_loss: -0.14089007597002717\n",
      "          total_loss: -0.15159029704001215\n",
      "          vf_explained_var: -0.33863160014152527\n",
      "          vf_loss: 0.00091124999582664\n",
      "    num_agent_steps_sampled: 157000\n",
      "    num_agent_steps_trained: 157000\n",
      "    num_steps_sampled: 157000\n",
      "    num_steps_trained: 157000\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.29827586206897\n",
      "    ram_util_percent: 59.937931034482766\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03873022999576218\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.616940499367125\n",
      "    mean_inference_ms: 1.5824352252754141\n",
      "    mean_raw_obs_processing_ms: 1.6360346263795542\n",
      "  time_since_restore: 3937.6029858589172\n",
      "  time_this_iter_s: 40.068886280059814\n",
      "  time_total_s: 3937.6029858589172\n",
      "  timers:\n",
      "    learn_throughput: 960.618\n",
      "    learn_time_ms: 1040.997\n",
      "    load_throughput: 53850.314\n",
      "    load_time_ms: 18.57\n",
      "    sample_throughput: 39.906\n",
      "    sample_time_ms: 25059.181\n",
      "    update_time_ms: 2.236\n",
      "  timestamp: 1633709737\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 157000\n",
      "  training_iteration: 157\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">          3937.6</td><td style=\"text-align: right;\">157000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            370.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 158000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-15-58\n",
      "  done: false\n",
      "  episode_len_mean: 369.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 423\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7687575976053873\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013708436618571879\n",
      "          policy_loss: -0.1018849387144049\n",
      "          total_loss: -0.11173018821411662\n",
      "          vf_explained_var: -0.5082760453224182\n",
      "          vf_loss: 0.0009024321893876833\n",
      "    num_agent_steps_sampled: 158000\n",
      "    num_agent_steps_trained: 158000\n",
      "    num_steps_sampled: 158000\n",
      "    num_steps_trained: 158000\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.07666666666666\n",
      "    ram_util_percent: 59.29333333333332\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03872720544668588\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.61748652456447\n",
      "    mean_inference_ms: 1.5823374303756463\n",
      "    mean_raw_obs_processing_ms: 1.6379506093016913\n",
      "  time_since_restore: 3958.614887714386\n",
      "  time_this_iter_s: 21.01190185546875\n",
      "  time_total_s: 3958.614887714386\n",
      "  timers:\n",
      "    learn_throughput: 961.322\n",
      "    learn_time_ms: 1040.234\n",
      "    load_throughput: 53858.957\n",
      "    load_time_ms: 18.567\n",
      "    sample_throughput: 40.208\n",
      "    sample_time_ms: 24870.465\n",
      "    update_time_ms: 2.234\n",
      "  timestamp: 1633709758\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 158000\n",
      "  training_iteration: 158\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">         3958.61</td><td style=\"text-align: right;\">158000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            369.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 159000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-16-21\n",
      "  done: false\n",
      "  episode_len_mean: 371.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 426\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8829814195632935\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012141412320974767\n",
      "          policy_loss: -0.07506667807077368\n",
      "          total_loss: -0.08713593504702051\n",
      "          vf_explained_var: -0.3569673001766205\n",
      "          vf_loss: 0.0006139648583484813\n",
      "    num_agent_steps_sampled: 159000\n",
      "    num_agent_steps_trained: 159000\n",
      "    num_steps_sampled: 159000\n",
      "    num_steps_trained: 159000\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.171875\n",
      "    ram_util_percent: 59.125\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038722615131970985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.617785409202494\n",
      "    mean_inference_ms: 1.5821904973841476\n",
      "    mean_raw_obs_processing_ms: 1.640770992376111\n",
      "  time_since_restore: 3981.0908467769623\n",
      "  time_this_iter_s: 22.475959062576294\n",
      "  time_total_s: 3981.0908467769623\n",
      "  timers:\n",
      "    learn_throughput: 961.34\n",
      "    learn_time_ms: 1040.215\n",
      "    load_throughput: 53667.513\n",
      "    load_time_ms: 18.633\n",
      "    sample_throughput: 40.488\n",
      "    sample_time_ms: 24698.512\n",
      "    update_time_ms: 2.235\n",
      "  timestamp: 1633709781\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 159000\n",
      "  training_iteration: 159\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">         3981.09</td><td style=\"text-align: right;\">159000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            371.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-16-43\n",
      "  done: false\n",
      "  episode_len_mean: 373.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 429\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9913883288701375\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010049314170669217\n",
      "          policy_loss: -0.134492452070117\n",
      "          total_loss: -0.1487791924013032\n",
      "          vf_explained_var: -0.9994450807571411\n",
      "          vf_loss: 0.0005396781435896022\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.141935483870974\n",
      "    ram_util_percent: 59.03225806451612\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038718081732674506\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.6174952839023\n",
      "    mean_inference_ms: 1.5820464786974568\n",
      "    mean_raw_obs_processing_ms: 1.6408487232900675\n",
      "  time_since_restore: 4003.064748287201\n",
      "  time_this_iter_s: 21.973901510238647\n",
      "  time_total_s: 4003.064748287201\n",
      "  timers:\n",
      "    learn_throughput: 961.232\n",
      "    learn_time_ms: 1040.331\n",
      "    load_throughput: 53995.548\n",
      "    load_time_ms: 18.52\n",
      "    sample_throughput: 41.599\n",
      "    sample_time_ms: 24039.025\n",
      "    update_time_ms: 2.239\n",
      "  timestamp: 1633709803\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 160\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         4003.06</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            373.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 161000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-17-04\n",
      "  done: false\n",
      "  episode_len_mean: 374.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 431\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.93893950647778\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016079960172925272\n",
      "          policy_loss: -0.09419868199361695\n",
      "          total_loss: -0.1044052552845743\n",
      "          vf_explained_var: -0.49022942781448364\n",
      "          vf_loss: 0.001042341695736266\n",
      "    num_agent_steps_sampled: 161000\n",
      "    num_agent_steps_trained: 161000\n",
      "    num_steps_sampled: 161000\n",
      "    num_steps_trained: 161000\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.19677419354839\n",
      "    ram_util_percent: 59.06774193548385\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03871505260061281\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.61698693979954\n",
      "    mean_inference_ms: 1.5819509235209954\n",
      "    mean_raw_obs_processing_ms: 1.6399746303177687\n",
      "  time_since_restore: 4024.572317838669\n",
      "  time_this_iter_s: 21.507569551467896\n",
      "  time_total_s: 4024.572317838669\n",
      "  timers:\n",
      "    learn_throughput: 963.383\n",
      "    learn_time_ms: 1038.009\n",
      "    load_throughput: 53822.327\n",
      "    load_time_ms: 18.58\n",
      "    sample_throughput: 42.173\n",
      "    sample_time_ms: 23711.762\n",
      "    update_time_ms: 2.256\n",
      "  timestamp: 1633709824\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 161000\n",
      "  training_iteration: 161\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         4024.57</td><td style=\"text-align: right;\">161000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            374.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 162000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-17-28\n",
      "  done: false\n",
      "  episode_len_mean: 373.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 434\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0564663264486525\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011980418011633394\n",
      "          policy_loss: -0.05987703483551741\n",
      "          total_loss: -0.07374081556788749\n",
      "          vf_explained_var: -0.9102351665496826\n",
      "          vf_loss: 0.0006357941863825544\n",
      "    num_agent_steps_sampled: 162000\n",
      "    num_agent_steps_trained: 162000\n",
      "    num_steps_sampled: 162000\n",
      "    num_steps_trained: 162000\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.349999999999994\n",
      "    ram_util_percent: 59.23529411764707\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03871047035146893\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.61637184223949\n",
      "    mean_inference_ms: 1.581808985478559\n",
      "    mean_raw_obs_processing_ms: 1.6386931737540145\n",
      "  time_since_restore: 4048.458254337311\n",
      "  time_this_iter_s: 23.885936498641968\n",
      "  time_total_s: 4048.458254337311\n",
      "  timers:\n",
      "    learn_throughput: 964.441\n",
      "    learn_time_ms: 1036.871\n",
      "    load_throughput: 53509.559\n",
      "    load_time_ms: 18.688\n",
      "    sample_throughput: 42.066\n",
      "    sample_time_ms: 23772.256\n",
      "    update_time_ms: 2.241\n",
      "  timestamp: 1633709848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 162000\n",
      "  training_iteration: 162\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         4048.46</td><td style=\"text-align: right;\">162000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">             373.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 163000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-17-49\n",
      "  done: false\n",
      "  episode_len_mean: 374.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 437\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.041716957092285\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013595140425460938\n",
      "          policy_loss: -0.11370076835155488\n",
      "          total_loss: -0.126710265999039\n",
      "          vf_explained_var: -0.5231282711029053\n",
      "          vf_loss: 0.0005251329699401847\n",
      "    num_agent_steps_sampled: 163000\n",
      "    num_agent_steps_trained: 163000\n",
      "    num_steps_sampled: 163000\n",
      "    num_steps_trained: 163000\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.03333333333333\n",
      "    ram_util_percent: 59.39000000000001\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03870585022807484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.61524940963398\n",
      "    mean_inference_ms: 1.5816697464448124\n",
      "    mean_raw_obs_processing_ms: 1.6376196991727332\n",
      "  time_since_restore: 4069.2913768291473\n",
      "  time_this_iter_s: 20.833122491836548\n",
      "  time_total_s: 4069.2913768291473\n",
      "  timers:\n",
      "    learn_throughput: 965.132\n",
      "    learn_time_ms: 1036.128\n",
      "    load_throughput: 53718.514\n",
      "    load_time_ms: 18.616\n",
      "    sample_throughput: 42.901\n",
      "    sample_time_ms: 23309.612\n",
      "    update_time_ms: 2.261\n",
      "  timestamp: 1633709869\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 163000\n",
      "  training_iteration: 163\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   163</td><td style=\"text-align: right;\">         4069.29</td><td style=\"text-align: right;\">163000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            374.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-18-10\n",
      "  done: false\n",
      "  episode_len_mean: 375.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 439\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9326165503925747\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012319282618303045\n",
      "          policy_loss: -0.0945716327884131\n",
      "          total_loss: -0.10657772620519003\n",
      "          vf_explained_var: -0.7224962711334229\n",
      "          vf_loss: 0.0010834328801138326\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.27\n",
      "    ram_util_percent: 59.52666666666665\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03870282571334632\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.614268860976296\n",
      "    mean_inference_ms: 1.581577233471429\n",
      "    mean_raw_obs_processing_ms: 1.6369174947841767\n",
      "  time_since_restore: 4090.6767432689667\n",
      "  time_this_iter_s: 21.385366439819336\n",
      "  time_total_s: 4090.6767432689667\n",
      "  timers:\n",
      "    learn_throughput: 964.535\n",
      "    learn_time_ms: 1036.769\n",
      "    load_throughput: 53593.384\n",
      "    load_time_ms: 18.659\n",
      "    sample_throughput: 43.442\n",
      "    sample_time_ms: 23019.206\n",
      "    update_time_ms: 2.261\n",
      "  timestamp: 1633709890\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 164\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         4090.68</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            375.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 165000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-18-33\n",
      "  done: false\n",
      "  episode_len_mean: 375.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 442\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.918850909339057\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012318474264312387\n",
      "          policy_loss: -0.03960621588759952\n",
      "          total_loss: -0.05175724311007394\n",
      "          vf_explained_var: -0.456969678401947\n",
      "          vf_loss: 0.0008012541728223571\n",
      "    num_agent_steps_sampled: 165000\n",
      "    num_agent_steps_trained: 165000\n",
      "    num_steps_sampled: 165000\n",
      "    num_steps_trained: 165000\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.04375\n",
      "    ram_util_percent: 59.659375\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03869826902016487\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.61277023671083\n",
      "    mean_inference_ms: 1.5814394389878546\n",
      "    mean_raw_obs_processing_ms: 1.6358920442100648\n",
      "  time_since_restore: 4113.122615098953\n",
      "  time_this_iter_s: 22.445871829986572\n",
      "  time_total_s: 4113.122615098953\n",
      "  timers:\n",
      "    learn_throughput: 966.055\n",
      "    learn_time_ms: 1035.138\n",
      "    load_throughput: 53187.25\n",
      "    load_time_ms: 18.801\n",
      "    sample_throughput: 43.644\n",
      "    sample_time_ms: 22912.912\n",
      "    update_time_ms: 2.259\n",
      "  timestamp: 1633709913\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 165000\n",
      "  training_iteration: 165\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">         4113.12</td><td style=\"text-align: right;\">165000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            375.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 166000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-18-53\n",
      "  done: false\n",
      "  episode_len_mean: 375.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 444\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0257582743962605\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009065531101438495\n",
      "          policy_loss: -0.049743371374077264\n",
      "          total_loss: -0.06472076210710738\n",
      "          vf_explained_var: -0.04097602516412735\n",
      "          vf_loss: 0.00069076677351\n",
      "    num_agent_steps_sampled: 166000\n",
      "    num_agent_steps_trained: 166000\n",
      "    num_steps_sampled: 166000\n",
      "    num_steps_trained: 166000\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.13793103448275\n",
      "    ram_util_percent: 59.78965517241381\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03869521680683312\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.611406562316898\n",
      "    mean_inference_ms: 1.581348537769345\n",
      "    mean_raw_obs_processing_ms: 1.6353000381084615\n",
      "  time_since_restore: 4133.161465883255\n",
      "  time_this_iter_s: 20.038850784301758\n",
      "  time_total_s: 4133.161465883255\n",
      "  timers:\n",
      "    learn_throughput: 965.221\n",
      "    learn_time_ms: 1036.033\n",
      "    load_throughput: 53727.185\n",
      "    load_time_ms: 18.613\n",
      "    sample_throughput: 44.439\n",
      "    sample_time_ms: 22502.62\n",
      "    update_time_ms: 2.274\n",
      "  timestamp: 1633709933\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 166000\n",
      "  training_iteration: 166\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         4133.16</td><td style=\"text-align: right;\">166000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">             375.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 167000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-19-16\n",
      "  done: false\n",
      "  episode_len_mean: 376.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 447\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8880534423722162\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011004306137442457\n",
      "          policy_loss: 0.005990348176823722\n",
      "          total_loss: -0.00670183797677358\n",
      "          vf_explained_var: -0.5427919626235962\n",
      "          vf_loss: 0.0006174160468314464\n",
      "    num_agent_steps_sampled: 167000\n",
      "    num_agent_steps_trained: 167000\n",
      "    num_steps_sampled: 167000\n",
      "    num_steps_trained: 167000\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.03939393939394\n",
      "    ram_util_percent: 59.93636363636364\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03869057620604138\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.609310466030458\n",
      "    mean_inference_ms: 1.581213280095202\n",
      "    mean_raw_obs_processing_ms: 1.6344375205047756\n",
      "  time_since_restore: 4156.253871202469\n",
      "  time_this_iter_s: 23.092405319213867\n",
      "  time_total_s: 4156.253871202469\n",
      "  timers:\n",
      "    learn_throughput: 964.735\n",
      "    learn_time_ms: 1036.554\n",
      "    load_throughput: 54125.08\n",
      "    load_time_ms: 18.476\n",
      "    sample_throughput: 48.066\n",
      "    sample_time_ms: 20804.61\n",
      "    update_time_ms: 2.273\n",
      "  timestamp: 1633709956\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 167000\n",
      "  training_iteration: 167\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         4156.25</td><td style=\"text-align: right;\">167000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">               376</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-19-56\n",
      "  done: false\n",
      "  episode_len_mean: 375.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 450\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7258743127187093\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015495417151805352\n",
      "          policy_loss: -0.07915541173683273\n",
      "          total_loss: -0.08733311345179876\n",
      "          vf_explained_var: -0.56572026014328\n",
      "          vf_loss: 0.0012364868773147464\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.22931034482759\n",
      "    ram_util_percent: 60.1\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0386859385728103\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.607115431503022\n",
      "    mean_inference_ms: 1.581079328619696\n",
      "    mean_raw_obs_processing_ms: 1.6366517649945098\n",
      "  time_since_restore: 4196.557964801788\n",
      "  time_this_iter_s: 40.30409359931946\n",
      "  time_total_s: 4196.557964801788\n",
      "  timers:\n",
      "    learn_throughput: 964.873\n",
      "    learn_time_ms: 1036.406\n",
      "    load_throughput: 54071.074\n",
      "    load_time_ms: 18.494\n",
      "    sample_throughput: 43.987\n",
      "    sample_time_ms: 22733.95\n",
      "    update_time_ms: 2.278\n",
      "  timestamp: 1633709996\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 168\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         4196.56</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            375.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 169000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-20-19\n",
      "  done: false\n",
      "  episode_len_mean: 376.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 453\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.869236299726698\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013846166457707396\n",
      "          policy_loss: 3.914886878596412e-05\n",
      "          total_loss: -0.010974485737582048\n",
      "          vf_explained_var: -0.6485059261322021\n",
      "          vf_loss: 0.0006691079547939201\n",
      "    num_agent_steps_sampled: 169000\n",
      "    num_agent_steps_trained: 169000\n",
      "    num_steps_sampled: 169000\n",
      "    num_steps_trained: 169000\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.742424242424235\n",
      "    ram_util_percent: 60.239393939393935\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03868135082207431\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.604925050573605\n",
      "    mean_inference_ms: 1.5809457872636181\n",
      "    mean_raw_obs_processing_ms: 1.638927444140034\n",
      "  time_since_restore: 4219.764188528061\n",
      "  time_this_iter_s: 23.206223726272583\n",
      "  time_total_s: 4219.764188528061\n",
      "  timers:\n",
      "    learn_throughput: 963.647\n",
      "    learn_time_ms: 1037.724\n",
      "    load_throughput: 54040.42\n",
      "    load_time_ms: 18.505\n",
      "    sample_throughput: 43.849\n",
      "    sample_time_ms: 22805.673\n",
      "    update_time_ms: 2.282\n",
      "  timestamp: 1633710019\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 169000\n",
      "  training_iteration: 169\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         4219.76</td><td style=\"text-align: right;\">169000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            376.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 170000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-20-42\n",
      "  done: false\n",
      "  episode_len_mean: 376.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 455\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8184724595811632\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011835459756657703\n",
      "          policy_loss: -0.03143194642745786\n",
      "          total_loss: -0.04289322965261009\n",
      "          vf_explained_var: -0.9890323877334595\n",
      "          vf_loss: 0.0007317392866955035\n",
      "    num_agent_steps_sampled: 170000\n",
      "    num_agent_steps_trained: 170000\n",
      "    num_steps_sampled: 170000\n",
      "    num_steps_trained: 170000\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.175\n",
      "    ram_util_percent: 60.234375\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03867825301295455\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.603587828752566\n",
      "    mean_inference_ms: 1.5808590989981104\n",
      "    mean_raw_obs_processing_ms: 1.6405161634864442\n",
      "  time_since_restore: 4242.1541793346405\n",
      "  time_this_iter_s: 22.38999080657959\n",
      "  time_total_s: 4242.1541793346405\n",
      "  timers:\n",
      "    learn_throughput: 963.767\n",
      "    learn_time_ms: 1037.595\n",
      "    load_throughput: 53574.559\n",
      "    load_time_ms: 18.666\n",
      "    sample_throughput: 43.769\n",
      "    sample_time_ms: 22847.245\n",
      "    update_time_ms: 2.307\n",
      "  timestamp: 1633710042\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 170000\n",
      "  training_iteration: 170\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">         4242.15</td><td style=\"text-align: right;\">170000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            376.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 171000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-21-04\n",
      "  done: false\n",
      "  episode_len_mean: 374.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 458\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5666803134812248\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013452947386988651\n",
      "          policy_loss: -0.07383636732896169\n",
      "          total_loss: -0.08196709948695369\n",
      "          vf_explained_var: -0.8850719928741455\n",
      "          vf_loss: 0.0007255120219067774\n",
      "    num_agent_steps_sampled: 171000\n",
      "    num_agent_steps_trained: 171000\n",
      "    num_steps_sampled: 171000\n",
      "    num_steps_trained: 171000\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.150000000000006\n",
      "    ram_util_percent: 59.940625000000004\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038673530797570216\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.601714887932363\n",
      "    mean_inference_ms: 1.5807306882959165\n",
      "    mean_raw_obs_processing_ms: 1.641655791553215\n",
      "  time_since_restore: 4264.614597320557\n",
      "  time_this_iter_s: 22.460417985916138\n",
      "  time_total_s: 4264.614597320557\n",
      "  timers:\n",
      "    learn_throughput: 962.6\n",
      "    learn_time_ms: 1038.853\n",
      "    load_throughput: 53637.933\n",
      "    load_time_ms: 18.644\n",
      "    sample_throughput: 43.589\n",
      "    sample_time_ms: 22941.32\n",
      "    update_time_ms: 2.288\n",
      "  timestamp: 1633710064\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 171000\n",
      "  training_iteration: 171\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         4264.61</td><td style=\"text-align: right;\">171000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            374.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-21-29\n",
      "  done: false\n",
      "  episode_len_mean: 374.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 461\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6096415797869363\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013585219769370759\n",
      "          policy_loss: -0.04964802919162644\n",
      "          total_loss: -0.057787385003434284\n",
      "          vf_explained_var: -0.7732987999916077\n",
      "          vf_loss: 0.0010795417442245202\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.3764705882353\n",
      "    ram_util_percent: 59.8235294117647\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866891574751316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.600320801691947\n",
      "    mean_inference_ms: 1.58060528143945\n",
      "    mean_raw_obs_processing_ms: 1.640372187885908\n",
      "  time_since_restore: 4288.841526031494\n",
      "  time_this_iter_s: 24.2269287109375\n",
      "  time_total_s: 4288.841526031494\n",
      "  timers:\n",
      "    learn_throughput: 961.383\n",
      "    learn_time_ms: 1040.169\n",
      "    load_throughput: 53959.704\n",
      "    load_time_ms: 18.532\n",
      "    sample_throughput: 43.527\n",
      "    sample_time_ms: 22974.183\n",
      "    update_time_ms: 2.297\n",
      "  timestamp: 1633710089\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 172\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         4288.84</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            374.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 173000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-21-50\n",
      "  done: false\n",
      "  episode_len_mean: 373.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 464\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8004816823535494\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015773970799080227\n",
      "          policy_loss: -0.03127457797527313\n",
      "          total_loss: -0.04073488194909361\n",
      "          vf_explained_var: -0.24859890341758728\n",
      "          vf_loss: 0.0005589398658937878\n",
      "    num_agent_steps_sampled: 173000\n",
      "    num_agent_steps_trained: 173000\n",
      "    num_steps_sampled: 173000\n",
      "    num_steps_trained: 173000\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.08333333333334\n",
      "    ram_util_percent: 59.81333333333332\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866446194275954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.599063196803314\n",
      "    mean_inference_ms: 1.5804817952607808\n",
      "    mean_raw_obs_processing_ms: 1.6392871828534883\n",
      "  time_since_restore: 4309.848034858704\n",
      "  time_this_iter_s: 21.006508827209473\n",
      "  time_total_s: 4309.848034858704\n",
      "  timers:\n",
      "    learn_throughput: 959.427\n",
      "    learn_time_ms: 1042.289\n",
      "    load_throughput: 53424.227\n",
      "    load_time_ms: 18.718\n",
      "    sample_throughput: 43.499\n",
      "    sample_time_ms: 22989.217\n",
      "    update_time_ms: 2.278\n",
      "  timestamp: 1633710110\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 173000\n",
      "  training_iteration: 173\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         4309.85</td><td style=\"text-align: right;\">173000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">             373.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 174000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-22-11\n",
      "  done: false\n",
      "  episode_len_mean: 372.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 466\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7835818727811177\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01961417071584132\n",
      "          policy_loss: -0.03484274869163831\n",
      "          total_loss: -0.04209214299917221\n",
      "          vf_explained_var: -0.5073517560958862\n",
      "          vf_loss: 0.0006567495810385379\n",
      "    num_agent_steps_sampled: 174000\n",
      "    num_agent_steps_trained: 174000\n",
      "    num_steps_sampled: 174000\n",
      "    num_steps_trained: 174000\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.074193548387086\n",
      "    ram_util_percent: 59.890322580645176\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03866162222569989\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.59852497731638\n",
      "    mean_inference_ms: 1.5804011639343043\n",
      "    mean_raw_obs_processing_ms: 1.6386852778986638\n",
      "  time_since_restore: 4331.131783723831\n",
      "  time_this_iter_s: 21.283748865127563\n",
      "  time_total_s: 4331.131783723831\n",
      "  timers:\n",
      "    learn_throughput: 959.304\n",
      "    learn_time_ms: 1042.423\n",
      "    load_throughput: 53886.428\n",
      "    load_time_ms: 18.558\n",
      "    sample_throughput: 43.518\n",
      "    sample_time_ms: 22979.08\n",
      "    update_time_ms: 2.273\n",
      "  timestamp: 1633710131\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 174000\n",
      "  training_iteration: 174\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         4331.13</td><td style=\"text-align: right;\">174000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            372.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 175000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-22-33\n",
      "  done: false\n",
      "  episode_len_mean: 372.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 469\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8240267660882739\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01125107347054648\n",
      "          policy_loss: -0.025908034967465535\n",
      "          total_loss: -0.03783278378347556\n",
      "          vf_explained_var: -0.5730635523796082\n",
      "          vf_loss: 0.0006196614325745031\n",
      "    num_agent_steps_sampled: 175000\n",
      "    num_agent_steps_trained: 175000\n",
      "    num_steps_sampled: 175000\n",
      "    num_steps_trained: 175000\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.20322580645162\n",
      "    ram_util_percent: 60.00645161290321\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038657378140491215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.597669947477076\n",
      "    mean_inference_ms: 1.5802797685464811\n",
      "    mean_raw_obs_processing_ms: 1.6377516200900908\n",
      "  time_since_restore: 4353.008996963501\n",
      "  time_this_iter_s: 21.8772132396698\n",
      "  time_total_s: 4353.008996963501\n",
      "  timers:\n",
      "    learn_throughput: 960.02\n",
      "    learn_time_ms: 1041.645\n",
      "    load_throughput: 53844.507\n",
      "    load_time_ms: 18.572\n",
      "    sample_throughput: 43.624\n",
      "    sample_time_ms: 22922.998\n",
      "    update_time_ms: 2.265\n",
      "  timestamp: 1633710153\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 175000\n",
      "  training_iteration: 175\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         4353.01</td><td style=\"text-align: right;\">175000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            372.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-22-53\n",
      "  done: false\n",
      "  episode_len_mean: 372.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 471\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9159700751304627\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012899747934790115\n",
      "          policy_loss: -0.03275435254391697\n",
      "          total_loss: -0.04471689789659447\n",
      "          vf_explained_var: -0.9232434034347534\n",
      "          vf_loss: 0.0006666564586339518\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.14827586206896\n",
      "    ram_util_percent: 60.1344827586207\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03865460293888303\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.596803365489407\n",
      "    mean_inference_ms: 1.5802005922249025\n",
      "    mean_raw_obs_processing_ms: 1.6372465115703436\n",
      "  time_since_restore: 4373.18933057785\n",
      "  time_this_iter_s: 20.180333614349365\n",
      "  time_total_s: 4373.18933057785\n",
      "  timers:\n",
      "    learn_throughput: 959.039\n",
      "    learn_time_ms: 1042.71\n",
      "    load_throughput: 53878.052\n",
      "    load_time_ms: 18.56\n",
      "    sample_throughput: 43.599\n",
      "    sample_time_ms: 22936.094\n",
      "    update_time_ms: 2.279\n",
      "  timestamp: 1633710173\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 176\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">         4373.19</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            372.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 177000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-23-16\n",
      "  done: false\n",
      "  episode_len_mean: 373.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 474\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6894914733039008\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011215986265889752\n",
      "          policy_loss: -0.08341115568247107\n",
      "          total_loss: -0.09406996493538221\n",
      "          vf_explained_var: -0.8895565271377563\n",
      "          vf_loss: 0.0005580144905252382\n",
      "    num_agent_steps_sampled: 177000\n",
      "    num_agent_steps_trained: 177000\n",
      "    num_steps_sampled: 177000\n",
      "    num_steps_trained: 177000\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.233333333333334\n",
      "    ram_util_percent: 60.40303030303029\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038650476523680866\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.59550689926701\n",
      "    mean_inference_ms: 1.5800820415247583\n",
      "    mean_raw_obs_processing_ms: 1.6364537064538316\n",
      "  time_since_restore: 4396.388612031937\n",
      "  time_this_iter_s: 23.199281454086304\n",
      "  time_total_s: 4396.388612031937\n",
      "  timers:\n",
      "    learn_throughput: 958.874\n",
      "    learn_time_ms: 1042.89\n",
      "    load_throughput: 54237.134\n",
      "    load_time_ms: 18.438\n",
      "    sample_throughput: 43.579\n",
      "    sample_time_ms: 22946.746\n",
      "    update_time_ms: 2.261\n",
      "  timestamp: 1633710196\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 177000\n",
      "  training_iteration: 177\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         4396.39</td><td style=\"text-align: right;\">177000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            373.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 178000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-23-38\n",
      "  done: false\n",
      "  episode_len_mean: 372.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 477\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8377912362416586\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012650593009092938\n",
      "          policy_loss: -0.060838012852602534\n",
      "          total_loss: -0.07234264926777946\n",
      "          vf_explained_var: -0.6160953044891357\n",
      "          vf_loss: 0.00046891497396346594\n",
      "    num_agent_steps_sampled: 178000\n",
      "    num_agent_steps_trained: 178000\n",
      "    num_steps_sampled: 178000\n",
      "    num_steps_trained: 178000\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.2258064516129\n",
      "    ram_util_percent: 60.67741935483872\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864639704753688\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.594029591921608\n",
      "    mean_inference_ms: 1.5799649340355257\n",
      "    mean_raw_obs_processing_ms: 1.635730101083881\n",
      "  time_since_restore: 4418.43795132637\n",
      "  time_this_iter_s: 22.049339294433594\n",
      "  time_total_s: 4418.43795132637\n",
      "  timers:\n",
      "    learn_throughput: 958.264\n",
      "    learn_time_ms: 1043.553\n",
      "    load_throughput: 54285.57\n",
      "    load_time_ms: 18.421\n",
      "    sample_throughput: 47.347\n",
      "    sample_time_ms: 21120.627\n",
      "    update_time_ms: 2.27\n",
      "  timestamp: 1633710218\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 178000\n",
      "  training_iteration: 178\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   178</td><td style=\"text-align: right;\">         4418.44</td><td style=\"text-align: right;\">178000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            372.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 179000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-23-59\n",
      "  done: false\n",
      "  episode_len_mean: 374.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -8.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 479\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7605139560169645\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011803046712310965\n",
      "          policy_loss: -0.048359200689527725\n",
      "          total_loss: -0.059208303462300035\n",
      "          vf_explained_var: -0.48473289608955383\n",
      "          vf_loss: 0.0007807437854353338\n",
      "    num_agent_steps_sampled: 179000\n",
      "    num_agent_steps_trained: 179000\n",
      "    num_steps_sampled: 179000\n",
      "    num_steps_trained: 179000\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.029999999999994\n",
      "    ram_util_percent: 60.70666666666668\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864370400392994\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.592644351736308\n",
      "    mean_inference_ms: 1.5798867304360475\n",
      "    mean_raw_obs_processing_ms: 1.635257898672149\n",
      "  time_since_restore: 4439.03337430954\n",
      "  time_this_iter_s: 20.595422983169556\n",
      "  time_total_s: 4439.03337430954\n",
      "  timers:\n",
      "    learn_throughput: 958.902\n",
      "    learn_time_ms: 1042.859\n",
      "    load_throughput: 54421.017\n",
      "    load_time_ms: 18.375\n",
      "    sample_throughput: 47.938\n",
      "    sample_time_ms: 20860.29\n",
      "    update_time_ms: 2.258\n",
      "  timestamp: 1633710239\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 179000\n",
      "  training_iteration: 179\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         4439.03</td><td style=\"text-align: right;\">179000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -8</td><td style=\"text-align: right;\">            374.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-24-35\n",
      "  done: false\n",
      "  episode_len_mean: 376.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.04\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 481\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8402733908759223\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012407573137483219\n",
      "          policy_loss: -0.06241836854153209\n",
      "          total_loss: -0.0735968782669968\n",
      "          vf_explained_var: -0.9442582726478577\n",
      "          vf_loss: 0.0009428900065055738\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.83076923076923\n",
      "    ram_util_percent: 60.69807692307692\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03864108576331324\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.59050482232412\n",
      "    mean_inference_ms: 1.5798086915297669\n",
      "    mean_raw_obs_processing_ms: 1.636691477104254\n",
      "  time_since_restore: 4475.580571174622\n",
      "  time_this_iter_s: 36.54719686508179\n",
      "  time_total_s: 4475.580571174622\n",
      "  timers:\n",
      "    learn_throughput: 958.409\n",
      "    learn_time_ms: 1043.396\n",
      "    load_throughput: 54628.413\n",
      "    load_time_ms: 18.305\n",
      "    sample_throughput: 44.892\n",
      "    sample_time_ms: 22275.557\n",
      "    update_time_ms: 2.222\n",
      "  timestamp: 1633710275\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 180\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         4475.58</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">   -0.04</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            376.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 181000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-24-55\n",
      "  done: false\n",
      "  episode_len_mean: 379.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.04\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 483\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8745234171549479\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013519526465237242\n",
      "          policy_loss: 0.019884487158722347\n",
      "          total_loss: 0.00851240646508005\n",
      "          vf_explained_var: -0.622148871421814\n",
      "          vf_loss: 0.0005288925054224415\n",
      "    num_agent_steps_sampled: 181000\n",
      "    num_agent_steps_trained: 181000\n",
      "    num_steps_sampled: 181000\n",
      "    num_steps_trained: 181000\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.10000000000001\n",
      "    ram_util_percent: 60.49310344827587\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038638460521912915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.58774015997962\n",
      "    mean_inference_ms: 1.5797292967364414\n",
      "    mean_raw_obs_processing_ms: 1.6380541314053008\n",
      "  time_since_restore: 4495.516985177994\n",
      "  time_this_iter_s: 19.936414003372192\n",
      "  time_total_s: 4495.516985177994\n",
      "  timers:\n",
      "    learn_throughput: 959.519\n",
      "    learn_time_ms: 1042.189\n",
      "    load_throughput: 53119.15\n",
      "    load_time_ms: 18.826\n",
      "    sample_throughput: 45.405\n",
      "    sample_time_ms: 22023.843\n",
      "    update_time_ms: 2.216\n",
      "  timestamp: 1633710295\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 181000\n",
      "  training_iteration: 181\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">         4495.52</td><td style=\"text-align: right;\">181000</td><td style=\"text-align: right;\">   -0.04</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            379.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 182000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-25-18\n",
      "  done: false\n",
      "  episode_len_mean: 381.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.04\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 486\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8125646763377719\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011111836676392489\n",
      "          policy_loss: -0.07088429414563709\n",
      "          total_loss: -0.08285742584202024\n",
      "          vf_explained_var: -0.7843126654624939\n",
      "          vf_loss: 0.0005271505816684415\n",
      "    num_agent_steps_sampled: 182000\n",
      "    num_agent_steps_trained: 182000\n",
      "    num_steps_sampled: 182000\n",
      "    num_steps_trained: 182000\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.028125\n",
      "    ram_util_percent: 60.193749999999994\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863452143381952\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.58370713299545\n",
      "    mean_inference_ms: 1.579612344001534\n",
      "    mean_raw_obs_processing_ms: 1.6400559444462877\n",
      "  time_since_restore: 4518.364439964294\n",
      "  time_this_iter_s: 22.84745478630066\n",
      "  time_total_s: 4518.364439964294\n",
      "  timers:\n",
      "    learn_throughput: 960.521\n",
      "    learn_time_ms: 1041.102\n",
      "    load_throughput: 52302.75\n",
      "    load_time_ms: 19.119\n",
      "    sample_throughput: 45.69\n",
      "    sample_time_ms: 21886.714\n",
      "    update_time_ms: 2.217\n",
      "  timestamp: 1633710318\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 182000\n",
      "  training_iteration: 182\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">         4518.36</td><td style=\"text-align: right;\">182000</td><td style=\"text-align: right;\">   -0.04</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            381.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 183000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-25-41\n",
      "  done: false\n",
      "  episode_len_mean: 382.92\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.04\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 488\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.914996980296241\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012657902862891035\n",
      "          policy_loss: -0.05756855391793781\n",
      "          total_loss: -0.06968001599113147\n",
      "          vf_explained_var: -0.8592236042022705\n",
      "          vf_loss: 0.0006304428560219498\n",
      "    num_agent_steps_sampled: 183000\n",
      "    num_agent_steps_trained: 183000\n",
      "    num_steps_sampled: 183000\n",
      "    num_steps_trained: 183000\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.93636363636364\n",
      "    ram_util_percent: 60.02424242424242\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03863197705986471\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.580624062923413\n",
      "    mean_inference_ms: 1.5795364686427285\n",
      "    mean_raw_obs_processing_ms: 1.639205122705261\n",
      "  time_since_restore: 4541.536399841309\n",
      "  time_this_iter_s: 23.17195987701416\n",
      "  time_total_s: 4541.536399841309\n",
      "  timers:\n",
      "    learn_throughput: 960.307\n",
      "    learn_time_ms: 1041.334\n",
      "    load_throughput: 52546.833\n",
      "    load_time_ms: 19.031\n",
      "    sample_throughput: 45.242\n",
      "    sample_time_ms: 22103.13\n",
      "    update_time_ms: 2.23\n",
      "  timestamp: 1633710341\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 183000\n",
      "  training_iteration: 183\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">         4541.54</td><td style=\"text-align: right;\">183000</td><td style=\"text-align: right;\">   -0.04</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            382.92</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-26-03\n",
      "  done: false\n",
      "  episode_len_mean: 385.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.04\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 491\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9159340182940165\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019493736052246517\n",
      "          policy_loss: -0.07411245240105523\n",
      "          total_loss: -0.08203077297657728\n",
      "          vf_explained_var: -0.5378494262695312\n",
      "          vf_loss: 0.0013723151616027785\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.87741935483872\n",
      "    ram_util_percent: 59.96129032258066\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862815510991464\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.57572991856086\n",
      "    mean_inference_ms: 1.5794226592491907\n",
      "    mean_raw_obs_processing_ms: 1.6377955348189281\n",
      "  time_since_restore: 4563.093243598938\n",
      "  time_this_iter_s: 21.556843757629395\n",
      "  time_total_s: 4563.093243598938\n",
      "  timers:\n",
      "    learn_throughput: 961.181\n",
      "    learn_time_ms: 1040.387\n",
      "    load_throughput: 52733.534\n",
      "    load_time_ms: 18.963\n",
      "    sample_throughput: 45.185\n",
      "    sample_time_ms: 22131.044\n",
      "    update_time_ms: 2.627\n",
      "  timestamp: 1633710363\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 184\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">         4563.09</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">   -0.04</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            385.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 185000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-26-28\n",
      "  done: false\n",
      "  episode_len_mean: 384.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.04\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 493\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.813580032189687\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010728915023941996\n",
      "          policy_loss: -0.09485724303457473\n",
      "          total_loss: -0.10683496449556616\n",
      "          vf_explained_var: -0.9753943681716919\n",
      "          vf_loss: 0.0007265658295687495\n",
      "    num_agent_steps_sampled: 185000\n",
      "    num_agent_steps_trained: 185000\n",
      "    num_steps_sampled: 185000\n",
      "    num_steps_trained: 185000\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.96666666666667\n",
      "    ram_util_percent: 59.99166666666666\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862568283170054\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.572574097143143\n",
      "    mean_inference_ms: 1.5793484417653532\n",
      "    mean_raw_obs_processing_ms: 1.6369373581069078\n",
      "  time_since_restore: 4588.076534032822\n",
      "  time_this_iter_s: 24.983290433883667\n",
      "  time_total_s: 4588.076534032822\n",
      "  timers:\n",
      "    learn_throughput: 959.406\n",
      "    learn_time_ms: 1042.312\n",
      "    load_throughput: 52741.955\n",
      "    load_time_ms: 18.96\n",
      "    sample_throughput: 44.564\n",
      "    sample_time_ms: 22439.733\n",
      "    update_time_ms: 2.629\n",
      "  timestamp: 1633710388\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 185000\n",
      "  training_iteration: 185\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">         4588.08</td><td style=\"text-align: right;\">185000</td><td style=\"text-align: right;\">   -0.04</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            384.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 186000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-26-52\n",
      "  done: false\n",
      "  episode_len_mean: 385.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.04\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 496\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8731303956773546\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013777822921426866\n",
      "          policy_loss: -0.04893901691668563\n",
      "          total_loss: -0.05970249507162306\n",
      "          vf_explained_var: -0.41115808486938477\n",
      "          vf_loss: 0.000992801008073406\n",
      "    num_agent_steps_sampled: 186000\n",
      "    num_agent_steps_trained: 186000\n",
      "    num_steps_sampled: 186000\n",
      "    num_steps_trained: 186000\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.07272727272728\n",
      "    ram_util_percent: 60.078787878787885\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03862200014180985\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.567769016742997\n",
      "    mean_inference_ms: 1.5792391866549098\n",
      "    mean_raw_obs_processing_ms: 1.6356732879028955\n",
      "  time_since_restore: 4611.686527729034\n",
      "  time_this_iter_s: 23.60999369621277\n",
      "  time_total_s: 4611.686527729034\n",
      "  timers:\n",
      "    learn_throughput: 959.737\n",
      "    learn_time_ms: 1041.953\n",
      "    load_throughput: 52733.733\n",
      "    load_time_ms: 18.963\n",
      "    sample_throughput: 43.892\n",
      "    sample_time_ms: 22783.048\n",
      "    update_time_ms: 2.608\n",
      "  timestamp: 1633710412\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 186000\n",
      "  training_iteration: 186\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">         4611.69</td><td style=\"text-align: right;\">186000</td><td style=\"text-align: right;\">   -0.04</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            385.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 187000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-27-17\n",
      "  done: false\n",
      "  episode_len_mean: 383.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 499\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.796680368317498\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009253276634497808\n",
      "          policy_loss: 0.002151023472348849\n",
      "          total_loss: -0.010642760826481714\n",
      "          vf_explained_var: -0.06492941826581955\n",
      "          vf_loss: 0.0004885468389450883\n",
      "    num_agent_steps_sampled: 187000\n",
      "    num_agent_steps_trained: 187000\n",
      "    num_steps_sampled: 187000\n",
      "    num_steps_trained: 187000\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.87777777777778\n",
      "    ram_util_percent: 60.21666666666667\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03861842184593296\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.563238403618687\n",
      "    mean_inference_ms: 1.579133742072719\n",
      "    mean_raw_obs_processing_ms: 1.63458083367815\n",
      "  time_since_restore: 4636.867548942566\n",
      "  time_this_iter_s: 25.181021213531494\n",
      "  time_total_s: 4636.867548942566\n",
      "  timers:\n",
      "    learn_throughput: 960.276\n",
      "    learn_time_ms: 1041.367\n",
      "    load_throughput: 52589.725\n",
      "    load_time_ms: 19.015\n",
      "    sample_throughput: 43.513\n",
      "    sample_time_ms: 22981.709\n",
      "    update_time_ms: 2.622\n",
      "  timestamp: 1633710437\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 187000\n",
      "  training_iteration: 187\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">         4636.87</td><td style=\"text-align: right;\">187000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            383.56</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-27-38\n",
      "  done: false\n",
      "  episode_len_mean: 385.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 501\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8553280247582329\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0028535067157017084\n",
      "          policy_loss: -0.11337836252318488\n",
      "          total_loss: -0.12947048081292045\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0010165740412452983\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.8774193548387\n",
      "    ram_util_percent: 60.358064516129055\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03861606055197683\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.559624848465194\n",
      "    mean_inference_ms: 1.5790633418214157\n",
      "    mean_raw_obs_processing_ms: 1.6338604470783036\n",
      "  time_since_restore: 4658.06925535202\n",
      "  time_this_iter_s: 21.201706409454346\n",
      "  time_total_s: 4658.06925535202\n",
      "  timers:\n",
      "    learn_throughput: 960.859\n",
      "    learn_time_ms: 1040.735\n",
      "    load_throughput: 52399.194\n",
      "    load_time_ms: 19.084\n",
      "    sample_throughput: 43.673\n",
      "    sample_time_ms: 22897.521\n",
      "    update_time_ms: 2.611\n",
      "  timestamp: 1633710458\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 188\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   188</td><td style=\"text-align: right;\">         4658.07</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">            385.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 189000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-27-59\n",
      "  done: false\n",
      "  episode_len_mean: 388.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.04\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 503\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7125278181499906\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014771187963253635\n",
      "          policy_loss: 0.000843747788005405\n",
      "          total_loss: -0.0006524051229159037\n",
      "          vf_explained_var: -0.15604214370250702\n",
      "          vf_loss: 0.011890167816373934\n",
      "    num_agent_steps_sampled: 189000\n",
      "    num_agent_steps_trained: 189000\n",
      "    num_steps_sampled: 189000\n",
      "    num_steps_trained: 189000\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.99655172413792\n",
      "    ram_util_percent: 60.403448275862075\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03861371936921885\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.55540095275001\n",
      "    mean_inference_ms: 1.5789920336999308\n",
      "    mean_raw_obs_processing_ms: 1.6330845221649497\n",
      "  time_since_restore: 4678.8213720321655\n",
      "  time_this_iter_s: 20.752116680145264\n",
      "  time_total_s: 4678.8213720321655\n",
      "  timers:\n",
      "    learn_throughput: 957.69\n",
      "    learn_time_ms: 1044.179\n",
      "    load_throughput: 52787.69\n",
      "    load_time_ms: 18.944\n",
      "    sample_throughput: 43.649\n",
      "    sample_time_ms: 22909.857\n",
      "    update_time_ms: 2.635\n",
      "  timestamp: 1633710479\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 189000\n",
      "  training_iteration: 189\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">         4678.82</td><td style=\"text-align: right;\">189000</td><td style=\"text-align: right;\">   -0.04</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">               388</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 190000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-28-22\n",
      "  done: false\n",
      "  episode_len_mean: 389.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.04\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 506\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9165257427427504\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017436686618649194\n",
      "          policy_loss: -0.08844248250954681\n",
      "          total_loss: -0.10167718190285895\n",
      "          vf_explained_var: 0.10656698048114777\n",
      "          vf_loss: 0.0015168975754123596\n",
      "    num_agent_steps_sampled: 190000\n",
      "    num_agent_steps_trained: 190000\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.89411764705882\n",
      "    ram_util_percent: 60.51176470588236\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038610319999067624\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.5488099485373\n",
      "    mean_inference_ms: 1.578887924026372\n",
      "    mean_raw_obs_processing_ms: 1.6320883406013058\n",
      "  time_since_restore: 4702.075687170029\n",
      "  time_this_iter_s: 23.25431513786316\n",
      "  time_total_s: 4702.075687170029\n",
      "  timers:\n",
      "    learn_throughput: 957.336\n",
      "    learn_time_ms: 1044.565\n",
      "    load_throughput: 53074.182\n",
      "    load_time_ms: 18.842\n",
      "    sample_throughput: 46.339\n",
      "    sample_time_ms: 21580.282\n",
      "    update_time_ms: 2.649\n",
      "  timestamp: 1633710502\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 190\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   190</td><td style=\"text-align: right;\">         4702.08</td><td style=\"text-align: right;\">190000</td><td style=\"text-align: right;\">   -0.04</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            389.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 191000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-28-45\n",
      "  done: false\n",
      "  episode_len_mean: 388.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.04\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 508\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.793864693906572\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01286621434468525\n",
      "          policy_loss: -0.10469502988788816\n",
      "          total_loss: -0.11839988931185669\n",
      "          vf_explained_var: -0.16438913345336914\n",
      "          vf_loss: 0.0009770268477344265\n",
      "    num_agent_steps_sampled: 191000\n",
      "    num_agent_steps_trained: 191000\n",
      "    num_steps_sampled: 191000\n",
      "    num_steps_trained: 191000\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.93636363636364\n",
      "    ram_util_percent: 60.584848484848486\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03860809699027269\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.54443997678678\n",
      "    mean_inference_ms: 1.5788177764912574\n",
      "    mean_raw_obs_processing_ms: 1.6314000654571468\n",
      "  time_since_restore: 4725.375494480133\n",
      "  time_this_iter_s: 23.29980731010437\n",
      "  time_total_s: 4725.375494480133\n",
      "  timers:\n",
      "    learn_throughput: 956.139\n",
      "    learn_time_ms: 1045.873\n",
      "    load_throughput: 54689.885\n",
      "    load_time_ms: 18.285\n",
      "    sample_throughput: 45.629\n",
      "    sample_time_ms: 21915.879\n",
      "    update_time_ms: 2.645\n",
      "  timestamp: 1633710525\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 191000\n",
      "  training_iteration: 191\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">         4725.38</td><td style=\"text-align: right;\">191000</td><td style=\"text-align: right;\">   -0.04</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            388.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-29-19\n",
      "  done: false\n",
      "  episode_len_mean: 390.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.04\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 510\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0219843983650208\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01999281092858642\n",
      "          policy_loss: -0.12090170428984695\n",
      "          total_loss: -0.1352092909730143\n",
      "          vf_explained_var: -0.4499540627002716\n",
      "          vf_loss: 0.000851576323232924\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.139583333333334\n",
      "    ram_util_percent: 60.38958333333334\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03860590128084752\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.53914524896548\n",
      "    mean_inference_ms: 1.5787460527468948\n",
      "    mean_raw_obs_processing_ms: 1.6325221959311125\n",
      "  time_since_restore: 4759.206032752991\n",
      "  time_this_iter_s: 33.830538272857666\n",
      "  time_total_s: 4759.206032752991\n",
      "  timers:\n",
      "    learn_throughput: 955.263\n",
      "    learn_time_ms: 1046.832\n",
      "    load_throughput: 55538.615\n",
      "    load_time_ms: 18.005\n",
      "    sample_throughput: 43.453\n",
      "    sample_time_ms: 23013.503\n",
      "    update_time_ms: 2.634\n",
      "  timestamp: 1633710559\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 192\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">         4759.21</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">   -0.04</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            390.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 193000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-29-43\n",
      "  done: false\n",
      "  episode_len_mean: 390.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.04\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 513\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9886003600226507\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014927328989922847\n",
      "          policy_loss: -0.06485431635131439\n",
      "          total_loss: -0.08021633304241631\n",
      "          vf_explained_var: -0.995894193649292\n",
      "          vf_loss: 0.0007455066981492565\n",
      "    num_agent_steps_sampled: 193000\n",
      "    num_agent_steps_trained: 193000\n",
      "    num_steps_sampled: 193000\n",
      "    num_steps_trained: 193000\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.83235294117647\n",
      "    ram_util_percent: 59.5970588235294\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03860262386991373\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.53122145014877\n",
      "    mean_inference_ms: 1.5786382259193075\n",
      "    mean_raw_obs_processing_ms: 1.634162641770806\n",
      "  time_since_restore: 4783.244847536087\n",
      "  time_this_iter_s: 24.038814783096313\n",
      "  time_total_s: 4783.244847536087\n",
      "  timers:\n",
      "    learn_throughput: 955.944\n",
      "    learn_time_ms: 1046.086\n",
      "    load_throughput: 55338.936\n",
      "    load_time_ms: 18.07\n",
      "    sample_throughput: 43.288\n",
      "    sample_time_ms: 23100.87\n",
      "    update_time_ms: 2.61\n",
      "  timestamp: 1633710583\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 193000\n",
      "  training_iteration: 193\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   193</td><td style=\"text-align: right;\">         4783.24</td><td style=\"text-align: right;\">193000</td><td style=\"text-align: right;\">   -0.04</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            390.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 194000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-30-05\n",
      "  done: false\n",
      "  episode_len_mean: 390.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.04\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 515\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5962148984273274\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012446898532778682\n",
      "          policy_loss: -0.07448152158823278\n",
      "          total_loss: -0.08656591702666548\n",
      "          vf_explained_var: -0.6995903253555298\n",
      "          vf_loss: 0.0007271298336692983\n",
      "    num_agent_steps_sampled: 194000\n",
      "    num_agent_steps_trained: 194000\n",
      "    num_steps_sampled: 194000\n",
      "    num_steps_trained: 194000\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.11612903225805\n",
      "    ram_util_percent: 59.61935483870969\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03860044859453121\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.525706818980133\n",
      "    mean_inference_ms: 1.5785664781203093\n",
      "    mean_raw_obs_processing_ms: 1.6353448039514595\n",
      "  time_since_restore: 4804.494625329971\n",
      "  time_this_iter_s: 21.249777793884277\n",
      "  time_total_s: 4804.494625329971\n",
      "  timers:\n",
      "    learn_throughput: 954.749\n",
      "    learn_time_ms: 1047.395\n",
      "    load_throughput: 55103.815\n",
      "    load_time_ms: 18.148\n",
      "    sample_throughput: 43.348\n",
      "    sample_time_ms: 23069.192\n",
      "    update_time_ms: 2.213\n",
      "  timestamp: 1633710605\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 194000\n",
      "  training_iteration: 194\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   194</td><td style=\"text-align: right;\">         4804.49</td><td style=\"text-align: right;\">194000</td><td style=\"text-align: right;\">   -0.04</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            390.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 195000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-30-24\n",
      "  done: false\n",
      "  episode_len_mean: 391.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.04\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 518\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8694313009579977\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018883358435678708\n",
      "          policy_loss: -0.05563349144326316\n",
      "          total_loss: -0.06839246151761877\n",
      "          vf_explained_var: -0.6191084384918213\n",
      "          vf_loss: 0.0011554935216231065\n",
      "    num_agent_steps_sampled: 195000\n",
      "    num_agent_steps_trained: 195000\n",
      "    num_steps_sampled: 195000\n",
      "    num_steps_trained: 195000\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.0\n",
      "    ram_util_percent: 59.61071428571428\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03859718928819782\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.516784158472255\n",
      "    mean_inference_ms: 1.5784572727617268\n",
      "    mean_raw_obs_processing_ms: 1.6370751762547007\n",
      "  time_since_restore: 4824.017237901688\n",
      "  time_this_iter_s: 19.52261257171631\n",
      "  time_total_s: 4824.017237901688\n",
      "  timers:\n",
      "    learn_throughput: 957.003\n",
      "    learn_time_ms: 1044.929\n",
      "    load_throughput: 54966.464\n",
      "    load_time_ms: 18.193\n",
      "    sample_throughput: 44.394\n",
      "    sample_time_ms: 22525.534\n",
      "    update_time_ms: 2.222\n",
      "  timestamp: 1633710624\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 195000\n",
      "  training_iteration: 195\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   195</td><td style=\"text-align: right;\">         4824.02</td><td style=\"text-align: right;\">195000</td><td style=\"text-align: right;\">   -0.04</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            391.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-30-46\n",
      "  done: false\n",
      "  episode_len_mean: 392.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.04\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 520\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7751324004597133\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017291272408898686\n",
      "          policy_loss: -0.08416059596670998\n",
      "          total_loss: -0.09674132636023892\n",
      "          vf_explained_var: -0.8577232956886292\n",
      "          vf_loss: 0.0007937422649572707\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.12666666666667\n",
      "    ram_util_percent: 59.62999999999999\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03859508079121111\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.510652887686682\n",
      "    mean_inference_ms: 1.5783849137192363\n",
      "    mean_raw_obs_processing_ms: 1.636179382138474\n",
      "  time_since_restore: 4845.388326406479\n",
      "  time_this_iter_s: 21.37108850479126\n",
      "  time_total_s: 4845.388326406479\n",
      "  timers:\n",
      "    learn_throughput: 957.572\n",
      "    learn_time_ms: 1044.307\n",
      "    load_throughput: 55049.211\n",
      "    load_time_ms: 18.166\n",
      "    sample_throughput: 44.838\n",
      "    sample_time_ms: 22302.302\n",
      "    update_time_ms: 2.235\n",
      "  timestamp: 1633710646\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 196\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   196</td><td style=\"text-align: right;\">         4845.39</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">   -0.04</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            392.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 197000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-31-06\n",
      "  done: false\n",
      "  episode_len_mean: 393.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.07\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 523\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7568364593717787\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012472003389562402\n",
      "          policy_loss: 0.006733475459946526\n",
      "          total_loss: 0.009796218077341715\n",
      "          vf_explained_var: -0.45449286699295044\n",
      "          vf_loss: 0.017474128399044275\n",
      "    num_agent_steps_sampled: 197000\n",
      "    num_agent_steps_trained: 197000\n",
      "    num_steps_sampled: 197000\n",
      "    num_steps_trained: 197000\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.94482758620689\n",
      "    ram_util_percent: 59.73448275862069\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03859191581100335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.501273993958083\n",
      "    mean_inference_ms: 1.5782761396914475\n",
      "    mean_raw_obs_processing_ms: 1.6348086423110968\n",
      "  time_since_restore: 4865.369523048401\n",
      "  time_this_iter_s: 19.981196641921997\n",
      "  time_total_s: 4865.369523048401\n",
      "  timers:\n",
      "    learn_throughput: 956.09\n",
      "    learn_time_ms: 1045.926\n",
      "    load_throughput: 54941.696\n",
      "    load_time_ms: 18.201\n",
      "    sample_throughput: 45.912\n",
      "    sample_time_ms: 21780.706\n",
      "    update_time_ms: 2.222\n",
      "  timestamp: 1633710666\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 197000\n",
      "  training_iteration: 197\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   197</td><td style=\"text-align: right;\">         4865.37</td><td style=\"text-align: right;\">197000</td><td style=\"text-align: right;\">   -0.07</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            393.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 198000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-31-25\n",
      "  done: false\n",
      "  episode_len_mean: 394.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.07\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 525\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6528635912471348\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015973022109393695\n",
      "          policy_loss: -0.049574008014880946\n",
      "          total_loss: -0.05853008265193138\n",
      "          vf_explained_var: -0.3964473307132721\n",
      "          vf_loss: 0.0035293898946191704\n",
      "    num_agent_steps_sampled: 198000\n",
      "    num_agent_steps_trained: 198000\n",
      "    num_steps_sampled: 198000\n",
      "    num_steps_trained: 198000\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.29642857142857\n",
      "    ram_util_percent: 59.878571428571426\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038589907847493395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.49489486805106\n",
      "    mean_inference_ms: 1.5782047894956686\n",
      "    mean_raw_obs_processing_ms: 1.6339987356532697\n",
      "  time_since_restore: 4884.984280824661\n",
      "  time_this_iter_s: 19.614757776260376\n",
      "  time_total_s: 4884.984280824661\n",
      "  timers:\n",
      "    learn_throughput: 955.741\n",
      "    learn_time_ms: 1046.309\n",
      "    load_throughput: 55252.769\n",
      "    load_time_ms: 18.099\n",
      "    sample_throughput: 46.25\n",
      "    sample_time_ms: 21621.732\n",
      "    update_time_ms: 2.213\n",
      "  timestamp: 1633710685\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 198000\n",
      "  training_iteration: 198\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   198</td><td style=\"text-align: right;\">         4884.98</td><td style=\"text-align: right;\">198000</td><td style=\"text-align: right;\">   -0.07</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            394.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 199000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-31-45\n",
      "  done: false\n",
      "  episode_len_mean: 395.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.07\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 527\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9452433188756306\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01836845116839435\n",
      "          policy_loss: -0.03293694696492619\n",
      "          total_loss: -0.04669422482450803\n",
      "          vf_explained_var: -0.47084978222846985\n",
      "          vf_loss: 0.0010456397330724737\n",
      "    num_agent_steps_sampled: 199000\n",
      "    num_agent_steps_trained: 199000\n",
      "    num_steps_sampled: 199000\n",
      "    num_steps_trained: 199000\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.05\n",
      "    ram_util_percent: 59.98928571428571\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03858790177046862\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.488264173656745\n",
      "    mean_inference_ms: 1.5781335683828257\n",
      "    mean_raw_obs_processing_ms: 1.6331330502703116\n",
      "  time_since_restore: 4904.695464134216\n",
      "  time_this_iter_s: 19.711183309555054\n",
      "  time_total_s: 4904.695464134216\n",
      "  timers:\n",
      "    learn_throughput: 957.241\n",
      "    learn_time_ms: 1044.669\n",
      "    load_throughput: 54782.031\n",
      "    load_time_ms: 18.254\n",
      "    sample_throughput: 46.47\n",
      "    sample_time_ms: 21519.157\n",
      "    update_time_ms: 2.199\n",
      "  timestamp: 1633710705\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 199000\n",
      "  training_iteration: 199\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">          4904.7</td><td style=\"text-align: right;\">199000</td><td style=\"text-align: right;\">   -0.07</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">            395.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-32-05\n",
      "  done: false\n",
      "  episode_len_mean: 396.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.13\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 530\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8840511269039577\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01750361333885798\n",
      "          policy_loss: -0.012483936382664575\n",
      "          total_loss: 0.10886496189567778\n",
      "          vf_explained_var: -0.3897337317466736\n",
      "          vf_loss: 0.13575881136995221\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.20357142857142\n",
      "    ram_util_percent: 60.11071428571429\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03858491554521473\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.478043586533843\n",
      "    mean_inference_ms: 1.578026079184587\n",
      "    mean_raw_obs_processing_ms: 1.6318546397490916\n",
      "  time_since_restore: 4924.275957584381\n",
      "  time_this_iter_s: 19.580493450164795\n",
      "  time_total_s: 4924.275957584381\n",
      "  timers:\n",
      "    learn_throughput: 958.665\n",
      "    learn_time_ms: 1043.118\n",
      "    load_throughput: 54609.849\n",
      "    load_time_ms: 18.312\n",
      "    sample_throughput: 47.274\n",
      "    sample_time_ms: 21153.276\n",
      "    update_time_ms: 2.198\n",
      "  timestamp: 1633710725\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 200\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         4924.28</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">   -0.13</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">            396.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 201000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-32-23\n",
      "  done: false\n",
      "  episode_len_mean: 397.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.13\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 532\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7976519677374099\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009023269195554937\n",
      "          policy_loss: -0.23797663930389618\n",
      "          total_loss: -0.23440036709523862\n",
      "          vf_explained_var: -0.3551900088787079\n",
      "          vf_loss: 0.019268778271766172\n",
      "    num_agent_steps_sampled: 201000\n",
      "    num_agent_steps_trained: 201000\n",
      "    num_steps_sampled: 201000\n",
      "    num_steps_trained: 201000\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.034615384615385\n",
      "    ram_util_percent: 60.2576923076923\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03858298127883418\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.470912948067863\n",
      "    mean_inference_ms: 1.5779547365995206\n",
      "    mean_raw_obs_processing_ms: 1.63107281308984\n",
      "  time_since_restore: 4942.838889360428\n",
      "  time_this_iter_s: 18.562931776046753\n",
      "  time_total_s: 4942.838889360428\n",
      "  timers:\n",
      "    learn_throughput: 958.703\n",
      "    learn_time_ms: 1043.076\n",
      "    load_throughput: 54551.395\n",
      "    load_time_ms: 18.331\n",
      "    sample_throughput: 48.357\n",
      "    sample_time_ms: 20679.588\n",
      "    update_time_ms: 2.216\n",
      "  timestamp: 1633710743\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 201000\n",
      "  training_iteration: 201\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   201</td><td style=\"text-align: right;\">         4942.84</td><td style=\"text-align: right;\">201000</td><td style=\"text-align: right;\">   -0.13</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">            397.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 202000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-32-47\n",
      "  done: false\n",
      "  episode_len_mean: 398.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.19\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 535\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6511035137706334\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016334514782069418\n",
      "          policy_loss: -0.08431878536939622\n",
      "          total_loss: -0.08386729641093148\n",
      "          vf_explained_var: 0.0219492856413126\n",
      "          vf_loss: 0.012827847727263968\n",
      "    num_agent_steps_sampled: 202000\n",
      "    num_agent_steps_trained: 202000\n",
      "    num_steps_sampled: 202000\n",
      "    num_steps_trained: 202000\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.161764705882355\n",
      "    ram_util_percent: 60.40882352941177\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03858011002813259\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.460416490533404\n",
      "    mean_inference_ms: 1.5778481004506466\n",
      "    mean_raw_obs_processing_ms: 1.6299158793874502\n",
      "  time_since_restore: 4966.48131608963\n",
      "  time_this_iter_s: 23.64242672920227\n",
      "  time_total_s: 4966.48131608963\n",
      "  timers:\n",
      "    learn_throughput: 959.575\n",
      "    learn_time_ms: 1042.129\n",
      "    load_throughput: 54004.656\n",
      "    load_time_ms: 18.517\n",
      "    sample_throughput: 50.861\n",
      "    sample_time_ms: 19661.566\n",
      "    update_time_ms: 2.21\n",
      "  timestamp: 1633710767\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 202000\n",
      "  training_iteration: 202\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   202</td><td style=\"text-align: right;\">         4966.48</td><td style=\"text-align: right;\">202000</td><td style=\"text-align: right;\">   -0.19</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">            398.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 203000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-33-07\n",
      "  done: false\n",
      "  episode_len_mean: 398.86\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.19\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 537\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8893209430906508\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015920360325301273\n",
      "          policy_loss: -0.1382987627138694\n",
      "          total_loss: -0.14290555069843927\n",
      "          vf_explained_var: -0.4388292133808136\n",
      "          vf_loss: 0.010256576554901484\n",
      "    num_agent_steps_sampled: 203000\n",
      "    num_agent_steps_trained: 203000\n",
      "    num_steps_sampled: 203000\n",
      "    num_steps_trained: 203000\n",
      "  iterations_since_restore: 203\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.03103448275862\n",
      "    ram_util_percent: 60.5103448275862\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038578197796388575\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.453381056098134\n",
      "    mean_inference_ms: 1.5777761558621575\n",
      "    mean_raw_obs_processing_ms: 1.6291215430243842\n",
      "  time_since_restore: 4987.012807369232\n",
      "  time_this_iter_s: 20.53149127960205\n",
      "  time_total_s: 4987.012807369232\n",
      "  timers:\n",
      "    learn_throughput: 960.091\n",
      "    learn_time_ms: 1041.568\n",
      "    load_throughput: 54090.598\n",
      "    load_time_ms: 18.488\n",
      "    sample_throughput: 51.783\n",
      "    sample_time_ms: 19311.428\n",
      "    update_time_ms: 2.221\n",
      "  timestamp: 1633710787\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 203000\n",
      "  training_iteration: 203\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   203</td><td style=\"text-align: right;\">         4987.01</td><td style=\"text-align: right;\">203000</td><td style=\"text-align: right;\">   -0.19</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">            398.86</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-33-25\n",
      "  done: false\n",
      "  episode_len_mean: 401.04\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.19\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 539\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9245957679218717\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015281436856833391\n",
      "          policy_loss: -0.0883073755643434\n",
      "          total_loss: -0.0980241929491361\n",
      "          vf_explained_var: -0.2684766948223114\n",
      "          vf_loss: 0.005661026011997213\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.196000000000005\n",
      "    ram_util_percent: 60.62000000000001\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03857627373670866\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.446037345687667\n",
      "    mean_inference_ms: 1.5777038364670204\n",
      "    mean_raw_obs_processing_ms: 1.6283665585061617\n",
      "  time_since_restore: 5004.372411727905\n",
      "  time_this_iter_s: 17.359604358673096\n",
      "  time_total_s: 5004.372411727905\n",
      "  timers:\n",
      "    learn_throughput: 960.851\n",
      "    learn_time_ms: 1040.744\n",
      "    load_throughput: 57499.462\n",
      "    load_time_ms: 17.391\n",
      "    sample_throughput: 52.842\n",
      "    sample_time_ms: 18924.342\n",
      "    update_time_ms: 2.217\n",
      "  timestamp: 1633710805\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 204\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   204</td><td style=\"text-align: right;\">         5004.37</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">   -0.19</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">            401.04</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 205000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-34-01\n",
      "  done: false\n",
      "  episode_len_mean: 401.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.19\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 541\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7908491863144769\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011493747314340337\n",
      "          policy_loss: 0.03143513624866803\n",
      "          total_loss: 0.018987955235772663\n",
      "          vf_explained_var: -0.694196879863739\n",
      "          vf_loss: 0.0025519562587659394\n",
      "    num_agent_steps_sampled: 205000\n",
      "    num_agent_steps_trained: 205000\n",
      "    num_steps_sampled: 205000\n",
      "    num_steps_trained: 205000\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.53653846153846\n",
      "    ram_util_percent: 60.66730769230769\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03857442637077941\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.43855409697202\n",
      "    mean_inference_ms: 1.5776318582646323\n",
      "    mean_raw_obs_processing_ms: 1.6291896212959835\n",
      "  time_since_restore: 5040.5754017829895\n",
      "  time_this_iter_s: 36.20299005508423\n",
      "  time_total_s: 5040.5754017829895\n",
      "  timers:\n",
      "    learn_throughput: 958.066\n",
      "    learn_time_ms: 1043.769\n",
      "    load_throughput: 57590.096\n",
      "    load_time_ms: 17.364\n",
      "    sample_throughput: 48.569\n",
      "    sample_time_ms: 20589.38\n",
      "    update_time_ms: 2.205\n",
      "  timestamp: 1633710841\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 205000\n",
      "  training_iteration: 205\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   205</td><td style=\"text-align: right;\">         5040.58</td><td style=\"text-align: right;\">205000</td><td style=\"text-align: right;\">   -0.19</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">            401.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 206000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-34-24\n",
      "  done: false\n",
      "  episode_len_mean: 402.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.19\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 544\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7747568607330322\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01775701846857917\n",
      "          policy_loss: -0.02722371473080582\n",
      "          total_loss: -0.03813681486580107\n",
      "          vf_explained_var: -0.41287147998809814\n",
      "          vf_loss: 0.002339724969998416\n",
      "    num_agent_steps_sampled: 206000\n",
      "    num_agent_steps_trained: 206000\n",
      "    num_steps_sampled: 206000\n",
      "    num_steps_trained: 206000\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.06666666666667\n",
      "    ram_util_percent: 60.654545454545456\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038571667087842404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.427717642385083\n",
      "    mean_inference_ms: 1.577524553617784\n",
      "    mean_raw_obs_processing_ms: 1.6303865722201782\n",
      "  time_since_restore: 5063.521908521652\n",
      "  time_this_iter_s: 22.94650673866272\n",
      "  time_total_s: 5063.521908521652\n",
      "  timers:\n",
      "    learn_throughput: 959.325\n",
      "    learn_time_ms: 1042.4\n",
      "    load_throughput: 56884.414\n",
      "    load_time_ms: 17.58\n",
      "    sample_throughput: 48.197\n",
      "    sample_time_ms: 20748.083\n",
      "    update_time_ms: 2.186\n",
      "  timestamp: 1633710864\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 206000\n",
      "  training_iteration: 206\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   206</td><td style=\"text-align: right;\">         5063.52</td><td style=\"text-align: right;\">206000</td><td style=\"text-align: right;\">   -0.19</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">             402.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 207000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-34-43\n",
      "  done: false\n",
      "  episode_len_mean: 403.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.19\n",
      "  episode_reward_min: -6.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 546\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.867535079850091\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005996923168881402\n",
      "          policy_loss: -0.2025565465291341\n",
      "          total_loss: -0.21808939708603753\n",
      "          vf_explained_var: -0.8204056620597839\n",
      "          vf_loss: 0.0016245265458969193\n",
      "    num_agent_steps_sampled: 207000\n",
      "    num_agent_steps_trained: 207000\n",
      "    num_steps_sampled: 207000\n",
      "    num_steps_trained: 207000\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.11071428571428\n",
      "    ram_util_percent: 60.675000000000004\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03856987872792552\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.420334017933428\n",
      "    mean_inference_ms: 1.5774537540715547\n",
      "    mean_raw_obs_processing_ms: 1.6312684014709624\n",
      "  time_since_restore: 5083.082380056381\n",
      "  time_this_iter_s: 19.560471534729004\n",
      "  time_total_s: 5083.082380056381\n",
      "  timers:\n",
      "    learn_throughput: 959.514\n",
      "    learn_time_ms: 1042.195\n",
      "    load_throughput: 56694.724\n",
      "    load_time_ms: 17.638\n",
      "    sample_throughput: 48.295\n",
      "    sample_time_ms: 20706.131\n",
      "    update_time_ms: 2.205\n",
      "  timestamp: 1633710883\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 207000\n",
      "  training_iteration: 207\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   207</td><td style=\"text-align: right;\">         5083.08</td><td style=\"text-align: right;\">207000</td><td style=\"text-align: right;\">   -0.19</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                  -6</td><td style=\"text-align: right;\">             403.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-35-05\n",
      "  done: false\n",
      "  episode_len_mean: 405.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.34\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 549\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7977701942125957\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006264823519233195\n",
      "          policy_loss: -0.2509004000160429\n",
      "          total_loss: -0.26173103319274055\n",
      "          vf_explained_var: -0.6482966542243958\n",
      "          vf_loss: 0.005561284533986408\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.23999999999999\n",
      "    ram_util_percent: 60.566666666666656\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03856712508633068\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.408958866573688\n",
      "    mean_inference_ms: 1.5773466512657603\n",
      "    mean_raw_obs_processing_ms: 1.6305539594978924\n",
      "  time_since_restore: 5104.157044887543\n",
      "  time_this_iter_s: 21.0746648311615\n",
      "  time_total_s: 5104.157044887543\n",
      "  timers:\n",
      "    learn_throughput: 959.406\n",
      "    learn_time_ms: 1042.312\n",
      "    load_throughput: 56634.017\n",
      "    load_time_ms: 17.657\n",
      "    sample_throughput: 47.957\n",
      "    sample_time_ms: 20851.988\n",
      "    update_time_ms: 2.203\n",
      "  timestamp: 1633710905\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 208\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   208</td><td style=\"text-align: right;\">         5104.16</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">   -0.34</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            405.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 209000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-35-26\n",
      "  done: false\n",
      "  episode_len_mean: 406.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.42\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 551\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8114952445030212\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01240582076146727\n",
      "          policy_loss: -0.0888348022268878\n",
      "          total_loss: 0.015283851656648847\n",
      "          vf_explained_var: -0.4257977306842804\n",
      "          vf_loss: 0.11909338425224027\n",
      "    num_agent_steps_sampled: 209000\n",
      "    num_agent_steps_trained: 209000\n",
      "    num_steps_sampled: 209000\n",
      "    num_steps_trained: 209000\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.089999999999996\n",
      "    ram_util_percent: 60.58999999999998\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03856528252029076\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.40119374836586\n",
      "    mean_inference_ms: 1.577275535327935\n",
      "    mean_raw_obs_processing_ms: 1.6294176782197645\n",
      "  time_since_restore: 5125.208365440369\n",
      "  time_this_iter_s: 21.051320552825928\n",
      "  time_total_s: 5125.208365440369\n",
      "  timers:\n",
      "    learn_throughput: 960.191\n",
      "    learn_time_ms: 1041.459\n",
      "    load_throughput: 56556.735\n",
      "    load_time_ms: 17.681\n",
      "    sample_throughput: 47.649\n",
      "    sample_time_ms: 20986.832\n",
      "    update_time_ms: 2.193\n",
      "  timestamp: 1633710926\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 209000\n",
      "  training_iteration: 209\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   209</td><td style=\"text-align: right;\">         5125.21</td><td style=\"text-align: right;\">209000</td><td style=\"text-align: right;\">   -0.42</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            406.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 210000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-35-45\n",
      "  done: false\n",
      "  episode_len_mean: 407.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.44\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 553\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7721235858069526\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013898697390054417\n",
      "          policy_loss: -0.19870787254638142\n",
      "          total_loss: -0.16446483896838293\n",
      "          vf_explained_var: -0.13102160394191742\n",
      "          vf_loss: 0.048446161369793114\n",
      "    num_agent_steps_sampled: 210000\n",
      "    num_agent_steps_trained: 210000\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.278571428571425\n",
      "    ram_util_percent: 60.585714285714275\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038563374581318\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.39312883382895\n",
      "    mean_inference_ms: 1.5772033049946936\n",
      "    mean_raw_obs_processing_ms: 1.6282311568946224\n",
      "  time_since_restore: 5144.936027765274\n",
      "  time_this_iter_s: 19.727662324905396\n",
      "  time_total_s: 5144.936027765274\n",
      "  timers:\n",
      "    learn_throughput: 960.232\n",
      "    learn_time_ms: 1041.415\n",
      "    load_throughput: 56642.889\n",
      "    load_time_ms: 17.654\n",
      "    sample_throughput: 47.615\n",
      "    sample_time_ms: 21001.627\n",
      "    update_time_ms: 2.186\n",
      "  timestamp: 1633710945\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 210\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   210</td><td style=\"text-align: right;\">         5144.94</td><td style=\"text-align: right;\">210000</td><td style=\"text-align: right;\">   -0.44</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            407.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 211000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-36-04\n",
      "  done: false\n",
      "  episode_len_mean: 409.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.54\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 555\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8072317242622375\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017839342323170172\n",
      "          policy_loss: 0.07724501296050018\n",
      "          total_loss: 0.13529977647380698\n",
      "          vf_explained_var: 0.05180232971906662\n",
      "          vf_loss: 0.07161149241858059\n",
      "    num_agent_steps_sampled: 211000\n",
      "    num_agent_steps_trained: 211000\n",
      "    num_steps_sampled: 211000\n",
      "    num_steps_trained: 211000\n",
      "  iterations_since_restore: 211\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.31111111111112\n",
      "    ram_util_percent: 60.62592592592594\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0385615321431894\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.38478406568857\n",
      "    mean_inference_ms: 1.5771308767447252\n",
      "    mean_raw_obs_processing_ms: 1.6270866396911494\n",
      "  time_since_restore: 5163.713745355606\n",
      "  time_this_iter_s: 18.77771759033203\n",
      "  time_total_s: 5163.713745355606\n",
      "  timers:\n",
      "    learn_throughput: 961.364\n",
      "    learn_time_ms: 1040.189\n",
      "    load_throughput: 56826.688\n",
      "    load_time_ms: 17.597\n",
      "    sample_throughput: 47.564\n",
      "    sample_time_ms: 21024.397\n",
      "    update_time_ms: 2.189\n",
      "  timestamp: 1633710964\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 211000\n",
      "  training_iteration: 211\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   211</td><td style=\"text-align: right;\">         5163.71</td><td style=\"text-align: right;\">211000</td><td style=\"text-align: right;\">   -0.54</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            409.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-36-27\n",
      "  done: false\n",
      "  episode_len_mean: 410.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.58\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 558\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6468134456210666\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01708197206049111\n",
      "          policy_loss: -0.04850892329381572\n",
      "          total_loss: -0.017662012784017456\n",
      "          vf_explained_var: -0.39359042048454285\n",
      "          vf_loss: 0.04299116911004401\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.209090909090904\n",
      "    ram_util_percent: 60.69393939393939\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038558827860963846\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.372433566012656\n",
      "    mean_inference_ms: 1.5770237551673076\n",
      "    mean_raw_obs_processing_ms: 1.6254317273343322\n",
      "  time_since_restore: 5186.7202615737915\n",
      "  time_this_iter_s: 23.006516218185425\n",
      "  time_total_s: 5186.7202615737915\n",
      "  timers:\n",
      "    learn_throughput: 961.385\n",
      "    learn_time_ms: 1040.166\n",
      "    load_throughput: 57127.851\n",
      "    load_time_ms: 17.505\n",
      "    sample_throughput: 47.708\n",
      "    sample_time_ms: 20960.909\n",
      "    update_time_ms: 2.197\n",
      "  timestamp: 1633710987\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 212\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   212</td><td style=\"text-align: right;\">         5186.72</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">   -0.58</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            410.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 213000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-36-47\n",
      "  done: false\n",
      "  episode_len_mean: 411.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.58\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 560\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7445227874649896\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01691609382672219\n",
      "          policy_loss: -0.055647748584548636\n",
      "          total_loss: -0.06335377279255125\n",
      "          vf_explained_var: 0.3805069327354431\n",
      "          vf_loss: 0.005457310229798572\n",
      "    num_agent_steps_sampled: 213000\n",
      "    num_agent_steps_trained: 213000\n",
      "    num_steps_sampled: 213000\n",
      "    num_steps_trained: 213000\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.935714285714276\n",
      "    ram_util_percent: 60.77857142857142\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03855703580953044\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.363790051452312\n",
      "    mean_inference_ms: 1.5769523344495382\n",
      "    mean_raw_obs_processing_ms: 1.624364574848824\n",
      "  time_since_restore: 5206.335369586945\n",
      "  time_this_iter_s: 19.615108013153076\n",
      "  time_total_s: 5206.335369586945\n",
      "  timers:\n",
      "    learn_throughput: 960.343\n",
      "    learn_time_ms: 1041.295\n",
      "    load_throughput: 57145.52\n",
      "    load_time_ms: 17.499\n",
      "    sample_throughput: 47.92\n",
      "    sample_time_ms: 20868.134\n",
      "    update_time_ms: 2.204\n",
      "  timestamp: 1633711007\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 213000\n",
      "  training_iteration: 213\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   213</td><td style=\"text-align: right;\">         5206.34</td><td style=\"text-align: right;\">213000</td><td style=\"text-align: right;\">   -0.58</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            411.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 214000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-37-08\n",
      "  done: false\n",
      "  episode_len_mean: 414.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.58\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 563\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9291408485836452\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015038454542610009\n",
      "          policy_loss: 0.03905247420900398\n",
      "          total_loss: 0.027119079149431652\n",
      "          vf_explained_var: 0.07517663389444351\n",
      "          vf_loss: 0.0035514039284963573\n",
      "    num_agent_steps_sampled: 214000\n",
      "    num_agent_steps_trained: 214000\n",
      "    num_steps_sampled: 214000\n",
      "    num_steps_trained: 214000\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.28965517241379\n",
      "    ram_util_percent: 60.78965517241379\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03855439137005688\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.350776202285207\n",
      "    mean_inference_ms: 1.5768472986781001\n",
      "    mean_raw_obs_processing_ms: 1.6227360832965458\n",
      "  time_since_restore: 5227.00620675087\n",
      "  time_this_iter_s: 20.67083716392517\n",
      "  time_total_s: 5227.00620675087\n",
      "  timers:\n",
      "    learn_throughput: 959.455\n",
      "    learn_time_ms: 1042.258\n",
      "    load_throughput: 53768.785\n",
      "    load_time_ms: 18.598\n",
      "    sample_throughput: 47.176\n",
      "    sample_time_ms: 21197.173\n",
      "    update_time_ms: 2.219\n",
      "  timestamp: 1633711028\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 214000\n",
      "  training_iteration: 214\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   214</td><td style=\"text-align: right;\">         5227.01</td><td style=\"text-align: right;\">214000</td><td style=\"text-align: right;\">   -0.58</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            414.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 215000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-37-29\n",
      "  done: false\n",
      "  episode_len_mean: 414.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 0.0\n",
      "  episode_reward_mean: -0.6\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 565\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9475851588779025\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018350366571047882\n",
      "          policy_loss: -0.05464676237251195\n",
      "          total_loss: -0.030754617053187557\n",
      "          vf_explained_var: -0.18536990880966187\n",
      "          vf_loss: 0.0387230595599653\n",
      "    num_agent_steps_sampled: 215000\n",
      "    num_agent_steps_trained: 215000\n",
      "    num_steps_sampled: 215000\n",
      "    num_steps_trained: 215000\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.32258064516129\n",
      "    ram_util_percent: 60.80645161290322\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038552664424738356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.34214873655304\n",
      "    mean_inference_ms: 1.576778396397923\n",
      "    mean_raw_obs_processing_ms: 1.6216600986348015\n",
      "  time_since_restore: 5248.2069799900055\n",
      "  time_this_iter_s: 21.200773239135742\n",
      "  time_total_s: 5248.2069799900055\n",
      "  timers:\n",
      "    learn_throughput: 959.935\n",
      "    learn_time_ms: 1041.737\n",
      "    load_throughput: 54426.172\n",
      "    load_time_ms: 18.374\n",
      "    sample_throughput: 50.767\n",
      "    sample_time_ms: 19697.668\n",
      "    update_time_ms: 2.239\n",
      "  timestamp: 1633711049\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 215000\n",
      "  training_iteration: 215\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   215</td><td style=\"text-align: right;\">         5248.21</td><td style=\"text-align: right;\">215000</td><td style=\"text-align: right;\">    -0.6</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">             414.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-37-54\n",
      "  done: false\n",
      "  episode_len_mean: 413.98\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: -0.56\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 568\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.895910045835707\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010895333507071924\n",
      "          policy_loss: -0.10083830257256826\n",
      "          total_loss: -0.04733137521478865\n",
      "          vf_explained_var: -0.43112173676490784\n",
      "          vf_loss: 0.06970814282540232\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.986111111111114\n",
      "    ram_util_percent: 60.82500000000001\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03855010969298682\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.329884515481908\n",
      "    mean_inference_ms: 1.5766780805678138\n",
      "    mean_raw_obs_processing_ms: 1.6201488673410922\n",
      "  time_since_restore: 5273.532489776611\n",
      "  time_this_iter_s: 25.325509786605835\n",
      "  time_total_s: 5273.532489776611\n",
      "  timers:\n",
      "    learn_throughput: 959.84\n",
      "    learn_time_ms: 1041.84\n",
      "    load_throughput: 54764.149\n",
      "    load_time_ms: 18.26\n",
      "    sample_throughput: 50.162\n",
      "    sample_time_ms: 19935.575\n",
      "    update_time_ms: 2.246\n",
      "  timestamp: 1633711074\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 216\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   216</td><td style=\"text-align: right;\">         5273.53</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">   -0.56</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            413.98</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 217000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-38-37\n",
      "  done: false\n",
      "  episode_len_mean: 411.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: -0.56\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 571\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.851579315132565\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017188013461675103\n",
      "          policy_loss: -0.0237536347988579\n",
      "          total_loss: -0.02768353931605816\n",
      "          vf_explained_var: -0.6210156083106995\n",
      "          vf_loss: 0.010235172603279353\n",
      "    num_agent_steps_sampled: 217000\n",
      "    num_agent_steps_trained: 217000\n",
      "    num_steps_sampled: 217000\n",
      "    num_steps_trained: 217000\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.0672131147541\n",
      "    ram_util_percent: 60.770491803278674\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03854760267626198\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.318364308292058\n",
      "    mean_inference_ms: 1.5765804093465514\n",
      "    mean_raw_obs_processing_ms: 1.6211327776534565\n",
      "  time_since_restore: 5316.283613204956\n",
      "  time_this_iter_s: 42.75112342834473\n",
      "  time_total_s: 5316.283613204956\n",
      "  timers:\n",
      "    learn_throughput: 961.201\n",
      "    learn_time_ms: 1040.365\n",
      "    load_throughput: 55459.231\n",
      "    load_time_ms: 18.031\n",
      "    sample_throughput: 44.931\n",
      "    sample_time_ms: 22256.351\n",
      "    update_time_ms: 2.228\n",
      "  timestamp: 1633711117\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 217000\n",
      "  training_iteration: 217\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   217</td><td style=\"text-align: right;\">         5316.28</td><td style=\"text-align: right;\">217000</td><td style=\"text-align: right;\">   -0.56</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            411.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 218000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-38-57\n",
      "  done: false\n",
      "  episode_len_mean: 412.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: -0.56\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 573\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9049794289800857\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008619140566857711\n",
      "          policy_loss: -0.17532596877879567\n",
      "          total_loss: -0.1866358315779103\n",
      "          vf_explained_var: 0.044381819665431976\n",
      "          vf_loss: 0.005558209726586938\n",
      "    num_agent_steps_sampled: 218000\n",
      "    num_agent_steps_trained: 218000\n",
      "    num_steps_sampled: 218000\n",
      "    num_steps_trained: 218000\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.42142857142857\n",
      "    ram_util_percent: 60.71071428571429\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03854596662448509\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.31053735071084\n",
      "    mean_inference_ms: 1.5765160295570357\n",
      "    mean_raw_obs_processing_ms: 1.6218695257824352\n",
      "  time_since_restore: 5336.104252338409\n",
      "  time_this_iter_s: 19.82063913345337\n",
      "  time_total_s: 5336.104252338409\n",
      "  timers:\n",
      "    learn_throughput: 961.672\n",
      "    learn_time_ms: 1039.856\n",
      "    load_throughput: 55701.544\n",
      "    load_time_ms: 17.953\n",
      "    sample_throughput: 45.184\n",
      "    sample_time_ms: 22131.548\n",
      "    update_time_ms: 2.236\n",
      "  timestamp: 1633711137\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 218000\n",
      "  training_iteration: 218\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   218</td><td style=\"text-align: right;\">          5336.1</td><td style=\"text-align: right;\">218000</td><td style=\"text-align: right;\">   -0.56</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">               412</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 219000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-39-19\n",
      "  done: false\n",
      "  episode_len_mean: 412.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 4.0\n",
      "  episode_reward_mean: -0.54\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 576\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.901395144727495\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02940529954112389\n",
      "          policy_loss: -0.06739611004789671\n",
      "          total_loss: 0.08129108018345303\n",
      "          vf_explained_var: 0.052619755268096924\n",
      "          vf_loss: 0.1602579291884063\n",
      "    num_agent_steps_sampled: 219000\n",
      "    num_agent_steps_trained: 219000\n",
      "    num_steps_sampled: 219000\n",
      "    num_steps_trained: 219000\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.258064516129025\n",
      "    ram_util_percent: 60.519354838709674\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03854352935726902\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.29880684608817\n",
      "    mean_inference_ms: 1.5764187071204256\n",
      "    mean_raw_obs_processing_ms: 1.622937319533999\n",
      "  time_since_restore: 5358.085408687592\n",
      "  time_this_iter_s: 21.98115634918213\n",
      "  time_total_s: 5358.085408687592\n",
      "  timers:\n",
      "    learn_throughput: 962.38\n",
      "    learn_time_ms: 1039.09\n",
      "    load_throughput: 55189.953\n",
      "    load_time_ms: 18.119\n",
      "    sample_throughput: 44.994\n",
      "    sample_time_ms: 22225.116\n",
      "    update_time_ms: 2.245\n",
      "  timestamp: 1633711159\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 219000\n",
      "  training_iteration: 219\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   219</td><td style=\"text-align: right;\">         5358.09</td><td style=\"text-align: right;\">219000</td><td style=\"text-align: right;\">   -0.54</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">             412.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-39-42\n",
      "  done: false\n",
      "  episode_len_mean: 411.78\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: -0.49\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 579\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.859428228272332\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011907475475780352\n",
      "          policy_loss: 0.087752657259504\n",
      "          total_loss: 0.09260056347896656\n",
      "          vf_explained_var: 0.26096105575561523\n",
      "          vf_loss: 0.018921068838487067\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.28529411764706\n",
      "    ram_util_percent: 60.311764705882354\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03854112520066258\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.287496721985132\n",
      "    mean_inference_ms: 1.5763233837947093\n",
      "    mean_raw_obs_processing_ms: 1.6240498180200715\n",
      "  time_since_restore: 5381.362206697464\n",
      "  time_this_iter_s: 23.276798009872437\n",
      "  time_total_s: 5381.362206697464\n",
      "  timers:\n",
      "    learn_throughput: 963.8\n",
      "    learn_time_ms: 1037.56\n",
      "    load_throughput: 55031.227\n",
      "    load_time_ms: 18.172\n",
      "    sample_throughput: 44.284\n",
      "    sample_time_ms: 22581.507\n",
      "    update_time_ms: 2.243\n",
      "  timestamp: 1633711182\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 220\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   220</td><td style=\"text-align: right;\">         5381.36</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">   -0.49</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            411.78</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 221000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-40-03\n",
      "  done: false\n",
      "  episode_len_mean: 411.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: -0.49\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 581\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8811166220241122\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01179806042237986\n",
      "          policy_loss: -0.019882534133891263\n",
      "          total_loss: 0.034896889556613235\n",
      "          vf_explained_var: 0.0766243115067482\n",
      "          vf_loss: 0.06911101409544547\n",
      "    num_agent_steps_sampled: 221000\n",
      "    num_agent_steps_trained: 221000\n",
      "    num_steps_sampled: 221000\n",
      "    num_steps_trained: 221000\n",
      "  iterations_since_restore: 221\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.24827586206897\n",
      "    ram_util_percent: 60.241379310344826\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03853955517315082\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.280232242651174\n",
      "    mean_inference_ms: 1.5762610954264955\n",
      "    mean_raw_obs_processing_ms: 1.6230068428460123\n",
      "  time_since_restore: 5401.859101295471\n",
      "  time_this_iter_s: 20.496894598007202\n",
      "  time_total_s: 5401.859101295471\n",
      "  timers:\n",
      "    learn_throughput: 962.193\n",
      "    learn_time_ms: 1039.293\n",
      "    load_throughput: 54894.453\n",
      "    load_time_ms: 18.217\n",
      "    sample_throughput: 43.953\n",
      "    sample_time_ms: 22751.617\n",
      "    update_time_ms: 2.236\n",
      "  timestamp: 1633711203\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 221000\n",
      "  training_iteration: 221\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   221</td><td style=\"text-align: right;\">         5401.86</td><td style=\"text-align: right;\">221000</td><td style=\"text-align: right;\">   -0.49</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            411.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 222000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-40-27\n",
      "  done: false\n",
      "  episode_len_mean: 407.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: -0.53\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 584\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5677875571780735\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018271048034552925\n",
      "          policy_loss: -0.034873166845904455\n",
      "          total_loss: 0.08265852077553669\n",
      "          vf_explained_var: 0.6186473965644836\n",
      "          vf_loss: 0.12627227467795213\n",
      "    num_agent_steps_sampled: 222000\n",
      "    num_agent_steps_trained: 222000\n",
      "    num_steps_sampled: 222000\n",
      "    num_steps_trained: 222000\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.13428571428572\n",
      "    ram_util_percent: 60.25714285714286\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038537273754979486\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.27010878174294\n",
      "    mean_inference_ms: 1.5761697086391186\n",
      "    mean_raw_obs_processing_ms: 1.6215814682110437\n",
      "  time_since_restore: 5426.222801208496\n",
      "  time_this_iter_s: 24.363699913024902\n",
      "  time_total_s: 5426.222801208496\n",
      "  timers:\n",
      "    learn_throughput: 963.07\n",
      "    learn_time_ms: 1038.346\n",
      "    load_throughput: 54692.666\n",
      "    load_time_ms: 18.284\n",
      "    sample_throughput: 43.691\n",
      "    sample_time_ms: 22888.21\n",
      "    update_time_ms: 2.24\n",
      "  timestamp: 1633711227\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 222000\n",
      "  training_iteration: 222\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   222</td><td style=\"text-align: right;\">         5426.22</td><td style=\"text-align: right;\">222000</td><td style=\"text-align: right;\">   -0.53</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            407.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 223000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-40-47\n",
      "  done: false\n",
      "  episode_len_mean: 407.68\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: -0.63\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 586\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8624824894799126\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01578647029581774\n",
      "          policy_loss: 0.04345868130524953\n",
      "          total_loss: 0.10628441936439938\n",
      "          vf_explained_var: -0.1018676832318306\n",
      "          vf_loss: 0.07545664731620086\n",
      "    num_agent_steps_sampled: 223000\n",
      "    num_agent_steps_trained: 223000\n",
      "    num_steps_sampled: 223000\n",
      "    num_steps_trained: 223000\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.317857142857136\n",
      "    ram_util_percent: 60.40357142857142\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03853576287316893\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.263092207600074\n",
      "    mean_inference_ms: 1.5761072311595803\n",
      "    mean_raw_obs_processing_ms: 1.6206083725434337\n",
      "  time_since_restore: 5445.791179418564\n",
      "  time_this_iter_s: 19.56837821006775\n",
      "  time_total_s: 5445.791179418564\n",
      "  timers:\n",
      "    learn_throughput: 963.495\n",
      "    learn_time_ms: 1037.888\n",
      "    load_throughput: 54854.393\n",
      "    load_time_ms: 18.23\n",
      "    sample_throughput: 43.699\n",
      "    sample_time_ms: 22884.058\n",
      "    update_time_ms: 2.238\n",
      "  timestamp: 1633711247\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 223000\n",
      "  training_iteration: 223\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   223</td><td style=\"text-align: right;\">         5445.79</td><td style=\"text-align: right;\">223000</td><td style=\"text-align: right;\">   -0.63</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            407.68</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-41-10\n",
      "  done: false\n",
      "  episode_len_mean: 406.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: -0.65\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 589\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8810475521617465\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016604036114689864\n",
      "          policy_loss: -0.0801762249527706\n",
      "          total_loss: 0.22793832366458244\n",
      "          vf_explained_var: 0.5855964422225952\n",
      "          vf_loss: 0.3206206783445345\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.161764705882355\n",
      "    ram_util_percent: 60.51176470588236\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03853349066793201\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.252803684632934\n",
      "    mean_inference_ms: 1.5760134299891515\n",
      "    mean_raw_obs_processing_ms: 1.619287141011283\n",
      "  time_since_restore: 5469.656794309616\n",
      "  time_this_iter_s: 23.865614891052246\n",
      "  time_total_s: 5469.656794309616\n",
      "  timers:\n",
      "    learn_throughput: 964.416\n",
      "    learn_time_ms: 1036.897\n",
      "    load_throughput: 55118.442\n",
      "    load_time_ms: 18.143\n",
      "    sample_throughput: 43.095\n",
      "    sample_time_ms: 23204.617\n",
      "    update_time_ms: 2.236\n",
      "  timestamp: 1633711270\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 224\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   224</td><td style=\"text-align: right;\">         5469.66</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">   -0.65</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            406.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 225000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-41-31\n",
      "  done: false\n",
      "  episode_len_mean: 406.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: -0.61\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 592\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.055989701218075\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009056903396950835\n",
      "          policy_loss: 0.07728389021423128\n",
      "          total_loss: 0.07544606971657938\n",
      "          vf_explained_var: -0.33321937918663025\n",
      "          vf_loss: 0.0152832825978597\n",
      "    num_agent_steps_sampled: 225000\n",
      "    num_agent_steps_trained: 225000\n",
      "    num_steps_sampled: 225000\n",
      "    num_steps_trained: 225000\n",
      "  iterations_since_restore: 225\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.39333333333333\n",
      "    ram_util_percent: 60.71333333333334\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03853122585648856\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.242357700911747\n",
      "    mean_inference_ms: 1.5759184289779258\n",
      "    mean_raw_obs_processing_ms: 1.618017184113853\n",
      "  time_since_restore: 5490.688477277756\n",
      "  time_this_iter_s: 21.03168296813965\n",
      "  time_total_s: 5490.688477277756\n",
      "  timers:\n",
      "    learn_throughput: 965.774\n",
      "    learn_time_ms: 1035.439\n",
      "    load_throughput: 54655.677\n",
      "    load_time_ms: 18.296\n",
      "    sample_throughput: 43.124\n",
      "    sample_time_ms: 23189.017\n",
      "    update_time_ms: 2.24\n",
      "  timestamp: 1633711291\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 225000\n",
      "  training_iteration: 225\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   225</td><td style=\"text-align: right;\">         5490.69</td><td style=\"text-align: right;\">225000</td><td style=\"text-align: right;\">   -0.61</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            406.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 226000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-41-55\n",
      "  done: false\n",
      "  episode_len_mean: 404.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: -0.73\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 595\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8565903902053833\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012045721889927958\n",
      "          policy_loss: 0.03179726716544893\n",
      "          total_loss: 0.2190544192989667\n",
      "          vf_explained_var: 0.7154343724250793\n",
      "          vf_loss: 0.20124944316016302\n",
      "    num_agent_steps_sampled: 226000\n",
      "    num_agent_steps_trained: 226000\n",
      "    num_steps_sampled: 226000\n",
      "    num_steps_trained: 226000\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.21764705882352\n",
      "    ram_util_percent: 60.82647058823529\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03852893065502967\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.231781645278055\n",
      "    mean_inference_ms: 1.5758224484119825\n",
      "    mean_raw_obs_processing_ms: 1.6168789897672533\n",
      "  time_since_restore: 5514.288561820984\n",
      "  time_this_iter_s: 23.60008454322815\n",
      "  time_total_s: 5514.288561820984\n",
      "  timers:\n",
      "    learn_throughput: 964.064\n",
      "    learn_time_ms: 1037.276\n",
      "    load_throughput: 54810.738\n",
      "    load_time_ms: 18.245\n",
      "    sample_throughput: 43.451\n",
      "    sample_time_ms: 23014.687\n",
      "    update_time_ms: 2.243\n",
      "  timestamp: 1633711315\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 226000\n",
      "  training_iteration: 226\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   226</td><td style=\"text-align: right;\">         5514.29</td><td style=\"text-align: right;\">226000</td><td style=\"text-align: right;\">   -0.73</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">             404.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 227000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-42-20\n",
      "  done: false\n",
      "  episode_len_mean: 404.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.64\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 598\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.876578688621521\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016889471054635764\n",
      "          policy_loss: 0.053758107125759125\n",
      "          total_loss: 0.18559402165313563\n",
      "          vf_explained_var: 0.7790495157241821\n",
      "          vf_loss: 0.1441889791438977\n",
      "    num_agent_steps_sampled: 227000\n",
      "    num_agent_steps_trained: 227000\n",
      "    num_steps_sampled: 227000\n",
      "    num_steps_trained: 227000\n",
      "  iterations_since_restore: 227\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.17777777777778\n",
      "    ram_util_percent: 60.816666666666684\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03852662235980836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.221291086312704\n",
      "    mean_inference_ms: 1.5757256484139663\n",
      "    mean_raw_obs_processing_ms: 1.6157890844470821\n",
      "  time_since_restore: 5539.540291547775\n",
      "  time_this_iter_s: 25.251729726791382\n",
      "  time_total_s: 5539.540291547775\n",
      "  timers:\n",
      "    learn_throughput: 962.529\n",
      "    learn_time_ms: 1038.93\n",
      "    load_throughput: 54353.104\n",
      "    load_time_ms: 18.398\n",
      "    sample_throughput: 47.03\n",
      "    sample_time_ms: 21262.956\n",
      "    update_time_ms: 2.237\n",
      "  timestamp: 1633711340\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 227000\n",
      "  training_iteration: 227\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   227</td><td style=\"text-align: right;\">         5539.54</td><td style=\"text-align: right;\">227000</td><td style=\"text-align: right;\">   -0.64</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            404.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-43-04\n",
      "  done: false\n",
      "  episode_len_mean: 401.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.62\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 601\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8527931213378905\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00998217138412719\n",
      "          policy_loss: -0.09111714793576134\n",
      "          total_loss: -0.0788366542922126\n",
      "          vf_explained_var: 0.40657779574394226\n",
      "          vf_loss: 0.027018319349735975\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.91612903225807\n",
      "    ram_util_percent: 60.698387096774184\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03852431687632686\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.211448572804077\n",
      "    mean_inference_ms: 1.5756302004356355\n",
      "    mean_raw_obs_processing_ms: 1.6169298038962925\n",
      "  time_since_restore: 5583.0891456604\n",
      "  time_this_iter_s: 43.54885411262512\n",
      "  time_total_s: 5583.0891456604\n",
      "  timers:\n",
      "    learn_throughput: 962.615\n",
      "    learn_time_ms: 1038.837\n",
      "    load_throughput: 53748.735\n",
      "    load_time_ms: 18.605\n",
      "    sample_throughput: 42.309\n",
      "    sample_time_ms: 23635.647\n",
      "    update_time_ms: 2.239\n",
      "  timestamp: 1633711384\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 228\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   228</td><td style=\"text-align: right;\">         5583.09</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">   -0.62</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            401.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 229000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-43-28\n",
      "  done: false\n",
      "  episode_len_mean: 397.26\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.58\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 604\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9647395862473382\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015198395936323718\n",
      "          policy_loss: -0.09301686849859025\n",
      "          total_loss: -0.0071224066325359875\n",
      "          vf_explained_var: 0.33109092712402344\n",
      "          vf_loss: 0.09977121698256168\n",
      "    num_agent_steps_sampled: 229000\n",
      "    num_agent_steps_trained: 229000\n",
      "    num_steps_sampled: 229000\n",
      "    num_steps_trained: 229000\n",
      "  iterations_since_restore: 229\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.08\n",
      "    ram_util_percent: 60.70000000000001\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038521974041761345\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.20232868359969\n",
      "    mean_inference_ms: 1.5755357252990771\n",
      "    mean_raw_obs_processing_ms: 1.6182638134688967\n",
      "  time_since_restore: 5607.628590583801\n",
      "  time_this_iter_s: 24.53944492340088\n",
      "  time_total_s: 5607.628590583801\n",
      "  timers:\n",
      "    learn_throughput: 961.031\n",
      "    learn_time_ms: 1040.549\n",
      "    load_throughput: 54195.296\n",
      "    load_time_ms: 18.452\n",
      "    sample_throughput: 41.859\n",
      "    sample_time_ms: 23889.914\n",
      "    update_time_ms: 2.248\n",
      "  timestamp: 1633711408\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 229000\n",
      "  training_iteration: 229\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   229</td><td style=\"text-align: right;\">         5607.63</td><td style=\"text-align: right;\">229000</td><td style=\"text-align: right;\">   -0.58</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            397.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 230000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-43-54\n",
      "  done: false\n",
      "  episode_len_mean: 396.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.57\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 607\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8147312813334995\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015893873904147544\n",
      "          policy_loss: -0.10019887275993825\n",
      "          total_loss: 0.03933009525967969\n",
      "          vf_explained_var: 0.23511174321174622\n",
      "          vf_loss: 0.15164157792977576\n",
      "    num_agent_steps_sampled: 230000\n",
      "    num_agent_steps_trained: 230000\n",
      "    num_steps_sampled: 230000\n",
      "    num_steps_trained: 230000\n",
      "  iterations_since_restore: 230\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.26216216216218\n",
      "    ram_util_percent: 60.61351351351351\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03851962976620546\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.193554494739345\n",
      "    mean_inference_ms: 1.5754419432547784\n",
      "    mean_raw_obs_processing_ms: 1.6196378492930006\n",
      "  time_since_restore: 5633.291044712067\n",
      "  time_this_iter_s: 25.66245412826538\n",
      "  time_total_s: 5633.291044712067\n",
      "  timers:\n",
      "    learn_throughput: 958.636\n",
      "    learn_time_ms: 1043.149\n",
      "    load_throughput: 53633.681\n",
      "    load_time_ms: 18.645\n",
      "    sample_throughput: 41.45\n",
      "    sample_time_ms: 24125.668\n",
      "    update_time_ms: 2.255\n",
      "  timestamp: 1633711434\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 230000\n",
      "  training_iteration: 230\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   230</td><td style=\"text-align: right;\">         5633.29</td><td style=\"text-align: right;\">230000</td><td style=\"text-align: right;\">   -0.57</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            396.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 231000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-44-21\n",
      "  done: false\n",
      "  episode_len_mean: 392.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.62\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 610\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6818129195107354\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018798413445011024\n",
      "          policy_loss: 0.0602013542316854\n",
      "          total_loss: 0.3516418524293436\n",
      "          vf_explained_var: 0.21419993042945862\n",
      "          vf_loss: 0.3011210998520255\n",
      "    num_agent_steps_sampled: 231000\n",
      "    num_agent_steps_trained: 231000\n",
      "    num_steps_sampled: 231000\n",
      "    num_steps_trained: 231000\n",
      "  iterations_since_restore: 231\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.31052631578948\n",
      "    ram_util_percent: 60.39210526315792\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038517291291983294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.185951331793145\n",
      "    mean_inference_ms: 1.5753522765362171\n",
      "    mean_raw_obs_processing_ms: 1.6193554112025166\n",
      "  time_since_restore: 5660.2014701366425\n",
      "  time_this_iter_s: 26.910425424575806\n",
      "  time_total_s: 5660.2014701366425\n",
      "  timers:\n",
      "    learn_throughput: 960.185\n",
      "    learn_time_ms: 1041.466\n",
      "    load_throughput: 53209.584\n",
      "    load_time_ms: 18.794\n",
      "    sample_throughput: 40.374\n",
      "    sample_time_ms: 24768.585\n",
      "    update_time_ms: 2.252\n",
      "  timestamp: 1633711461\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 231000\n",
      "  training_iteration: 231\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   231</td><td style=\"text-align: right;\">          5660.2</td><td style=\"text-align: right;\">231000</td><td style=\"text-align: right;\">   -0.62</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            392.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-44-42\n",
      "  done: false\n",
      "  episode_len_mean: 391.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.67\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 612\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7011392765574984\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.026788343345382347\n",
      "          policy_loss: -0.034251856638325584\n",
      "          total_loss: 0.33849589741892283\n",
      "          vf_explained_var: 0.8358519077301025\n",
      "          vf_loss: 0.3795879511369599\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.36\n",
      "    ram_util_percent: 60.33000000000001\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03851576091695632\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.180854692205294\n",
      "    mean_inference_ms: 1.5752932818573253\n",
      "    mean_raw_obs_processing_ms: 1.618654937507352\n",
      "  time_since_restore: 5681.211438894272\n",
      "  time_this_iter_s: 21.009968757629395\n",
      "  time_total_s: 5681.211438894272\n",
      "  timers:\n",
      "    learn_throughput: 959.115\n",
      "    learn_time_ms: 1042.628\n",
      "    load_throughput: 53562.518\n",
      "    load_time_ms: 18.67\n",
      "    sample_throughput: 40.93\n",
      "    sample_time_ms: 24432.193\n",
      "    update_time_ms: 2.239\n",
      "  timestamp: 1633711482\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 232\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   232</td><td style=\"text-align: right;\">         5681.21</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">   -0.67</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">            391.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 233000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-45-06\n",
      "  done: false\n",
      "  episode_len_mean: 391.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.67\n",
      "  episode_reward_min: -10.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 615\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4873423059781392\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012992772660528888\n",
      "          policy_loss: -0.17804868585533565\n",
      "          total_loss: 0.12697169677250916\n",
      "          vf_explained_var: 0.8173605799674988\n",
      "          vf_loss: 0.3124940147002538\n",
      "    num_agent_steps_sampled: 233000\n",
      "    num_agent_steps_trained: 233000\n",
      "    num_steps_sampled: 233000\n",
      "    num_steps_trained: 233000\n",
      "  iterations_since_restore: 233\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.141176470588235\n",
      "    ram_util_percent: 60.35882352941176\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03851348949942412\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.17350210066728\n",
      "    mean_inference_ms: 1.5752058716401658\n",
      "    mean_raw_obs_processing_ms: 1.6175782321233292\n",
      "  time_since_restore: 5704.93926692009\n",
      "  time_this_iter_s: 23.72782802581787\n",
      "  time_total_s: 5704.93926692009\n",
      "  timers:\n",
      "    learn_throughput: 960.246\n",
      "    learn_time_ms: 1041.4\n",
      "    load_throughput: 53389.681\n",
      "    load_time_ms: 18.73\n",
      "    sample_throughput: 40.243\n",
      "    sample_time_ms: 24849.294\n",
      "    update_time_ms: 2.25\n",
      "  timestamp: 1633711506\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 233000\n",
      "  training_iteration: 233\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   233</td><td style=\"text-align: right;\">         5704.94</td><td style=\"text-align: right;\">233000</td><td style=\"text-align: right;\">   -0.67</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -10</td><td style=\"text-align: right;\">             391.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 234000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-45-24\n",
      "  done: false\n",
      "  episode_len_mean: 391.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.8\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 617\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.017648312780592\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016783737684333923\n",
      "          policy_loss: -0.14429269458891616\n",
      "          total_loss: 0.0679180816643768\n",
      "          vf_explained_var: 0.7145189642906189\n",
      "          vf_loss: 0.22282839661671056\n",
      "    num_agent_steps_sampled: 234000\n",
      "    num_agent_steps_trained: 234000\n",
      "    num_steps_sampled: 234000\n",
      "    num_steps_trained: 234000\n",
      "  iterations_since_restore: 234\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.63076923076923\n",
      "    ram_util_percent: 60.54230769230769\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038512023976052544\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.168669549555833\n",
      "    mean_inference_ms: 1.5751490493055036\n",
      "    mean_raw_obs_processing_ms: 1.6169404539907208\n",
      "  time_since_restore: 5723.497186660767\n",
      "  time_this_iter_s: 18.55791974067688\n",
      "  time_total_s: 5723.497186660767\n",
      "  timers:\n",
      "    learn_throughput: 960.236\n",
      "    learn_time_ms: 1041.41\n",
      "    load_throughput: 53180.911\n",
      "    load_time_ms: 18.804\n",
      "    sample_throughput: 41.121\n",
      "    sample_time_ms: 24318.445\n",
      "    update_time_ms: 2.252\n",
      "  timestamp: 1633711524\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 234000\n",
      "  training_iteration: 234\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   234</td><td style=\"text-align: right;\">          5723.5</td><td style=\"text-align: right;\">234000</td><td style=\"text-align: right;\">    -0.8</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            391.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 235000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-45-46\n",
      "  done: false\n",
      "  episode_len_mean: 392.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.69\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 620\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8345951941278247\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018246121394657946\n",
      "          policy_loss: -0.07761490320165952\n",
      "          total_loss: 0.2510639583071073\n",
      "          vf_explained_var: 0.6998655200004578\n",
      "          vf_loss: 0.33663307713965573\n",
      "    num_agent_steps_sampled: 235000\n",
      "    num_agent_steps_trained: 235000\n",
      "    num_steps_sampled: 235000\n",
      "    num_steps_trained: 235000\n",
      "  iterations_since_restore: 235\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.54333333333333\n",
      "    ram_util_percent: 60.68333333333335\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03850982655940824\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.1613512094978\n",
      "    mean_inference_ms: 1.575064589047995\n",
      "    mean_raw_obs_processing_ms: 1.6159590740771224\n",
      "  time_since_restore: 5744.528066635132\n",
      "  time_this_iter_s: 21.030879974365234\n",
      "  time_total_s: 5744.528066635132\n",
      "  timers:\n",
      "    learn_throughput: 959.772\n",
      "    learn_time_ms: 1041.914\n",
      "    load_throughput: 53201.013\n",
      "    load_time_ms: 18.797\n",
      "    sample_throughput: 41.122\n",
      "    sample_time_ms: 24317.927\n",
      "    update_time_ms: 2.217\n",
      "  timestamp: 1633711546\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 235000\n",
      "  training_iteration: 235\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   235</td><td style=\"text-align: right;\">         5744.53</td><td style=\"text-align: right;\">235000</td><td style=\"text-align: right;\">   -0.69</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            392.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-46-08\n",
      "  done: false\n",
      "  episode_len_mean: 390.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.63\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 623\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6439431620968712\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008985174371432928\n",
      "          policy_loss: 0.05297842918015602\n",
      "          total_loss: 0.25325900233971577\n",
      "          vf_explained_var: 0.7879564762115479\n",
      "          vf_loss: 0.2116026633315616\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "  iterations_since_restore: 236\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.25151515151515\n",
      "    ram_util_percent: 60.77575757575758\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03850772216430611\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.154572538571802\n",
      "    mean_inference_ms: 1.5749823658464035\n",
      "    mean_raw_obs_processing_ms: 1.6150986735906327\n",
      "  time_since_restore: 5767.160455226898\n",
      "  time_this_iter_s: 22.632388591766357\n",
      "  time_total_s: 5767.160455226898\n",
      "  timers:\n",
      "    learn_throughput: 960.495\n",
      "    learn_time_ms: 1041.13\n",
      "    load_throughput: 52865.802\n",
      "    load_time_ms: 18.916\n",
      "    sample_throughput: 41.285\n",
      "    sample_time_ms: 24221.824\n",
      "    update_time_ms: 2.215\n",
      "  timestamp: 1633711568\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 236\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   236</td><td style=\"text-align: right;\">         5767.16</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">   -0.63</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            390.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 237000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-46-31\n",
      "  done: false\n",
      "  episode_len_mean: 389.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.76\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 625\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.580175953441196\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015631738460364926\n",
      "          policy_loss: -0.05812002279692226\n",
      "          total_loss: 0.26530669720636474\n",
      "          vf_explained_var: 0.7134262323379517\n",
      "          vf_loss: 0.3303257140848372\n",
      "    num_agent_steps_sampled: 237000\n",
      "    num_agent_steps_trained: 237000\n",
      "    num_steps_sampled: 237000\n",
      "    num_steps_trained: 237000\n",
      "  iterations_since_restore: 237\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.31515151515151\n",
      "    ram_util_percent: 60.81818181818182\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03850634458564348\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.150412126296484\n",
      "    mean_inference_ms: 1.5749290320559057\n",
      "    mean_raw_obs_processing_ms: 1.614554286819007\n",
      "  time_since_restore: 5790.132880449295\n",
      "  time_this_iter_s: 22.97242522239685\n",
      "  time_total_s: 5790.132880449295\n",
      "  timers:\n",
      "    learn_throughput: 961.217\n",
      "    learn_time_ms: 1040.348\n",
      "    load_throughput: 52930.849\n",
      "    load_time_ms: 18.893\n",
      "    sample_throughput: 41.676\n",
      "    sample_time_ms: 23994.688\n",
      "    update_time_ms: 2.221\n",
      "  timestamp: 1633711591\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 237000\n",
      "  training_iteration: 237\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   237</td><td style=\"text-align: right;\">         5790.13</td><td style=\"text-align: right;\">237000</td><td style=\"text-align: right;\">   -0.76</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            389.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 238000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-46-52\n",
      "  done: false\n",
      "  episode_len_mean: 387.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.78\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 628\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8380936834547255\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010603259492902773\n",
      "          policy_loss: -0.029681210302644305\n",
      "          total_loss: 0.14243688020441267\n",
      "          vf_explained_var: 0.683272659778595\n",
      "          vf_loss: 0.18446013507329756\n",
      "    num_agent_steps_sampled: 238000\n",
      "    num_agent_steps_trained: 238000\n",
      "    num_steps_sampled: 238000\n",
      "    num_steps_trained: 238000\n",
      "  iterations_since_restore: 238\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.32333333333334\n",
      "    ram_util_percent: 60.830000000000005\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03850430206437992\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.14463702201768\n",
      "    mean_inference_ms: 1.574849122964919\n",
      "    mean_raw_obs_processing_ms: 1.6138575128031518\n",
      "  time_since_restore: 5811.26921081543\n",
      "  time_this_iter_s: 21.136330366134644\n",
      "  time_total_s: 5811.26921081543\n",
      "  timers:\n",
      "    learn_throughput: 960.256\n",
      "    learn_time_ms: 1041.389\n",
      "    load_throughput: 53421.097\n",
      "    load_time_ms: 18.719\n",
      "    sample_throughput: 45.972\n",
      "    sample_time_ms: 21752.553\n",
      "    update_time_ms: 2.222\n",
      "  timestamp: 1633711612\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 238000\n",
      "  training_iteration: 238\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   238</td><td style=\"text-align: right;\">         5811.27</td><td style=\"text-align: right;\">238000</td><td style=\"text-align: right;\">   -0.78</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            387.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 239000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-47-30\n",
      "  done: false\n",
      "  episode_len_mean: 387.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.72\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 630\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.049943443139394\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014308629651735119\n",
      "          policy_loss: -0.132966182132562\n",
      "          total_loss: 0.08910452988412645\n",
      "          vf_explained_var: 0.28676483035087585\n",
      "          vf_loss: 0.23442092945592272\n",
      "    num_agent_steps_sampled: 239000\n",
      "    num_agent_steps_trained: 239000\n",
      "    num_steps_sampled: 239000\n",
      "    num_steps_trained: 239000\n",
      "  iterations_since_restore: 239\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.69074074074074\n",
      "    ram_util_percent: 60.85555555555557\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038502965858436065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.140879578697014\n",
      "    mean_inference_ms: 1.5747958828531592\n",
      "    mean_raw_obs_processing_ms: 1.6147697075031757\n",
      "  time_since_restore: 5849.2039296627045\n",
      "  time_this_iter_s: 37.93471884727478\n",
      "  time_total_s: 5849.2039296627045\n",
      "  timers:\n",
      "    learn_throughput: 961.933\n",
      "    learn_time_ms: 1039.573\n",
      "    load_throughput: 52834.969\n",
      "    load_time_ms: 18.927\n",
      "    sample_throughput: 43.302\n",
      "    sample_time_ms: 23093.71\n",
      "    update_time_ms: 2.212\n",
      "  timestamp: 1633711650\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 239000\n",
      "  training_iteration: 239\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   239</td><td style=\"text-align: right;\">          5849.2</td><td style=\"text-align: right;\">239000</td><td style=\"text-align: right;\">   -0.72</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            387.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-47-59\n",
      "  done: false\n",
      "  episode_len_mean: 383.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.6\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 634\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1310551206270854\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01371288642161201\n",
      "          policy_loss: -0.047220775816175675\n",
      "          total_loss: 0.30060601234436035\n",
      "          vf_explained_var: 0.8321429491043091\n",
      "          vf_loss: 0.351327418618732\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.86097560975609\n",
      "    ram_util_percent: 60.79512195121951\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038500414241916386\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.135124266809335\n",
      "    mean_inference_ms: 1.5746960838745299\n",
      "    mean_raw_obs_processing_ms: 1.6167900333363152\n",
      "  time_since_restore: 5877.66033244133\n",
      "  time_this_iter_s: 28.45640277862549\n",
      "  time_total_s: 5877.66033244133\n",
      "  timers:\n",
      "    learn_throughput: 963.66\n",
      "    learn_time_ms: 1037.711\n",
      "    load_throughput: 53351.243\n",
      "    load_time_ms: 18.744\n",
      "    sample_throughput: 42.78\n",
      "    sample_time_ms: 23375.143\n",
      "    update_time_ms: 2.22\n",
      "  timestamp: 1633711679\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 240\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   240</td><td style=\"text-align: right;\">         5877.66</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">    -0.6</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">             383.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 241000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-48-27\n",
      "  done: false\n",
      "  episode_len_mean: 379.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.49\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 637\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4080556803279454\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012955213830603699\n",
      "          policy_loss: -0.11946879774332046\n",
      "          total_loss: 0.10343332896526489\n",
      "          vf_explained_var: 0.8497580885887146\n",
      "          vf_loss: 0.2296042886045244\n",
      "    num_agent_steps_sampled: 241000\n",
      "    num_agent_steps_trained: 241000\n",
      "    num_steps_sampled: 241000\n",
      "    num_steps_trained: 241000\n",
      "  iterations_since_restore: 241\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.29\n",
      "    ram_util_percent: 60.495000000000005\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03849853839148783\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.131699552971455\n",
      "    mean_inference_ms: 1.574624716015351\n",
      "    mean_raw_obs_processing_ms: 1.6183734124205742\n",
      "  time_since_restore: 5906.104640245438\n",
      "  time_this_iter_s: 28.444307804107666\n",
      "  time_total_s: 5906.104640245438\n",
      "  timers:\n",
      "    learn_throughput: 963.925\n",
      "    learn_time_ms: 1037.425\n",
      "    load_throughput: 53328.858\n",
      "    load_time_ms: 18.752\n",
      "    sample_throughput: 42.501\n",
      "    sample_time_ms: 23528.821\n",
      "    update_time_ms: 2.216\n",
      "  timestamp: 1633711707\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 241000\n",
      "  training_iteration: 241\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   241</td><td style=\"text-align: right;\">          5906.1</td><td style=\"text-align: right;\">241000</td><td style=\"text-align: right;\">   -0.49</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            379.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 242000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-48-45\n",
      "  done: false\n",
      "  episode_len_mean: 378.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.49\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 639\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1878535853491887\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012939603733027915\n",
      "          policy_loss: -0.0309298704067866\n",
      "          total_loss: 0.028567548634277448\n",
      "          vf_explained_var: 0.4372209310531616\n",
      "          vf_loss: 0.07400644538510177\n",
      "    num_agent_steps_sampled: 242000\n",
      "    num_agent_steps_trained: 242000\n",
      "    num_steps_sampled: 242000\n",
      "    num_steps_trained: 242000\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.449999999999996\n",
      "    ram_util_percent: 60.3423076923077\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03849729885952314\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.129634131049325\n",
      "    mean_inference_ms: 1.5745780417619166\n",
      "    mean_raw_obs_processing_ms: 1.6194972289528846\n",
      "  time_since_restore: 5923.931238889694\n",
      "  time_this_iter_s: 17.826598644256592\n",
      "  time_total_s: 5923.931238889694\n",
      "  timers:\n",
      "    learn_throughput: 964.221\n",
      "    learn_time_ms: 1037.107\n",
      "    load_throughput: 56373.993\n",
      "    load_time_ms: 17.739\n",
      "    sample_throughput: 43.082\n",
      "    sample_time_ms: 23211.724\n",
      "    update_time_ms: 2.31\n",
      "  timestamp: 1633711725\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 242000\n",
      "  training_iteration: 242\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   242</td><td style=\"text-align: right;\">         5923.93</td><td style=\"text-align: right;\">242000</td><td style=\"text-align: right;\">   -0.49</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            378.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 243000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-49-14\n",
      "  done: false\n",
      "  episode_len_mean: 373.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.35\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 643\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2171195069948832\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006592684298368094\n",
      "          policy_loss: -0.09504106636676524\n",
      "          total_loss: 0.17169238535894288\n",
      "          vf_explained_var: 0.828678548336029\n",
      "          vf_loss: 0.27514990650945237\n",
      "    num_agent_steps_sampled: 243000\n",
      "    num_agent_steps_trained: 243000\n",
      "    num_steps_sampled: 243000\n",
      "    num_steps_trained: 243000\n",
      "  iterations_since_restore: 243\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.0675\n",
      "    ram_util_percent: 60.33\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038494881584245653\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.126936750004546\n",
      "    mean_inference_ms: 1.5744891360109867\n",
      "    mean_raw_obs_processing_ms: 1.6188540423072104\n",
      "  time_since_restore: 5952.464692592621\n",
      "  time_this_iter_s: 28.533453702926636\n",
      "  time_total_s: 5952.464692592621\n",
      "  timers:\n",
      "    learn_throughput: 962.449\n",
      "    learn_time_ms: 1039.017\n",
      "    load_throughput: 56265.925\n",
      "    load_time_ms: 17.773\n",
      "    sample_throughput: 42.211\n",
      "    sample_time_ms: 23690.349\n",
      "    update_time_ms: 2.305\n",
      "  timestamp: 1633711754\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 243000\n",
      "  training_iteration: 243\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   243</td><td style=\"text-align: right;\">         5952.46</td><td style=\"text-align: right;\">243000</td><td style=\"text-align: right;\">   -0.35</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            373.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-49-33\n",
      "  done: false\n",
      "  episode_len_mean: 373.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.36\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 645\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0406707339816625\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012658612691448095\n",
      "          policy_loss: -0.10617785942223337\n",
      "          total_loss: 0.20115311245123546\n",
      "          vf_explained_var: 0.26404425501823425\n",
      "          vf_loss: 0.3205282046770056\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_agent_steps_trained: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.45357142857143\n",
      "    ram_util_percent: 60.464285714285715\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03849366600289495\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.125454756087507\n",
      "    mean_inference_ms: 1.5744447855112162\n",
      "    mean_raw_obs_processing_ms: 1.618556201258251\n",
      "  time_since_restore: 5971.712441205978\n",
      "  time_this_iter_s: 19.247748613357544\n",
      "  time_total_s: 5971.712441205978\n",
      "  timers:\n",
      "    learn_throughput: 960.644\n",
      "    learn_time_ms: 1040.969\n",
      "    load_throughput: 56714.886\n",
      "    load_time_ms: 17.632\n",
      "    sample_throughput: 42.092\n",
      "    sample_time_ms: 23757.525\n",
      "    update_time_ms: 2.299\n",
      "  timestamp: 1633711773\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 244\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   244</td><td style=\"text-align: right;\">         5971.71</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">   -0.36</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            373.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 245000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-49-56\n",
      "  done: false\n",
      "  episode_len_mean: 372.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.38\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 648\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7974757300482855\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019668378792443836\n",
      "          policy_loss: -0.06362537137336201\n",
      "          total_loss: 0.3461078782462411\n",
      "          vf_explained_var: 0.7757164239883423\n",
      "          vf_loss: 0.4165062384472953\n",
      "    num_agent_steps_sampled: 245000\n",
      "    num_agent_steps_trained: 245000\n",
      "    num_steps_sampled: 245000\n",
      "    num_steps_trained: 245000\n",
      "  iterations_since_restore: 245\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.51212121212121\n",
      "    ram_util_percent: 60.56666666666666\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03849185136812901\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.12372586128979\n",
      "    mean_inference_ms: 1.574381038683823\n",
      "    mean_raw_obs_processing_ms: 1.6181831667004551\n",
      "  time_since_restore: 5995.158694267273\n",
      "  time_this_iter_s: 23.446253061294556\n",
      "  time_total_s: 5995.158694267273\n",
      "  timers:\n",
      "    learn_throughput: 959.826\n",
      "    learn_time_ms: 1041.855\n",
      "    load_throughput: 56003.13\n",
      "    load_time_ms: 17.856\n",
      "    sample_throughput: 41.67\n",
      "    sample_time_ms: 23997.922\n",
      "    update_time_ms: 2.318\n",
      "  timestamp: 1633711796\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 245000\n",
      "  training_iteration: 245\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   245</td><td style=\"text-align: right;\">         5995.16</td><td style=\"text-align: right;\">245000</td><td style=\"text-align: right;\">   -0.38</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            372.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 246000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-50-19\n",
      "  done: false\n",
      "  episode_len_mean: 371.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.3\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 651\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8975734935866462\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017017376975614024\n",
      "          policy_loss: -0.1731310925549931\n",
      "          total_loss: 0.12768534090783862\n",
      "          vf_explained_var: 0.28759440779685974\n",
      "          vf_loss: 0.31010023990852964\n",
      "    num_agent_steps_sampled: 246000\n",
      "    num_agent_steps_trained: 246000\n",
      "    num_steps_sampled: 246000\n",
      "    num_steps_trained: 246000\n",
      "  iterations_since_restore: 246\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.22121212121211\n",
      "    ram_util_percent: 60.727272727272734\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03849005968465044\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.12226597650311\n",
      "    mean_inference_ms: 1.5743184845256055\n",
      "    mean_raw_obs_processing_ms: 1.6178476513684532\n",
      "  time_since_restore: 6018.04564499855\n",
      "  time_this_iter_s: 22.886950731277466\n",
      "  time_total_s: 6018.04564499855\n",
      "  timers:\n",
      "    learn_throughput: 960.286\n",
      "    learn_time_ms: 1041.356\n",
      "    load_throughput: 56486.28\n",
      "    load_time_ms: 17.703\n",
      "    sample_throughput: 41.625\n",
      "    sample_time_ms: 24024.034\n",
      "    update_time_ms: 2.312\n",
      "  timestamp: 1633711819\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 246000\n",
      "  training_iteration: 246\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   246</td><td style=\"text-align: right;\">         6018.05</td><td style=\"text-align: right;\">246000</td><td style=\"text-align: right;\">    -0.3</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            371.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 247000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-50-40\n",
      "  done: false\n",
      "  episode_len_mean: 371.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.29\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 653\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9911890321307713\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01573189729968413\n",
      "          policy_loss: -0.05977326950265301\n",
      "          total_loss: 0.21039259785579312\n",
      "          vf_explained_var: 0.7785248756408691\n",
      "          vf_loss: 0.28111794735822415\n",
      "    num_agent_steps_sampled: 247000\n",
      "    num_agent_steps_trained: 247000\n",
      "    num_steps_sampled: 247000\n",
      "    num_steps_trained: 247000\n",
      "  iterations_since_restore: 247\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.400000000000006\n",
      "    ram_util_percent: 60.841379310344834\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03848892695451713\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.121457192153983\n",
      "    mean_inference_ms: 1.5742776702386487\n",
      "    mean_raw_obs_processing_ms: 1.6176927995709907\n",
      "  time_since_restore: 6038.413847923279\n",
      "  time_this_iter_s: 20.368202924728394\n",
      "  time_total_s: 6038.413847923279\n",
      "  timers:\n",
      "    learn_throughput: 959.455\n",
      "    learn_time_ms: 1042.259\n",
      "    load_throughput: 56441.129\n",
      "    load_time_ms: 17.718\n",
      "    sample_throughput: 42.083\n",
      "    sample_time_ms: 23762.677\n",
      "    update_time_ms: 2.328\n",
      "  timestamp: 1633711840\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 247000\n",
      "  training_iteration: 247\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   247</td><td style=\"text-align: right;\">         6038.41</td><td style=\"text-align: right;\">247000</td><td style=\"text-align: right;\">   -0.29</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            371.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-51-00\n",
      "  done: false\n",
      "  episode_len_mean: 370.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.16\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 655\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.772161340713501\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019692503753865588\n",
      "          policy_loss: -0.12998621857000722\n",
      "          total_loss: 0.3163806205822362\n",
      "          vf_explained_var: 0.2753642797470093\n",
      "          vf_loss: 0.4528729597727458\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "  iterations_since_restore: 248\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.21\n",
      "    ram_util_percent: 60.86666666666668\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03848781016011361\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.12086731877793\n",
      "    mean_inference_ms: 1.574237744972103\n",
      "    mean_raw_obs_processing_ms: 1.6175597706556153\n",
      "  time_since_restore: 6059.144865512848\n",
      "  time_this_iter_s: 20.731017589569092\n",
      "  time_total_s: 6059.144865512848\n",
      "  timers:\n",
      "    learn_throughput: 960.29\n",
      "    learn_time_ms: 1041.352\n",
      "    load_throughput: 55416.366\n",
      "    load_time_ms: 18.045\n",
      "    sample_throughput: 42.154\n",
      "    sample_time_ms: 23722.723\n",
      "    update_time_ms: 2.328\n",
      "  timestamp: 1633711860\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 248\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   248</td><td style=\"text-align: right;\">         6059.14</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\">   -0.16</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">             370.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 249000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-51-22\n",
      "  done: false\n",
      "  episode_len_mean: 371.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.13\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 658\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6645124978489345\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011332433796854207\n",
      "          policy_loss: -0.22815210413601664\n",
      "          total_loss: -0.06424168418678973\n",
      "          vf_explained_var: 0.7862867712974548\n",
      "          vf_loss: 0.17410136581295066\n",
      "    num_agent_steps_sampled: 249000\n",
      "    num_agent_steps_trained: 249000\n",
      "    num_steps_sampled: 249000\n",
      "    num_steps_trained: 249000\n",
      "  iterations_since_restore: 249\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.29354838709678\n",
      "    ram_util_percent: 60.909677419354864\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03848618148997915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.119803975729866\n",
      "    mean_inference_ms: 1.5741783574221457\n",
      "    mean_raw_obs_processing_ms: 1.6173928067079575\n",
      "  time_since_restore: 6080.670515537262\n",
      "  time_this_iter_s: 21.525650024414062\n",
      "  time_total_s: 6080.670515537262\n",
      "  timers:\n",
      "    learn_throughput: 958.503\n",
      "    learn_time_ms: 1043.293\n",
      "    load_throughput: 56160.752\n",
      "    load_time_ms: 17.806\n",
      "    sample_throughput: 45.29\n",
      "    sample_time_ms: 22080.118\n",
      "    update_time_ms: 2.334\n",
      "  timestamp: 1633711882\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 249000\n",
      "  training_iteration: 249\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   249</td><td style=\"text-align: right;\">         6080.67</td><td style=\"text-align: right;\">249000</td><td style=\"text-align: right;\">   -0.13</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            371.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 250000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-52-02\n",
      "  done: false\n",
      "  episode_len_mean: 369.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: -0.12\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 660\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7504501673910353\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009429414877732849\n",
      "          policy_loss: -0.1366593012172315\n",
      "          total_loss: -0.05764403724008137\n",
      "          vf_explained_var: 0.7054709792137146\n",
      "          vf_loss: 0.09114942015666101\n",
      "    num_agent_steps_sampled: 250000\n",
      "    num_agent_steps_trained: 250000\n",
      "    num_steps_sampled: 250000\n",
      "    num_steps_trained: 250000\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.841071428571425\n",
      "    ram_util_percent: 60.830357142857146\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03848506027906768\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.119427363964824\n",
      "    mean_inference_ms: 1.574139581497033\n",
      "    mean_raw_obs_processing_ms: 1.6186312622105254\n",
      "  time_since_restore: 6120.429366827011\n",
      "  time_this_iter_s: 39.758851289749146\n",
      "  time_total_s: 6120.429366827011\n",
      "  timers:\n",
      "    learn_throughput: 957.288\n",
      "    learn_time_ms: 1044.617\n",
      "    load_throughput: 56091.805\n",
      "    load_time_ms: 17.828\n",
      "    sample_throughput: 43.087\n",
      "    sample_time_ms: 23209.04\n",
      "    update_time_ms: 2.319\n",
      "  timestamp: 1633711922\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 250000\n",
      "  training_iteration: 250\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   250</td><td style=\"text-align: right;\">         6120.43</td><td style=\"text-align: right;\">250000</td><td style=\"text-align: right;\">   -0.12</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            369.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 251000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-52-24\n",
      "  done: false\n",
      "  episode_len_mean: 368.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 6.0\n",
      "  episode_reward_mean: 0.0\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 663\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6108633756637574\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007152368797762934\n",
      "          policy_loss: -0.025463758181366654\n",
      "          total_loss: 0.02756546904436416\n",
      "          vf_explained_var: 0.548769474029541\n",
      "          vf_loss: 0.06506436462902153\n",
      "    num_agent_steps_sampled: 251000\n",
      "    num_agent_steps_trained: 251000\n",
      "    num_steps_sampled: 251000\n",
      "    num_steps_trained: 251000\n",
      "  iterations_since_restore: 251\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.034375\n",
      "    ram_util_percent: 60.715624999999996\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03848334534390487\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.119098404372824\n",
      "    mean_inference_ms: 1.574081263017129\n",
      "    mean_raw_obs_processing_ms: 1.6205147890678864\n",
      "  time_since_restore: 6142.675240755081\n",
      "  time_this_iter_s: 22.24587392807007\n",
      "  time_total_s: 6142.675240755081\n",
      "  timers:\n",
      "    learn_throughput: 955.981\n",
      "    learn_time_ms: 1046.046\n",
      "    load_throughput: 56477.837\n",
      "    load_time_ms: 17.706\n",
      "    sample_throughput: 44.272\n",
      "    sample_time_ms: 22587.886\n",
      "    update_time_ms: 2.318\n",
      "  timestamp: 1633711944\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 251000\n",
      "  training_iteration: 251\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   251</td><td style=\"text-align: right;\">         6142.68</td><td style=\"text-align: right;\">251000</td><td style=\"text-align: right;\">       0</td><td style=\"text-align: right;\">                   6</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            368.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-53-13\n",
      "  done: false\n",
      "  episode_len_mean: 364.33\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 0.22\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 667\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.961530007256402\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010064390597585791\n",
      "          policy_loss: -0.08996925188435448\n",
      "          total_loss: 0.09097933578822348\n",
      "          vf_explained_var: 0.5959858894348145\n",
      "          vf_loss: 0.19483190369792283\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_agent_steps_trained: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "  iterations_since_restore: 252\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.14\n",
      "    ram_util_percent: 60.62142857142856\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03848111682294608\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.118507595957446\n",
      "    mean_inference_ms: 1.574005045454295\n",
      "    mean_raw_obs_processing_ms: 1.627550609673727\n",
      "  time_since_restore: 6191.480413913727\n",
      "  time_this_iter_s: 48.80517315864563\n",
      "  time_total_s: 6191.480413913727\n",
      "  timers:\n",
      "    learn_throughput: 954.245\n",
      "    learn_time_ms: 1047.949\n",
      "    load_throughput: 52939.0\n",
      "    load_time_ms: 18.89\n",
      "    sample_throughput: 38.937\n",
      "    sample_time_ms: 25682.726\n",
      "    update_time_ms: 2.23\n",
      "  timestamp: 1633711993\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 252\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   252</td><td style=\"text-align: right;\">         6191.48</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">    0.22</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            364.33</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 253000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-53-32\n",
      "  done: false\n",
      "  episode_len_mean: 366.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 0.3\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 669\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.017258707682292\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00972956972623041\n",
      "          policy_loss: -0.06855903168519338\n",
      "          total_loss: 0.16708281661073368\n",
      "          vf_explained_var: 0.7516634464263916\n",
      "          vf_loss: 0.25027314480394125\n",
      "    num_agent_steps_sampled: 253000\n",
      "    num_agent_steps_trained: 253000\n",
      "    num_steps_sampled: 253000\n",
      "    num_steps_trained: 253000\n",
      "  iterations_since_restore: 253\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.77777777777778\n",
      "    ram_util_percent: 60.55925925925926\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038480068032216506\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.11770185714399\n",
      "    mean_inference_ms: 1.57396617498256\n",
      "    mean_raw_obs_processing_ms: 1.630259097091803\n",
      "  time_since_restore: 6210.585485935211\n",
      "  time_this_iter_s: 19.105072021484375\n",
      "  time_total_s: 6210.585485935211\n",
      "  timers:\n",
      "    learn_throughput: 955.445\n",
      "    learn_time_ms: 1046.633\n",
      "    load_throughput: 52579.836\n",
      "    load_time_ms: 19.019\n",
      "    sample_throughput: 40.419\n",
      "    sample_time_ms: 24741.073\n",
      "    update_time_ms: 2.217\n",
      "  timestamp: 1633712012\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 253000\n",
      "  training_iteration: 253\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   253</td><td style=\"text-align: right;\">         6210.59</td><td style=\"text-align: right;\">253000</td><td style=\"text-align: right;\">     0.3</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            366.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 254000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-53-50\n",
      "  done: false\n",
      "  episode_len_mean: 369.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 0.37\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 671\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1682334303855897\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0072958908766151765\n",
      "          policy_loss: -0.06329788176549805\n",
      "          total_loss: -0.015817909066875777\n",
      "          vf_explained_var: 0.17522971332073212\n",
      "          vf_loss: 0.06500706690777507\n",
      "    num_agent_steps_sampled: 254000\n",
      "    num_agent_steps_trained: 254000\n",
      "    num_steps_sampled: 254000\n",
      "    num_steps_trained: 254000\n",
      "  iterations_since_restore: 254\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.276923076923076\n",
      "    ram_util_percent: 60.28846153846152\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03847897225056891\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.11640810099089\n",
      "    mean_inference_ms: 1.5739255679206776\n",
      "    mean_raw_obs_processing_ms: 1.6320948857927113\n",
      "  time_since_restore: 6228.575546503067\n",
      "  time_this_iter_s: 17.990060567855835\n",
      "  time_total_s: 6228.575546503067\n",
      "  timers:\n",
      "    learn_throughput: 955.575\n",
      "    learn_time_ms: 1046.49\n",
      "    load_throughput: 51783.768\n",
      "    load_time_ms: 19.311\n",
      "    sample_throughput: 40.625\n",
      "    sample_time_ms: 24615.149\n",
      "    update_time_ms: 2.217\n",
      "  timestamp: 1633712030\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 254000\n",
      "  training_iteration: 254\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   254</td><td style=\"text-align: right;\">         6228.58</td><td style=\"text-align: right;\">254000</td><td style=\"text-align: right;\">    0.37</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            369.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 255000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-54-10\n",
      "  done: false\n",
      "  episode_len_mean: 368.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 0.38\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 673\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8513628310627408\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020913452063980643\n",
      "          policy_loss: 0.08072423011892371\n",
      "          total_loss: 0.3357741917586989\n",
      "          vf_explained_var: 0.5043972730636597\n",
      "          vf_loss: 0.2616527250657479\n",
      "    num_agent_steps_sampled: 255000\n",
      "    num_agent_steps_trained: 255000\n",
      "    num_steps_sampled: 255000\n",
      "    num_steps_trained: 255000\n",
      "  iterations_since_restore: 255\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.34137931034482\n",
      "    ram_util_percent: 60.06206896551724\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03847788080955168\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.115205644293052\n",
      "    mean_inference_ms: 1.5738851675653893\n",
      "    mean_raw_obs_processing_ms: 1.633945024915493\n",
      "  time_since_restore: 6249.001040935516\n",
      "  time_this_iter_s: 20.42549443244934\n",
      "  time_total_s: 6249.001040935516\n",
      "  timers:\n",
      "    learn_throughput: 955.289\n",
      "    learn_time_ms: 1046.803\n",
      "    load_throughput: 52204.256\n",
      "    load_time_ms: 19.156\n",
      "    sample_throughput: 41.13\n",
      "    sample_time_ms: 24312.918\n",
      "    update_time_ms: 2.211\n",
      "  timestamp: 1633712050\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 255000\n",
      "  training_iteration: 255\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   255</td><td style=\"text-align: right;\">            6249</td><td style=\"text-align: right;\">255000</td><td style=\"text-align: right;\">    0.38</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            368.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-54-30\n",
      "  done: false\n",
      "  episode_len_mean: 370.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 0.5\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 676\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9234434790081447\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008088855558863588\n",
      "          policy_loss: -0.03454114720225334\n",
      "          total_loss: 0.09668780721517073\n",
      "          vf_explained_var: 0.48950493335723877\n",
      "          vf_loss: 0.14355310350656508\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.47037037037038\n",
      "    ram_util_percent: 59.937037037037044\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038476206178219306\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.113092496144056\n",
      "    mean_inference_ms: 1.5738248758153797\n",
      "    mean_raw_obs_processing_ms: 1.6367402383870369\n",
      "  time_since_restore: 6268.134623527527\n",
      "  time_this_iter_s: 19.133582592010498\n",
      "  time_total_s: 6268.134623527527\n",
      "  timers:\n",
      "    learn_throughput: 955.094\n",
      "    learn_time_ms: 1047.017\n",
      "    load_throughput: 51177.385\n",
      "    load_time_ms: 19.54\n",
      "    sample_throughput: 41.776\n",
      "    sample_time_ms: 23936.987\n",
      "    update_time_ms: 2.216\n",
      "  timestamp: 1633712070\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 256\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   256</td><td style=\"text-align: right;\">         6268.13</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">     0.5</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            370.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 257000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-54-48\n",
      "  done: false\n",
      "  episode_len_mean: 372.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 0.62\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 678\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.237511506345537\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005658520609898016\n",
      "          policy_loss: 0.019824627372953627\n",
      "          total_loss: 0.019548669043514464\n",
      "          vf_explained_var: 0.42332667112350464\n",
      "          vf_loss: 0.01726509854197502\n",
      "    num_agent_steps_sampled: 257000\n",
      "    num_agent_steps_trained: 257000\n",
      "    num_steps_sampled: 257000\n",
      "    num_steps_trained: 257000\n",
      "  iterations_since_restore: 257\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.30384615384616\n",
      "    ram_util_percent: 59.96923076923077\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03847510047060954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.11127294567932\n",
      "    mean_inference_ms: 1.573784335669842\n",
      "    mean_raw_obs_processing_ms: 1.638614672741559\n",
      "  time_since_restore: 6286.103327035904\n",
      "  time_this_iter_s: 17.968703508377075\n",
      "  time_total_s: 6286.103327035904\n",
      "  timers:\n",
      "    learn_throughput: 955.855\n",
      "    learn_time_ms: 1046.184\n",
      "    load_throughput: 54307.852\n",
      "    load_time_ms: 18.414\n",
      "    sample_throughput: 42.196\n",
      "    sample_time_ms: 23698.999\n",
      "    update_time_ms: 2.227\n",
      "  timestamp: 1633712088\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 257000\n",
      "  training_iteration: 257\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   257</td><td style=\"text-align: right;\">          6286.1</td><td style=\"text-align: right;\">257000</td><td style=\"text-align: right;\">    0.62</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            372.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 258000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-55-06\n",
      "  done: false\n",
      "  episode_len_mean: 373.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 0.73\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 680\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.263381263944838\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006258942850286727\n",
      "          policy_loss: -0.1287168186571863\n",
      "          total_loss: -0.10046021391948064\n",
      "          vf_explained_var: 0.9776836633682251\n",
      "          vf_loss: 0.045543422032561565\n",
      "    num_agent_steps_sampled: 258000\n",
      "    num_agent_steps_trained: 258000\n",
      "    num_steps_sampled: 258000\n",
      "    num_steps_trained: 258000\n",
      "  iterations_since_restore: 258\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.42307692307692\n",
      "    ram_util_percent: 60.0576923076923\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038473975384488254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.109212449615878\n",
      "    mean_inference_ms: 1.5737432379967282\n",
      "    mean_raw_obs_processing_ms: 1.64043486008052\n",
      "  time_since_restore: 6304.426089763641\n",
      "  time_this_iter_s: 18.322762727737427\n",
      "  time_total_s: 6304.426089763641\n",
      "  timers:\n",
      "    learn_throughput: 955.432\n",
      "    learn_time_ms: 1046.647\n",
      "    load_throughput: 55023.43\n",
      "    load_time_ms: 18.174\n",
      "    sample_throughput: 42.63\n",
      "    sample_time_ms: 23457.683\n",
      "    update_time_ms: 2.509\n",
      "  timestamp: 1633712106\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 258000\n",
      "  training_iteration: 258\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   258</td><td style=\"text-align: right;\">         6304.43</td><td style=\"text-align: right;\">258000</td><td style=\"text-align: right;\">    0.73</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            373.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 259000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-55-23\n",
      "  done: false\n",
      "  episode_len_mean: 375.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 0.83\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 682\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2999992847442625\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005082127152250631\n",
      "          policy_loss: -0.02072404854827457\n",
      "          total_loss: -0.004647849003473917\n",
      "          vf_explained_var: 0.3160251975059509\n",
      "          vf_loss: 0.03473454787033713\n",
      "    num_agent_steps_sampled: 259000\n",
      "    num_agent_steps_trained: 259000\n",
      "    num_steps_sampled: 259000\n",
      "    num_steps_trained: 259000\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.468\n",
      "    ram_util_percent: 60.175999999999995\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03847283549422689\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.10679001675609\n",
      "    mean_inference_ms: 1.5737016646145987\n",
      "    mean_raw_obs_processing_ms: 1.6422663149357328\n",
      "  time_since_restore: 6321.955683231354\n",
      "  time_this_iter_s: 17.529593467712402\n",
      "  time_total_s: 6321.955683231354\n",
      "  timers:\n",
      "    learn_throughput: 955.253\n",
      "    learn_time_ms: 1046.843\n",
      "    load_throughput: 58650.028\n",
      "    load_time_ms: 17.05\n",
      "    sample_throughput: 43.367\n",
      "    sample_time_ms: 23059.0\n",
      "    update_time_ms: 2.495\n",
      "  timestamp: 1633712123\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 259000\n",
      "  training_iteration: 259\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   259</td><td style=\"text-align: right;\">         6321.96</td><td style=\"text-align: right;\">259000</td><td style=\"text-align: right;\">    0.83</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            375.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-55-45\n",
      "  done: false\n",
      "  episode_len_mean: 376.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 0.98\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 685\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8472361935509576\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011267509848945645\n",
      "          policy_loss: -0.033380376588967114\n",
      "          total_loss: 0.11613969136443403\n",
      "          vf_explained_var: 0.7170355319976807\n",
      "          vf_loss: 0.15836663184066613\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_agent_steps_trained: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "  iterations_since_restore: 260\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.21612903225806\n",
      "    ram_util_percent: 60.29354838709677\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03847112370151866\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.10304591410577\n",
      "    mean_inference_ms: 1.5736404114893663\n",
      "    mean_raw_obs_processing_ms: 1.64500364564132\n",
      "  time_since_restore: 6343.414080142975\n",
      "  time_this_iter_s: 21.458396911621094\n",
      "  time_total_s: 6343.414080142975\n",
      "  timers:\n",
      "    learn_throughput: 954.223\n",
      "    learn_time_ms: 1047.973\n",
      "    load_throughput: 59075.536\n",
      "    load_time_ms: 16.927\n",
      "    sample_throughput: 47.108\n",
      "    sample_time_ms: 21227.949\n",
      "    update_time_ms: 2.498\n",
      "  timestamp: 1633712145\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 260\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   260</td><td style=\"text-align: right;\">         6343.41</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">    0.98</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            376.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 261000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-56-04\n",
      "  done: false\n",
      "  episode_len_mean: 377.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 1.22\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 687\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.854296875\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.3697206166055467\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003639991396104768\n",
      "          policy_loss: -0.09570948692659537\n",
      "          total_loss: -0.05693870653501815\n",
      "          vf_explained_var: 0.3004295527935028\n",
      "          vf_loss: 0.05935835181218055\n",
      "    num_agent_steps_sampled: 261000\n",
      "    num_agent_steps_trained: 261000\n",
      "    num_steps_sampled: 261000\n",
      "    num_steps_trained: 261000\n",
      "  iterations_since_restore: 261\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.28846153846155\n",
      "    ram_util_percent: 60.45\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03847004700005636\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.10038893803366\n",
      "    mean_inference_ms: 1.5736005948796015\n",
      "    mean_raw_obs_processing_ms: 1.646860648151854\n",
      "  time_since_restore: 6362.038371324539\n",
      "  time_this_iter_s: 18.62429118156433\n",
      "  time_total_s: 6362.038371324539\n",
      "  timers:\n",
      "    learn_throughput: 953.185\n",
      "    learn_time_ms: 1049.114\n",
      "    load_throughput: 59316.901\n",
      "    load_time_ms: 16.859\n",
      "    sample_throughput: 47.928\n",
      "    sample_time_ms: 20864.696\n",
      "    update_time_ms: 2.498\n",
      "  timestamp: 1633712164\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 261000\n",
      "  training_iteration: 261\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   261</td><td style=\"text-align: right;\">         6362.04</td><td style=\"text-align: right;\">261000</td><td style=\"text-align: right;\">    1.22</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            377.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 262000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-56-26\n",
      "  done: false\n",
      "  episode_len_mean: 378.49\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 1.41\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 690\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8614953676859538\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011688945501982254\n",
      "          policy_loss: -0.06737923117147551\n",
      "          total_loss: -0.006533927346269289\n",
      "          vf_explained_var: 0.8216189742088318\n",
      "          vf_loss: 0.07446734070415711\n",
      "    num_agent_steps_sampled: 262000\n",
      "    num_agent_steps_trained: 262000\n",
      "    num_steps_sampled: 262000\n",
      "    num_steps_trained: 262000\n",
      "  iterations_since_restore: 262\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.087500000000006\n",
      "    ram_util_percent: 60.584374999999994\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038468441807012345\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.09638144302078\n",
      "    mean_inference_ms: 1.573541361892138\n",
      "    mean_raw_obs_processing_ms: 1.649631451912133\n",
      "  time_since_restore: 6384.478289842606\n",
      "  time_this_iter_s: 22.439918518066406\n",
      "  time_total_s: 6384.478289842606\n",
      "  timers:\n",
      "    learn_throughput: 953.276\n",
      "    learn_time_ms: 1049.014\n",
      "    load_throughput: 59402.842\n",
      "    load_time_ms: 16.834\n",
      "    sample_throughput: 54.86\n",
      "    sample_time_ms: 18228.287\n",
      "    update_time_ms: 2.518\n",
      "  timestamp: 1633712186\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 262000\n",
      "  training_iteration: 262\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   262</td><td style=\"text-align: right;\">         6384.48</td><td style=\"text-align: right;\">262000</td><td style=\"text-align: right;\">    1.41</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            378.49</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 263000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-56-53\n",
      "  done: false\n",
      "  episode_len_mean: 375.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 1.54\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 693\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.561756130721834\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009161160748305846\n",
      "          policy_loss: -0.024014590432246526\n",
      "          total_loss: 0.0254985015011496\n",
      "          vf_explained_var: 0.5255440473556519\n",
      "          vf_loss: 0.06121747959405184\n",
      "    num_agent_steps_sampled: 263000\n",
      "    num_agent_steps_trained: 263000\n",
      "    num_steps_sampled: 263000\n",
      "    num_steps_trained: 263000\n",
      "  iterations_since_restore: 263\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.135000000000005\n",
      "    ram_util_percent: 60.69250000000001\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03846681919110026\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.09303135527198\n",
      "    mean_inference_ms: 1.5734843747717746\n",
      "    mean_raw_obs_processing_ms: 1.6524191612754446\n",
      "  time_since_restore: 6411.82337141037\n",
      "  time_this_iter_s: 27.345081567764282\n",
      "  time_total_s: 6411.82337141037\n",
      "  timers:\n",
      "    learn_throughput: 953.957\n",
      "    learn_time_ms: 1048.265\n",
      "    load_throughput: 59363.243\n",
      "    load_time_ms: 16.845\n",
      "    sample_throughput: 52.485\n",
      "    sample_time_ms: 19053.017\n",
      "    update_time_ms: 2.516\n",
      "  timestamp: 1633712213\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 263000\n",
      "  training_iteration: 263\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   263</td><td style=\"text-align: right;\">         6411.82</td><td style=\"text-align: right;\">263000</td><td style=\"text-align: right;\">    1.54</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">             375.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-57-30\n",
      "  done: false\n",
      "  episode_len_mean: 376.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 1.78\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 696\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.034387781884935\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010597390520622281\n",
      "          policy_loss: -0.07645262396997876\n",
      "          total_loss: -0.00462298807170656\n",
      "          vf_explained_var: 0.9667097330093384\n",
      "          vf_loss: 0.08764685311147737\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "  iterations_since_restore: 264\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.151923076923076\n",
      "    ram_util_percent: 60.83653846153847\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03846524004331233\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.089213878208724\n",
      "    mean_inference_ms: 1.5734271747325126\n",
      "    mean_raw_obs_processing_ms: 1.657123126212575\n",
      "  time_since_restore: 6448.647528171539\n",
      "  time_this_iter_s: 36.824156761169434\n",
      "  time_total_s: 6448.647528171539\n",
      "  timers:\n",
      "    learn_throughput: 955.129\n",
      "    learn_time_ms: 1046.979\n",
      "    load_throughput: 59990.703\n",
      "    load_time_ms: 16.669\n",
      "    sample_throughput: 47.76\n",
      "    sample_time_ms: 20937.888\n",
      "    update_time_ms: 2.511\n",
      "  timestamp: 1633712250\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 264\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   264</td><td style=\"text-align: right;\">         6448.65</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\">    1.78</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            376.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 265000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-57-49\n",
      "  done: false\n",
      "  episode_len_mean: 378.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 1.79\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 698\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.982243882285224\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011979668714064574\n",
      "          policy_loss: -0.22448963075876235\n",
      "          total_loss: -0.19656957512100537\n",
      "          vf_explained_var: 0.47613295912742615\n",
      "          vf_loss: 0.04262539359430472\n",
      "    num_agent_steps_sampled: 265000\n",
      "    num_agent_steps_trained: 265000\n",
      "    num_steps_sampled: 265000\n",
      "    num_steps_trained: 265000\n",
      "  iterations_since_restore: 265\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.72592592592593\n",
      "    ram_util_percent: 60.807407407407396\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03846417875884792\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.08623997843913\n",
      "    mean_inference_ms: 1.5733884462083605\n",
      "    mean_raw_obs_processing_ms: 1.6602210492089033\n",
      "  time_since_restore: 6467.5324993133545\n",
      "  time_this_iter_s: 18.884971141815186\n",
      "  time_total_s: 6467.5324993133545\n",
      "  timers:\n",
      "    learn_throughput: 956.845\n",
      "    learn_time_ms: 1045.101\n",
      "    load_throughput: 58214.523\n",
      "    load_time_ms: 17.178\n",
      "    sample_throughput: 48.111\n",
      "    sample_time_ms: 20785.199\n",
      "    update_time_ms: 2.516\n",
      "  timestamp: 1633712269\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 265000\n",
      "  training_iteration: 265\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   265</td><td style=\"text-align: right;\">         6467.53</td><td style=\"text-align: right;\">265000</td><td style=\"text-align: right;\">    1.79</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">             378.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 266000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-58-06\n",
      "  done: false\n",
      "  episode_len_mean: 382.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 9.0\n",
      "  episode_reward_mean: 1.88\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 700\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.45582228369183\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007308662315843822\n",
      "          policy_loss: 0.03605615645647049\n",
      "          total_loss: 0.029596983972522948\n",
      "          vf_explained_var: 0.9347833395004272\n",
      "          vf_loss: 0.014977166676221208\n",
      "    num_agent_steps_sampled: 266000\n",
      "    num_agent_steps_trained: 266000\n",
      "    num_steps_sampled: 266000\n",
      "    num_steps_trained: 266000\n",
      "  iterations_since_restore: 266\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.479166666666664\n",
      "    ram_util_percent: 60.84583333333334\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03846306904533444\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.082485822243363\n",
      "    mean_inference_ms: 1.5733485319396965\n",
      "    mean_raw_obs_processing_ms: 1.6618695776940517\n",
      "  time_since_restore: 6484.597078800201\n",
      "  time_this_iter_s: 17.064579486846924\n",
      "  time_total_s: 6484.597078800201\n",
      "  timers:\n",
      "    learn_throughput: 957.579\n",
      "    learn_time_ms: 1044.3\n",
      "    load_throughput: 63742.35\n",
      "    load_time_ms: 15.688\n",
      "    sample_throughput: 48.59\n",
      "    sample_time_ms: 20580.565\n",
      "    update_time_ms: 2.523\n",
      "  timestamp: 1633712286\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 266000\n",
      "  training_iteration: 266\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   266</td><td style=\"text-align: right;\">          6484.6</td><td style=\"text-align: right;\">266000</td><td style=\"text-align: right;\">    1.88</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            382.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 267000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-59-04\n",
      "  done: false\n",
      "  episode_len_mean: 380.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 2.11\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 703\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9631349192725287\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011278317337268282\n",
      "          policy_loss: -0.08418158723248376\n",
      "          total_loss: 0.006324106454849243\n",
      "          vf_explained_var: 0.9494575262069702\n",
      "          vf_loss: 0.10531952360437977\n",
      "    num_agent_steps_sampled: 267000\n",
      "    num_agent_steps_trained: 267000\n",
      "    num_steps_sampled: 267000\n",
      "    num_steps_trained: 267000\n",
      "  iterations_since_restore: 267\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.354878048780492\n",
      "    ram_util_percent: 60.580487804878054\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03846136498410465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.076196984850377\n",
      "    mean_inference_ms: 1.573287275860488\n",
      "    mean_raw_obs_processing_ms: 1.6687120849114871\n",
      "  time_since_restore: 6542.029959440231\n",
      "  time_this_iter_s: 57.43288064002991\n",
      "  time_total_s: 6542.029959440231\n",
      "  timers:\n",
      "    learn_throughput: 957.96\n",
      "    learn_time_ms: 1043.885\n",
      "    load_throughput: 59435.25\n",
      "    load_time_ms: 16.825\n",
      "    sample_throughput: 40.773\n",
      "    sample_time_ms: 24526.26\n",
      "    update_time_ms: 2.511\n",
      "  timestamp: 1633712344\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 267000\n",
      "  training_iteration: 267\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   267</td><td style=\"text-align: right;\">         6542.03</td><td style=\"text-align: right;\">267000</td><td style=\"text-align: right;\">    2.11</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            380.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 268000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-59-22\n",
      "  done: false\n",
      "  episode_len_mean: 384.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 2.22\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 705\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.126993641588423\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008837982711752524\n",
      "          policy_loss: 0.014880634616646501\n",
      "          total_loss: 0.07535214939465125\n",
      "          vf_explained_var: 0.7154663801193237\n",
      "          vf_loss: 0.07796632051467896\n",
      "    num_agent_steps_sampled: 268000\n",
      "    num_agent_steps_trained: 268000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "  iterations_since_restore: 268\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.51111111111111\n",
      "    ram_util_percent: 60.72592592592592\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03846024324873594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.071537375658785\n",
      "    mean_inference_ms: 1.5732455043852724\n",
      "    mean_raw_obs_processing_ms: 1.6732504196963054\n",
      "  time_since_restore: 6560.658451795578\n",
      "  time_this_iter_s: 18.62849235534668\n",
      "  time_total_s: 6560.658451795578\n",
      "  timers:\n",
      "    learn_throughput: 956.952\n",
      "    learn_time_ms: 1044.984\n",
      "    load_throughput: 59555.003\n",
      "    load_time_ms: 16.791\n",
      "    sample_throughput: 40.723\n",
      "    sample_time_ms: 24556.044\n",
      "    update_time_ms: 2.23\n",
      "  timestamp: 1633712362\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 268\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   268</td><td style=\"text-align: right;\">         6560.66</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">    2.22</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            384.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 269000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_16-59-44\n",
      "  done: false\n",
      "  episode_len_mean: 387.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 2.36\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 708\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484375\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.157940740386645\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.030902537346837632\n",
      "          policy_loss: -0.026766697896851432\n",
      "          total_loss: 0.20947781412137878\n",
      "          vf_explained_var: 0.9403523802757263\n",
      "          vf_loss: 0.23462393906795317\n",
      "    num_agent_steps_sampled: 269000\n",
      "    num_agent_steps_trained: 269000\n",
      "    num_steps_sampled: 269000\n",
      "    num_steps_trained: 269000\n",
      "  iterations_since_restore: 269\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.35483870967741\n",
      "    ram_util_percent: 60.551612903225816\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03845853958309966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.06408384270177\n",
      "    mean_inference_ms: 1.573181945755812\n",
      "    mean_raw_obs_processing_ms: 1.6800262147062872\n",
      "  time_since_restore: 6582.367213726044\n",
      "  time_this_iter_s: 21.7087619304657\n",
      "  time_total_s: 6582.367213726044\n",
      "  timers:\n",
      "    learn_throughput: 952.921\n",
      "    learn_time_ms: 1049.405\n",
      "    load_throughput: 55820.228\n",
      "    load_time_ms: 17.915\n",
      "    sample_throughput: 40.051\n",
      "    sample_time_ms: 24968.384\n",
      "    update_time_ms: 2.256\n",
      "  timestamp: 1633712384\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 269000\n",
      "  training_iteration: 269\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   269</td><td style=\"text-align: right;\">         6582.37</td><td style=\"text-align: right;\">269000</td><td style=\"text-align: right;\">    2.36</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            387.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 270000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-00-04\n",
      "  done: false\n",
      "  episode_len_mean: 388.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 2.55\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 710\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1198598080211215\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007467050940542934\n",
      "          policy_loss: -0.13492941810852951\n",
      "          total_loss: -0.13216598100132412\n",
      "          vf_explained_var: 0.4515605568885803\n",
      "          vf_loss: 0.01917772547652324\n",
      "    num_agent_steps_sampled: 270000\n",
      "    num_agent_steps_trained: 270000\n",
      "    num_steps_sampled: 270000\n",
      "    num_steps_trained: 270000\n",
      "  iterations_since_restore: 270\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.186206896551724\n",
      "    ram_util_percent: 59.83793103448276\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03845740053950151\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.05868790055185\n",
      "    mean_inference_ms: 1.5731389895292784\n",
      "    mean_raw_obs_processing_ms: 1.6845001027491333\n",
      "  time_since_restore: 6602.3697555065155\n",
      "  time_this_iter_s: 20.0025417804718\n",
      "  time_total_s: 6602.3697555065155\n",
      "  timers:\n",
      "    learn_throughput: 954.391\n",
      "    learn_time_ms: 1047.789\n",
      "    load_throughput: 55619.997\n",
      "    load_time_ms: 17.979\n",
      "    sample_throughput: 40.283\n",
      "    sample_time_ms: 24824.347\n",
      "    update_time_ms: 2.261\n",
      "  timestamp: 1633712404\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 270000\n",
      "  training_iteration: 270\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   270</td><td style=\"text-align: right;\">         6602.37</td><td style=\"text-align: right;\">270000</td><td style=\"text-align: right;\">    2.55</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            388.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 271000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-00-21\n",
      "  done: false\n",
      "  episode_len_mean: 390.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 2.67\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 712\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.093434045049879\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016067724015139034\n",
      "          policy_loss: -0.14251029822561476\n",
      "          total_loss: 0.359278260750903\n",
      "          vf_explained_var: 0.644807755947113\n",
      "          vf_loss: 0.5124279564453496\n",
      "    num_agent_steps_sampled: 271000\n",
      "    num_agent_steps_trained: 271000\n",
      "    num_steps_sampled: 271000\n",
      "    num_steps_trained: 271000\n",
      "  iterations_since_restore: 271\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.291666666666664\n",
      "    ram_util_percent: 59.5875\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03845626997681979\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.053031642447298\n",
      "    mean_inference_ms: 1.5730961904029805\n",
      "    mean_raw_obs_processing_ms: 1.688975756758914\n",
      "  time_since_restore: 6619.22877573967\n",
      "  time_this_iter_s: 16.859020233154297\n",
      "  time_total_s: 6619.22877573967\n",
      "  timers:\n",
      "    learn_throughput: 956.54\n",
      "    learn_time_ms: 1045.435\n",
      "    load_throughput: 59318.243\n",
      "    load_time_ms: 16.858\n",
      "    sample_throughput: 40.566\n",
      "    sample_time_ms: 24651.302\n",
      "    update_time_ms: 2.266\n",
      "  timestamp: 1633712421\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 271000\n",
      "  training_iteration: 271\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   271</td><td style=\"text-align: right;\">         6619.23</td><td style=\"text-align: right;\">271000</td><td style=\"text-align: right;\">    2.67</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            390.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-00-43\n",
      "  done: false\n",
      "  episode_len_mean: 391.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 2.75\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 715\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6259966995981006\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013058094114830925\n",
      "          policy_loss: -0.03388712008794149\n",
      "          total_loss: 0.6304652776776088\n",
      "          vf_explained_var: 0.643039345741272\n",
      "          vf_loss: 0.6722457459403409\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "  iterations_since_restore: 272\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.364516129032246\n",
      "    ram_util_percent: 59.57096774193546\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03845457419582313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.04439785666211\n",
      "    mean_inference_ms: 1.5730320080341516\n",
      "    mean_raw_obs_processing_ms: 1.6956900760492888\n",
      "  time_since_restore: 6641.429006099701\n",
      "  time_this_iter_s: 22.200230360031128\n",
      "  time_total_s: 6641.429006099701\n",
      "  timers:\n",
      "    learn_throughput: 957.765\n",
      "    learn_time_ms: 1044.097\n",
      "    load_throughput: 59643.078\n",
      "    load_time_ms: 16.766\n",
      "    sample_throughput: 40.603\n",
      "    sample_time_ms: 24628.763\n",
      "    update_time_ms: 2.265\n",
      "  timestamp: 1633712443\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 272\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   272</td><td style=\"text-align: right;\">         6641.43</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\">    2.75</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            391.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 273000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-01-01\n",
      "  done: false\n",
      "  episode_len_mean: 392.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 2.96\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 717\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8434553451008266\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01311815820342874\n",
      "          policy_loss: -0.04214322335190243\n",
      "          total_loss: 0.11137437563803461\n",
      "          vf_explained_var: 0.44581323862075806\n",
      "          vf_loss: 0.1635470473104053\n",
      "    num_agent_steps_sampled: 273000\n",
      "    num_agent_steps_trained: 273000\n",
      "    num_steps_sampled: 273000\n",
      "    num_steps_trained: 273000\n",
      "  iterations_since_restore: 273\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.136\n",
      "    ram_util_percent: 59.684000000000005\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03845343071481139\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.03863618755898\n",
      "    mean_inference_ms: 1.5729895842000525\n",
      "    mean_raw_obs_processing_ms: 1.7001674294316655\n",
      "  time_since_restore: 6658.880649805069\n",
      "  time_this_iter_s: 17.451643705368042\n",
      "  time_total_s: 6658.880649805069\n",
      "  timers:\n",
      "    learn_throughput: 956.296\n",
      "    learn_time_ms: 1045.701\n",
      "    load_throughput: 64628.873\n",
      "    load_time_ms: 15.473\n",
      "    sample_throughput: 42.303\n",
      "    sample_time_ms: 23639.124\n",
      "    update_time_ms: 2.285\n",
      "  timestamp: 1633712461\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 273000\n",
      "  training_iteration: 273\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   273</td><td style=\"text-align: right;\">         6658.88</td><td style=\"text-align: right;\">273000</td><td style=\"text-align: right;\">    2.96</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            392.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 274000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-01-56\n",
      "  done: false\n",
      "  episode_len_mean: 390.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.01\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 720\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8143129348754883\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010094477474770864\n",
      "          policy_loss: 0.10504164128667778\n",
      "          total_loss: 0.39572473011083076\n",
      "          vf_explained_var: 0.5539809465408325\n",
      "          vf_loss: 0.3023584625373284\n",
      "    num_agent_steps_sampled: 274000\n",
      "    num_agent_steps_trained: 274000\n",
      "    num_steps_sampled: 274000\n",
      "    num_steps_trained: 274000\n",
      "  iterations_since_restore: 274\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.3625\n",
      "    ram_util_percent: 59.78375000000001\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038451670215763886\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.02955165886284\n",
      "    mean_inference_ms: 1.5729248874526434\n",
      "    mean_raw_obs_processing_ms: 1.7111741716746343\n",
      "  time_since_restore: 6714.415770530701\n",
      "  time_this_iter_s: 55.535120725631714\n",
      "  time_total_s: 6714.415770530701\n",
      "  timers:\n",
      "    learn_throughput: 956.606\n",
      "    learn_time_ms: 1045.362\n",
      "    load_throughput: 64679.801\n",
      "    load_time_ms: 15.461\n",
      "    sample_throughput: 39.199\n",
      "    sample_time_ms: 25510.559\n",
      "    update_time_ms: 2.292\n",
      "  timestamp: 1633712516\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 274000\n",
      "  training_iteration: 274\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   274</td><td style=\"text-align: right;\">         6714.42</td><td style=\"text-align: right;\">274000</td><td style=\"text-align: right;\">    3.01</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            390.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 275000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-02-14\n",
      "  done: false\n",
      "  episode_len_mean: 393.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.0\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 722\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8575262599521214\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015496967838168117\n",
      "          policy_loss: 0.004260279072655572\n",
      "          total_loss: 0.3433641817420721\n",
      "          vf_explained_var: 0.4517231285572052\n",
      "          vf_loss: 0.3477499097585678\n",
      "    num_agent_steps_sampled: 275000\n",
      "    num_agent_steps_trained: 275000\n",
      "    num_steps_sampled: 275000\n",
      "    num_steps_trained: 275000\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.876000000000005\n",
      "    ram_util_percent: 60.272\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03845047140297588\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.02315901277327\n",
      "    mean_inference_ms: 1.5728819453546288\n",
      "    mean_raw_obs_processing_ms: 1.7184988924279572\n",
      "  time_since_restore: 6731.998296499252\n",
      "  time_this_iter_s: 17.582525968551636\n",
      "  time_total_s: 6731.998296499252\n",
      "  timers:\n",
      "    learn_throughput: 955.77\n",
      "    learn_time_ms: 1046.277\n",
      "    load_throughput: 72301.375\n",
      "    load_time_ms: 13.831\n",
      "    sample_throughput: 39.4\n",
      "    sample_time_ms: 25381.025\n",
      "    update_time_ms: 2.293\n",
      "  timestamp: 1633712534\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 275000\n",
      "  training_iteration: 275\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   275</td><td style=\"text-align: right;\">            6732</td><td style=\"text-align: right;\">275000</td><td style=\"text-align: right;\">       3</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">             393.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 276000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-02-32\n",
      "  done: false\n",
      "  episode_len_mean: 394.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.17\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 724\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9086019582218594\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010601476579059346\n",
      "          policy_loss: -0.06571758410169018\n",
      "          total_loss: 0.11838294756081369\n",
      "          vf_explained_var: 0.5173894762992859\n",
      "          vf_loss: 0.19639394476802813\n",
      "    num_agent_steps_sampled: 276000\n",
      "    num_agent_steps_trained: 276000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "  iterations_since_restore: 276\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.592\n",
      "    ram_util_percent: 60.32\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03844926558351317\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.01643144567483\n",
      "    mean_inference_ms: 1.5728384804183184\n",
      "    mean_raw_obs_processing_ms: 1.725753201443845\n",
      "  time_since_restore: 6750.018805027008\n",
      "  time_this_iter_s: 18.020508527755737\n",
      "  time_total_s: 6750.018805027008\n",
      "  timers:\n",
      "    learn_throughput: 954.988\n",
      "    learn_time_ms: 1047.134\n",
      "    load_throughput: 66927.143\n",
      "    load_time_ms: 14.942\n",
      "    sample_throughput: 39.255\n",
      "    sample_time_ms: 25474.679\n",
      "    update_time_ms: 2.275\n",
      "  timestamp: 1633712552\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 276\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   276</td><td style=\"text-align: right;\">         6750.02</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">    3.17</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            394.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 277000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-02-50\n",
      "  done: false\n",
      "  episode_len_mean: 395.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.37\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 726\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4981771111488342\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010381009084773076\n",
      "          policy_loss: -0.052573849426375496\n",
      "          total_loss: 0.11851866567093465\n",
      "          vf_explained_var: 0.7676146030426025\n",
      "          vf_loss: 0.17942294027242395\n",
      "    num_agent_steps_sampled: 277000\n",
      "    num_agent_steps_trained: 277000\n",
      "    num_steps_sampled: 277000\n",
      "    num_steps_trained: 277000\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.12692307692308\n",
      "    ram_util_percent: 60.28846153846154\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03844803417795171\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.009458226356315\n",
      "    mean_inference_ms: 1.5727953449640382\n",
      "    mean_raw_obs_processing_ms: 1.7329967698035573\n",
      "  time_since_restore: 6768.157825469971\n",
      "  time_this_iter_s: 18.139020442962646\n",
      "  time_total_s: 6768.157825469971\n",
      "  timers:\n",
      "    learn_throughput: 954.763\n",
      "    learn_time_ms: 1047.38\n",
      "    load_throughput: 67054.577\n",
      "    load_time_ms: 14.913\n",
      "    sample_throughput: 46.414\n",
      "    sample_time_ms: 21545.082\n",
      "    update_time_ms: 2.27\n",
      "  timestamp: 1633712570\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 277000\n",
      "  training_iteration: 277\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   277</td><td style=\"text-align: right;\">         6768.16</td><td style=\"text-align: right;\">277000</td><td style=\"text-align: right;\">    3.37</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">             395.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 278000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-03-06\n",
      "  done: false\n",
      "  episode_len_mean: 398.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.42\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 728\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.671782500214047\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011988570713336744\n",
      "          policy_loss: -0.02788255765206284\n",
      "          total_loss: 0.3645545883725087\n",
      "          vf_explained_var: 0.8168433308601379\n",
      "          vf_loss: 0.40147361738814247\n",
      "    num_agent_steps_sampled: 278000\n",
      "    num_agent_steps_trained: 278000\n",
      "    num_steps_sampled: 278000\n",
      "    num_steps_trained: 278000\n",
      "  iterations_since_restore: 278\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.1625\n",
      "    ram_util_percent: 60.225\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038446777328652634\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.00211275641438\n",
      "    mean_inference_ms: 1.5727514930671704\n",
      "    mean_raw_obs_processing_ms: 1.7401695165576763\n",
      "  time_since_restore: 6784.483830690384\n",
      "  time_this_iter_s: 16.326005220413208\n",
      "  time_total_s: 6784.483830690384\n",
      "  timers:\n",
      "    learn_throughput: 955.207\n",
      "    learn_time_ms: 1046.893\n",
      "    load_throughput: 74443.516\n",
      "    load_time_ms: 13.433\n",
      "    sample_throughput: 46.911\n",
      "    sample_time_ms: 21316.803\n",
      "    update_time_ms: 2.262\n",
      "  timestamp: 1633712586\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 278000\n",
      "  training_iteration: 278\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   278</td><td style=\"text-align: right;\">         6784.48</td><td style=\"text-align: right;\">278000</td><td style=\"text-align: right;\">    3.42</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            398.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 279000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-03-25\n",
      "  done: false\n",
      "  episode_len_mean: 399.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.48\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 730\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.08619571129481\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009402888604078343\n",
      "          policy_loss: -0.028131017254458532\n",
      "          total_loss: 0.13264850390454133\n",
      "          vf_explained_var: 0.8665416240692139\n",
      "          vf_loss: 0.1756168363822831\n",
      "    num_agent_steps_sampled: 279000\n",
      "    num_agent_steps_trained: 279000\n",
      "    num_steps_sampled: 279000\n",
      "    num_steps_trained: 279000\n",
      "  iterations_since_restore: 279\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.58846153846154\n",
      "    ram_util_percent: 60.30384615384616\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03844551642261592\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.994599306566577\n",
      "    mean_inference_ms: 1.572708197953101\n",
      "    mean_raw_obs_processing_ms: 1.7459324176874538\n",
      "  time_since_restore: 6802.6874368190765\n",
      "  time_this_iter_s: 18.203606128692627\n",
      "  time_total_s: 6802.6874368190765\n",
      "  timers:\n",
      "    learn_throughput: 959.6\n",
      "    learn_time_ms: 1042.101\n",
      "    load_throughput: 74230.082\n",
      "    load_time_ms: 13.472\n",
      "    sample_throughput: 47.685\n",
      "    sample_time_ms: 20970.945\n",
      "    update_time_ms: 2.346\n",
      "  timestamp: 1633712605\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 279000\n",
      "  training_iteration: 279\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   279</td><td style=\"text-align: right;\">         6802.69</td><td style=\"text-align: right;\">279000</td><td style=\"text-align: right;\">    3.48</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">             399.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-03-43\n",
      "  done: false\n",
      "  episode_len_mean: 403.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.55\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 733\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0447369204627144\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007263089050159345\n",
      "          policy_loss: -0.057497191429138186\n",
      "          total_loss: 0.1700943077603976\n",
      "          vf_explained_var: 0.8311173915863037\n",
      "          vf_loss: 0.24338524701694647\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "  iterations_since_restore: 280\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.357692307692304\n",
      "    ram_util_percent: 60.330769230769235\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038443585010761346\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.982164856618255\n",
      "    mean_inference_ms: 1.5726416341785392\n",
      "    mean_raw_obs_processing_ms: 1.7545673145707397\n",
      "  time_since_restore: 6820.753712415695\n",
      "  time_this_iter_s: 18.066275596618652\n",
      "  time_total_s: 6820.753712415695\n",
      "  timers:\n",
      "    learn_throughput: 959.354\n",
      "    learn_time_ms: 1042.368\n",
      "    load_throughput: 78864.761\n",
      "    load_time_ms: 12.68\n",
      "    sample_throughput: 48.128\n",
      "    sample_time_ms: 20777.821\n",
      "    update_time_ms: 2.341\n",
      "  timestamp: 1633712623\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 280\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   280</td><td style=\"text-align: right;\">         6820.75</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\">    3.55</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            403.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 281000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-03-59\n",
      "  done: false\n",
      "  episode_len_mean: 407.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.65\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 735\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7751004238923391\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008073381467086636\n",
      "          policy_loss: -0.13437647303152417\n",
      "          total_loss: 0.08380766369195448\n",
      "          vf_explained_var: 0.07794912904500961\n",
      "          vf_loss: 0.23076234391580025\n",
      "    num_agent_steps_sampled: 281000\n",
      "    num_agent_steps_trained: 281000\n",
      "    num_steps_sampled: 281000\n",
      "    num_steps_trained: 281000\n",
      "  iterations_since_restore: 281\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.30869565217392\n",
      "    ram_util_percent: 60.42173913043479\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03844229193884354\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.973201480038647\n",
      "    mean_inference_ms: 1.5725959166643153\n",
      "    mean_raw_obs_processing_ms: 1.7602575764184303\n",
      "  time_since_restore: 6836.985976219177\n",
      "  time_this_iter_s: 16.232263803482056\n",
      "  time_total_s: 6836.985976219177\n",
      "  timers:\n",
      "    learn_throughput: 958.678\n",
      "    learn_time_ms: 1043.103\n",
      "    load_throughput: 81366.17\n",
      "    load_time_ms: 12.29\n",
      "    sample_throughput: 48.275\n",
      "    sample_time_ms: 20714.794\n",
      "    update_time_ms: 2.349\n",
      "  timestamp: 1633712639\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 281000\n",
      "  training_iteration: 281\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   281</td><td style=\"text-align: right;\">         6836.99</td><td style=\"text-align: right;\">281000</td><td style=\"text-align: right;\">    3.65</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            407.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 282000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-04-23\n",
      "  done: false\n",
      "  episode_len_mean: 407.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.69\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 738\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2836639821529388\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01902391100382693\n",
      "          policy_loss: -0.11470280906392469\n",
      "          total_loss: 0.11782637081212467\n",
      "          vf_explained_var: 0.7578635215759277\n",
      "          vf_loss: 0.2331767681365212\n",
      "    num_agent_steps_sampled: 282000\n",
      "    num_agent_steps_trained: 282000\n",
      "    num_steps_sampled: 282000\n",
      "    num_steps_trained: 282000\n",
      "  iterations_since_restore: 282\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.48823529411764\n",
      "    ram_util_percent: 60.54705882352941\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038440365261036974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.959816061428604\n",
      "    mean_inference_ms: 1.5725283188985832\n",
      "    mean_raw_obs_processing_ms: 1.7687594641474116\n",
      "  time_since_restore: 6861.298905134201\n",
      "  time_this_iter_s: 24.312928915023804\n",
      "  time_total_s: 6861.298905134201\n",
      "  timers:\n",
      "    learn_throughput: 959.343\n",
      "    learn_time_ms: 1042.38\n",
      "    load_throughput: 81566.968\n",
      "    load_time_ms: 12.26\n",
      "    sample_throughput: 47.786\n",
      "    sample_time_ms: 20926.475\n",
      "    update_time_ms: 2.673\n",
      "  timestamp: 1633712663\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 282000\n",
      "  training_iteration: 282\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   282</td><td style=\"text-align: right;\">          6861.3</td><td style=\"text-align: right;\">282000</td><td style=\"text-align: right;\">    3.69</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            407.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 283000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-04-44\n",
      "  done: false\n",
      "  episode_len_mean: 406.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.71\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 740\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5717877957555983\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013896964149954666\n",
      "          policy_loss: 0.015649333596229553\n",
      "          total_loss: 0.17673367477125593\n",
      "          vf_explained_var: 0.8516474962234497\n",
      "          vf_loss: 0.16789811924099923\n",
      "    num_agent_steps_sampled: 283000\n",
      "    num_agent_steps_trained: 283000\n",
      "    num_steps_sampled: 283000\n",
      "    num_steps_trained: 283000\n",
      "  iterations_since_restore: 283\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.120000000000005\n",
      "    ram_util_percent: 60.68\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03843909104163124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.95079718833385\n",
      "    mean_inference_ms: 1.5724837641136462\n",
      "    mean_raw_obs_processing_ms: 1.7744407245088536\n",
      "  time_since_restore: 6881.646063566208\n",
      "  time_this_iter_s: 20.347158432006836\n",
      "  time_total_s: 6881.646063566208\n",
      "  timers:\n",
      "    learn_throughput: 959.637\n",
      "    learn_time_ms: 1042.06\n",
      "    load_throughput: 74842.154\n",
      "    load_time_ms: 13.361\n",
      "    sample_throughput: 47.136\n",
      "    sample_time_ms: 21215.22\n",
      "    update_time_ms: 2.67\n",
      "  timestamp: 1633712684\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 283000\n",
      "  training_iteration: 283\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   283</td><td style=\"text-align: right;\">         6881.65</td><td style=\"text-align: right;\">283000</td><td style=\"text-align: right;\">    3.71</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            406.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 284000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-05-00\n",
      "  done: false\n",
      "  episode_len_mean: 411.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.79\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 742\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0050316744380527\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012750110969160408\n",
      "          policy_loss: -0.04124150783237484\n",
      "          total_loss: 0.005681258170968957\n",
      "          vf_explained_var: 0.6155575513839722\n",
      "          vf_loss: 0.05880379769951105\n",
      "    num_agent_steps_sampled: 284000\n",
      "    num_agent_steps_trained: 284000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "  iterations_since_restore: 284\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.43636363636364\n",
      "    ram_util_percent: 60.84090909090909\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038437791072805566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.94109832783598\n",
      "    mean_inference_ms: 1.5724380013166217\n",
      "    mean_raw_obs_processing_ms: 1.7800553826342491\n",
      "  time_since_restore: 6897.692257404327\n",
      "  time_this_iter_s: 16.046193838119507\n",
      "  time_total_s: 6897.692257404327\n",
      "  timers:\n",
      "    learn_throughput: 958.866\n",
      "    learn_time_ms: 1042.898\n",
      "    load_throughput: 84117.74\n",
      "    load_time_ms: 11.888\n",
      "    sample_throughput: 57.914\n",
      "    sample_time_ms: 17266.965\n",
      "    update_time_ms: 2.666\n",
      "  timestamp: 1633712700\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 284\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   284</td><td style=\"text-align: right;\">         6897.69</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">    3.79</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            411.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 285000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-05-17\n",
      "  done: false\n",
      "  episode_len_mean: 413.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.83\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 744\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6482304414113362\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007466761918287388\n",
      "          policy_loss: -0.02754914557768239\n",
      "          total_loss: 0.1465708757026328\n",
      "          vf_explained_var: 0.39014294743537903\n",
      "          vf_loss: 0.18581819995823834\n",
      "    num_agent_steps_sampled: 285000\n",
      "    num_agent_steps_trained: 285000\n",
      "    num_steps_sampled: 285000\n",
      "    num_steps_trained: 285000\n",
      "  iterations_since_restore: 285\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.304\n",
      "    ram_util_percent: 60.62\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038436458101536154\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.931157482087624\n",
      "    mean_inference_ms: 1.5723918478394352\n",
      "    mean_raw_obs_processing_ms: 1.7856073234151761\n",
      "  time_since_restore: 6914.549617052078\n",
      "  time_this_iter_s: 16.857359647750854\n",
      "  time_total_s: 6914.549617052078\n",
      "  timers:\n",
      "    learn_throughput: 959.076\n",
      "    learn_time_ms: 1042.67\n",
      "    load_throughput: 80492.824\n",
      "    load_time_ms: 12.423\n",
      "    sample_throughput: 58.159\n",
      "    sample_time_ms: 17194.148\n",
      "    update_time_ms: 2.658\n",
      "  timestamp: 1633712717\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 285000\n",
      "  training_iteration: 285\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   285</td><td style=\"text-align: right;\">         6914.55</td><td style=\"text-align: right;\">285000</td><td style=\"text-align: right;\">    3.83</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            413.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 286000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-05-32\n",
      "  done: false\n",
      "  episode_len_mean: 416.08\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.09\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 746\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.85783109664917\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011462179847189762\n",
      "          policy_loss: -0.07761013760334916\n",
      "          total_loss: 0.04095861537175046\n",
      "          vf_explained_var: 0.8758978843688965\n",
      "          vf_loss: 0.12980298702087667\n",
      "    num_agent_steps_sampled: 286000\n",
      "    num_agent_steps_trained: 286000\n",
      "    num_steps_sampled: 286000\n",
      "    num_steps_trained: 286000\n",
      "  iterations_since_restore: 286\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.6047619047619\n",
      "    ram_util_percent: 60.59999999999999\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03843514462688403\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.920855024873905\n",
      "    mean_inference_ms: 1.57234539140473\n",
      "    mean_raw_obs_processing_ms: 1.7911552810039046\n",
      "  time_since_restore: 6929.87366437912\n",
      "  time_this_iter_s: 15.324047327041626\n",
      "  time_total_s: 6929.87366437912\n",
      "  timers:\n",
      "    learn_throughput: 959.418\n",
      "    learn_time_ms: 1042.298\n",
      "    load_throughput: 91603.891\n",
      "    load_time_ms: 10.917\n",
      "    sample_throughput: 59.079\n",
      "    sample_time_ms: 16926.375\n",
      "    update_time_ms: 2.667\n",
      "  timestamp: 1633712732\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 286000\n",
      "  training_iteration: 286\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   286</td><td style=\"text-align: right;\">         6929.87</td><td style=\"text-align: right;\">286000</td><td style=\"text-align: right;\">    4.09</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            416.08</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 287000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-05-50\n",
      "  done: false\n",
      "  episode_len_mean: 418.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.08\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 748\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8101416852739123\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009242923066657922\n",
      "          policy_loss: -0.037875067525439794\n",
      "          total_loss: 0.05734352775745922\n",
      "          vf_explained_var: 0.8556803464889526\n",
      "          vf_loss: 0.10739786091984974\n",
      "    num_agent_steps_sampled: 287000\n",
      "    num_agent_steps_trained: 287000\n",
      "    num_steps_sampled: 287000\n",
      "    num_steps_trained: 287000\n",
      "  iterations_since_restore: 287\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.364\n",
      "    ram_util_percent: 60.592\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03843383660282063\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.910192679069745\n",
      "    mean_inference_ms: 1.5722977203988466\n",
      "    mean_raw_obs_processing_ms: 1.7966398263375647\n",
      "  time_since_restore: 6947.396295785904\n",
      "  time_this_iter_s: 17.522631406784058\n",
      "  time_total_s: 6947.396295785904\n",
      "  timers:\n",
      "    learn_throughput: 959.222\n",
      "    learn_time_ms: 1042.512\n",
      "    load_throughput: 96633.598\n",
      "    load_time_ms: 10.348\n",
      "    sample_throughput: 59.294\n",
      "    sample_time_ms: 16865.106\n",
      "    update_time_ms: 2.652\n",
      "  timestamp: 1633712750\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 287000\n",
      "  training_iteration: 287\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   287</td><td style=\"text-align: right;\">          6947.4</td><td style=\"text-align: right;\">287000</td><td style=\"text-align: right;\">    4.08</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            418.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-06-22\n",
      "  done: false\n",
      "  episode_len_mean: 421.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.15\n",
      "  episode_reward_min: -12.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 750\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2093524992465974\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01362787750493442\n",
      "          policy_loss: -0.032030496001243594\n",
      "          total_loss: 0.3852212616552909\n",
      "          vf_explained_var: 0.6145175099372864\n",
      "          vf_loss: 0.42061359302865137\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "  iterations_since_restore: 288\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.541304347826095\n",
      "    ram_util_percent: 60.58695652173913\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038432586317092496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.899061022959685\n",
      "    mean_inference_ms: 1.572249191193484\n",
      "    mean_raw_obs_processing_ms: 1.8032760601295967\n",
      "  time_since_restore: 6979.567036151886\n",
      "  time_this_iter_s: 32.170740365982056\n",
      "  time_total_s: 6979.567036151886\n",
      "  timers:\n",
      "    learn_throughput: 959.878\n",
      "    learn_time_ms: 1041.8\n",
      "    load_throughput: 85156.963\n",
      "    load_time_ms: 11.743\n",
      "    sample_throughput: 54.204\n",
      "    sample_time_ms: 18448.898\n",
      "    update_time_ms: 2.658\n",
      "  timestamp: 1633712782\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 288\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   288</td><td style=\"text-align: right;\">         6979.57</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\">    4.15</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -12</td><td style=\"text-align: right;\">            421.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 289000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-06-39\n",
      "  done: false\n",
      "  episode_len_mean: 423.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.37\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 752\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7196778376897177\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011095439330245692\n",
      "          policy_loss: -0.0015987845758597056\n",
      "          total_loss: 0.4520568241675695\n",
      "          vf_explained_var: 0.7573316693305969\n",
      "          vf_loss: 0.4637432807849513\n",
      "    num_agent_steps_sampled: 289000\n",
      "    num_agent_steps_trained: 289000\n",
      "    num_steps_sampled: 289000\n",
      "    num_steps_trained: 289000\n",
      "  iterations_since_restore: 289\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.636\n",
      "    ram_util_percent: 60.38\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038431320930712755\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.887658413422955\n",
      "    mean_inference_ms: 1.5721999798861919\n",
      "    mean_raw_obs_processing_ms: 1.8098474933540538\n",
      "  time_since_restore: 6996.521171092987\n",
      "  time_this_iter_s: 16.954134941101074\n",
      "  time_total_s: 6996.521171092987\n",
      "  timers:\n",
      "    learn_throughput: 960.893\n",
      "    learn_time_ms: 1040.699\n",
      "    load_throughput: 94369.811\n",
      "    load_time_ms: 10.597\n",
      "    sample_throughput: 54.566\n",
      "    sample_time_ms: 18326.299\n",
      "    update_time_ms: 2.579\n",
      "  timestamp: 1633712799\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 289000\n",
      "  training_iteration: 289\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   289</td><td style=\"text-align: right;\">         6996.52</td><td style=\"text-align: right;\">289000</td><td style=\"text-align: right;\">    4.37</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            423.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 290000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-06-56\n",
      "  done: false\n",
      "  episode_len_mean: 424.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.41\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 754\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5524844884872437\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017093273249784918\n",
      "          policy_loss: -0.07503005547655953\n",
      "          total_loss: 0.1031030338878433\n",
      "          vf_explained_var: 0.5990929007530212\n",
      "          vf_loss: 0.18270588542024294\n",
      "    num_agent_steps_sampled: 290000\n",
      "    num_agent_steps_trained: 290000\n",
      "    num_steps_sampled: 290000\n",
      "    num_steps_trained: 290000\n",
      "  iterations_since_restore: 290\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.24583333333334\n",
      "    ram_util_percent: 60.22083333333334\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03843004952543793\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.876084204822956\n",
      "    mean_inference_ms: 1.5721510313111906\n",
      "    mean_raw_obs_processing_ms: 1.8164131664263925\n",
      "  time_since_restore: 7013.570109128952\n",
      "  time_this_iter_s: 17.048938035964966\n",
      "  time_total_s: 7013.570109128952\n",
      "  timers:\n",
      "    learn_throughput: 961.485\n",
      "    learn_time_ms: 1040.058\n",
      "    load_throughput: 96983.733\n",
      "    load_time_ms: 10.311\n",
      "    sample_throughput: 54.868\n",
      "    sample_time_ms: 18225.518\n",
      "    update_time_ms: 2.585\n",
      "  timestamp: 1633712816\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 290000\n",
      "  training_iteration: 290\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   290</td><td style=\"text-align: right;\">         7013.57</td><td style=\"text-align: right;\">290000</td><td style=\"text-align: right;\">    4.41</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             424.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 291000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-07-12\n",
      "  done: false\n",
      "  episode_len_mean: 426.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.4\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 756\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5873633126417797\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012023894825905835\n",
      "          policy_loss: 0.1009873017668724\n",
      "          total_loss: 0.2801448056474328\n",
      "          vf_explained_var: 0.7820607423782349\n",
      "          vf_loss: 0.18732715919613838\n",
      "    num_agent_steps_sampled: 291000\n",
      "    num_agent_steps_trained: 291000\n",
      "    num_steps_sampled: 291000\n",
      "    num_steps_trained: 291000\n",
      "  iterations_since_restore: 291\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.38260869565217\n",
      "    ram_util_percent: 60.15652173913042\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03842879376260319\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.864240612783178\n",
      "    mean_inference_ms: 1.5721017993870852\n",
      "    mean_raw_obs_processing_ms: 1.8229713972644968\n",
      "  time_since_restore: 7029.8200607299805\n",
      "  time_this_iter_s: 16.249951601028442\n",
      "  time_total_s: 7029.8200607299805\n",
      "  timers:\n",
      "    learn_throughput: 961.998\n",
      "    learn_time_ms: 1039.503\n",
      "    load_throughput: 97011.773\n",
      "    load_time_ms: 10.308\n",
      "    sample_throughput: 54.861\n",
      "    sample_time_ms: 18227.85\n",
      "    update_time_ms: 2.573\n",
      "  timestamp: 1633712832\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 291000\n",
      "  training_iteration: 291\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   291</td><td style=\"text-align: right;\">         7029.82</td><td style=\"text-align: right;\">291000</td><td style=\"text-align: right;\">     4.4</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            426.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 292000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-07-29\n",
      "  done: false\n",
      "  episode_len_mean: 427.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.51\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 758\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8243105265829298\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012764947236982889\n",
      "          policy_loss: -0.0894015277011527\n",
      "          total_loss: 0.11639550094389253\n",
      "          vf_explained_var: 0.7165001034736633\n",
      "          vf_loss: 0.21586134129514298\n",
      "    num_agent_steps_sampled: 292000\n",
      "    num_agent_steps_trained: 292000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "  iterations_since_restore: 292\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.62\n",
      "    ram_util_percent: 60.16000000000001\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03842752295496167\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.85211769795598\n",
      "    mean_inference_ms: 1.5720518551216094\n",
      "    mean_raw_obs_processing_ms: 1.8294647189928646\n",
      "  time_since_restore: 7047.212520837784\n",
      "  time_this_iter_s: 17.392460107803345\n",
      "  time_total_s: 7047.212520837784\n",
      "  timers:\n",
      "    learn_throughput: 961.42\n",
      "    learn_time_ms: 1040.128\n",
      "    load_throughput: 108587.865\n",
      "    load_time_ms: 9.209\n",
      "    sample_throughput: 57.024\n",
      "    sample_time_ms: 17536.626\n",
      "    update_time_ms: 2.227\n",
      "  timestamp: 1633712849\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 292\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   292</td><td style=\"text-align: right;\">         7047.21</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">    4.51</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            427.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 293000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-07-46\n",
      "  done: false\n",
      "  episode_len_mean: 430.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.59\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 760\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9222833765877618\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011699441298868605\n",
      "          policy_loss: -0.13427579262190395\n",
      "          total_loss: -0.05831376554237472\n",
      "          vf_explained_var: 0.791107714176178\n",
      "          vf_loss: 0.08768876268424922\n",
      "    num_agent_steps_sampled: 293000\n",
      "    num_agent_steps_trained: 293000\n",
      "    num_steps_sampled: 293000\n",
      "    num_steps_trained: 293000\n",
      "  iterations_since_restore: 293\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.38333333333333\n",
      "    ram_util_percent: 60.12916666666666\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03842629927064299\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.839612242212315\n",
      "    mean_inference_ms: 1.572001726799238\n",
      "    mean_raw_obs_processing_ms: 1.8346248788749073\n",
      "  time_since_restore: 7064.190150976181\n",
      "  time_this_iter_s: 16.977630138397217\n",
      "  time_total_s: 7064.190150976181\n",
      "  timers:\n",
      "    learn_throughput: 960.41\n",
      "    learn_time_ms: 1041.222\n",
      "    load_throughput: 123570.134\n",
      "    load_time_ms: 8.093\n",
      "    sample_throughput: 58.141\n",
      "    sample_time_ms: 17199.708\n",
      "    update_time_ms: 2.231\n",
      "  timestamp: 1633712866\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 293000\n",
      "  training_iteration: 293\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   293</td><td style=\"text-align: right;\">         7064.19</td><td style=\"text-align: right;\">293000</td><td style=\"text-align: right;\">    4.59</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            430.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 294000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-08-06\n",
      "  done: false\n",
      "  episode_len_mean: 431.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.58\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 763\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7536607093281216\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0053725740308301\n",
      "          policy_loss: -0.0702342356244723\n",
      "          total_loss: 0.019368946469492384\n",
      "          vf_explained_var: 0.7484950423240662\n",
      "          vf_loss: 0.10369745660573244\n",
      "    num_agent_steps_sampled: 294000\n",
      "    num_agent_steps_trained: 294000\n",
      "    num_steps_sampled: 294000\n",
      "    num_steps_trained: 294000\n",
      "  iterations_since_restore: 294\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.168965517241375\n",
      "    ram_util_percent: 60.15862068965518\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03842443978055657\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.820686479888792\n",
      "    mean_inference_ms: 1.571926147329437\n",
      "    mean_raw_obs_processing_ms: 1.8423628809046897\n",
      "  time_since_restore: 7084.029675722122\n",
      "  time_this_iter_s: 19.839524745941162\n",
      "  time_total_s: 7084.029675722122\n",
      "  timers:\n",
      "    learn_throughput: 959.549\n",
      "    learn_time_ms: 1042.156\n",
      "    load_throughput: 103717.998\n",
      "    load_time_ms: 9.642\n",
      "    sample_throughput: 56.894\n",
      "    sample_time_ms: 17576.543\n",
      "    update_time_ms: 2.255\n",
      "  timestamp: 1633712886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 294000\n",
      "  training_iteration: 294\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   294</td><td style=\"text-align: right;\">         7084.03</td><td style=\"text-align: right;\">294000</td><td style=\"text-align: right;\">    4.58</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            431.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 295000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-08-27\n",
      "  done: false\n",
      "  episode_len_mean: 430.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.53\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 765\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7835397316349877\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013614154979025272\n",
      "          policy_loss: 0.013771314918994904\n",
      "          total_loss: 0.1416669194897016\n",
      "          vf_explained_var: 0.8234504461288452\n",
      "          vf_loss: 0.13700810329367716\n",
      "    num_agent_steps_sampled: 295000\n",
      "    num_agent_steps_trained: 295000\n",
      "    num_steps_sampled: 295000\n",
      "    num_steps_trained: 295000\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.40689655172414\n",
      "    ram_util_percent: 60.29655172413793\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038423205416408576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.808067273330106\n",
      "    mean_inference_ms: 1.5718756795054671\n",
      "    mean_raw_obs_processing_ms: 1.8453385447757733\n",
      "  time_since_restore: 7104.3608775138855\n",
      "  time_this_iter_s: 20.331201791763306\n",
      "  time_total_s: 7104.3608775138855\n",
      "  timers:\n",
      "    learn_throughput: 960.185\n",
      "    learn_time_ms: 1041.466\n",
      "    load_throughput: 96560.406\n",
      "    load_time_ms: 10.356\n",
      "    sample_throughput: 55.791\n",
      "    sample_time_ms: 17923.914\n",
      "    update_time_ms: 2.266\n",
      "  timestamp: 1633712907\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 295000\n",
      "  training_iteration: 295\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   295</td><td style=\"text-align: right;\">         7104.36</td><td style=\"text-align: right;\">295000</td><td style=\"text-align: right;\">    4.53</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            430.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-09-25\n",
      "  done: false\n",
      "  episode_len_mean: 435.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.46\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 767\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.399872339434094\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01849792011192181\n",
      "          policy_loss: -0.05921126852432887\n",
      "          total_loss: 0.6911232378747728\n",
      "          vf_explained_var: 0.40038952231407166\n",
      "          vf_loss: 0.7524811956617568\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "  iterations_since_restore: 296\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.044578313253016\n",
      "    ram_util_percent: 60.44578313253011\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03842195020261139\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.795203122323173\n",
      "    mean_inference_ms: 1.571824473584886\n",
      "    mean_raw_obs_processing_ms: 1.8508986918143022\n",
      "  time_since_restore: 7163.050046682358\n",
      "  time_this_iter_s: 58.68916916847229\n",
      "  time_total_s: 7163.050046682358\n",
      "  timers:\n",
      "    learn_throughput: 959.51\n",
      "    learn_time_ms: 1042.199\n",
      "    load_throughput: 84835.903\n",
      "    load_time_ms: 11.787\n",
      "    sample_throughput: 44.927\n",
      "    sample_time_ms: 22258.255\n",
      "    update_time_ms: 2.266\n",
      "  timestamp: 1633712965\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 296\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   296</td><td style=\"text-align: right;\">         7163.05</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\">    4.46</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            435.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 297000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-09-42\n",
      "  done: false\n",
      "  episode_len_mean: 436.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.52\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 770\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1739113357332016\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00665885349465262\n",
      "          policy_loss: -0.05278146856774887\n",
      "          total_loss: -0.022681471415691906\n",
      "          vf_explained_var: 0.7821878790855408\n",
      "          vf_loss: 0.04757263453470336\n",
      "    num_agent_steps_sampled: 297000\n",
      "    num_agent_steps_trained: 297000\n",
      "    num_steps_sampled: 297000\n",
      "    num_steps_trained: 297000\n",
      "  iterations_since_restore: 297\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.111999999999995\n",
      "    ram_util_percent: 60.54\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03842004324546452\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.77600797506187\n",
      "    mean_inference_ms: 1.5717500251422332\n",
      "    mean_raw_obs_processing_ms: 1.8593056014800282\n",
      "  time_since_restore: 7179.945753574371\n",
      "  time_this_iter_s: 16.89570689201355\n",
      "  time_total_s: 7179.945753574371\n",
      "  timers:\n",
      "    learn_throughput: 961.176\n",
      "    learn_time_ms: 1040.392\n",
      "    load_throughput: 84873.67\n",
      "    load_time_ms: 11.782\n",
      "    sample_throughput: 45.05\n",
      "    sample_time_ms: 22197.373\n",
      "    update_time_ms: 2.277\n",
      "  timestamp: 1633712982\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 297000\n",
      "  training_iteration: 297\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   297</td><td style=\"text-align: right;\">         7179.95</td><td style=\"text-align: right;\">297000</td><td style=\"text-align: right;\">    4.52</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            436.89</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 298000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-09-59\n",
      "  done: false\n",
      "  episode_len_mean: 438.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.56\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 772\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0523120548990037\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009305711270799414\n",
      "          policy_loss: 0.019430678255028196\n",
      "          total_loss: 0.13734477882583937\n",
      "          vf_explained_var: 0.7930223941802979\n",
      "          vf_loss: 0.132474843516118\n",
      "    num_agent_steps_sampled: 298000\n",
      "    num_agent_steps_trained: 298000\n",
      "    num_steps_sampled: 298000\n",
      "    num_steps_trained: 298000\n",
      "  iterations_since_restore: 298\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.55416666666667\n",
      "    ram_util_percent: 60.51666666666667\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03841881888290719\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.76320163929213\n",
      "    mean_inference_ms: 1.5717015205556786\n",
      "    mean_raw_obs_processing_ms: 1.8649303277185527\n",
      "  time_since_restore: 7196.743999958038\n",
      "  time_this_iter_s: 16.798246383666992\n",
      "  time_total_s: 7196.743999958038\n",
      "  timers:\n",
      "    learn_throughput: 961.913\n",
      "    learn_time_ms: 1039.596\n",
      "    load_throughput: 88994.734\n",
      "    load_time_ms: 11.237\n",
      "    sample_throughput: 48.399\n",
      "    sample_time_ms: 20661.472\n",
      "    update_time_ms: 2.271\n",
      "  timestamp: 1633712999\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 298000\n",
      "  training_iteration: 298\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   298</td><td style=\"text-align: right;\">         7196.74</td><td style=\"text-align: right;\">298000</td><td style=\"text-align: right;\">    4.56</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">               438</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 299000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-10-17\n",
      "  done: false\n",
      "  episode_len_mean: 438.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.64\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 774\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0918477972348533\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014371694402279466\n",
      "          policy_loss: 0.02854283567931917\n",
      "          total_loss: 0.19496196516686015\n",
      "          vf_explained_var: 0.6250301003456116\n",
      "          vf_loss: 0.1781293334128956\n",
      "    num_agent_steps_sampled: 299000\n",
      "    num_agent_steps_trained: 299000\n",
      "    num_steps_sampled: 299000\n",
      "    num_steps_trained: 299000\n",
      "  iterations_since_restore: 299\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.50769230769231\n",
      "    ram_util_percent: 60.31153846153846\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03841763761939723\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.75037467956257\n",
      "    mean_inference_ms: 1.57165365814388\n",
      "    mean_raw_obs_processing_ms: 1.870552308787988\n",
      "  time_since_restore: 7214.996798753738\n",
      "  time_this_iter_s: 18.252798795700073\n",
      "  time_total_s: 7214.996798753738\n",
      "  timers:\n",
      "    learn_throughput: 963.106\n",
      "    learn_time_ms: 1038.308\n",
      "    load_throughput: 81056.244\n",
      "    load_time_ms: 12.337\n",
      "    sample_throughput: 48.097\n",
      "    sample_time_ms: 20791.528\n",
      "    update_time_ms: 2.249\n",
      "  timestamp: 1633713017\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 299000\n",
      "  training_iteration: 299\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">            7215</td><td style=\"text-align: right;\">299000</td><td style=\"text-align: right;\">    4.64</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">               438</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-10-37\n",
      "  done: false\n",
      "  episode_len_mean: 437.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.52\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 776\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7053083803918627\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014541340531721483\n",
      "          policy_loss: 0.015039909217092726\n",
      "          total_loss: 0.44593113027513026\n",
      "          vf_explained_var: 0.6258677840232849\n",
      "          vf_loss: 0.43862734370761447\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_agent_steps_trained: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "  iterations_since_restore: 300\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.388888888888886\n",
      "    ram_util_percent: 60.1888888888889\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038416475124298105\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.737499201797387\n",
      "    mean_inference_ms: 1.5716059619653646\n",
      "    mean_raw_obs_processing_ms: 1.8761105804169922\n",
      "  time_since_restore: 7234.3085289001465\n",
      "  time_this_iter_s: 19.31173014640808\n",
      "  time_total_s: 7234.3085289001465\n",
      "  timers:\n",
      "    learn_throughput: 961.678\n",
      "    learn_time_ms: 1039.849\n",
      "    load_throughput: 74387.14\n",
      "    load_time_ms: 13.443\n",
      "    sample_throughput: 47.586\n",
      "    sample_time_ms: 21014.62\n",
      "    update_time_ms: 2.778\n",
      "  timestamp: 1633713037\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 300\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   300</td><td style=\"text-align: right;\">         7234.31</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">    4.52</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            437.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 301000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-10-57\n",
      "  done: false\n",
      "  episode_len_mean: 436.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.42\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 779\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226562500001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3973762883080376\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.021650209352891952\n",
      "          policy_loss: 0.023269061620036762\n",
      "          total_loss: 0.3375765544672807\n",
      "          vf_explained_var: 0.6210007071495056\n",
      "          vf_loss: 0.31440947329004604\n",
      "    num_agent_steps_sampled: 301000\n",
      "    num_agent_steps_trained: 301000\n",
      "    num_steps_sampled: 301000\n",
      "    num_steps_trained: 301000\n",
      "  iterations_since_restore: 301\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.03793103448275\n",
      "    ram_util_percent: 60.17241379310344\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038414754437527204\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.71871112978353\n",
      "    mean_inference_ms: 1.5715358537476787\n",
      "    mean_raw_obs_processing_ms: 1.8845105661562362\n",
      "  time_since_restore: 7254.344615697861\n",
      "  time_this_iter_s: 20.036086797714233\n",
      "  time_total_s: 7254.344615697861\n",
      "  timers:\n",
      "    learn_throughput: 960.093\n",
      "    learn_time_ms: 1041.565\n",
      "    load_throughput: 66823.714\n",
      "    load_time_ms: 14.965\n",
      "    sample_throughput: 46.751\n",
      "    sample_time_ms: 21390.003\n",
      "    update_time_ms: 2.78\n",
      "  timestamp: 1633713057\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 301000\n",
      "  training_iteration: 301\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   301</td><td style=\"text-align: right;\">         7254.34</td><td style=\"text-align: right;\">301000</td><td style=\"text-align: right;\">    4.42</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            436.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 302000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-11-13\n",
      "  done: false\n",
      "  episode_len_mean: 438.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.37\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 781\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9610839843749996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4555010855197907\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013902478328056588\n",
      "          policy_loss: -0.07784572256108126\n",
      "          total_loss: 0.17611882955663735\n",
      "          vf_explained_var: 0.82222580909729\n",
      "          vf_loss: 0.2551581107907825\n",
      "    num_agent_steps_sampled: 302000\n",
      "    num_agent_steps_trained: 302000\n",
      "    num_steps_sampled: 302000\n",
      "    num_steps_trained: 302000\n",
      "  iterations_since_restore: 302\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.38260869565217\n",
      "    ram_util_percent: 60.182608695652185\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03841362759241956\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.706248956077015\n",
      "    mean_inference_ms: 1.571489871213094\n",
      "    mean_raw_obs_processing_ms: 1.8901295931383384\n",
      "  time_since_restore: 7270.870977163315\n",
      "  time_this_iter_s: 16.5263614654541\n",
      "  time_total_s: 7270.870977163315\n",
      "  timers:\n",
      "    learn_throughput: 960.853\n",
      "    learn_time_ms: 1040.742\n",
      "    load_throughput: 64959.384\n",
      "    load_time_ms: 15.394\n",
      "    sample_throughput: 46.94\n",
      "    sample_time_ms: 21303.809\n",
      "    update_time_ms: 2.773\n",
      "  timestamp: 1633713073\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 302000\n",
      "  training_iteration: 302\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   302</td><td style=\"text-align: right;\">         7270.87</td><td style=\"text-align: right;\">302000</td><td style=\"text-align: right;\">    4.37</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">             438.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 303000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-11-31\n",
      "  done: false\n",
      "  episode_len_mean: 438.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.34\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 783\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9610839843749996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6764468206299676\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010308828955836959\n",
      "          policy_loss: -0.06345781824655003\n",
      "          total_loss: 0.16307987835672166\n",
      "          vf_explained_var: 0.12066594511270523\n",
      "          vf_loss: 0.23339451336198383\n",
      "    num_agent_steps_sampled: 303000\n",
      "    num_agent_steps_trained: 303000\n",
      "    num_steps_sampled: 303000\n",
      "    num_steps_trained: 303000\n",
      "  iterations_since_restore: 303\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.44615384615384\n",
      "    ram_util_percent: 60.29230769230769\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03841251879092057\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.693745193519657\n",
      "    mean_inference_ms: 1.5714440123240365\n",
      "    mean_raw_obs_processing_ms: 1.895745149461974\n",
      "  time_since_restore: 7288.487460136414\n",
      "  time_this_iter_s: 17.616482973098755\n",
      "  time_total_s: 7288.487460136414\n",
      "  timers:\n",
      "    learn_throughput: 962.655\n",
      "    learn_time_ms: 1038.794\n",
      "    load_throughput: 64834.571\n",
      "    load_time_ms: 15.424\n",
      "    sample_throughput: 46.795\n",
      "    sample_time_ms: 21369.609\n",
      "    update_time_ms: 2.781\n",
      "  timestamp: 1633713091\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 303000\n",
      "  training_iteration: 303\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   303</td><td style=\"text-align: right;\">         7288.49</td><td style=\"text-align: right;\">303000</td><td style=\"text-align: right;\">    4.34</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            438.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-11-48\n",
      "  done: false\n",
      "  episode_len_mean: 440.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.37\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 785\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9610839843749996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.763102040025923\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008119829865529284\n",
      "          policy_loss: -0.1506979723771413\n",
      "          total_loss: 0.0021228470736079745\n",
      "          vf_explained_var: 0.5869736671447754\n",
      "          vf_loss: 0.1626480032586389\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "  iterations_since_restore: 304\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.24583333333333\n",
      "    ram_util_percent: 60.39166666666666\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038411407265096786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.68095731282537\n",
      "    mean_inference_ms: 1.571397376526341\n",
      "    mean_raw_obs_processing_ms: 1.9012980039946017\n",
      "  time_since_restore: 7305.504861354828\n",
      "  time_this_iter_s: 17.017401218414307\n",
      "  time_total_s: 7305.504861354828\n",
      "  timers:\n",
      "    learn_throughput: 964.537\n",
      "    learn_time_ms: 1036.767\n",
      "    load_throughput: 69305.802\n",
      "    load_time_ms: 14.429\n",
      "    sample_throughput: 47.415\n",
      "    sample_time_ms: 21090.444\n",
      "    update_time_ms: 2.765\n",
      "  timestamp: 1633713108\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 304\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   304</td><td style=\"text-align: right;\">          7305.5</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\">    4.37</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            440.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 305000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-12-08\n",
      "  done: false\n",
      "  episode_len_mean: 439.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.26\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 787\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9610839843749996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.818873107433319\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008433997480264329\n",
      "          policy_loss: -0.08392707568903764\n",
      "          total_loss: 0.04182145881156127\n",
      "          vf_explained_var: 0.6062718629837036\n",
      "          vf_loss: 0.13583148208757242\n",
      "    num_agent_steps_sampled: 305000\n",
      "    num_agent_steps_trained: 305000\n",
      "    num_steps_sampled: 305000\n",
      "    num_steps_trained: 305000\n",
      "  iterations_since_restore: 305\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.417857142857144\n",
      "    ram_util_percent: 60.47142857142857\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03841027064024655\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.66833468180959\n",
      "    mean_inference_ms: 1.57135084471076\n",
      "    mean_raw_obs_processing_ms: 1.9068498121207273\n",
      "  time_since_restore: 7325.290085077286\n",
      "  time_this_iter_s: 19.785223722457886\n",
      "  time_total_s: 7325.290085077286\n",
      "  timers:\n",
      "    learn_throughput: 963.419\n",
      "    learn_time_ms: 1037.97\n",
      "    load_throughput: 69831.162\n",
      "    load_time_ms: 14.32\n",
      "    sample_throughput: 47.54\n",
      "    sample_time_ms: 21034.733\n",
      "    update_time_ms: 2.766\n",
      "  timestamp: 1633713128\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 305000\n",
      "  training_iteration: 305\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   305</td><td style=\"text-align: right;\">         7325.29</td><td style=\"text-align: right;\">305000</td><td style=\"text-align: right;\">    4.26</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            439.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 306000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-12-25\n",
      "  done: false\n",
      "  episode_len_mean: 442.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.28\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 789\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9610839843749996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1713587946361965\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008407599638416377\n",
      "          policy_loss: -0.0565200075507164\n",
      "          total_loss: 0.14410326172494226\n",
      "          vf_explained_var: 0.5004452466964722\n",
      "          vf_loss: 0.21425644839182495\n",
      "    num_agent_steps_sampled: 306000\n",
      "    num_agent_steps_trained: 306000\n",
      "    num_steps_sampled: 306000\n",
      "    num_steps_trained: 306000\n",
      "  iterations_since_restore: 306\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.37500000000001\n",
      "    ram_util_percent: 60.570833333333326\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038409141725603856\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.655401080791655\n",
      "    mean_inference_ms: 1.571304063922172\n",
      "    mean_raw_obs_processing_ms: 1.912397516332577\n",
      "  time_since_restore: 7342.191197872162\n",
      "  time_this_iter_s: 16.9011127948761\n",
      "  time_total_s: 7342.191197872162\n",
      "  timers:\n",
      "    learn_throughput: 964.616\n",
      "    learn_time_ms: 1036.682\n",
      "    load_throughput: 75360.0\n",
      "    load_time_ms: 13.27\n",
      "    sample_throughput: 59.318\n",
      "    sample_time_ms: 16858.296\n",
      "    update_time_ms: 2.748\n",
      "  timestamp: 1633713145\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 306000\n",
      "  training_iteration: 306\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   306</td><td style=\"text-align: right;\">         7342.19</td><td style=\"text-align: right;\">306000</td><td style=\"text-align: right;\">    4.28</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            442.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 307000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-12-42\n",
      "  done: false\n",
      "  episode_len_mean: 443.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.14\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 791\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9610839843749996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0772716893090144\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007520558971429169\n",
      "          policy_loss: -0.06054219546624356\n",
      "          total_loss: 0.04891807559049792\n",
      "          vf_explained_var: 0.16913650929927826\n",
      "          vf_loss: 0.12300509895301527\n",
      "    num_agent_steps_sampled: 307000\n",
      "    num_agent_steps_trained: 307000\n",
      "    num_steps_sampled: 307000\n",
      "    num_steps_trained: 307000\n",
      "  iterations_since_restore: 307\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.376\n",
      "    ram_util_percent: 60.6\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03840801581730173\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.641998571581198\n",
      "    mean_inference_ms: 1.571256428112796\n",
      "    mean_raw_obs_processing_ms: 1.9178841371726463\n",
      "  time_since_restore: 7359.7716290950775\n",
      "  time_this_iter_s: 17.58043122291565\n",
      "  time_total_s: 7359.7716290950775\n",
      "  timers:\n",
      "    learn_throughput: 962.716\n",
      "    learn_time_ms: 1038.728\n",
      "    load_throughput: 78146.53\n",
      "    load_time_ms: 12.796\n",
      "    sample_throughput: 59.083\n",
      "    sample_time_ms: 16925.202\n",
      "    update_time_ms: 2.74\n",
      "  timestamp: 1633713162\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 307000\n",
      "  training_iteration: 307\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   307</td><td style=\"text-align: right;\">         7359.77</td><td style=\"text-align: right;\">307000</td><td style=\"text-align: right;\">    4.14</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            443.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-13-00\n",
      "  done: false\n",
      "  episode_len_mean: 448.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.15\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 794\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9610839843749996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.061639404296875\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0074991091050111945\n",
      "          policy_loss: 0.0332437157837881\n",
      "          total_loss: 0.11014671062843667\n",
      "          vf_explained_var: 0.26194655895233154\n",
      "          vf_loss: 0.09031211144497825\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_agent_steps_trained: 308000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "  iterations_since_restore: 308\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.21538461538462\n",
      "    ram_util_percent: 60.599999999999994\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03840634566206786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.621326842508083\n",
      "    mean_inference_ms: 1.5711836924912672\n",
      "    mean_raw_obs_processing_ms: 1.9254501255184604\n",
      "  time_since_restore: 7377.65363574028\n",
      "  time_this_iter_s: 17.882006645202637\n",
      "  time_total_s: 7377.65363574028\n",
      "  timers:\n",
      "    learn_throughput: 962.532\n",
      "    learn_time_ms: 1038.927\n",
      "    load_throughput: 81204.858\n",
      "    load_time_ms: 12.315\n",
      "    sample_throughput: 58.707\n",
      "    sample_time_ms: 17033.846\n",
      "    update_time_ms: 2.752\n",
      "  timestamp: 1633713180\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 308\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   308</td><td style=\"text-align: right;\">         7377.65</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">    4.15</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            448.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 309000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-13-18\n",
      "  done: false\n",
      "  episode_len_mean: 450.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.99\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 796\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9610839843749996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.040237573782603\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015972232810895885\n",
      "          policy_loss: 0.0387748707499769\n",
      "          total_loss: 0.357785514742136\n",
      "          vf_explained_var: 0.7350740432739258\n",
      "          vf_loss: 0.3240623661213451\n",
      "    num_agent_steps_sampled: 309000\n",
      "    num_agent_steps_trained: 309000\n",
      "    num_steps_sampled: 309000\n",
      "    num_steps_trained: 309000\n",
      "  iterations_since_restore: 309\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.21923076923077\n",
      "    ram_util_percent: 60.607692307692304\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03840522466493281\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.607429624445615\n",
      "    mean_inference_ms: 1.571135198905682\n",
      "    mean_raw_obs_processing_ms: 1.9296080473645028\n",
      "  time_since_restore: 7395.832875013351\n",
      "  time_this_iter_s: 18.17923927307129\n",
      "  time_total_s: 7395.832875013351\n",
      "  timers:\n",
      "    learn_throughput: 960.497\n",
      "    learn_time_ms: 1041.127\n",
      "    load_throughput: 89295.418\n",
      "    load_time_ms: 11.199\n",
      "    sample_throughput: 58.736\n",
      "    sample_time_ms: 17025.413\n",
      "    update_time_ms: 2.769\n",
      "  timestamp: 1633713198\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 309000\n",
      "  training_iteration: 309\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   309</td><td style=\"text-align: right;\">         7395.83</td><td style=\"text-align: right;\">309000</td><td style=\"text-align: right;\">    3.99</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            450.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 310000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-13-52\n",
      "  done: false\n",
      "  episode_len_mean: 451.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.98\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 798\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.9610839843749996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1281428257624309\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003212878993435374\n",
      "          policy_loss: -0.22051541970835792\n",
      "          total_loss: -0.16389482923679882\n",
      "          vf_explained_var: -0.2827160656452179\n",
      "          vf_loss: 0.06481417265410225\n",
      "    num_agent_steps_sampled: 310000\n",
      "    num_agent_steps_trained: 310000\n",
      "    num_steps_sampled: 310000\n",
      "    num_steps_trained: 310000\n",
      "  iterations_since_restore: 310\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.07872340425531\n",
      "    ram_util_percent: 60.5595744680851\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038404108768846736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.59346327599982\n",
      "    mean_inference_ms: 1.571086715331378\n",
      "    mean_raw_obs_processing_ms: 1.934847315126171\n",
      "  time_since_restore: 7428.974469423294\n",
      "  time_this_iter_s: 33.14159440994263\n",
      "  time_total_s: 7428.974469423294\n",
      "  timers:\n",
      "    learn_throughput: 962.259\n",
      "    learn_time_ms: 1039.222\n",
      "    load_throughput: 89349.822\n",
      "    load_time_ms: 11.192\n",
      "    sample_throughput: 54.317\n",
      "    sample_time_ms: 18410.342\n",
      "    update_time_ms: 2.741\n",
      "  timestamp: 1633713232\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 310000\n",
      "  training_iteration: 310\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   310</td><td style=\"text-align: right;\">         7428.97</td><td style=\"text-align: right;\">310000</td><td style=\"text-align: right;\">    3.98</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            451.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 311000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-14-09\n",
      "  done: false\n",
      "  episode_len_mean: 451.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.91\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 800\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4805419921874998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2664751794603135\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02015513130285054\n",
      "          policy_loss: -0.05325193140241835\n",
      "          total_loss: 0.012692615886529287\n",
      "          vf_explained_var: -0.046519935131073\n",
      "          vf_loss: 0.078923910876943\n",
      "    num_agent_steps_sampled: 311000\n",
      "    num_agent_steps_trained: 311000\n",
      "    num_steps_sampled: 311000\n",
      "    num_steps_trained: 311000\n",
      "  iterations_since_restore: 311\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.06\n",
      "    ram_util_percent: 60.52799999999999\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038403026351747034\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.579613207004318\n",
      "    mean_inference_ms: 1.571038170786627\n",
      "    mean_raw_obs_processing_ms: 1.9400862780803652\n",
      "  time_since_restore: 7446.166699171066\n",
      "  time_this_iter_s: 17.192229747772217\n",
      "  time_total_s: 7446.166699171066\n",
      "  timers:\n",
      "    learn_throughput: 963.837\n",
      "    learn_time_ms: 1037.519\n",
      "    load_throughput: 98670.236\n",
      "    load_time_ms: 10.135\n",
      "    sample_throughput: 55.161\n",
      "    sample_time_ms: 18128.713\n",
      "    update_time_ms: 2.742\n",
      "  timestamp: 1633713249\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 311000\n",
      "  training_iteration: 311\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   311</td><td style=\"text-align: right;\">         7446.17</td><td style=\"text-align: right;\">311000</td><td style=\"text-align: right;\">    3.91</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            451.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-14-28\n",
      "  done: false\n",
      "  episode_len_mean: 450.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.89\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 802\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2043986214531794\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01295901329116802\n",
      "          policy_loss: -0.11262529840071996\n",
      "          total_loss: 0.09587334924274021\n",
      "          vf_explained_var: 0.5735534429550171\n",
      "          vf_loss: 0.2212016132970651\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "  iterations_since_restore: 312\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.42857142857143\n",
      "    ram_util_percent: 60.421428571428564\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03840198404378331\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.56595180133348\n",
      "    mean_inference_ms: 1.5709908087133722\n",
      "    mean_raw_obs_processing_ms: 1.9423789607469377\n",
      "  time_since_restore: 7465.748492240906\n",
      "  time_this_iter_s: 19.581793069839478\n",
      "  time_total_s: 7465.748492240906\n",
      "  timers:\n",
      "    learn_throughput: 962.056\n",
      "    learn_time_ms: 1039.44\n",
      "    load_throughput: 92958.041\n",
      "    load_time_ms: 10.758\n",
      "    sample_throughput: 54.254\n",
      "    sample_time_ms: 18431.702\n",
      "    update_time_ms: 2.75\n",
      "  timestamp: 1633713268\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 312\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   312</td><td style=\"text-align: right;\">         7465.75</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\">    3.89</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            450.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 313000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-14-46\n",
      "  done: false\n",
      "  episode_len_mean: 454.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.83\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 804\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2183524356948006\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009952094328739728\n",
      "          policy_loss: -0.021815314557817246\n",
      "          total_loss: 0.11908439675139057\n",
      "          vf_explained_var: 0.1031179130077362\n",
      "          vf_loss: 0.15590963564657917\n",
      "    num_agent_steps_sampled: 313000\n",
      "    num_agent_steps_trained: 313000\n",
      "    num_steps_sampled: 313000\n",
      "    num_steps_trained: 313000\n",
      "  iterations_since_restore: 313\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.355999999999995\n",
      "    ram_util_percent: 60.192\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03840090813953157\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.5521590027308\n",
      "    mean_inference_ms: 1.570943120744911\n",
      "    mean_raw_obs_processing_ms: 1.9446181380375607\n",
      "  time_since_restore: 7483.320623397827\n",
      "  time_this_iter_s: 17.572131156921387\n",
      "  time_total_s: 7483.320623397827\n",
      "  timers:\n",
      "    learn_throughput: 962.102\n",
      "    learn_time_ms: 1039.391\n",
      "    load_throughput: 93067.155\n",
      "    load_time_ms: 10.745\n",
      "    sample_throughput: 54.267\n",
      "    sample_time_ms: 18427.331\n",
      "    update_time_ms: 2.746\n",
      "  timestamp: 1633713286\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 313000\n",
      "  training_iteration: 313\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   313</td><td style=\"text-align: right;\">         7483.32</td><td style=\"text-align: right;\">313000</td><td style=\"text-align: right;\">    3.83</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            454.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 314000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-15-03\n",
      "  done: false\n",
      "  episode_len_mean: 452.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.85\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 806\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9471333980560304\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009259090992712524\n",
      "          policy_loss: -0.07147814631462097\n",
      "          total_loss: -0.032348400271601144\n",
      "          vf_explained_var: 0.7541350722312927\n",
      "          vf_loss: 0.051927002684937586\n",
      "    num_agent_steps_sampled: 314000\n",
      "    num_agent_steps_trained: 314000\n",
      "    num_steps_sampled: 314000\n",
      "    num_steps_trained: 314000\n",
      "  iterations_since_restore: 314\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.2875\n",
      "    ram_util_percent: 60.104166666666664\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03839983581673041\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.538215570033653\n",
      "    mean_inference_ms: 1.5708956066106197\n",
      "    mean_raw_obs_processing_ms: 1.9468671575032295\n",
      "  time_since_restore: 7499.908510923386\n",
      "  time_this_iter_s: 16.58788752555847\n",
      "  time_total_s: 7499.908510923386\n",
      "  timers:\n",
      "    learn_throughput: 962.474\n",
      "    learn_time_ms: 1038.99\n",
      "    load_throughput: 89943.216\n",
      "    load_time_ms: 11.118\n",
      "    sample_throughput: 54.394\n",
      "    sample_time_ms: 18384.413\n",
      "    update_time_ms: 2.739\n",
      "  timestamp: 1633713303\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 314000\n",
      "  training_iteration: 314\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   314</td><td style=\"text-align: right;\">         7499.91</td><td style=\"text-align: right;\">314000</td><td style=\"text-align: right;\">    3.85</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            452.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 315000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-15-21\n",
      "  done: false\n",
      "  episode_len_mean: 456.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.81\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 809\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8393680744700962\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012125391348487547\n",
      "          policy_loss: 0.006796630968650182\n",
      "          total_loss: 0.3588255542847845\n",
      "          vf_explained_var: 0.7707377672195435\n",
      "          vf_loss: 0.36168246641755103\n",
      "    num_agent_steps_sampled: 315000\n",
      "    num_agent_steps_trained: 315000\n",
      "    num_steps_sampled: 315000\n",
      "    num_steps_trained: 315000\n",
      "  iterations_since_restore: 315\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.323076923076925\n",
      "    ram_util_percent: 60.07307692307691\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03839824340370936\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.51709442691667\n",
      "    mean_inference_ms: 1.5708237807333696\n",
      "    mean_raw_obs_processing_ms: 1.950227588805528\n",
      "  time_since_restore: 7518.105433702469\n",
      "  time_this_iter_s: 18.196922779083252\n",
      "  time_total_s: 7518.105433702469\n",
      "  timers:\n",
      "    learn_throughput: 962.643\n",
      "    learn_time_ms: 1038.807\n",
      "    load_throughput: 89269.951\n",
      "    load_time_ms: 11.202\n",
      "    sample_throughput: 54.869\n",
      "    sample_time_ms: 18225.283\n",
      "    update_time_ms: 3.112\n",
      "  timestamp: 1633713321\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 315000\n",
      "  training_iteration: 315\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   315</td><td style=\"text-align: right;\">         7518.11</td><td style=\"text-align: right;\">315000</td><td style=\"text-align: right;\">    3.81</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            456.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 316000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-15-35\n",
      "  done: false\n",
      "  episode_len_mean: 458.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.77\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 810\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1324348429838815\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01438471199058532\n",
      "          policy_loss: -0.003971436123053233\n",
      "          total_loss: 0.19166828451885118\n",
      "          vf_explained_var: 0.6712617874145508\n",
      "          vf_loss: 0.19659538625015152\n",
      "    num_agent_steps_sampled: 316000\n",
      "    num_agent_steps_trained: 316000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "  iterations_since_restore: 316\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.98\n",
      "    ram_util_percent: 60.105\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03839771332583403\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.50988069767343\n",
      "    mean_inference_ms: 1.570799411323232\n",
      "    mean_raw_obs_processing_ms: 1.951331141137429\n",
      "  time_since_restore: 7532.46719455719\n",
      "  time_this_iter_s: 14.36176085472107\n",
      "  time_total_s: 7532.46719455719\n",
      "  timers:\n",
      "    learn_throughput: 960.819\n",
      "    learn_time_ms: 1040.779\n",
      "    load_throughput: 92309.507\n",
      "    load_time_ms: 10.833\n",
      "    sample_throughput: 55.649\n",
      "    sample_time_ms: 17969.717\n",
      "    update_time_ms: 3.133\n",
      "  timestamp: 1633713335\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 316\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   316</td><td style=\"text-align: right;\">         7532.47</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">    3.77</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            458.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 317000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-15-53\n",
      "  done: false\n",
      "  episode_len_mean: 459.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.83\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 812\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8305006477567884\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014721027279443275\n",
      "          policy_loss: -0.015557835830582513\n",
      "          total_loss: 0.22376658601893318\n",
      "          vf_explained_var: 0.6706510186195374\n",
      "          vf_loss: 0.24701831887165707\n",
      "    num_agent_steps_sampled: 317000\n",
      "    num_agent_steps_trained: 317000\n",
      "    num_steps_sampled: 317000\n",
      "    num_steps_trained: 317000\n",
      "  iterations_since_restore: 317\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.11538461538461\n",
      "    ram_util_percent: 60.215384615384615\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03839667423551928\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.495617122895887\n",
      "    mean_inference_ms: 1.5707510380278609\n",
      "    mean_raw_obs_processing_ms: 1.9535495271634926\n",
      "  time_since_restore: 7550.224335670471\n",
      "  time_this_iter_s: 17.75714111328125\n",
      "  time_total_s: 7550.224335670471\n",
      "  timers:\n",
      "    learn_throughput: 962.381\n",
      "    learn_time_ms: 1039.089\n",
      "    load_throughput: 83029.054\n",
      "    load_time_ms: 12.044\n",
      "    sample_throughput: 55.593\n",
      "    sample_time_ms: 17987.863\n",
      "    update_time_ms: 3.139\n",
      "  timestamp: 1633713353\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 317000\n",
      "  training_iteration: 317\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   317</td><td style=\"text-align: right;\">         7550.22</td><td style=\"text-align: right;\">317000</td><td style=\"text-align: right;\">    3.83</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            459.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 318000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-16-11\n",
      "  done: false\n",
      "  episode_len_mean: 460.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.93\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 815\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7208129882812502\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1301155514187284\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003797432060768793\n",
      "          policy_loss: -0.07839834002984894\n",
      "          total_loss: -0.05972943405310313\n",
      "          vf_explained_var: 0.34971973299980164\n",
      "          vf_loss: 0.03723282189263652\n",
      "    num_agent_steps_sampled: 318000\n",
      "    num_agent_steps_trained: 318000\n",
      "    num_steps_sampled: 318000\n",
      "    num_steps_trained: 318000\n",
      "  iterations_since_restore: 318\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.364\n",
      "    ram_util_percent: 60.279999999999994\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03839516743812035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.473863645938053\n",
      "    mean_inference_ms: 1.5706782053077282\n",
      "    mean_raw_obs_processing_ms: 1.9568927862480168\n",
      "  time_since_restore: 7567.808315753937\n",
      "  time_this_iter_s: 17.583980083465576\n",
      "  time_total_s: 7567.808315753937\n",
      "  timers:\n",
      "    learn_throughput: 961.261\n",
      "    learn_time_ms: 1040.3\n",
      "    load_throughput: 82963.854\n",
      "    load_time_ms: 12.053\n",
      "    sample_throughput: 55.689\n",
      "    sample_time_ms: 17956.841\n",
      "    update_time_ms: 3.139\n",
      "  timestamp: 1633713371\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 318000\n",
      "  training_iteration: 318\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   318</td><td style=\"text-align: right;\">         7567.81</td><td style=\"text-align: right;\">318000</td><td style=\"text-align: right;\">    3.93</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            460.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 319000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-16-32\n",
      "  done: false\n",
      "  episode_len_mean: 459.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.91\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 817\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9237023380067613\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010808682455744586\n",
      "          policy_loss: -0.11321612646182379\n",
      "          total_loss: -0.02989012168513404\n",
      "          vf_explained_var: -0.3418753445148468\n",
      "          vf_loss: 0.09866750644416444\n",
      "    num_agent_steps_sampled: 319000\n",
      "    num_agent_steps_trained: 319000\n",
      "    num_steps_sampled: 319000\n",
      "    num_steps_trained: 319000\n",
      "  iterations_since_restore: 319\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.34333333333334\n",
      "    ram_util_percent: 60.393333333333345\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03839419825447257\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.459679253309755\n",
      "    mean_inference_ms: 1.5706299548662077\n",
      "    mean_raw_obs_processing_ms: 1.959132272257856\n",
      "  time_since_restore: 7588.719136714935\n",
      "  time_this_iter_s: 20.910820960998535\n",
      "  time_total_s: 7588.719136714935\n",
      "  timers:\n",
      "    learn_throughput: 962.936\n",
      "    learn_time_ms: 1038.491\n",
      "    load_throughput: 75463.453\n",
      "    load_time_ms: 13.251\n",
      "    sample_throughput: 54.854\n",
      "    sample_time_ms: 18230.105\n",
      "    update_time_ms: 3.462\n",
      "  timestamp: 1633713392\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 319000\n",
      "  training_iteration: 319\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   319</td><td style=\"text-align: right;\">         7588.72</td><td style=\"text-align: right;\">319000</td><td style=\"text-align: right;\">    3.91</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            459.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-16-50\n",
      "  done: false\n",
      "  episode_len_mean: 460.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.87\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 820\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064941406251\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.769646696249644\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020141421046538474\n",
      "          policy_loss: -0.10630584646844202\n",
      "          total_loss: 0.12850174233317374\n",
      "          vf_explained_var: 0.5104609727859497\n",
      "          vf_loss: 0.2452449599901835\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "  iterations_since_restore: 320\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.455555555555556\n",
      "    ram_util_percent: 60.5037037037037\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03839277068686836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.43881196880535\n",
      "    mean_inference_ms: 1.5705586926219877\n",
      "    mean_raw_obs_processing_ms: 1.958214340334586\n",
      "  time_since_restore: 7607.656603097916\n",
      "  time_this_iter_s: 18.937466382980347\n",
      "  time_total_s: 7607.656603097916\n",
      "  timers:\n",
      "    learn_throughput: 961.058\n",
      "    learn_time_ms: 1040.52\n",
      "    load_throughput: 74541.419\n",
      "    load_time_ms: 13.415\n",
      "    sample_throughput: 59.495\n",
      "    sample_time_ms: 16807.999\n",
      "    update_time_ms: 2.954\n",
      "  timestamp: 1633713410\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 320\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   320</td><td style=\"text-align: right;\">         7607.66</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\">    3.87</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            460.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 321000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-17-08\n",
      "  done: false\n",
      "  episode_len_mean: 460.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.89\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 822\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9072558999061584\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014284447453929486\n",
      "          policy_loss: -0.03128850865695212\n",
      "          total_loss: 0.1572323412530952\n",
      "          vf_explained_var: 0.2971436679363251\n",
      "          vf_loss: 0.19987109624263313\n",
      "    num_agent_steps_sampled: 321000\n",
      "    num_agent_steps_trained: 321000\n",
      "    num_steps_sampled: 321000\n",
      "    num_steps_trained: 321000\n",
      "  iterations_since_restore: 321\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.36\n",
      "    ram_util_percent: 60.583999999999996\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0383918398945965\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.425016211968423\n",
      "    mean_inference_ms: 1.570511586140248\n",
      "    mean_raw_obs_processing_ms: 1.9576229937248935\n",
      "  time_since_restore: 7625.545667171478\n",
      "  time_this_iter_s: 17.889064073562622\n",
      "  time_total_s: 7625.545667171478\n",
      "  timers:\n",
      "    learn_throughput: 960.386\n",
      "    learn_time_ms: 1041.248\n",
      "    load_throughput: 74959.458\n",
      "    load_time_ms: 13.341\n",
      "    sample_throughput: 59.252\n",
      "    sample_time_ms: 16877.024\n",
      "    update_time_ms: 2.955\n",
      "  timestamp: 1633713428\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 321000\n",
      "  training_iteration: 321\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   321</td><td style=\"text-align: right;\">         7625.55</td><td style=\"text-align: right;\">321000</td><td style=\"text-align: right;\">    3.89</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            460.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 322000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-17-27\n",
      "  done: false\n",
      "  episode_len_mean: 460.18\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.85\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 824\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2390225225024754\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011056177595968133\n",
      "          policy_loss: 0.02108359556231234\n",
      "          total_loss: 0.13057014817992846\n",
      "          vf_explained_var: 0.6044613718986511\n",
      "          vf_loss: 0.12589970127575928\n",
      "    num_agent_steps_sampled: 322000\n",
      "    num_agent_steps_trained: 322000\n",
      "    num_steps_sampled: 322000\n",
      "    num_steps_trained: 322000\n",
      "  iterations_since_restore: 322\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.61481481481482\n",
      "    ram_util_percent: 60.62962962962964\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03839092060166906\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.41136569575169\n",
      "    mean_inference_ms: 1.5704648907854966\n",
      "    mean_raw_obs_processing_ms: 1.9570521761222\n",
      "  time_since_restore: 7644.43683385849\n",
      "  time_this_iter_s: 18.89116668701172\n",
      "  time_total_s: 7644.43683385849\n",
      "  timers:\n",
      "    learn_throughput: 961.943\n",
      "    learn_time_ms: 1039.563\n",
      "    load_throughput: 74685.831\n",
      "    load_time_ms: 13.389\n",
      "    sample_throughput: 59.49\n",
      "    sample_time_ms: 16809.605\n",
      "    update_time_ms: 2.96\n",
      "  timestamp: 1633713447\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 322000\n",
      "  training_iteration: 322\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   322</td><td style=\"text-align: right;\">         7644.44</td><td style=\"text-align: right;\">322000</td><td style=\"text-align: right;\">    3.85</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            460.18</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 323000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-18-04\n",
      "  done: false\n",
      "  episode_len_mean: 456.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.93\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 827\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4626359681288401\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011464443558533767\n",
      "          policy_loss: 0.091991460778647\n",
      "          total_loss: 0.16266903856562243\n",
      "          vf_explained_var: 0.9324560165405273\n",
      "          vf_loss: 0.07910614781495598\n",
      "    num_agent_steps_sampled: 323000\n",
      "    num_agent_steps_trained: 323000\n",
      "    num_steps_sampled: 323000\n",
      "    num_steps_trained: 323000\n",
      "  iterations_since_restore: 323\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.18679245283018\n",
      "    ram_util_percent: 60.607547169811326\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038389615697853305\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.391453230558298\n",
      "    mean_inference_ms: 1.570396640340617\n",
      "    mean_raw_obs_processing_ms: 1.957851366621202\n",
      "  time_since_restore: 7681.601006269455\n",
      "  time_this_iter_s: 37.164172410964966\n",
      "  time_total_s: 7681.601006269455\n",
      "  timers:\n",
      "    learn_throughput: 960.672\n",
      "    learn_time_ms: 1040.938\n",
      "    load_throughput: 68697.142\n",
      "    load_time_ms: 14.557\n",
      "    sample_throughput: 53.287\n",
      "    sample_time_ms: 18766.274\n",
      "    update_time_ms: 2.956\n",
      "  timestamp: 1633713484\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 323000\n",
      "  training_iteration: 323\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   323</td><td style=\"text-align: right;\">          7681.6</td><td style=\"text-align: right;\">323000</td><td style=\"text-align: right;\">    3.93</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            456.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 324000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-18-28\n",
      "  done: false\n",
      "  episode_len_mean: 454.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.95\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 829\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5530571871333652\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009066894247017308\n",
      "          policy_loss: 0.05534056706560982\n",
      "          total_loss: 0.16229440238740708\n",
      "          vf_explained_var: 0.7286220192909241\n",
      "          vf_loss: 0.11758275929217538\n",
      "    num_agent_steps_sampled: 324000\n",
      "    num_agent_steps_trained: 324000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "  iterations_since_restore: 324\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.93636363636364\n",
      "    ram_util_percent: 60.47575757575758\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03838878115438213\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.378707677983368\n",
      "    mean_inference_ms: 1.5703524426031328\n",
      "    mean_raw_obs_processing_ms: 1.958421793399546\n",
      "  time_since_restore: 7704.69868683815\n",
      "  time_this_iter_s: 23.09768056869507\n",
      "  time_total_s: 7704.69868683815\n",
      "  timers:\n",
      "    learn_throughput: 959.796\n",
      "    learn_time_ms: 1041.888\n",
      "    load_throughput: 65496.191\n",
      "    load_time_ms: 15.268\n",
      "    sample_throughput: 51.505\n",
      "    sample_time_ms: 19415.571\n",
      "    update_time_ms: 2.962\n",
      "  timestamp: 1633713508\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 324\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   324</td><td style=\"text-align: right;\">          7704.7</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">    3.95</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            454.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 325000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-18-46\n",
      "  done: false\n",
      "  episode_len_mean: 454.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.91\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 832\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4676397773954604\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010907545450707603\n",
      "          policy_loss: -0.16828270703554155\n",
      "          total_loss: 0.13972067659099896\n",
      "          vf_explained_var: 0.7229114770889282\n",
      "          vf_loss: 0.3167830565219952\n",
      "    num_agent_steps_sampled: 325000\n",
      "    num_agent_steps_trained: 325000\n",
      "    num_steps_sampled: 325000\n",
      "    num_steps_trained: 325000\n",
      "  iterations_since_restore: 325\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.24814814814816\n",
      "    ram_util_percent: 60.4888888888889\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03838755210870964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.35985342025919\n",
      "    mean_inference_ms: 1.5702868384883195\n",
      "    mean_raw_obs_processing_ms: 1.9593342097011572\n",
      "  time_since_restore: 7723.47562789917\n",
      "  time_this_iter_s: 18.776941061019897\n",
      "  time_total_s: 7723.47562789917\n",
      "  timers:\n",
      "    learn_throughput: 959.574\n",
      "    learn_time_ms: 1042.129\n",
      "    load_throughput: 65711.676\n",
      "    load_time_ms: 15.218\n",
      "    sample_throughput: 51.351\n",
      "    sample_time_ms: 19473.775\n",
      "    update_time_ms: 2.581\n",
      "  timestamp: 1633713526\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 325000\n",
      "  training_iteration: 325\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   325</td><td style=\"text-align: right;\">         7723.48</td><td style=\"text-align: right;\">325000</td><td style=\"text-align: right;\">    3.91</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            454.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 326000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-19-02\n",
      "  done: false\n",
      "  episode_len_mean: 455.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.88\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 833\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2962282293372684\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008964367054569787\n",
      "          policy_loss: -0.1162688150174088\n",
      "          total_loss: 0.14011994954198598\n",
      "          vf_explained_var: 0.6103463768959045\n",
      "          vf_loss: 0.2645048204395506\n",
      "    num_agent_steps_sampled: 326000\n",
      "    num_agent_steps_trained: 326000\n",
      "    num_steps_sampled: 326000\n",
      "    num_steps_trained: 326000\n",
      "  iterations_since_restore: 326\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.09130434782608\n",
      "    ram_util_percent: 60.27391304347827\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038387142385426426\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.353439497959386\n",
      "    mean_inference_ms: 1.5702645819599288\n",
      "    mean_raw_obs_processing_ms: 1.9596024152742422\n",
      "  time_since_restore: 7739.371914625168\n",
      "  time_this_iter_s: 15.896286725997925\n",
      "  time_total_s: 7739.371914625168\n",
      "  timers:\n",
      "    learn_throughput: 961.003\n",
      "    learn_time_ms: 1040.579\n",
      "    load_throughput: 65777.217\n",
      "    load_time_ms: 15.203\n",
      "    sample_throughput: 50.946\n",
      "    sample_time_ms: 19628.731\n",
      "    update_time_ms: 2.629\n",
      "  timestamp: 1633713542\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 326000\n",
      "  training_iteration: 326\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   326</td><td style=\"text-align: right;\">         7739.37</td><td style=\"text-align: right;\">326000</td><td style=\"text-align: right;\">    3.88</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">               455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 327000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-19-20\n",
      "  done: false\n",
      "  episode_len_mean: 457.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.94\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 836\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9319677551587422\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00982125670963374\n",
      "          policy_loss: -0.12582981627848414\n",
      "          total_loss: -0.03866312777002653\n",
      "          vf_explained_var: 0.5101766586303711\n",
      "          vf_loss: 0.1011768984556612\n",
      "    num_agent_steps_sampled: 327000\n",
      "    num_agent_steps_trained: 327000\n",
      "    num_steps_sampled: 327000\n",
      "    num_steps_trained: 327000\n",
      "  iterations_since_restore: 327\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.372\n",
      "    ram_util_percent: 60.132\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03838595246985536\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.334418466675587\n",
      "    mean_inference_ms: 1.570198822899346\n",
      "    mean_raw_obs_processing_ms: 1.9604961540660395\n",
      "  time_since_restore: 7757.062414646149\n",
      "  time_this_iter_s: 17.690500020980835\n",
      "  time_total_s: 7757.062414646149\n",
      "  timers:\n",
      "    learn_throughput: 960.766\n",
      "    learn_time_ms: 1040.836\n",
      "    load_throughput: 65915.322\n",
      "    load_time_ms: 15.171\n",
      "    sample_throughput: 50.964\n",
      "    sample_time_ms: 19621.825\n",
      "    update_time_ms: 2.643\n",
      "  timestamp: 1633713560\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 327000\n",
      "  training_iteration: 327\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   327</td><td style=\"text-align: right;\">         7757.06</td><td style=\"text-align: right;\">327000</td><td style=\"text-align: right;\">    3.94</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">             457.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-19-38\n",
      "  done: false\n",
      "  episode_len_mean: 459.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.93\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 838\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.868061782254113\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013979060337675462\n",
      "          policy_loss: 0.0129107094473309\n",
      "          total_loss: 0.24477464778141844\n",
      "          vf_explained_var: 0.6253398656845093\n",
      "          vf_loss: 0.24298733952972623\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "  iterations_since_restore: 328\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.542307692307695\n",
      "    ram_util_percent: 60.11538461538461\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03838517728142812\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.321423309098865\n",
      "    mean_inference_ms: 1.5701542947637364\n",
      "    mean_raw_obs_processing_ms: 1.9610641341340938\n",
      "  time_since_restore: 7775.161128759384\n",
      "  time_this_iter_s: 18.098714113235474\n",
      "  time_total_s: 7775.161128759384\n",
      "  timers:\n",
      "    learn_throughput: 959.974\n",
      "    learn_time_ms: 1041.695\n",
      "    load_throughput: 61714.612\n",
      "    load_time_ms: 16.204\n",
      "    sample_throughput: 50.835\n",
      "    sample_time_ms: 19671.404\n",
      "    update_time_ms: 2.649\n",
      "  timestamp: 1633713578\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 328\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   328</td><td style=\"text-align: right;\">         7775.16</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\">    3.93</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            459.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 329000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-19-55\n",
      "  done: false\n",
      "  episode_len_mean: 460.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.92\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 840\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1741900549994573\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009602696093840122\n",
      "          policy_loss: -0.12485654883914524\n",
      "          total_loss: -0.09345808592107561\n",
      "          vf_explained_var: 0.26115572452545166\n",
      "          vf_loss: 0.04794905175610135\n",
      "    num_agent_steps_sampled: 329000\n",
      "    num_agent_steps_trained: 329000\n",
      "    num_steps_sampled: 329000\n",
      "    num_steps_trained: 329000\n",
      "  iterations_since_restore: 329\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.34583333333333\n",
      "    ram_util_percent: 60.09583333333334\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03838441352695065\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.308284398473806\n",
      "    mean_inference_ms: 1.5701097869521845\n",
      "    mean_raw_obs_processing_ms: 1.9616479555490818\n",
      "  time_since_restore: 7792.106955528259\n",
      "  time_this_iter_s: 16.945826768875122\n",
      "  time_total_s: 7792.106955528259\n",
      "  timers:\n",
      "    learn_throughput: 960.269\n",
      "    learn_time_ms: 1041.375\n",
      "    load_throughput: 66644.803\n",
      "    load_time_ms: 15.005\n",
      "    sample_throughput: 51.875\n",
      "    sample_time_ms: 19276.924\n",
      "    update_time_ms: 2.316\n",
      "  timestamp: 1633713595\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 329000\n",
      "  training_iteration: 329\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   329</td><td style=\"text-align: right;\">         7792.11</td><td style=\"text-align: right;\">329000</td><td style=\"text-align: right;\">    3.92</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            460.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 330000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-20-11\n",
      "  done: false\n",
      "  episode_len_mean: 462.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.81\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 842\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.1303625583648682\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013947274553746385\n",
      "          policy_loss: -0.042929955240752965\n",
      "          total_loss: 0.17257234876354535\n",
      "          vf_explained_var: 0.6511344313621521\n",
      "          vf_loss: 0.21926590020043982\n",
      "    num_agent_steps_sampled: 330000\n",
      "    num_agent_steps_trained: 330000\n",
      "    num_steps_sampled: 330000\n",
      "    num_steps_trained: 330000\n",
      "  iterations_since_restore: 330\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.30909090909092\n",
      "    ram_util_percent: 60.14545454545455\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03838366887981799\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.295216360650535\n",
      "    mean_inference_ms: 1.5700654865533852\n",
      "    mean_raw_obs_processing_ms: 1.9622473368162008\n",
      "  time_since_restore: 7807.6195549964905\n",
      "  time_this_iter_s: 15.512599468231201\n",
      "  time_total_s: 7807.6195549964905\n",
      "  timers:\n",
      "    learn_throughput: 960.509\n",
      "    learn_time_ms: 1041.114\n",
      "    load_throughput: 75081.431\n",
      "    load_time_ms: 13.319\n",
      "    sample_throughput: 52.808\n",
      "    sample_time_ms: 18936.397\n",
      "    update_time_ms: 2.302\n",
      "  timestamp: 1633713611\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 330000\n",
      "  training_iteration: 330\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   330</td><td style=\"text-align: right;\">         7807.62</td><td style=\"text-align: right;\">330000</td><td style=\"text-align: right;\">    3.81</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            462.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 331000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-20-28\n",
      "  done: false\n",
      "  episode_len_mean: 462.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.74\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 844\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.047568558322059\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008647248619823245\n",
      "          policy_loss: -0.060435087896055645\n",
      "          total_loss: -0.028866303174032104\n",
      "          vf_explained_var: 0.06295796483755112\n",
      "          vf_loss: 0.047369681746284996\n",
      "    num_agent_steps_sampled: 331000\n",
      "    num_agent_steps_trained: 331000\n",
      "    num_steps_sampled: 331000\n",
      "    num_steps_trained: 331000\n",
      "  iterations_since_restore: 331\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.38\n",
      "    ram_util_percent: 60.236\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03838296275219505\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.282267779386686\n",
      "    mean_inference_ms: 1.5700214331893279\n",
      "    mean_raw_obs_processing_ms: 1.9628617773593198\n",
      "  time_since_restore: 7824.88395524025\n",
      "  time_this_iter_s: 17.264400243759155\n",
      "  time_total_s: 7824.88395524025\n",
      "  timers:\n",
      "    learn_throughput: 960.044\n",
      "    learn_time_ms: 1041.619\n",
      "    load_throughput: 75026.903\n",
      "    load_time_ms: 13.329\n",
      "    sample_throughput: 52.985\n",
      "    sample_time_ms: 18873.419\n",
      "    update_time_ms: 2.307\n",
      "  timestamp: 1633713628\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 331000\n",
      "  training_iteration: 331\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   331</td><td style=\"text-align: right;\">         7824.88</td><td style=\"text-align: right;\">331000</td><td style=\"text-align: right;\">    3.74</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            462.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 332000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-20-44\n",
      "  done: false\n",
      "  episode_len_mean: 461.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.67\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 846\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.55557292898496\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007743939964348259\n",
      "          policy_loss: -0.037028031961785425\n",
      "          total_loss: 0.09530268997574846\n",
      "          vf_explained_var: 0.6236047744750977\n",
      "          vf_loss: 0.14370000254776744\n",
      "    num_agent_steps_sampled: 332000\n",
      "    num_agent_steps_trained: 332000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "  iterations_since_restore: 332\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.3608695652174\n",
      "    ram_util_percent: 60.35652173913046\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038382292589165576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.269490960676816\n",
      "    mean_inference_ms: 1.569978474648849\n",
      "    mean_raw_obs_processing_ms: 1.9634911635207988\n",
      "  time_since_restore: 7841.290176153183\n",
      "  time_this_iter_s: 16.40622091293335\n",
      "  time_total_s: 7841.290176153183\n",
      "  timers:\n",
      "    learn_throughput: 958.08\n",
      "    learn_time_ms: 1043.754\n",
      "    load_throughput: 84573.152\n",
      "    load_time_ms: 11.824\n",
      "    sample_throughput: 53.693\n",
      "    sample_time_ms: 18624.297\n",
      "    update_time_ms: 2.302\n",
      "  timestamp: 1633713644\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 332\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   332</td><td style=\"text-align: right;\">         7841.29</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">    3.67</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">             461.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 333000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-21-02\n",
      "  done: false\n",
      "  episode_len_mean: 461.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.65\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 848\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5406097412109376\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9016746785905627\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02119843946731982\n",
      "          policy_loss: -0.034287264280849036\n",
      "          total_loss: 0.31400866951379514\n",
      "          vf_explained_var: 0.6439001560211182\n",
      "          vf_loss: 0.35585260002149477\n",
      "    num_agent_steps_sampled: 333000\n",
      "    num_agent_steps_trained: 333000\n",
      "    num_steps_sampled: 333000\n",
      "    num_steps_trained: 333000\n",
      "  iterations_since_restore: 333\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.16153846153846\n",
      "    ram_util_percent: 60.42307692307692\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03838162823528079\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.256833411196205\n",
      "    mean_inference_ms: 1.5699358728133388\n",
      "    mean_raw_obs_processing_ms: 1.964135521322657\n",
      "  time_since_restore: 7859.38033413887\n",
      "  time_this_iter_s: 18.090157985687256\n",
      "  time_total_s: 7859.38033413887\n",
      "  timers:\n",
      "    learn_throughput: 958.429\n",
      "    learn_time_ms: 1043.374\n",
      "    load_throughput: 84949.65\n",
      "    load_time_ms: 11.772\n",
      "    sample_throughput: 59.818\n",
      "    sample_time_ms: 16717.331\n",
      "    update_time_ms: 2.291\n",
      "  timestamp: 1633713662\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 333000\n",
      "  training_iteration: 333\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   333</td><td style=\"text-align: right;\">         7859.38</td><td style=\"text-align: right;\">333000</td><td style=\"text-align: right;\">    3.65</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            461.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 334000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-21-20\n",
      "  done: false\n",
      "  episode_len_mean: 460.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.62\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 850\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6891500572363536\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011996708811249531\n",
      "          policy_loss: 0.012036735895607206\n",
      "          total_loss: 0.2584383759026726\n",
      "          vf_explained_var: 0.4743422567844391\n",
      "          vf_loss: 0.2535648343463739\n",
      "    num_agent_steps_sampled: 334000\n",
      "    num_agent_steps_trained: 334000\n",
      "    num_steps_sampled: 334000\n",
      "    num_steps_trained: 334000\n",
      "  iterations_since_restore: 334\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.208\n",
      "    ram_util_percent: 60.512\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03838091347944686\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.244394343964963\n",
      "    mean_inference_ms: 1.5698936577252665\n",
      "    mean_raw_obs_processing_ms: 1.9636369247095282\n",
      "  time_since_restore: 7876.79229259491\n",
      "  time_this_iter_s: 17.41195845603943\n",
      "  time_total_s: 7876.79229259491\n",
      "  timers:\n",
      "    learn_throughput: 957.548\n",
      "    learn_time_ms: 1044.334\n",
      "    load_throughput: 95062.804\n",
      "    load_time_ms: 10.519\n",
      "    sample_throughput: 61.923\n",
      "    sample_time_ms: 16149.068\n",
      "    update_time_ms: 2.286\n",
      "  timestamp: 1633713680\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 334000\n",
      "  training_iteration: 334\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   334</td><td style=\"text-align: right;\">         7876.79</td><td style=\"text-align: right;\">334000</td><td style=\"text-align: right;\">    3.62</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            460.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 335000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-21-41\n",
      "  done: false\n",
      "  episode_len_mean: 457.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.6\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 853\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9207294278674656\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008566784423731318\n",
      "          policy_loss: -0.2060086061971055\n",
      "          total_loss: -0.13189551482597986\n",
      "          vf_explained_var: 0.7325615882873535\n",
      "          vf_loss: 0.08637345053462518\n",
      "    num_agent_steps_sampled: 335000\n",
      "    num_agent_steps_trained: 335000\n",
      "    num_steps_sampled: 335000\n",
      "    num_steps_trained: 335000\n",
      "  iterations_since_restore: 335\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.43225806451614\n",
      "    ram_util_percent: 60.60645161290321\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038379885720157644\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.226440729498467\n",
      "    mean_inference_ms: 1.5698322638846565\n",
      "    mean_raw_obs_processing_ms: 1.9629820357401104\n",
      "  time_since_restore: 7898.128445863724\n",
      "  time_this_iter_s: 21.336153268814087\n",
      "  time_total_s: 7898.128445863724\n",
      "  timers:\n",
      "    learn_throughput: 959.192\n",
      "    learn_time_ms: 1042.545\n",
      "    load_throughput: 95729.548\n",
      "    load_time_ms: 10.446\n",
      "    sample_throughput: 60.95\n",
      "    sample_time_ms: 16406.868\n",
      "    update_time_ms: 2.286\n",
      "  timestamp: 1633713701\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 335000\n",
      "  training_iteration: 335\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   335</td><td style=\"text-align: right;\">         7898.13</td><td style=\"text-align: right;\">335000</td><td style=\"text-align: right;\">     3.6</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">             457.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-22-01\n",
      "  done: false\n",
      "  episode_len_mean: 455.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.58\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 855\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.554168364736769\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011945115472573854\n",
      "          policy_loss: -0.10824622636040052\n",
      "          total_loss: 0.17022531545824474\n",
      "          vf_explained_var: 0.6172411441802979\n",
      "          vf_loss: 0.28432675848404565\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_agent_steps_trained: 336000\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "  iterations_since_restore: 336\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.27857142857143\n",
      "    ram_util_percent: 60.63571428571428\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03837922311505836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.214821493378988\n",
      "    mean_inference_ms: 1.5697919142407324\n",
      "    mean_raw_obs_processing_ms: 1.9625831973085355\n",
      "  time_since_restore: 7918.145442485809\n",
      "  time_this_iter_s: 20.01699662208557\n",
      "  time_total_s: 7918.145442485809\n",
      "  timers:\n",
      "    learn_throughput: 959.303\n",
      "    learn_time_ms: 1042.424\n",
      "    load_throughput: 80278.677\n",
      "    load_time_ms: 12.457\n",
      "    sample_throughput: 59.463\n",
      "    sample_time_ms: 16817.103\n",
      "    update_time_ms: 2.237\n",
      "  timestamp: 1633713721\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 336\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   336</td><td style=\"text-align: right;\">         7918.15</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\">    3.58</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            455.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 337000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-22-38\n",
      "  done: false\n",
      "  episode_len_mean: 453.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.58\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 857\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3826473673184714\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012570561186647526\n",
      "          policy_loss: -0.02264343591199981\n",
      "          total_loss: 0.2474446619550387\n",
      "          vf_explained_var: 0.7148337364196777\n",
      "          vf_loss: 0.2737209168573221\n",
      "    num_agent_steps_sampled: 337000\n",
      "    num_agent_steps_trained: 337000\n",
      "    num_steps_sampled: 337000\n",
      "    num_steps_trained: 337000\n",
      "  iterations_since_restore: 337\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.03584905660377\n",
      "    ram_util_percent: 60.515094339622635\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03837859494639442\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.203493744167467\n",
      "    mean_inference_ms: 1.5697523802080626\n",
      "    mean_raw_obs_processing_ms: 1.9631907693461839\n",
      "  time_since_restore: 7955.133815288544\n",
      "  time_this_iter_s: 36.988372802734375\n",
      "  time_total_s: 7955.133815288544\n",
      "  timers:\n",
      "    learn_throughput: 957.47\n",
      "    learn_time_ms: 1044.419\n",
      "    load_throughput: 81459.088\n",
      "    load_time_ms: 12.276\n",
      "    sample_throughput: 53.347\n",
      "    sample_time_ms: 18745.086\n",
      "    update_time_ms: 2.231\n",
      "  timestamp: 1633713758\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 337000\n",
      "  training_iteration: 337\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   337</td><td style=\"text-align: right;\">         7955.13</td><td style=\"text-align: right;\">337000</td><td style=\"text-align: right;\">    3.58</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            453.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 338000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-22-58\n",
      "  done: false\n",
      "  episode_len_mean: 451.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.6\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 860\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9210068252351549\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011309351563507125\n",
      "          policy_loss: -0.08590994922237263\n",
      "          total_loss: 0.13485439862642024\n",
      "          vf_explained_var: 0.5943502187728882\n",
      "          vf_loss: 0.23080349597666\n",
      "    num_agent_steps_sampled: 338000\n",
      "    num_agent_steps_trained: 338000\n",
      "    num_steps_sampled: 338000\n",
      "    num_steps_trained: 338000\n",
      "  iterations_since_restore: 338\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.671428571428564\n",
      "    ram_util_percent: 60.51071428571429\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03837763330980947\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.1869443749431\n",
      "    mean_inference_ms: 1.569694030503303\n",
      "    mean_raw_obs_processing_ms: 1.9641586601913008\n",
      "  time_since_restore: 7974.971369504929\n",
      "  time_this_iter_s: 19.837554216384888\n",
      "  time_total_s: 7974.971369504929\n",
      "  timers:\n",
      "    learn_throughput: 957.758\n",
      "    learn_time_ms: 1044.106\n",
      "    load_throughput: 80891.009\n",
      "    load_time_ms: 12.362\n",
      "    sample_throughput: 52.856\n",
      "    sample_time_ms: 18919.217\n",
      "    update_time_ms: 2.222\n",
      "  timestamp: 1633713778\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 338000\n",
      "  training_iteration: 338\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   338</td><td style=\"text-align: right;\">         7974.97</td><td style=\"text-align: right;\">338000</td><td style=\"text-align: right;\">     3.6</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            451.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 339000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-23-20\n",
      "  done: false\n",
      "  episode_len_mean: 451.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.62\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 862\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7615779519081116\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0119106597344002\n",
      "          policy_loss: -0.1549626215464539\n",
      "          total_loss: -0.08189410865306854\n",
      "          vf_explained_var: 0.5900276303291321\n",
      "          vf_loss: 0.08102576125206219\n",
      "    num_agent_steps_sampled: 339000\n",
      "    num_agent_steps_trained: 339000\n",
      "    num_steps_sampled: 339000\n",
      "    num_steps_trained: 339000\n",
      "  iterations_since_restore: 339\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.09677419354838\n",
      "    ram_util_percent: 60.41935483870969\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038377037989151964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.176183220558904\n",
      "    mean_inference_ms: 1.5696559446538776\n",
      "    mean_raw_obs_processing_ms: 1.964855309470845\n",
      "  time_since_restore: 7996.371787071228\n",
      "  time_this_iter_s: 21.40041756629944\n",
      "  time_total_s: 7996.371787071228\n",
      "  timers:\n",
      "    learn_throughput: 955.855\n",
      "    learn_time_ms: 1046.184\n",
      "    load_throughput: 73469.05\n",
      "    load_time_ms: 13.611\n",
      "    sample_throughput: 51.649\n",
      "    sample_time_ms: 19361.344\n",
      "    update_time_ms: 2.219\n",
      "  timestamp: 1633713800\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 339000\n",
      "  training_iteration: 339\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   339</td><td style=\"text-align: right;\">         7996.37</td><td style=\"text-align: right;\">339000</td><td style=\"text-align: right;\">    3.62</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            451.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 340000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-23-39\n",
      "  done: false\n",
      "  episode_len_mean: 452.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.69\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 865\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.14492523405287\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008062675829892967\n",
      "          policy_loss: -0.11600251181258095\n",
      "          total_loss: -0.07455838177767064\n",
      "          vf_explained_var: 0.3532291650772095\n",
      "          vf_loss: 0.056355240806523293\n",
      "    num_agent_steps_sampled: 340000\n",
      "    num_agent_steps_trained: 340000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "  iterations_since_restore: 340\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.555555555555564\n",
      "    ram_util_percent: 60.3\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0383761332865009\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.15993455889871\n",
      "    mean_inference_ms: 1.5695990211565192\n",
      "    mean_raw_obs_processing_ms: 1.9658652062171678\n",
      "  time_since_restore: 8015.467646121979\n",
      "  time_this_iter_s: 19.095859050750732\n",
      "  time_total_s: 8015.467646121979\n",
      "  timers:\n",
      "    learn_throughput: 956.421\n",
      "    learn_time_ms: 1045.565\n",
      "    load_throughput: 66246.808\n",
      "    load_time_ms: 15.095\n",
      "    sample_throughput: 50.713\n",
      "    sample_time_ms: 19718.773\n",
      "    update_time_ms: 2.245\n",
      "  timestamp: 1633713819\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 340\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   340</td><td style=\"text-align: right;\">         8015.47</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\">    3.69</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            452.66</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 341000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-23-57\n",
      "  done: false\n",
      "  episode_len_mean: 453.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.72\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 867\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0523857209417553\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010170966707744988\n",
      "          policy_loss: 0.0748191687795851\n",
      "          total_loss: 0.17324211013813814\n",
      "          vf_explained_var: 0.12415122985839844\n",
      "          vf_loss: 0.110699009274443\n",
      "    num_agent_steps_sampled: 341000\n",
      "    num_agent_steps_trained: 341000\n",
      "    num_steps_sampled: 341000\n",
      "    num_steps_trained: 341000\n",
      "  iterations_since_restore: 341\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.05185185185186\n",
      "    ram_util_percent: 60.1888888888889\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03837556453778691\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.149196024212223\n",
      "    mean_inference_ms: 1.569561645871151\n",
      "    mean_raw_obs_processing_ms: 1.9638897654250984\n",
      "  time_since_restore: 8033.913430452347\n",
      "  time_this_iter_s: 18.445784330368042\n",
      "  time_total_s: 8033.913430452347\n",
      "  timers:\n",
      "    learn_throughput: 957.02\n",
      "    learn_time_ms: 1044.911\n",
      "    load_throughput: 61981.918\n",
      "    load_time_ms: 16.134\n",
      "    sample_throughput: 50.412\n",
      "    sample_time_ms: 19836.541\n",
      "    update_time_ms: 2.243\n",
      "  timestamp: 1633713837\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 341000\n",
      "  training_iteration: 341\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   341</td><td style=\"text-align: right;\">         8033.91</td><td style=\"text-align: right;\">341000</td><td style=\"text-align: right;\">    3.72</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            453.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 342000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-24-20\n",
      "  done: false\n",
      "  episode_len_mean: 452.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.68\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 869\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8295127444797092\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016071849677073008\n",
      "          policy_loss: -0.02760607832007938\n",
      "          total_loss: 0.36009878458248246\n",
      "          vf_explained_var: 0.48933202028274536\n",
      "          vf_loss: 0.39296709299087523\n",
      "    num_agent_steps_sampled: 342000\n",
      "    num_agent_steps_trained: 342000\n",
      "    num_steps_sampled: 342000\n",
      "    num_steps_trained: 342000\n",
      "  iterations_since_restore: 342\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.2939393939394\n",
      "    ram_util_percent: 60.16060606060606\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038374989321205144\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.138903774426616\n",
      "    mean_inference_ms: 1.5695249170129864\n",
      "    mean_raw_obs_processing_ms: 1.961934362930959\n",
      "  time_since_restore: 8057.081543684006\n",
      "  time_this_iter_s: 23.168113231658936\n",
      "  time_total_s: 8057.081543684006\n",
      "  timers:\n",
      "    learn_throughput: 957.631\n",
      "    learn_time_ms: 1044.243\n",
      "    load_throughput: 56382.784\n",
      "    load_time_ms: 17.736\n",
      "    sample_throughput: 48.752\n",
      "    sample_time_ms: 20511.798\n",
      "    update_time_ms: 2.243\n",
      "  timestamp: 1633713860\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 342000\n",
      "  training_iteration: 342\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   342</td><td style=\"text-align: right;\">         8057.08</td><td style=\"text-align: right;\">342000</td><td style=\"text-align: right;\">    3.68</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            452.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 343000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-24-47\n",
      "  done: false\n",
      "  episode_len_mean: 446.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 8.0\n",
      "  episode_reward_mean: 3.69\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 873\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.695815176433987\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018879391386598363\n",
      "          policy_loss: 0.012258690264489915\n",
      "          total_loss: 0.29541467212968403\n",
      "          vf_explained_var: 0.9019767642021179\n",
      "          vf_loss: 0.28480456132027837\n",
      "    num_agent_steps_sampled: 343000\n",
      "    num_agent_steps_trained: 343000\n",
      "    num_steps_sampled: 343000\n",
      "    num_steps_trained: 343000\n",
      "  iterations_since_restore: 343\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.29473684210526\n",
      "    ram_util_percent: 60.168421052631594\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03837384630277498\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.11956929775696\n",
      "    mean_inference_ms: 1.5694540454665706\n",
      "    mean_raw_obs_processing_ms: 1.9580768432141178\n",
      "  time_since_restore: 8083.800840377808\n",
      "  time_this_iter_s: 26.71929669380188\n",
      "  time_total_s: 8083.800840377808\n",
      "  timers:\n",
      "    learn_throughput: 958.262\n",
      "    learn_time_ms: 1043.555\n",
      "    load_throughput: 56379.979\n",
      "    load_time_ms: 17.737\n",
      "    sample_throughput: 46.783\n",
      "    sample_time_ms: 21375.409\n",
      "    update_time_ms: 2.236\n",
      "  timestamp: 1633713887\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 343000\n",
      "  training_iteration: 343\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   343</td><td style=\"text-align: right;\">          8083.8</td><td style=\"text-align: right;\">343000</td><td style=\"text-align: right;\">    3.69</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            446.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-26-10\n",
      "  done: false\n",
      "  episode_len_mean: 438.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.84\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 877\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9118536710739136\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013131232822320626\n",
      "          policy_loss: 0.042705236706468794\n",
      "          total_loss: 0.20264157843258646\n",
      "          vf_explained_var: 0.8950369358062744\n",
      "          vf_loss: 0.16840656931615539\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "  iterations_since_restore: 344\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.494871794871795\n",
      "    ram_util_percent: 60.299145299145295\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03837273443600384\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.101122361027674\n",
      "    mean_inference_ms: 1.5693860886336437\n",
      "    mean_raw_obs_processing_ms: 1.9616273541399705\n",
      "  time_since_restore: 8166.220144748688\n",
      "  time_this_iter_s: 82.41930437088013\n",
      "  time_total_s: 8166.220144748688\n",
      "  timers:\n",
      "    learn_throughput: 959.582\n",
      "    learn_time_ms: 1042.121\n",
      "    load_throughput: 53158.399\n",
      "    load_time_ms: 18.812\n",
      "    sample_throughput: 35.873\n",
      "    sample_time_ms: 27876.496\n",
      "    update_time_ms: 2.251\n",
      "  timestamp: 1633713970\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 344\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   344</td><td style=\"text-align: right;\">         8166.22</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\">    3.84</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            438.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 345000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-26-59\n",
      "  done: false\n",
      "  episode_len_mean: 434.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 3.98\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 880\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8479359083705478\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007685027642990535\n",
      "          policy_loss: -0.1192965974410375\n",
      "          total_loss: 0.021991092753079203\n",
      "          vf_explained_var: 0.6471982002258301\n",
      "          vf_loss: 0.15353514932923848\n",
      "    num_agent_steps_sampled: 345000\n",
      "    num_agent_steps_trained: 345000\n",
      "    num_steps_sampled: 345000\n",
      "    num_steps_trained: 345000\n",
      "  iterations_since_restore: 345\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.768571428571423\n",
      "    ram_util_percent: 60.492857142857154\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038371922415609265\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.087625545788708\n",
      "    mean_inference_ms: 1.569336043443925\n",
      "    mean_raw_obs_processing_ms: 1.9668175777015944\n",
      "  time_since_restore: 8215.165746927261\n",
      "  time_this_iter_s: 48.94560217857361\n",
      "  time_total_s: 8215.165746927261\n",
      "  timers:\n",
      "    learn_throughput: 957.292\n",
      "    learn_time_ms: 1044.613\n",
      "    load_throughput: 53105.228\n",
      "    load_time_ms: 18.831\n",
      "    sample_throughput: 32.642\n",
      "    sample_time_ms: 30634.942\n",
      "    update_time_ms: 2.254\n",
      "  timestamp: 1633714019\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 345000\n",
      "  training_iteration: 345\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   345</td><td style=\"text-align: right;\">         8215.17</td><td style=\"text-align: right;\">345000</td><td style=\"text-align: right;\">    3.98</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">             434.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 346000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-27-57\n",
      "  done: false\n",
      "  episode_len_mean: 427.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.06\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 883\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3428085247675579\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008725124734505445\n",
      "          policy_loss: -0.08435883190896776\n",
      "          total_loss: 0.16545281459887823\n",
      "          vf_explained_var: 0.7138420939445496\n",
      "          vf_loss: 0.25616440052787465\n",
      "    num_agent_steps_sampled: 346000\n",
      "    num_agent_steps_trained: 346000\n",
      "    num_steps_sampled: 346000\n",
      "    num_steps_trained: 346000\n",
      "  iterations_since_restore: 346\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.379518072289155\n",
      "    ram_util_percent: 60.49879518072289\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03837114576397221\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.074857236850384\n",
      "    mean_inference_ms: 1.56928768293125\n",
      "    mean_raw_obs_processing_ms: 1.9751888958321246\n",
      "  time_since_restore: 8273.409259319305\n",
      "  time_this_iter_s: 58.24351239204407\n",
      "  time_total_s: 8273.409259319305\n",
      "  timers:\n",
      "    learn_throughput: 957.488\n",
      "    learn_time_ms: 1044.4\n",
      "    load_throughput: 54570.7\n",
      "    load_time_ms: 18.325\n",
      "    sample_throughput: 29.021\n",
      "    sample_time_ms: 34458.318\n",
      "    update_time_ms: 2.253\n",
      "  timestamp: 1633714077\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 346000\n",
      "  training_iteration: 346\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   346</td><td style=\"text-align: right;\">         8273.41</td><td style=\"text-align: right;\">346000</td><td style=\"text-align: right;\">    4.06</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            427.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 347000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-28-15\n",
      "  done: false\n",
      "  episode_len_mean: 426.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.07\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 885\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7972911596298218\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006755257361699327\n",
      "          policy_loss: -0.17879992959400018\n",
      "          total_loss: -0.08767904918640852\n",
      "          vf_explained_var: 0.7548989057540894\n",
      "          vf_loss: 0.10361585463914606\n",
      "    num_agent_steps_sampled: 347000\n",
      "    num_agent_steps_trained: 347000\n",
      "    num_steps_sampled: 347000\n",
      "    num_steps_trained: 347000\n",
      "  iterations_since_restore: 347\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.33461538461539\n",
      "    ram_util_percent: 60.300000000000004\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03837065366020394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.0665628002933\n",
      "    mean_inference_ms: 1.5692559879836698\n",
      "    mean_raw_obs_processing_ms: 1.9808089846344377\n",
      "  time_since_restore: 8291.319775342941\n",
      "  time_this_iter_s: 17.910516023635864\n",
      "  time_total_s: 8291.319775342941\n",
      "  timers:\n",
      "    learn_throughput: 958.628\n",
      "    learn_time_ms: 1043.157\n",
      "    load_throughput: 54973.596\n",
      "    load_time_ms: 18.191\n",
      "    sample_throughput: 30.72\n",
      "    sample_time_ms: 32551.893\n",
      "    update_time_ms: 2.265\n",
      "  timestamp: 1633714095\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 347000\n",
      "  training_iteration: 347\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   347</td><td style=\"text-align: right;\">         8291.32</td><td style=\"text-align: right;\">347000</td><td style=\"text-align: right;\">    4.07</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            426.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 348000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-28-32\n",
      "  done: false\n",
      "  episode_len_mean: 427.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.11\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 887\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5048319538434347\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013626781605726477\n",
      "          policy_loss: 0.01696529487768809\n",
      "          total_loss: 0.35929884927140343\n",
      "          vf_explained_var: 0.8596631288528442\n",
      "          vf_loss: 0.3463317286223173\n",
      "    num_agent_steps_sampled: 348000\n",
      "    num_agent_steps_trained: 348000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "  iterations_since_restore: 348\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.225\n",
      "    ram_util_percent: 59.574999999999996\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03837015151936172\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.05814545029492\n",
      "    mean_inference_ms: 1.5692241493235235\n",
      "    mean_raw_obs_processing_ms: 1.9864276038438755\n",
      "  time_since_restore: 8308.187145233154\n",
      "  time_this_iter_s: 16.867369890213013\n",
      "  time_total_s: 8308.187145233154\n",
      "  timers:\n",
      "    learn_throughput: 959.566\n",
      "    learn_time_ms: 1042.138\n",
      "    load_throughput: 58415.595\n",
      "    load_time_ms: 17.119\n",
      "    sample_throughput: 31.001\n",
      "    sample_time_ms: 32256.945\n",
      "    update_time_ms: 2.269\n",
      "  timestamp: 1633714112\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 348\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   348</td><td style=\"text-align: right;\">         8308.19</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\">    4.11</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            427.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 349000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-28-51\n",
      "  done: false\n",
      "  episode_len_mean: 427.15\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.11\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 890\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5788703163464863\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015468958720513855\n",
      "          policy_loss: -0.030575277159611385\n",
      "          total_loss: 0.36328605545891657\n",
      "          vf_explained_var: 0.8444334864616394\n",
      "          vf_loss: 0.39710603290134006\n",
      "    num_agent_steps_sampled: 349000\n",
      "    num_agent_steps_trained: 349000\n",
      "    num_steps_sampled: 349000\n",
      "    num_steps_trained: 349000\n",
      "  iterations_since_restore: 349\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.53928571428572\n",
      "    ram_util_percent: 59.27857142857142\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038369429499620786\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.04597855048531\n",
      "    mean_inference_ms: 1.5691772025119315\n",
      "    mean_raw_obs_processing_ms: 1.994915944125429\n",
      "  time_since_restore: 8327.602547883987\n",
      "  time_this_iter_s: 19.41540265083313\n",
      "  time_total_s: 8327.602547883987\n",
      "  timers:\n",
      "    learn_throughput: 960.206\n",
      "    learn_time_ms: 1041.443\n",
      "    load_throughput: 58817.646\n",
      "    load_time_ms: 17.002\n",
      "    sample_throughput: 31.192\n",
      "    sample_time_ms: 32059.251\n",
      "    update_time_ms: 2.269\n",
      "  timestamp: 1633714131\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 349000\n",
      "  training_iteration: 349\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   349</td><td style=\"text-align: right;\">          8327.6</td><td style=\"text-align: right;\">349000</td><td style=\"text-align: right;\">    4.11</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            427.15</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 350000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-29-12\n",
      "  done: false\n",
      "  episode_len_mean: 426.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.18\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 892\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0034446239471437\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011588518948018588\n",
      "          policy_loss: -0.07664408572018147\n",
      "          total_loss: 0.02781716858347257\n",
      "          vf_explained_var: 0.6550095677375793\n",
      "          vf_loss: 0.11509839893422193\n",
      "    num_agent_steps_sampled: 350000\n",
      "    num_agent_steps_trained: 350000\n",
      "    num_steps_sampled: 350000\n",
      "    num_steps_trained: 350000\n",
      "  iterations_since_restore: 350\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.36896551724139\n",
      "    ram_util_percent: 59.30344827586206\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03836895481744481\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.038133497721944\n",
      "    mean_inference_ms: 1.5691467917859174\n",
      "    mean_raw_obs_processing_ms: 2.0005913422613\n",
      "  time_since_restore: 8348.103714227676\n",
      "  time_this_iter_s: 20.501166343688965\n",
      "  time_total_s: 8348.103714227676\n",
      "  timers:\n",
      "    learn_throughput: 961.336\n",
      "    learn_time_ms: 1040.219\n",
      "    load_throughput: 58791.923\n",
      "    load_time_ms: 17.009\n",
      "    sample_throughput: 31.055\n",
      "    sample_time_ms: 32200.77\n",
      "    update_time_ms: 2.497\n",
      "  timestamp: 1633714152\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 350000\n",
      "  training_iteration: 350\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   350</td><td style=\"text-align: right;\">          8348.1</td><td style=\"text-align: right;\">350000</td><td style=\"text-align: right;\">    4.18</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            426.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 351000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-29-28\n",
      "  done: false\n",
      "  episode_len_mean: 425.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.22\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 894\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.954554471704695\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010909862254822794\n",
      "          policy_loss: 0.049268796377711824\n",
      "          total_loss: 0.2100483213033941\n",
      "          vf_explained_var: 0.7557373046875\n",
      "          vf_loss: 0.1614780995580885\n",
      "    num_agent_steps_sampled: 351000\n",
      "    num_agent_steps_trained: 351000\n",
      "    num_steps_sampled: 351000\n",
      "    num_steps_trained: 351000\n",
      "  iterations_since_restore: 351\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.49999999999999\n",
      "    ram_util_percent: 59.416666666666664\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03836846141211791\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.030134416897496\n",
      "    mean_inference_ms: 1.5691160608065815\n",
      "    mean_raw_obs_processing_ms: 2.006205184134116\n",
      "  time_since_restore: 8364.691186189651\n",
      "  time_this_iter_s: 16.587471961975098\n",
      "  time_total_s: 8364.691186189651\n",
      "  timers:\n",
      "    learn_throughput: 961.218\n",
      "    learn_time_ms: 1040.347\n",
      "    load_throughput: 62591.463\n",
      "    load_time_ms: 15.977\n",
      "    sample_throughput: 31.235\n",
      "    sample_time_ms: 32015.825\n",
      "    update_time_ms: 2.5\n",
      "  timestamp: 1633714168\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 351000\n",
      "  training_iteration: 351\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   351</td><td style=\"text-align: right;\">         8364.69</td><td style=\"text-align: right;\">351000</td><td style=\"text-align: right;\">    4.22</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            425.95</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-29-49\n",
      "  done: false\n",
      "  episode_len_mean: 423.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.32\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 897\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.730429810947842\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009475559078829734\n",
      "          policy_loss: -0.08162351610759894\n",
      "          total_loss: 0.08814454277356466\n",
      "          vf_explained_var: 0.7621672749519348\n",
      "          vf_loss: 0.17938848692509862\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_agent_steps_trained: 352000\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "  iterations_since_restore: 352\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.59310344827586\n",
      "    ram_util_percent: 59.55172413793102\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03836776027529968\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.018613522182044\n",
      "    mean_inference_ms: 1.5690709886468182\n",
      "    mean_raw_obs_processing_ms: 2.0141458266200196\n",
      "  time_since_restore: 8385.41837477684\n",
      "  time_this_iter_s: 20.72718858718872\n",
      "  time_total_s: 8385.41837477684\n",
      "  timers:\n",
      "    learn_throughput: 962.46\n",
      "    learn_time_ms: 1039.005\n",
      "    load_throughput: 63585.996\n",
      "    load_time_ms: 15.727\n",
      "    sample_throughput: 31.473\n",
      "    sample_time_ms: 31773.051\n",
      "    update_time_ms: 2.756\n",
      "  timestamp: 1633714189\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 352\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   352</td><td style=\"text-align: right;\">         8385.42</td><td style=\"text-align: right;\">352000</td><td style=\"text-align: right;\">    4.32</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             423.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 353000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-30-09\n",
      "  done: false\n",
      "  episode_len_mean: 422.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.3\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 899\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9236743582619562\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010895463909356924\n",
      "          policy_loss: -0.03435766332679325\n",
      "          total_loss: 0.08512851546208064\n",
      "          vf_explained_var: 0.8617824912071228\n",
      "          vf_loss: 0.12988762805859247\n",
      "    num_agent_steps_sampled: 353000\n",
      "    num_agent_steps_trained: 353000\n",
      "    num_steps_sampled: 353000\n",
      "    num_steps_trained: 353000\n",
      "  iterations_since_restore: 353\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.30344827586208\n",
      "    ram_util_percent: 59.696551724137926\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03836733072291673\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.01124618588882\n",
      "    mean_inference_ms: 1.5690417725417687\n",
      "    mean_raw_obs_processing_ms: 2.018740383993556\n",
      "  time_since_restore: 8405.601796150208\n",
      "  time_this_iter_s: 20.18342137336731\n",
      "  time_total_s: 8405.601796150208\n",
      "  timers:\n",
      "    learn_throughput: 963.065\n",
      "    learn_time_ms: 1038.352\n",
      "    load_throughput: 63145.546\n",
      "    load_time_ms: 15.836\n",
      "    sample_throughput: 32.134\n",
      "    sample_time_ms: 31119.994\n",
      "    update_time_ms: 2.757\n",
      "  timestamp: 1633714209\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 353000\n",
      "  training_iteration: 353\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   353</td><td style=\"text-align: right;\">          8405.6</td><td style=\"text-align: right;\">353000</td><td style=\"text-align: right;\">     4.3</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            422.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 354000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-30-29\n",
      "  done: false\n",
      "  episode_len_mean: 420.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.38\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 902\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5058815267350938\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007397038047492228\n",
      "          policy_loss: -0.12247354007429546\n",
      "          total_loss: -0.02755670232905282\n",
      "          vf_explained_var: 0.9551764726638794\n",
      "          vf_loss: 0.10397728667077091\n",
      "    num_agent_steps_sampled: 354000\n",
      "    num_agent_steps_trained: 354000\n",
      "    num_steps_sampled: 354000\n",
      "    num_steps_trained: 354000\n",
      "  iterations_since_restore: 354\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.41034482758621\n",
      "    ram_util_percent: 59.83793103448277\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03836667532660826\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.00047144998711\n",
      "    mean_inference_ms: 1.568998597134424\n",
      "    mean_raw_obs_processing_ms: 2.025666586790466\n",
      "  time_since_restore: 8425.953568458557\n",
      "  time_this_iter_s: 20.35177230834961\n",
      "  time_total_s: 8425.953568458557\n",
      "  timers:\n",
      "    learn_throughput: 961.64\n",
      "    learn_time_ms: 1039.89\n",
      "    load_throughput: 62948.237\n",
      "    load_time_ms: 15.886\n",
      "    sample_throughput: 40.142\n",
      "    sample_time_ms: 24911.672\n",
      "    update_time_ms: 2.74\n",
      "  timestamp: 1633714229\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 354000\n",
      "  training_iteration: 354\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   354</td><td style=\"text-align: right;\">         8425.95</td><td style=\"text-align: right;\">354000</td><td style=\"text-align: right;\">    4.38</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            420.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 355000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-30-46\n",
      "  done: false\n",
      "  episode_len_mean: 420.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.42\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 904\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6852982534302605\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012536135053452154\n",
      "          policy_loss: -0.010053643501467174\n",
      "          total_loss: 0.20770572796463965\n",
      "          vf_explained_var: 0.7153950333595276\n",
      "          vf_loss: 0.22444661830862364\n",
      "    num_agent_steps_sampled: 355000\n",
      "    num_agent_steps_trained: 355000\n",
      "    num_steps_sampled: 355000\n",
      "    num_steps_trained: 355000\n",
      "  iterations_since_restore: 355\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.0625\n",
      "    ram_util_percent: 59.99166666666667\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03836628139171614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.993337805081264\n",
      "    mean_inference_ms: 1.5689700554755441\n",
      "    mean_raw_obs_processing_ms: 2.0303250244483184\n",
      "  time_since_restore: 8442.503274440765\n",
      "  time_this_iter_s: 16.549705982208252\n",
      "  time_total_s: 8442.503274440765\n",
      "  timers:\n",
      "    learn_throughput: 963.87\n",
      "    learn_time_ms: 1037.484\n",
      "    load_throughput: 67789.142\n",
      "    load_time_ms: 14.752\n",
      "    sample_throughput: 46.135\n",
      "    sample_time_ms: 21675.622\n",
      "    update_time_ms: 2.731\n",
      "  timestamp: 1633714246\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 355000\n",
      "  training_iteration: 355\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   355</td><td style=\"text-align: right;\">          8442.5</td><td style=\"text-align: right;\">355000</td><td style=\"text-align: right;\">    4.42</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            420.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 356000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-31-05\n",
      "  done: false\n",
      "  episode_len_mean: 421.53\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.45\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 906\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3228692240185207\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007232155730358593\n",
      "          policy_loss: -0.14723942635787857\n",
      "          total_loss: -0.015337789555390675\n",
      "          vf_explained_var: 0.8293382525444031\n",
      "          vf_loss: 0.13926566988229752\n",
      "    num_agent_steps_sampled: 356000\n",
      "    num_agent_steps_trained: 356000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "  iterations_since_restore: 356\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.338461538461544\n",
      "    ram_util_percent: 60.10384615384616\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03836588363761639\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.986369450261112\n",
      "    mean_inference_ms: 1.568941613733832\n",
      "    mean_raw_obs_processing_ms: 2.0349838562797777\n",
      "  time_since_restore: 8460.952297210693\n",
      "  time_this_iter_s: 18.44902276992798\n",
      "  time_total_s: 8460.952297210693\n",
      "  timers:\n",
      "    learn_throughput: 963.707\n",
      "    learn_time_ms: 1037.66\n",
      "    load_throughput: 67096.948\n",
      "    load_time_ms: 14.904\n",
      "    sample_throughput: 56.51\n",
      "    sample_time_ms: 17695.84\n",
      "    update_time_ms: 2.73\n",
      "  timestamp: 1633714265\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 356\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   356</td><td style=\"text-align: right;\">         8460.95</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\">    4.45</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            421.53</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 357000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-31-26\n",
      "  done: false\n",
      "  episode_len_mean: 419.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.29\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 909\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3835640483432345\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011978289172448697\n",
      "          policy_loss: 0.10809783322943581\n",
      "          total_loss: 0.3117563569711314\n",
      "          vf_explained_var: 0.5812875628471375\n",
      "          vf_loss: 0.20778079016341103\n",
      "    num_agent_steps_sampled: 357000\n",
      "    num_agent_steps_trained: 357000\n",
      "    num_steps_sampled: 357000\n",
      "    num_steps_trained: 357000\n",
      "  iterations_since_restore: 357\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.52903225806452\n",
      "    ram_util_percent: 60.248387096774195\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03836524855229849\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.97626826582813\n",
      "    mean_inference_ms: 1.5689001872921944\n",
      "    mean_raw_obs_processing_ms: 2.0419729495541215\n",
      "  time_since_restore: 8482.483522891998\n",
      "  time_this_iter_s: 21.53122568130493\n",
      "  time_total_s: 8482.483522891998\n",
      "  timers:\n",
      "    learn_throughput: 962.212\n",
      "    learn_time_ms: 1039.273\n",
      "    load_throughput: 66570.971\n",
      "    load_time_ms: 15.022\n",
      "    sample_throughput: 55.383\n",
      "    sample_time_ms: 18056.203\n",
      "    update_time_ms: 2.712\n",
      "  timestamp: 1633714286\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 357000\n",
      "  training_iteration: 357\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   357</td><td style=\"text-align: right;\">         8482.48</td><td style=\"text-align: right;\">357000</td><td style=\"text-align: right;\">    4.29</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            419.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 358000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-31-45\n",
      "  done: false\n",
      "  episode_len_mean: 416.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.25\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 911\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2666997571786245\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016599814517717703\n",
      "          policy_loss: -0.008590690460469988\n",
      "          total_loss: 0.25317744618902605\n",
      "          vf_explained_var: 0.511573851108551\n",
      "          vf_loss: 0.2609740972932842\n",
      "    num_agent_steps_sampled: 358000\n",
      "    num_agent_steps_trained: 358000\n",
      "    num_steps_sampled: 358000\n",
      "    num_steps_trained: 358000\n",
      "  iterations_since_restore: 358\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.232142857142854\n",
      "    ram_util_percent: 60.32142857142857\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03836480937124659\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.97001331325976\n",
      "    mean_inference_ms: 1.5688738531825306\n",
      "    mean_raw_obs_processing_ms: 2.0466921253391996\n",
      "  time_since_restore: 8501.740720510483\n",
      "  time_this_iter_s: 19.257197618484497\n",
      "  time_total_s: 8501.740720510483\n",
      "  timers:\n",
      "    learn_throughput: 961.093\n",
      "    learn_time_ms: 1040.482\n",
      "    load_throughput: 62102.967\n",
      "    load_time_ms: 16.102\n",
      "    sample_throughput: 54.666\n",
      "    sample_time_ms: 18292.92\n",
      "    update_time_ms: 2.699\n",
      "  timestamp: 1633714305\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 358000\n",
      "  training_iteration: 358\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   358</td><td style=\"text-align: right;\">         8501.74</td><td style=\"text-align: right;\">358000</td><td style=\"text-align: right;\">    4.25</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            416.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 359000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-32-26\n",
      "  done: false\n",
      "  episode_len_mean: 413.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.25\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 914\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.003218940893809\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005569051783947209\n",
      "          policy_loss: 0.03317138254642486\n",
      "          total_loss: 0.04843176110751099\n",
      "          vf_explained_var: 0.9653071165084839\n",
      "          vf_loss: 0.030776542280283238\n",
      "    num_agent_steps_sampled: 359000\n",
      "    num_agent_steps_trained: 359000\n",
      "    num_steps_sampled: 359000\n",
      "    num_steps_trained: 359000\n",
      "  iterations_since_restore: 359\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.291228070175436\n",
      "    ram_util_percent: 60.40175438596492\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03836413399671776\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.961249135144705\n",
      "    mean_inference_ms: 1.5688355233695606\n",
      "    mean_raw_obs_processing_ms: 2.0552011874361473\n",
      "  time_since_restore: 8541.896271705627\n",
      "  time_this_iter_s: 40.15555119514465\n",
      "  time_total_s: 8541.896271705627\n",
      "  timers:\n",
      "    learn_throughput: 961.561\n",
      "    learn_time_ms: 1039.976\n",
      "    load_throughput: 61377.124\n",
      "    load_time_ms: 16.293\n",
      "    sample_throughput: 49.099\n",
      "    sample_time_ms: 20367.204\n",
      "    update_time_ms: 2.77\n",
      "  timestamp: 1633714346\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 359000\n",
      "  training_iteration: 359\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   359</td><td style=\"text-align: right;\">          8541.9</td><td style=\"text-align: right;\">359000</td><td style=\"text-align: right;\">    4.25</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">             413.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-32-48\n",
      "  done: false\n",
      "  episode_len_mean: 411.69\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.28\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 916\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.654538971847958\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008144822370488924\n",
      "          policy_loss: -0.17672978714108467\n",
      "          total_loss: -0.05409870110452175\n",
      "          vf_explained_var: 0.7756829261779785\n",
      "          vf_loss: 0.1325717187176148\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "  iterations_since_restore: 360\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.328125\n",
      "    ram_util_percent: 60.703125\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038363639894722086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.955579982752266\n",
      "    mean_inference_ms: 1.5688106381085867\n",
      "    mean_raw_obs_processing_ms: 2.0608516681534503\n",
      "  time_since_restore: 8564.458896636963\n",
      "  time_this_iter_s: 22.56262493133545\n",
      "  time_total_s: 8564.458896636963\n",
      "  timers:\n",
      "    learn_throughput: 959.868\n",
      "    learn_time_ms: 1041.81\n",
      "    load_throughput: 60891.943\n",
      "    load_time_ms: 16.423\n",
      "    sample_throughput: 48.611\n",
      "    sample_time_ms: 20571.634\n",
      "    update_time_ms: 2.524\n",
      "  timestamp: 1633714368\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 360\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   360</td><td style=\"text-align: right;\">         8564.46</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\">    4.28</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            411.69</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 361000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-33-08\n",
      "  done: false\n",
      "  episode_len_mean: 413.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.24\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 919\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7516070551342435\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011887560597898666\n",
      "          policy_loss: 0.012145275870958963\n",
      "          total_loss: 0.5003868684586551\n",
      "          vf_explained_var: 0.24835027754306793\n",
      "          vf_loss: 0.49611786918507683\n",
      "    num_agent_steps_sampled: 361000\n",
      "    num_agent_steps_trained: 361000\n",
      "    num_steps_sampled: 361000\n",
      "    num_steps_trained: 361000\n",
      "  iterations_since_restore: 361\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.16896551724138\n",
      "    ram_util_percent: 60.53103448275864\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03836289431351903\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.94719452825696\n",
      "    mean_inference_ms: 1.5687741796139614\n",
      "    mean_raw_obs_processing_ms: 2.069352777611331\n",
      "  time_since_restore: 8584.708072423935\n",
      "  time_this_iter_s: 20.249175786972046\n",
      "  time_total_s: 8584.708072423935\n",
      "  timers:\n",
      "    learn_throughput: 959.237\n",
      "    learn_time_ms: 1042.495\n",
      "    load_throughput: 57126.84\n",
      "    load_time_ms: 17.505\n",
      "    sample_throughput: 47.764\n",
      "    sample_time_ms: 20936.057\n",
      "    update_time_ms: 2.508\n",
      "  timestamp: 1633714388\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 361000\n",
      "  training_iteration: 361\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   361</td><td style=\"text-align: right;\">         8584.71</td><td style=\"text-align: right;\">361000</td><td style=\"text-align: right;\">    4.24</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            413.84</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 362000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-33-32\n",
      "  done: false\n",
      "  episode_len_mean: 411.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.32\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 922\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.715621128347185\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009330757353957608\n",
      "          policy_loss: -0.020883556372589537\n",
      "          total_loss: 0.173964666430321\n",
      "          vf_explained_var: 0.5875048041343689\n",
      "          vf_loss: 0.20443799131446413\n",
      "    num_agent_steps_sampled: 362000\n",
      "    num_agent_steps_trained: 362000\n",
      "    num_steps_sampled: 362000\n",
      "    num_steps_trained: 362000\n",
      "  iterations_since_restore: 362\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.45294117647059\n",
      "    ram_util_percent: 60.36764705882353\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0383621323988957\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.939337377125796\n",
      "    mean_inference_ms: 1.568739032163016\n",
      "    mean_raw_obs_processing_ms: 2.077851935075171\n",
      "  time_since_restore: 8608.60422539711\n",
      "  time_this_iter_s: 23.89615297317505\n",
      "  time_total_s: 8608.60422539711\n",
      "  timers:\n",
      "    learn_throughput: 959.204\n",
      "    learn_time_ms: 1042.531\n",
      "    load_throughput: 55885.008\n",
      "    load_time_ms: 17.894\n",
      "    sample_throughput: 47.053\n",
      "    sample_time_ms: 21252.778\n",
      "    update_time_ms: 2.268\n",
      "  timestamp: 1633714412\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 362000\n",
      "  training_iteration: 362\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   362</td><td style=\"text-align: right;\">          8608.6</td><td style=\"text-align: right;\">362000</td><td style=\"text-align: right;\">    4.32</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            411.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 363000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-33-50\n",
      "  done: false\n",
      "  episode_len_mean: 411.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.37\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 923\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6924083352088928\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006516915334932112\n",
      "          policy_loss: 0.053104611072275376\n",
      "          total_loss: 0.13218544949260022\n",
      "          vf_explained_var: 0.7674354314804077\n",
      "          vf_loss: 0.09072025937752591\n",
      "    num_agent_steps_sampled: 363000\n",
      "    num_agent_steps_trained: 363000\n",
      "    num_steps_sampled: 363000\n",
      "    num_steps_trained: 363000\n",
      "  iterations_since_restore: 363\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.248000000000005\n",
      "    ram_util_percent: 60.388000000000005\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038361875254907425\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.936742405975803\n",
      "    mean_inference_ms: 1.5687274757991543\n",
      "    mean_raw_obs_processing_ms: 2.0807008541000034\n",
      "  time_since_restore: 8626.16259765625\n",
      "  time_this_iter_s: 17.558372259140015\n",
      "  time_total_s: 8626.16259765625\n",
      "  timers:\n",
      "    learn_throughput: 957.185\n",
      "    learn_time_ms: 1044.731\n",
      "    load_throughput: 59891.934\n",
      "    load_time_ms: 16.697\n",
      "    sample_throughput: 47.643\n",
      "    sample_time_ms: 20989.293\n",
      "    update_time_ms: 2.268\n",
      "  timestamp: 1633714430\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 363000\n",
      "  training_iteration: 363\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   363</td><td style=\"text-align: right;\">         8626.16</td><td style=\"text-align: right;\">363000</td><td style=\"text-align: right;\">    4.37</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            411.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 364000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-34-11\n",
      "  done: false\n",
      "  episode_len_mean: 413.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.34\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 926\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7421093795034621\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01446760652831287\n",
      "          policy_loss: -0.04816474649641249\n",
      "          total_loss: 0.17404458026091257\n",
      "          vf_explained_var: 0.4739888906478882\n",
      "          vf_loss: 0.22789842885815434\n",
      "    num_agent_steps_sampled: 364000\n",
      "    num_agent_steps_trained: 364000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "  iterations_since_restore: 364\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.03225806451613\n",
      "    ram_util_percent: 60.35483870967741\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0383610852562422\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.929013821315184\n",
      "    mean_inference_ms: 1.5686928840555134\n",
      "    mean_raw_obs_processing_ms: 2.088150665213179\n",
      "  time_since_restore: 8647.393638849258\n",
      "  time_this_iter_s: 21.231041193008423\n",
      "  time_total_s: 8647.393638849258\n",
      "  timers:\n",
      "    learn_throughput: 956.688\n",
      "    learn_time_ms: 1045.273\n",
      "    load_throughput: 59841.006\n",
      "    load_time_ms: 16.711\n",
      "    sample_throughput: 47.446\n",
      "    sample_time_ms: 21076.626\n",
      "    update_time_ms: 2.279\n",
      "  timestamp: 1633714451\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 364\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   364</td><td style=\"text-align: right;\">         8647.39</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\">    4.34</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            413.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 365000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-34-29\n",
      "  done: false\n",
      "  episode_len_mean: 415.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.22\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 928\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6098508530192905\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009848542265055452\n",
      "          policy_loss: 0.011823818625675307\n",
      "          total_loss: 0.22638341509219673\n",
      "          vf_explained_var: 0.6338852643966675\n",
      "          vf_loss: 0.22267177616142564\n",
      "    num_agent_steps_sampled: 365000\n",
      "    num_agent_steps_trained: 365000\n",
      "    num_steps_sampled: 365000\n",
      "    num_steps_trained: 365000\n",
      "  iterations_since_restore: 365\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.16400000000001\n",
      "    ram_util_percent: 60.407999999999994\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03836053386937395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.923635599381118\n",
      "    mean_inference_ms: 1.5686692406538503\n",
      "    mean_raw_obs_processing_ms: 2.092750652657776\n",
      "  time_since_restore: 8664.992706775665\n",
      "  time_this_iter_s: 17.59906792640686\n",
      "  time_total_s: 8664.992706775665\n",
      "  timers:\n",
      "    learn_throughput: 955.241\n",
      "    learn_time_ms: 1046.857\n",
      "    load_throughput: 58634.368\n",
      "    load_time_ms: 17.055\n",
      "    sample_throughput: 47.215\n",
      "    sample_time_ms: 21179.635\n",
      "    update_time_ms: 2.286\n",
      "  timestamp: 1633714469\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 365000\n",
      "  training_iteration: 365\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   365</td><td style=\"text-align: right;\">         8664.99</td><td style=\"text-align: right;\">365000</td><td style=\"text-align: right;\">    4.22</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            415.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 366000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-35-34\n",
      "  done: false\n",
      "  episode_len_mean: 409.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.47\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 932\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6188291284773084\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006176034525515206\n",
      "          policy_loss: 0.13391905021336342\n",
      "          total_loss: 0.15745431996054118\n",
      "          vf_explained_var: 0.3653255105018616\n",
      "          vf_loss: 0.03471532436605129\n",
      "    num_agent_steps_sampled: 366000\n",
      "    num_agent_steps_trained: 366000\n",
      "    num_steps_sampled: 366000\n",
      "    num_steps_trained: 366000\n",
      "  iterations_since_restore: 366\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.045161290322582\n",
      "    ram_util_percent: 60.55053763440863\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038359385693458196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.913499444108993\n",
      "    mean_inference_ms: 1.568623175232909\n",
      "    mean_raw_obs_processing_ms: 2.1063712707219784\n",
      "  time_since_restore: 8730.25390458107\n",
      "  time_this_iter_s: 65.26119780540466\n",
      "  time_total_s: 8730.25390458107\n",
      "  timers:\n",
      "    learn_throughput: 955.019\n",
      "    learn_time_ms: 1047.1\n",
      "    load_throughput: 57946.062\n",
      "    load_time_ms: 17.257\n",
      "    sample_throughput: 38.669\n",
      "    sample_time_ms: 25860.402\n",
      "    update_time_ms: 2.288\n",
      "  timestamp: 1633714534\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 366000\n",
      "  training_iteration: 366\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   366</td><td style=\"text-align: right;\">         8730.25</td><td style=\"text-align: right;\">366000</td><td style=\"text-align: right;\">    4.47</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            409.54</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 367000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-36-40\n",
      "  done: false\n",
      "  episode_len_mean: 399.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.51\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 936\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.379351704650455\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010298102375058033\n",
      "          policy_loss: 0.062351032594839734\n",
      "          total_loss: 0.27894492741260263\n",
      "          vf_explained_var: 0.8677743673324585\n",
      "          vf_loss: 0.22203652991188896\n",
      "    num_agent_steps_sampled: 367000\n",
      "    num_agent_steps_trained: 367000\n",
      "    num_steps_sampled: 367000\n",
      "    num_steps_trained: 367000\n",
      "  iterations_since_restore: 367\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.556382978723406\n",
      "    ram_util_percent: 60.70957446808512\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03835821853501752\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.905130513206927\n",
      "    mean_inference_ms: 1.5685811700122443\n",
      "    mean_raw_obs_processing_ms: 2.1244836609570643\n",
      "  time_since_restore: 8796.246537923813\n",
      "  time_this_iter_s: 65.99263334274292\n",
      "  time_total_s: 8796.246537923813\n",
      "  timers:\n",
      "    learn_throughput: 956.276\n",
      "    learn_time_ms: 1045.723\n",
      "    load_throughput: 57817.141\n",
      "    load_time_ms: 17.296\n",
      "    sample_throughput: 32.995\n",
      "    sample_time_ms: 30307.884\n",
      "    update_time_ms: 2.276\n",
      "  timestamp: 1633714600\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 367000\n",
      "  training_iteration: 367\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   367</td><td style=\"text-align: right;\">         8796.25</td><td style=\"text-align: right;\">367000</td><td style=\"text-align: right;\">    4.51</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            399.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 368000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-36-59\n",
      "  done: false\n",
      "  episode_len_mean: 399.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.54\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 938\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0770042485660976\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008666030119758123\n",
      "          policy_loss: -0.01930840313434601\n",
      "          total_loss: 0.07304027531709936\n",
      "          vf_explained_var: 0.25478607416152954\n",
      "          vf_loss: 0.10609130778660376\n",
      "    num_agent_steps_sampled: 368000\n",
      "    num_agent_steps_trained: 368000\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "  iterations_since_restore: 368\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.11785714285714\n",
      "    ram_util_percent: 60.624999999999986\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03835762041120925\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.901115169730957\n",
      "    mean_inference_ms: 1.5685608201977277\n",
      "    mean_raw_obs_processing_ms: 2.1335542325515586\n",
      "  time_since_restore: 8815.58197069168\n",
      "  time_this_iter_s: 19.335432767868042\n",
      "  time_total_s: 8815.58197069168\n",
      "  timers:\n",
      "    learn_throughput: 957.62\n",
      "    learn_time_ms: 1044.256\n",
      "    load_throughput: 58012.744\n",
      "    load_time_ms: 17.238\n",
      "    sample_throughput: 32.985\n",
      "    sample_time_ms: 30317.237\n",
      "    update_time_ms: 2.282\n",
      "  timestamp: 1633714619\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 368\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   368</td><td style=\"text-align: right;\">         8815.58</td><td style=\"text-align: right;\">368000</td><td style=\"text-align: right;\">    4.54</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            399.77</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 369000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-38-18\n",
      "  done: false\n",
      "  episode_len_mean: 390.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.78\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 942\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7500200496779548\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007485080647776717\n",
      "          policy_loss: -0.004459962621331215\n",
      "          total_loss: 0.05974778139756785\n",
      "          vf_explained_var: 0.7373389601707458\n",
      "          vf_loss: 0.07563818173689975\n",
      "    num_agent_steps_sampled: 369000\n",
      "    num_agent_steps_trained: 369000\n",
      "    num_steps_sampled: 369000\n",
      "    num_steps_trained: 369000\n",
      "  iterations_since_restore: 369\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.030630630630636\n",
      "    ram_util_percent: 60.22702702702704\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03835645370451646\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.893746387835453\n",
      "    mean_inference_ms: 1.5685227499361463\n",
      "    mean_raw_obs_processing_ms: 2.1582287363947175\n",
      "  time_since_restore: 8893.832664012909\n",
      "  time_this_iter_s: 78.25069332122803\n",
      "  time_total_s: 8893.832664012909\n",
      "  timers:\n",
      "    learn_throughput: 956.896\n",
      "    learn_time_ms: 1045.045\n",
      "    load_throughput: 58170.441\n",
      "    load_time_ms: 17.191\n",
      "    sample_throughput: 29.303\n",
      "    sample_time_ms: 34126.084\n",
      "    update_time_ms: 2.205\n",
      "  timestamp: 1633714698\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 369000\n",
      "  training_iteration: 369\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   369</td><td style=\"text-align: right;\">         8893.83</td><td style=\"text-align: right;\">369000</td><td style=\"text-align: right;\">    4.78</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            390.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 370000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-38-38\n",
      "  done: false\n",
      "  episode_len_mean: 390.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.85\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 944\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5710423641734652\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012975762307613298\n",
      "          policy_loss: -0.013992163497540686\n",
      "          total_loss: 0.15493773031565877\n",
      "          vf_explained_var: 0.5770056247711182\n",
      "          vf_loss: 0.17411807597511345\n",
      "    num_agent_steps_sampled: 370000\n",
      "    num_agent_steps_trained: 370000\n",
      "    num_steps_sampled: 370000\n",
      "    num_steps_trained: 370000\n",
      "  iterations_since_restore: 370\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.05862068965517\n",
      "    ram_util_percent: 60.537931034482746\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038355900047003316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.890453805487855\n",
      "    mean_inference_ms: 1.5685050757852463\n",
      "    mean_raw_obs_processing_ms: 2.1705981451081677\n",
      "  time_since_restore: 8914.102761745453\n",
      "  time_this_iter_s: 20.270097732543945\n",
      "  time_total_s: 8914.102761745453\n",
      "  timers:\n",
      "    learn_throughput: 956.771\n",
      "    learn_time_ms: 1045.182\n",
      "    load_throughput: 58864.037\n",
      "    load_time_ms: 16.988\n",
      "    sample_throughput: 29.501\n",
      "    sample_time_ms: 33896.904\n",
      "    update_time_ms: 2.211\n",
      "  timestamp: 1633714718\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 370000\n",
      "  training_iteration: 370\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   370</td><td style=\"text-align: right;\">          8914.1</td><td style=\"text-align: right;\">370000</td><td style=\"text-align: right;\">    4.85</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">             390.1</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 371000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-38-52\n",
      "  done: false\n",
      "  episode_len_mean: 390.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 4.88\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 946\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.8109146118164059\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.26333523425791\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004040184704739626\n",
      "          policy_loss: -0.08035351874099837\n",
      "          total_loss: -0.0252299175494247\n",
      "          vf_explained_var: 0.3035734295845032\n",
      "          vf_loss: 0.06448070977090134\n",
      "    num_agent_steps_sampled: 371000\n",
      "    num_agent_steps_trained: 371000\n",
      "    num_steps_sampled: 371000\n",
      "    num_steps_trained: 371000\n",
      "  iterations_since_restore: 371\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.52380952380953\n",
      "    ram_util_percent: 60.46666666666666\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038355341788241605\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.8870950314313\n",
      "    mean_inference_ms: 1.5684865664627534\n",
      "    mean_raw_obs_processing_ms: 2.182945485376594\n",
      "  time_since_restore: 8928.430871725082\n",
      "  time_this_iter_s: 14.328109979629517\n",
      "  time_total_s: 8928.430871725082\n",
      "  timers:\n",
      "    learn_throughput: 958.846\n",
      "    learn_time_ms: 1042.92\n",
      "    load_throughput: 64535.3\n",
      "    load_time_ms: 15.495\n",
      "    sample_throughput: 30.022\n",
      "    sample_time_ms: 33308.538\n",
      "    update_time_ms: 2.227\n",
      "  timestamp: 1633714732\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 371000\n",
      "  training_iteration: 371\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   371</td><td style=\"text-align: right;\">         8928.43</td><td style=\"text-align: right;\">371000</td><td style=\"text-align: right;\">    4.88</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            390.99</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 372000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-39-50\n",
      "  done: false\n",
      "  episode_len_mean: 385.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.02\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 949\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8036207331551446\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010198235660821817\n",
      "          policy_loss: -0.04521011614965068\n",
      "          total_loss: 0.06359517977883418\n",
      "          vf_explained_var: 0.3054672181606293\n",
      "          vf_loss: 0.1227065556579166\n",
      "    num_agent_steps_sampled: 372000\n",
      "    num_agent_steps_trained: 372000\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "  iterations_since_restore: 372\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.42682926829269\n",
      "    ram_util_percent: 60.08170731707316\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0383545416320204\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.882376961840183\n",
      "    mean_inference_ms: 1.568460704169896\n",
      "    mean_raw_obs_processing_ms: 2.2045020975791974\n",
      "  time_since_restore: 8985.663534164429\n",
      "  time_this_iter_s: 57.23266243934631\n",
      "  time_total_s: 8985.663534164429\n",
      "  timers:\n",
      "    learn_throughput: 957.806\n",
      "    learn_time_ms: 1044.053\n",
      "    load_throughput: 65059.843\n",
      "    load_time_ms: 15.37\n",
      "    sample_throughput: 27.292\n",
      "    sample_time_ms: 36641.178\n",
      "    update_time_ms: 2.222\n",
      "  timestamp: 1633714790\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 372\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   372</td><td style=\"text-align: right;\">         8985.66</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\">    5.02</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            385.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 373000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-40-09\n",
      "  done: false\n",
      "  episode_len_mean: 385.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.13\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 951\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.804124497042762\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01133546591567275\n",
      "          policy_loss: -0.025848662956721252\n",
      "          total_loss: -0.01624097288068798\n",
      "          vf_explained_var: 0.793643593788147\n",
      "          vf_loss: 0.02305288752540946\n",
      "    num_agent_steps_sampled: 373000\n",
      "    num_agent_steps_trained: 373000\n",
      "    num_steps_sampled: 373000\n",
      "    num_steps_trained: 373000\n",
      "  iterations_since_restore: 373\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.64074074074074\n",
      "    ram_util_percent: 60.41481481481482\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038354030224221736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.87928228149883\n",
      "    mean_inference_ms: 1.5684439655106206\n",
      "    mean_raw_obs_processing_ms: 2.2188618292582016\n",
      "  time_since_restore: 9004.710230588913\n",
      "  time_this_iter_s: 19.046696424484253\n",
      "  time_total_s: 9004.710230588913\n",
      "  timers:\n",
      "    learn_throughput: 959.518\n",
      "    learn_time_ms: 1042.19\n",
      "    load_throughput: 60805.08\n",
      "    load_time_ms: 16.446\n",
      "    sample_throughput: 27.181\n",
      "    sample_time_ms: 36790.776\n",
      "    update_time_ms: 2.229\n",
      "  timestamp: 1633714809\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 373000\n",
      "  training_iteration: 373\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   373</td><td style=\"text-align: right;\">         9004.71</td><td style=\"text-align: right;\">373000</td><td style=\"text-align: right;\">    5.13</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">               385</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 374000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-40-26\n",
      "  done: false\n",
      "  episode_len_mean: 386.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.21\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 953\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5512685391638015\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015704520251174456\n",
      "          policy_loss: -0.01873057766093148\n",
      "          total_loss: 0.05616562621047099\n",
      "          vf_explained_var: 0.8303840756416321\n",
      "          vf_loss: 0.0840413776329822\n",
      "    num_agent_steps_sampled: 374000\n",
      "    num_agent_steps_trained: 374000\n",
      "    num_steps_sampled: 374000\n",
      "    num_steps_trained: 374000\n",
      "  iterations_since_restore: 374\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.7125\n",
      "    ram_util_percent: 59.945833333333326\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03835352920511564\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.87595458597996\n",
      "    mean_inference_ms: 1.5684266764335857\n",
      "    mean_raw_obs_processing_ms: 2.2331411556901766\n",
      "  time_since_restore: 9021.722454071045\n",
      "  time_this_iter_s: 17.012223482131958\n",
      "  time_total_s: 9021.722454071045\n",
      "  timers:\n",
      "    learn_throughput: 960.609\n",
      "    learn_time_ms: 1041.006\n",
      "    load_throughput: 65308.238\n",
      "    load_time_ms: 15.312\n",
      "    sample_throughput: 27.494\n",
      "    sample_time_ms: 36371.234\n",
      "    update_time_ms: 2.224\n",
      "  timestamp: 1633714826\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 374000\n",
      "  training_iteration: 374\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   374</td><td style=\"text-align: right;\">         9021.72</td><td style=\"text-align: right;\">374000</td><td style=\"text-align: right;\">    5.21</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            386.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 375000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-41-28\n",
      "  done: false\n",
      "  episode_len_mean: 384.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.46\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 956\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4755149788326687\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01597804960731981\n",
      "          policy_loss: -0.10984786310129696\n",
      "          total_loss: 0.032219856811894314\n",
      "          vf_explained_var: 0.6209450364112854\n",
      "          vf_loss: 0.15034445693923368\n",
      "    num_agent_steps_sampled: 375000\n",
      "    num_agent_steps_trained: 375000\n",
      "    num_steps_sampled: 375000\n",
      "    num_steps_trained: 375000\n",
      "  iterations_since_restore: 375\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.332584269662924\n",
      "    ram_util_percent: 59.586516853932594\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038352809390137216\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.87121759475214\n",
      "    mean_inference_ms: 1.568401745382572\n",
      "    mean_raw_obs_processing_ms: 2.2572690429814264\n",
      "  time_since_restore: 9083.903660535812\n",
      "  time_this_iter_s: 62.181206464767456\n",
      "  time_total_s: 9083.903660535812\n",
      "  timers:\n",
      "    learn_throughput: 961.627\n",
      "    learn_time_ms: 1039.904\n",
      "    load_throughput: 62188.786\n",
      "    load_time_ms: 16.08\n",
      "    sample_throughput: 24.492\n",
      "    sample_time_ms: 40829.766\n",
      "    update_time_ms: 2.227\n",
      "  timestamp: 1633714888\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 375000\n",
      "  training_iteration: 375\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   375</td><td style=\"text-align: right;\">          9083.9</td><td style=\"text-align: right;\">375000</td><td style=\"text-align: right;\">    5.46</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            384.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-42-26\n",
      "  done: false\n",
      "  episode_len_mean: 380.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.61\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 960\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9282672312524585\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011042164140623041\n",
      "          policy_loss: -0.061518556997179986\n",
      "          total_loss: -0.023311550832457013\n",
      "          vf_explained_var: 0.32085084915161133\n",
      "          vf_loss: 0.05301255044113431\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_agent_steps_trained: 376000\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "  iterations_since_restore: 376\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.228915662650607\n",
      "    ram_util_percent: 60.303614457831344\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03835188260432516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.865230194131392\n",
      "    mean_inference_ms: 1.5683693467675324\n",
      "    mean_raw_obs_processing_ms: 2.2919776090125294\n",
      "  time_since_restore: 9141.917805433273\n",
      "  time_this_iter_s: 58.01414489746094\n",
      "  time_total_s: 9141.917805433273\n",
      "  timers:\n",
      "    learn_throughput: 960.802\n",
      "    learn_time_ms: 1040.798\n",
      "    load_throughput: 63813.048\n",
      "    load_time_ms: 15.671\n",
      "    sample_throughput: 24.935\n",
      "    sample_time_ms: 40104.592\n",
      "    update_time_ms: 2.221\n",
      "  timestamp: 1633714946\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 376\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   376</td><td style=\"text-align: right;\">         9141.92</td><td style=\"text-align: right;\">376000</td><td style=\"text-align: right;\">    5.61</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            380.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 377000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-42-48\n",
      "  done: false\n",
      "  episode_len_mean: 381.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.65\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 962\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1251539561483597\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01675214265838567\n",
      "          policy_loss: -0.15052046622667048\n",
      "          total_loss: -0.09118927733765708\n",
      "          vf_explained_var: 0.20505733788013458\n",
      "          vf_loss: 0.07379044902821381\n",
      "    num_agent_steps_sampled: 377000\n",
      "    num_agent_steps_trained: 377000\n",
      "    num_steps_sampled: 377000\n",
      "    num_steps_trained: 377000\n",
      "  iterations_since_restore: 377\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.79375\n",
      "    ram_util_percent: 60.5\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038351415515035424\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.862327622424214\n",
      "    mean_inference_ms: 1.5683536536205847\n",
      "    mean_raw_obs_processing_ms: 2.309322117331871\n",
      "  time_since_restore: 9164.269524812698\n",
      "  time_this_iter_s: 22.35171937942505\n",
      "  time_total_s: 9164.269524812698\n",
      "  timers:\n",
      "    learn_throughput: 961.675\n",
      "    learn_time_ms: 1039.853\n",
      "    load_throughput: 62963.64\n",
      "    load_time_ms: 15.882\n",
      "    sample_throughput: 27.979\n",
      "    sample_time_ms: 35741.204\n",
      "    update_time_ms: 2.248\n",
      "  timestamp: 1633714968\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 377000\n",
      "  training_iteration: 377\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   377</td><td style=\"text-align: right;\">         9164.27</td><td style=\"text-align: right;\">377000</td><td style=\"text-align: right;\">    5.65</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            381.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 378000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-43-51\n",
      "  done: false\n",
      "  episode_len_mean: 374.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.68\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 966\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.40545730590820295\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8607893294758266\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0236427940531639\n",
      "          policy_loss: -0.052777079823944306\n",
      "          total_loss: 0.3178852492322524\n",
      "          vf_explained_var: 0.5482766628265381\n",
      "          vf_loss: 0.3796840842399332\n",
      "    num_agent_steps_sampled: 378000\n",
      "    num_agent_steps_trained: 378000\n",
      "    num_steps_sampled: 378000\n",
      "    num_steps_trained: 378000\n",
      "  iterations_since_restore: 378\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.04659090909091\n",
      "    ram_util_percent: 59.80340909090908\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038350447388883754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.85720869803837\n",
      "    mean_inference_ms: 1.5683236325092207\n",
      "    mean_raw_obs_processing_ms: 2.3480160170280424\n",
      "  time_since_restore: 9226.51556801796\n",
      "  time_this_iter_s: 62.24604320526123\n",
      "  time_total_s: 9226.51556801796\n",
      "  timers:\n",
      "    learn_throughput: 960.537\n",
      "    learn_time_ms: 1041.084\n",
      "    load_throughput: 62886.041\n",
      "    load_time_ms: 15.902\n",
      "    sample_throughput: 24.981\n",
      "    sample_time_ms: 40030.947\n",
      "    update_time_ms: 2.303\n",
      "  timestamp: 1633715031\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 378000\n",
      "  training_iteration: 378\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   378</td><td style=\"text-align: right;\">         9226.52</td><td style=\"text-align: right;\">378000</td><td style=\"text-align: right;\">    5.68</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            374.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 379000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-44-07\n",
      "  done: false\n",
      "  episode_len_mean: 373.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.69\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 968\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2308708906173704\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005115655586964193\n",
      "          policy_loss: -0.12184673075874647\n",
      "          total_loss: -0.09549220734172398\n",
      "          vf_explained_var: 0.5118066668510437\n",
      "          vf_loss: 0.04555196393436442\n",
      "    num_agent_steps_sampled: 379000\n",
      "    num_agent_steps_trained: 379000\n",
      "    num_steps_sampled: 379000\n",
      "    num_steps_trained: 379000\n",
      "  iterations_since_restore: 379\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.93333333333334\n",
      "    ram_util_percent: 60.45000000000001\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038349957563737755\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.854490223028428\n",
      "    mean_inference_ms: 1.568308017472803\n",
      "    mean_raw_obs_processing_ms: 2.3673476643379003\n",
      "  time_since_restore: 9243.115186929703\n",
      "  time_this_iter_s: 16.599618911743164\n",
      "  time_total_s: 9243.115186929703\n",
      "  timers:\n",
      "    learn_throughput: 960.353\n",
      "    learn_time_ms: 1041.283\n",
      "    load_throughput: 68350.882\n",
      "    load_time_ms: 14.63\n",
      "    sample_throughput: 29.527\n",
      "    sample_time_ms: 33866.895\n",
      "    update_time_ms: 2.316\n",
      "  timestamp: 1633715047\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 379000\n",
      "  training_iteration: 379\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   379</td><td style=\"text-align: right;\">         9243.12</td><td style=\"text-align: right;\">379000</td><td style=\"text-align: right;\">    5.69</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">             373.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 380000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-44-23\n",
      "  done: false\n",
      "  episode_len_mean: 377.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.74\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 970\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3346169690291088\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012816326518006753\n",
      "          policy_loss: -0.014209635059038798\n",
      "          total_loss: 0.2661493538878858\n",
      "          vf_explained_var: 0.6711324453353882\n",
      "          vf_loss: 0.2859104483284884\n",
      "    num_agent_steps_sampled: 380000\n",
      "    num_agent_steps_trained: 380000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "  iterations_since_restore: 380\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.27391304347827\n",
      "    ram_util_percent: 60.40000000000001\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038349463365573835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.8512793761023\n",
      "    mean_inference_ms: 1.5682911016921526\n",
      "    mean_raw_obs_processing_ms: 2.386636124967318\n",
      "  time_since_restore: 9258.814499616623\n",
      "  time_this_iter_s: 15.699312686920166\n",
      "  time_total_s: 9258.814499616623\n",
      "  timers:\n",
      "    learn_throughput: 961.905\n",
      "    learn_time_ms: 1039.603\n",
      "    load_throughput: 75600.425\n",
      "    load_time_ms: 13.227\n",
      "    sample_throughput: 29.929\n",
      "    sample_time_ms: 33412.793\n",
      "    update_time_ms: 2.413\n",
      "  timestamp: 1633715063\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 380\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   380</td><td style=\"text-align: right;\">         9258.81</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\">    5.74</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            377.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 381000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-44-42\n",
      "  done: false\n",
      "  episode_len_mean: 379.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.81\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 972\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7184542377789815\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01150601395077461\n",
      "          policy_loss: -0.011930094949073261\n",
      "          total_loss: 0.04261097022228771\n",
      "          vf_explained_var: 0.8187891840934753\n",
      "          vf_loss: 0.06472781172229183\n",
      "    num_agent_steps_sampled: 381000\n",
      "    num_agent_steps_trained: 381000\n",
      "    num_steps_sampled: 381000\n",
      "    num_steps_trained: 381000\n",
      "  iterations_since_restore: 381\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.46666666666666\n",
      "    ram_util_percent: 59.93333333333333\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03834896679956242\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.847803326143033\n",
      "    mean_inference_ms: 1.5682734349503897\n",
      "    mean_raw_obs_processing_ms: 2.4058311952370826\n",
      "  time_since_restore: 9278.079375982285\n",
      "  time_this_iter_s: 19.26487636566162\n",
      "  time_total_s: 9278.079375982285\n",
      "  timers:\n",
      "    learn_throughput: 961.681\n",
      "    learn_time_ms: 1039.846\n",
      "    load_throughput: 67899.321\n",
      "    load_time_ms: 14.728\n",
      "    sample_throughput: 29.495\n",
      "    sample_time_ms: 33903.976\n",
      "    update_time_ms: 3.165\n",
      "  timestamp: 1633715082\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 381000\n",
      "  training_iteration: 381\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   381</td><td style=\"text-align: right;\">         9278.08</td><td style=\"text-align: right;\">381000</td><td style=\"text-align: right;\">    5.81</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            379.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 382000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-45-37\n",
      "  done: false\n",
      "  episode_len_mean: 379.72\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.85\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 976\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.841387512948778\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011181314078458569\n",
      "          policy_loss: -0.07092296249336666\n",
      "          total_loss: 0.3068631436261866\n",
      "          vf_explained_var: 0.8768668174743652\n",
      "          vf_loss: 0.38939965690175693\n",
      "    num_agent_steps_sampled: 382000\n",
      "    num_agent_steps_trained: 382000\n",
      "    num_steps_sampled: 382000\n",
      "    num_steps_trained: 382000\n",
      "  iterations_since_restore: 382\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.375949367088612\n",
      "    ram_util_percent: 59.762025316455706\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03834798233306231\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.840757235976305\n",
      "    mean_inference_ms: 1.5682375644866804\n",
      "    mean_raw_obs_processing_ms: 2.44241418470073\n",
      "  time_since_restore: 9333.299067735672\n",
      "  time_this_iter_s: 55.21969175338745\n",
      "  time_total_s: 9333.299067735672\n",
      "  timers:\n",
      "    learn_throughput: 961.308\n",
      "    learn_time_ms: 1040.25\n",
      "    load_throughput: 68623.073\n",
      "    load_time_ms: 14.572\n",
      "    sample_throughput: 29.671\n",
      "    sample_time_ms: 33702.425\n",
      "    update_time_ms: 3.169\n",
      "  timestamp: 1633715137\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 382000\n",
      "  training_iteration: 382\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   382</td><td style=\"text-align: right;\">          9333.3</td><td style=\"text-align: right;\">382000</td><td style=\"text-align: right;\">    5.85</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            379.72</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 383000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-45-56\n",
      "  done: false\n",
      "  episode_len_mean: 381.35\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.84\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 978\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7956297755241395\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013694528970318448\n",
      "          policy_loss: 0.035999013690484895\n",
      "          total_loss: 0.2241380325725509\n",
      "          vf_explained_var: 0.8564919233322144\n",
      "          vf_loss: 0.19776649399557047\n",
      "    num_agent_steps_sampled: 383000\n",
      "    num_agent_steps_trained: 383000\n",
      "    num_steps_sampled: 383000\n",
      "    num_steps_trained: 383000\n",
      "  iterations_since_restore: 383\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.25\n",
      "    ram_util_percent: 60.661538461538456\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038347467788036986\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.83711457323864\n",
      "    mean_inference_ms: 1.5682191074317888\n",
      "    mean_raw_obs_processing_ms: 2.458933465382998\n",
      "  time_since_restore: 9351.850177288055\n",
      "  time_this_iter_s: 18.551109552383423\n",
      "  time_total_s: 9351.850177288055\n",
      "  timers:\n",
      "    learn_throughput: 961.839\n",
      "    learn_time_ms: 1039.675\n",
      "    load_throughput: 68530.572\n",
      "    load_time_ms: 14.592\n",
      "    sample_throughput: 29.715\n",
      "    sample_time_ms: 33653.395\n",
      "    update_time_ms: 3.166\n",
      "  timestamp: 1633715156\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 383000\n",
      "  training_iteration: 383\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   383</td><td style=\"text-align: right;\">         9351.85</td><td style=\"text-align: right;\">383000</td><td style=\"text-align: right;\">    5.84</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            381.35</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 384000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-46-15\n",
      "  done: false\n",
      "  episode_len_mean: 385.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.78\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 981\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9363287806510925\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008926100298252729\n",
      "          policy_loss: -0.08245632747809092\n",
      "          total_loss: 0.03448993671271536\n",
      "          vf_explained_var: 0.28134670853614807\n",
      "          vf_loss: 0.13088082037866117\n",
      "    num_agent_steps_sampled: 384000\n",
      "    num_agent_steps_trained: 384000\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "  iterations_since_restore: 384\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.1888888888889\n",
      "    ram_util_percent: 60.21111111111112\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038346695651883825\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.831441442413812\n",
      "    mean_inference_ms: 1.5681907584751327\n",
      "    mean_raw_obs_processing_ms: 2.481374740479396\n",
      "  time_since_restore: 9370.416002035141\n",
      "  time_this_iter_s: 18.56582474708557\n",
      "  time_total_s: 9370.416002035141\n",
      "  timers:\n",
      "    learn_throughput: 962.793\n",
      "    learn_time_ms: 1038.645\n",
      "    load_throughput: 63226.456\n",
      "    load_time_ms: 15.816\n",
      "    sample_throughput: 29.578\n",
      "    sample_time_ms: 33808.575\n",
      "    update_time_ms: 3.157\n",
      "  timestamp: 1633715175\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 384\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   384</td><td style=\"text-align: right;\">         9370.42</td><td style=\"text-align: right;\">384000</td><td style=\"text-align: right;\">    5.78</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            385.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 385000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-46-32\n",
      "  done: false\n",
      "  episode_len_mean: 392.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.8\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 983\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9294763697518242\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01458678755162036\n",
      "          policy_loss: -0.058871866369413\n",
      "          total_loss: 9.3714768687884e-05\n",
      "          vf_explained_var: 0.8877391219139099\n",
      "          vf_loss: 0.06938886580367883\n",
      "    num_agent_steps_sampled: 385000\n",
      "    num_agent_steps_trained: 385000\n",
      "    num_steps_sampled: 385000\n",
      "    num_steps_trained: 385000\n",
      "  iterations_since_restore: 385\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.408\n",
      "    ram_util_percent: 59.824\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03834616910210489\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.827439138957075\n",
      "    mean_inference_ms: 1.5681712339327538\n",
      "    mean_raw_obs_processing_ms: 2.4948849961198483\n",
      "  time_since_restore: 9388.186002016068\n",
      "  time_this_iter_s: 17.769999980926514\n",
      "  time_total_s: 9388.186002016068\n",
      "  timers:\n",
      "    learn_throughput: 961.484\n",
      "    learn_time_ms: 1040.059\n",
      "    load_throughput: 67981.419\n",
      "    load_time_ms: 14.71\n",
      "    sample_throughput: 34.052\n",
      "    sample_time_ms: 29367.141\n",
      "    update_time_ms: 3.15\n",
      "  timestamp: 1633715192\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 385000\n",
      "  training_iteration: 385\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   385</td><td style=\"text-align: right;\">         9388.19</td><td style=\"text-align: right;\">385000</td><td style=\"text-align: right;\">     5.8</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            392.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 386000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-47-13\n",
      "  done: false\n",
      "  episode_len_mean: 384.91\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.94\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 987\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3526114573081334\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01153227447652268\n",
      "          policy_loss: 0.01688614326218764\n",
      "          total_loss: 0.24745873742633395\n",
      "          vf_explained_var: 0.7064476609230042\n",
      "          vf_loss: 0.23708494388394885\n",
      "    num_agent_steps_sampled: 386000\n",
      "    num_agent_steps_trained: 386000\n",
      "    num_steps_sampled: 386000\n",
      "    num_steps_trained: 386000\n",
      "  iterations_since_restore: 386\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.62542372881355\n",
      "    ram_util_percent: 59.793220338983055\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03834513314696452\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.820363233560983\n",
      "    mean_inference_ms: 1.568135599809079\n",
      "    mean_raw_obs_processing_ms: 2.523782748209261\n",
      "  time_since_restore: 9429.128714323044\n",
      "  time_this_iter_s: 40.94271230697632\n",
      "  time_total_s: 9429.128714323044\n",
      "  timers:\n",
      "    learn_throughput: 961.818\n",
      "    learn_time_ms: 1039.698\n",
      "    load_throughput: 67770.193\n",
      "    load_time_ms: 14.756\n",
      "    sample_throughput: 36.153\n",
      "    sample_time_ms: 27660.302\n",
      "    update_time_ms: 3.151\n",
      "  timestamp: 1633715233\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 386000\n",
      "  training_iteration: 386\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   386</td><td style=\"text-align: right;\">         9429.13</td><td style=\"text-align: right;\">386000</td><td style=\"text-align: right;\">    5.94</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            384.91</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 387000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-47-31\n",
      "  done: false\n",
      "  episode_len_mean: 384.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.98\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 989\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.094419929716322\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007042172377487393\n",
      "          policy_loss: 0.07979510679013199\n",
      "          total_loss: 0.14705312078197796\n",
      "          vf_explained_var: 0.7041260004043579\n",
      "          vf_loss: 0.08391926432442334\n",
      "    num_agent_steps_sampled: 387000\n",
      "    num_agent_steps_trained: 387000\n",
      "    num_steps_sampled: 387000\n",
      "    num_steps_trained: 387000\n",
      "  iterations_since_restore: 387\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.892307692307696\n",
      "    ram_util_percent: 60.39615384615385\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03834461131921142\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.816918344901786\n",
      "    mean_inference_ms: 1.5681183842690865\n",
      "    mean_raw_obs_processing_ms: 2.53826396897462\n",
      "  time_since_restore: 9447.142051458359\n",
      "  time_this_iter_s: 18.01333713531494\n",
      "  time_total_s: 9447.142051458359\n",
      "  timers:\n",
      "    learn_throughput: 961.235\n",
      "    learn_time_ms: 1040.328\n",
      "    load_throughput: 74276.88\n",
      "    load_time_ms: 13.463\n",
      "    sample_throughput: 36.728\n",
      "    sample_time_ms: 27227.131\n",
      "    update_time_ms: 3.137\n",
      "  timestamp: 1633715251\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 387000\n",
      "  training_iteration: 387\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   387</td><td style=\"text-align: right;\">         9447.14</td><td style=\"text-align: right;\">387000</td><td style=\"text-align: right;\">    5.98</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">             384.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 388000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-47-51\n",
      "  done: false\n",
      "  episode_len_mean: 384.52\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.01\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 991\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9738041838010152\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008812024535868594\n",
      "          policy_loss: -0.14537332066231304\n",
      "          total_loss: -0.050307838908500144\n",
      "          vf_explained_var: 0.868851900100708\n",
      "          vf_loss: 0.10944417297012277\n",
      "    num_agent_steps_sampled: 388000\n",
      "    num_agent_steps_trained: 388000\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "  iterations_since_restore: 388\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.43333333333334\n",
      "    ram_util_percent: 60.31851851851852\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03834407913526944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.813396485807065\n",
      "    mean_inference_ms: 1.5681006807026523\n",
      "    mean_raw_obs_processing_ms: 2.5526599199730384\n",
      "  time_since_restore: 9466.540464639664\n",
      "  time_this_iter_s: 19.39841318130493\n",
      "  time_total_s: 9466.540464639664\n",
      "  timers:\n",
      "    learn_throughput: 961.46\n",
      "    learn_time_ms: 1040.084\n",
      "    load_throughput: 74572.829\n",
      "    load_time_ms: 13.41\n",
      "    sample_throughput: 43.587\n",
      "    sample_time_ms: 22942.71\n",
      "    update_time_ms: 3.09\n",
      "  timestamp: 1633715271\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 388\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   388</td><td style=\"text-align: right;\">         9466.54</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\">    6.01</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            384.52</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 389000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-48-52\n",
      "  done: false\n",
      "  episode_len_mean: 379.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.08\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 995\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.076567061742147\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007336625075732023\n",
      "          policy_loss: 0.10593364867899153\n",
      "          total_loss: 0.15224777025481065\n",
      "          vf_explained_var: 0.30936476588249207\n",
      "          vf_loss: 0.06261776226262251\n",
      "    num_agent_steps_sampled: 389000\n",
      "    num_agent_steps_trained: 389000\n",
      "    num_steps_sampled: 389000\n",
      "    num_steps_trained: 389000\n",
      "  iterations_since_restore: 389\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.073863636363637\n",
      "    ram_util_percent: 59.98295454545455\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03834305788987841\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.806669895868016\n",
      "    mean_inference_ms: 1.5680670800350356\n",
      "    mean_raw_obs_processing_ms: 2.5858174077846052\n",
      "  time_since_restore: 9527.749006271362\n",
      "  time_this_iter_s: 61.20854163169861\n",
      "  time_total_s: 9527.749006271362\n",
      "  timers:\n",
      "    learn_throughput: 962.747\n",
      "    learn_time_ms: 1038.694\n",
      "    load_throughput: 68848.698\n",
      "    load_time_ms: 14.525\n",
      "    sample_throughput: 36.491\n",
      "    sample_time_ms: 27403.865\n",
      "    update_time_ms: 3.091\n",
      "  timestamp: 1633715332\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 389000\n",
      "  training_iteration: 389\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   389</td><td style=\"text-align: right;\">         9527.75</td><td style=\"text-align: right;\">389000</td><td style=\"text-align: right;\">    6.08</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            379.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 390000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-49-16\n",
      "  done: false\n",
      "  episode_len_mean: 378.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.09\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 997\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4193598369757334\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009829077049640105\n",
      "          policy_loss: 0.02928828563955095\n",
      "          total_loss: 0.13950107180409962\n",
      "          vf_explained_var: 0.7012165188789368\n",
      "          vf_loss: 0.11842847673429382\n",
      "    num_agent_steps_sampled: 390000\n",
      "    num_agent_steps_trained: 390000\n",
      "    num_steps_sampled: 390000\n",
      "    num_steps_trained: 390000\n",
      "  iterations_since_restore: 390\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.93939393939394\n",
      "    ram_util_percent: 60.4\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038342536833646634\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.803528555023178\n",
      "    mean_inference_ms: 1.568051179903739\n",
      "    mean_raw_obs_processing_ms: 2.602364248099087\n",
      "  time_since_restore: 9551.371778011322\n",
      "  time_this_iter_s: 23.622771739959717\n",
      "  time_total_s: 9551.371778011322\n",
      "  timers:\n",
      "    learn_throughput: 962.872\n",
      "    learn_time_ms: 1038.56\n",
      "    load_throughput: 61995.019\n",
      "    load_time_ms: 16.13\n",
      "    sample_throughput: 35.468\n",
      "    sample_time_ms: 28194.824\n",
      "    update_time_ms: 2.994\n",
      "  timestamp: 1633715356\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 390000\n",
      "  training_iteration: 390\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   390</td><td style=\"text-align: right;\">         9551.37</td><td style=\"text-align: right;\">390000</td><td style=\"text-align: right;\">    6.09</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            378.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 391000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-49-37\n",
      "  done: false\n",
      "  episode_len_mean: 378.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.17\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1000\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.925078308582306\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010765469457978977\n",
      "          policy_loss: 0.02937929452293449\n",
      "          total_loss: 0.07170626856386661\n",
      "          vf_explained_var: 0.9645277261734009\n",
      "          vf_loss: 0.0550303487210638\n",
      "    num_agent_steps_sampled: 391000\n",
      "    num_agent_steps_trained: 391000\n",
      "    num_steps_sampled: 391000\n",
      "    num_steps_trained: 391000\n",
      "  iterations_since_restore: 391\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.66774193548387\n",
      "    ram_util_percent: 60.6967741935484\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03834171685357133\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.79898859022338\n",
      "    mean_inference_ms: 1.5680276398222974\n",
      "    mean_raw_obs_processing_ms: 2.627198479638782\n",
      "  time_since_restore: 9572.984375476837\n",
      "  time_this_iter_s: 21.612597465515137\n",
      "  time_total_s: 9572.984375476837\n",
      "  timers:\n",
      "    learn_throughput: 962.542\n",
      "    learn_time_ms: 1038.916\n",
      "    load_throughput: 61746.229\n",
      "    load_time_ms: 16.195\n",
      "    sample_throughput: 35.174\n",
      "    sample_time_ms: 28429.932\n",
      "    update_time_ms: 2.237\n",
      "  timestamp: 1633715377\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 391000\n",
      "  training_iteration: 391\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   391</td><td style=\"text-align: right;\">         9572.98</td><td style=\"text-align: right;\">391000</td><td style=\"text-align: right;\">    6.17</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            378.37</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-50-17\n",
      "  done: false\n",
      "  episode_len_mean: 373.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.17\n",
      "  episode_reward_min: -7.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1003\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.963170666164822\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012549588521156944\n",
      "          policy_loss: -0.0987851944234636\n",
      "          total_loss: 0.044344087193409605\n",
      "          vf_explained_var: 0.9507886171340942\n",
      "          vf_loss: 0.15512850810256268\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "  iterations_since_restore: 392\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.792857142857144\n",
      "    ram_util_percent: 60.53392857142857\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038340876143862966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.794754930105906\n",
      "    mean_inference_ms: 1.5680050420934806\n",
      "    mean_raw_obs_processing_ms: 2.6532643933266167\n",
      "  time_since_restore: 9612.354852676392\n",
      "  time_this_iter_s: 39.37047719955444\n",
      "  time_total_s: 9612.354852676392\n",
      "  timers:\n",
      "    learn_throughput: 963.343\n",
      "    learn_time_ms: 1038.052\n",
      "    load_throughput: 61514.31\n",
      "    load_time_ms: 16.256\n",
      "    sample_throughput: 37.25\n",
      "    sample_time_ms: 26845.837\n",
      "    update_time_ms: 2.225\n",
      "  timestamp: 1633715417\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 392\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   392</td><td style=\"text-align: right;\">         9612.35</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\">    6.17</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -7</td><td style=\"text-align: right;\">            373.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 393000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-51-12\n",
      "  done: false\n",
      "  episode_len_mean: 368.46\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.42\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1007\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0010391182369656\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008673616574870839\n",
      "          policy_loss: 0.03342527062114742\n",
      "          total_loss: 0.041621829393423265\n",
      "          vf_explained_var: 0.8318814039230347\n",
      "          vf_loss: 0.02293177692530056\n",
      "    num_agent_steps_sampled: 393000\n",
      "    num_agent_steps_trained: 393000\n",
      "    num_steps_sampled: 393000\n",
      "    num_steps_trained: 393000\n",
      "  iterations_since_restore: 393\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.26125\n",
      "    ram_util_percent: 60.332499999999996\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038339748149826394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.789563803863235\n",
      "    mean_inference_ms: 1.5679777850093672\n",
      "    mean_raw_obs_processing_ms: 2.6917884716374\n",
      "  time_since_restore: 9668.026179075241\n",
      "  time_this_iter_s: 55.67132639884949\n",
      "  time_total_s: 9668.026179075241\n",
      "  timers:\n",
      "    learn_throughput: 963.717\n",
      "    learn_time_ms: 1037.649\n",
      "    load_throughput: 61369.85\n",
      "    load_time_ms: 16.295\n",
      "    sample_throughput: 32.724\n",
      "    sample_time_ms: 30558.252\n",
      "    update_time_ms: 2.226\n",
      "  timestamp: 1633715472\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 393000\n",
      "  training_iteration: 393\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   393</td><td style=\"text-align: right;\">         9668.03</td><td style=\"text-align: right;\">393000</td><td style=\"text-align: right;\">    6.42</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            368.46</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 394000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-52-21\n",
      "  done: false\n",
      "  episode_len_mean: 360.71\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.61\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1011\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6081859588623049\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1916149112913343\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0035805950532690842\n",
      "          policy_loss: 0.007667905754513211\n",
      "          total_loss: 0.020513161395986874\n",
      "          vf_explained_var: 0.1413925439119339\n",
      "          vf_loss: 0.03258373618074176\n",
      "    num_agent_steps_sampled: 394000\n",
      "    num_agent_steps_trained: 394000\n",
      "    num_steps_sampled: 394000\n",
      "    num_steps_trained: 394000\n",
      "  iterations_since_restore: 394\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.352040816326532\n",
      "    ram_util_percent: 60.45714285714287\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0383387042549024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.784780405665227\n",
      "    mean_inference_ms: 1.5679515787358411\n",
      "    mean_raw_obs_processing_ms: 2.7349709093239345\n",
      "  time_since_restore: 9736.720572471619\n",
      "  time_this_iter_s: 68.69439339637756\n",
      "  time_total_s: 9736.720572471619\n",
      "  timers:\n",
      "    learn_throughput: 963.806\n",
      "    learn_time_ms: 1037.554\n",
      "    load_throughput: 60830.213\n",
      "    load_time_ms: 16.439\n",
      "    sample_throughput: 28.113\n",
      "    sample_time_ms: 35571.053\n",
      "    update_time_ms: 2.229\n",
      "  timestamp: 1633715541\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 394000\n",
      "  training_iteration: 394\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   394</td><td style=\"text-align: right;\">         9736.72</td><td style=\"text-align: right;\">394000</td><td style=\"text-align: right;\">    6.61</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            360.71</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 395000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-53-08\n",
      "  done: false\n",
      "  episode_len_mean: 358.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.64\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1014\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9274248666233487\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01188157897314519\n",
      "          policy_loss: 0.009079036116600037\n",
      "          total_loss: 0.11210259513722526\n",
      "          vf_explained_var: 0.734556257724762\n",
      "          vf_loss: 0.11868470204580162\n",
      "    num_agent_steps_sampled: 395000\n",
      "    num_agent_steps_trained: 395000\n",
      "    num_steps_sampled: 395000\n",
      "    num_steps_trained: 395000\n",
      "  iterations_since_restore: 395\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.52238805970149\n",
      "    ram_util_percent: 60.468656716417904\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03833793765265077\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.781138972538017\n",
      "    mean_inference_ms: 1.5679324767204441\n",
      "    mean_raw_obs_processing_ms: 2.767941041909361\n",
      "  time_since_restore: 9783.983575820923\n",
      "  time_this_iter_s: 47.2630033493042\n",
      "  time_total_s: 9783.983575820923\n",
      "  timers:\n",
      "    learn_throughput: 964.491\n",
      "    learn_time_ms: 1036.817\n",
      "    load_throughput: 56925.873\n",
      "    load_time_ms: 17.567\n",
      "    sample_throughput: 25.961\n",
      "    sample_time_ms: 38519.972\n",
      "    update_time_ms: 2.231\n",
      "  timestamp: 1633715588\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 395000\n",
      "  training_iteration: 395\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   395</td><td style=\"text-align: right;\">         9783.98</td><td style=\"text-align: right;\">395000</td><td style=\"text-align: right;\">    6.64</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            358.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 396000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-53-28\n",
      "  done: false\n",
      "  episode_len_mean: 359.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.62\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1017\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297943115243\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9866739339298671\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020040418922737994\n",
      "          policy_loss: -0.010643381708198124\n",
      "          total_loss: 0.1730095576081011\n",
      "          vf_explained_var: 0.3460218906402588\n",
      "          vf_loss: 0.19742552960912388\n",
      "    num_agent_steps_sampled: 396000\n",
      "    num_agent_steps_trained: 396000\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "  iterations_since_restore: 396\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.939285714285724\n",
      "    ram_util_percent: 60.45357142857142\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03833723498703181\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.777292422369552\n",
      "    mean_inference_ms: 1.5679130226792928\n",
      "    mean_raw_obs_processing_ms: 2.8009028013506514\n",
      "  time_since_restore: 9803.273128032684\n",
      "  time_this_iter_s: 19.289552211761475\n",
      "  time_total_s: 9803.273128032684\n",
      "  timers:\n",
      "    learn_throughput: 964.841\n",
      "    learn_time_ms: 1036.441\n",
      "    load_throughput: 57010.289\n",
      "    load_time_ms: 17.541\n",
      "    sample_throughput: 27.506\n",
      "    sample_time_ms: 36355.064\n",
      "    update_time_ms: 2.245\n",
      "  timestamp: 1633715608\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 396\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   396</td><td style=\"text-align: right;\">         9803.27</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">    6.62</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             359.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 397000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-53-47\n",
      "  done: false\n",
      "  episode_len_mean: 359.09\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.67\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1019\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.124930187066396\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009052377928280899\n",
      "          policy_loss: 0.023309642614589798\n",
      "          total_loss: 0.05122054869102107\n",
      "          vf_explained_var: 0.3369148373603821\n",
      "          vf_loss: 0.04503106120456424\n",
      "    num_agent_steps_sampled: 397000\n",
      "    num_agent_steps_trained: 397000\n",
      "    num_steps_sampled: 397000\n",
      "    num_steps_trained: 397000\n",
      "  iterations_since_restore: 397\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.33214285714286\n",
      "    ram_util_percent: 59.73571428571428\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03833677366329145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.774682913048114\n",
      "    mean_inference_ms: 1.5678999043232986\n",
      "    mean_raw_obs_processing_ms: 2.8227927177996683\n",
      "  time_since_restore: 9822.842545747757\n",
      "  time_this_iter_s: 19.569417715072632\n",
      "  time_total_s: 9822.842545747757\n",
      "  timers:\n",
      "    learn_throughput: 965.29\n",
      "    learn_time_ms: 1035.958\n",
      "    load_throughput: 53629.977\n",
      "    load_time_ms: 18.646\n",
      "    sample_throughput: 27.39\n",
      "    sample_time_ms: 36510.058\n",
      "    update_time_ms: 2.242\n",
      "  timestamp: 1633715627\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 397000\n",
      "  training_iteration: 397\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   397</td><td style=\"text-align: right;\">         9822.84</td><td style=\"text-align: right;\">397000</td><td style=\"text-align: right;\">    6.67</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            359.09</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 398000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-54-06\n",
      "  done: false\n",
      "  episode_len_mean: 360.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.7\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1021\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.107327604293823\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010380805683660909\n",
      "          policy_loss: -0.049075401822725934\n",
      "          total_loss: -0.028209493392043643\n",
      "          vf_explained_var: 0.8433452844619751\n",
      "          vf_loss: 0.03720409087836742\n",
      "    num_agent_steps_sampled: 398000\n",
      "    num_agent_steps_trained: 398000\n",
      "    num_steps_sampled: 398000\n",
      "    num_steps_trained: 398000\n",
      "  iterations_since_restore: 398\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.266666666666666\n",
      "    ram_util_percent: 59.44074074074074\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038336317700086636\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.771815326006582\n",
      "    mean_inference_ms: 1.5678862752134364\n",
      "    mean_raw_obs_processing_ms: 2.84463573765054\n",
      "  time_since_restore: 9841.645032167435\n",
      "  time_this_iter_s: 18.802486419677734\n",
      "  time_total_s: 9841.645032167435\n",
      "  timers:\n",
      "    learn_throughput: 965.415\n",
      "    learn_time_ms: 1035.824\n",
      "    load_throughput: 53421.233\n",
      "    load_time_ms: 18.719\n",
      "    sample_throughput: 27.434\n",
      "    sample_time_ms: 36450.525\n",
      "    update_time_ms: 2.237\n",
      "  timestamp: 1633715646\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 398000\n",
      "  training_iteration: 398\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   398</td><td style=\"text-align: right;\">         9841.65</td><td style=\"text-align: right;\">398000</td><td style=\"text-align: right;\">     6.7</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            360.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 399000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-54-26\n",
      "  done: false\n",
      "  episode_len_mean: 359.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.67\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1024\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0528573089175755\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008944171497360124\n",
      "          policy_loss: 0.015588107208410898\n",
      "          total_loss: 0.11307972214288181\n",
      "          vf_explained_var: 0.9474902749061584\n",
      "          vf_loss: 0.11394039430759019\n",
      "    num_agent_steps_sampled: 399000\n",
      "    num_agent_steps_trained: 399000\n",
      "    num_steps_sampled: 399000\n",
      "    num_steps_trained: 399000\n",
      "  iterations_since_restore: 399\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.310714285714276\n",
      "    ram_util_percent: 59.42500000000001\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038335627436001\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.767620534528202\n",
      "    mean_inference_ms: 1.567865765009432\n",
      "    mean_raw_obs_processing_ms: 2.8773370943315917\n",
      "  time_since_restore: 9861.463453054428\n",
      "  time_this_iter_s: 19.818420886993408\n",
      "  time_total_s: 9861.463453054428\n",
      "  timers:\n",
      "    learn_throughput: 964.126\n",
      "    learn_time_ms: 1037.208\n",
      "    load_throughput: 53685.786\n",
      "    load_time_ms: 18.627\n",
      "    sample_throughput: 30.95\n",
      "    sample_time_ms: 32310.222\n",
      "    update_time_ms: 2.236\n",
      "  timestamp: 1633715666\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 399000\n",
      "  training_iteration: 399\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   399</td><td style=\"text-align: right;\">         9861.46</td><td style=\"text-align: right;\">399000</td><td style=\"text-align: right;\">    6.67</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            359.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-54-48\n",
      "  done: false\n",
      "  episode_len_mean: 356.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.74\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1026\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7460898942417569\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00966125930754921\n",
      "          policy_loss: -0.0653795657058557\n",
      "          total_loss: -0.037263470060295525\n",
      "          vf_explained_var: 0.5016469359397888\n",
      "          vf_loss: 0.041170112840417356\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_agent_steps_trained: 400000\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "  iterations_since_restore: 400\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.378125\n",
      "    ram_util_percent: 59.587500000000006\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03833514283436082\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.764944210430652\n",
      "    mean_inference_ms: 1.5678518241453911\n",
      "    mean_raw_obs_processing_ms: 2.899092889166913\n",
      "  time_since_restore: 9883.526762247086\n",
      "  time_this_iter_s: 22.06330919265747\n",
      "  time_total_s: 9883.526762247086\n",
      "  timers:\n",
      "    learn_throughput: 963.036\n",
      "    learn_time_ms: 1038.382\n",
      "    load_throughput: 53805.826\n",
      "    load_time_ms: 18.585\n",
      "    sample_throughput: 31.101\n",
      "    sample_time_ms: 32153.143\n",
      "    update_time_ms: 2.247\n",
      "  timestamp: 1633715688\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 400\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   400</td><td style=\"text-align: right;\">         9883.53</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\">    6.74</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">            356.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 401000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-55-46\n",
      "  done: false\n",
      "  episode_len_mean: 354.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.8\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1030\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.044999913374583\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016827864841145828\n",
      "          policy_loss: -0.003898665433128675\n",
      "          total_loss: 0.40895575046953225\n",
      "          vf_explained_var: 0.7155667543411255\n",
      "          vf_loss: 0.4256285604917341\n",
      "    num_agent_steps_sampled: 401000\n",
      "    num_agent_steps_trained: 401000\n",
      "    num_steps_sampled: 401000\n",
      "    num_steps_trained: 401000\n",
      "  iterations_since_restore: 401\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.597590361445782\n",
      "    ram_util_percent: 59.780722891566256\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03833419977744335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.759681720850367\n",
      "    mean_inference_ms: 1.5678247166793597\n",
      "    mean_raw_obs_processing_ms: 2.944255198169784\n",
      "  time_since_restore: 9941.681461334229\n",
      "  time_this_iter_s: 58.154699087142944\n",
      "  time_total_s: 9941.681461334229\n",
      "  timers:\n",
      "    learn_throughput: 962.656\n",
      "    learn_time_ms: 1038.793\n",
      "    load_throughput: 53932.228\n",
      "    load_time_ms: 18.542\n",
      "    sample_throughput: 27.928\n",
      "    sample_time_ms: 35806.935\n",
      "    update_time_ms: 2.28\n",
      "  timestamp: 1633715746\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 401000\n",
      "  training_iteration: 401\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   401</td><td style=\"text-align: right;\">         9941.68</td><td style=\"text-align: right;\">401000</td><td style=\"text-align: right;\">     6.8</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">            354.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 402000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-56-07\n",
      "  done: false\n",
      "  episode_len_mean: 358.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.77\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 1032\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1054836644066706\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005064050266975843\n",
      "          policy_loss: -0.17403813031398588\n",
      "          total_loss: -0.14743856456544663\n",
      "          vf_explained_var: 0.3105683922767639\n",
      "          vf_loss: 0.04534448907555391\n",
      "    num_agent_steps_sampled: 402000\n",
      "    num_agent_steps_trained: 402000\n",
      "    num_steps_sampled: 402000\n",
      "    num_steps_trained: 402000\n",
      "  iterations_since_restore: 402\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.53333333333334\n",
      "    ram_util_percent: 60.40000000000001\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038333738228332415\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.756934217034686\n",
      "    mean_inference_ms: 1.567810698728985\n",
      "    mean_raw_obs_processing_ms: 2.965639804149423\n",
      "  time_since_restore: 9962.880945920944\n",
      "  time_this_iter_s: 21.1994845867157\n",
      "  time_total_s: 9962.880945920944\n",
      "  timers:\n",
      "    learn_throughput: 962.353\n",
      "    learn_time_ms: 1039.12\n",
      "    load_throughput: 54049.613\n",
      "    load_time_ms: 18.502\n",
      "    sample_throughput: 29.421\n",
      "    sample_time_ms: 33989.522\n",
      "    update_time_ms: 2.291\n",
      "  timestamp: 1633715767\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 402000\n",
      "  training_iteration: 402\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   402</td><td style=\"text-align: right;\">         9962.88</td><td style=\"text-align: right;\">402000</td><td style=\"text-align: right;\">    6.77</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">            358.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 403000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-57-07\n",
      "  done: false\n",
      "  episode_len_mean: 356.61\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.69\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1035\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7228209680981106\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019182970160141713\n",
      "          policy_loss: -0.04831321231193013\n",
      "          total_loss: 0.2701676624102725\n",
      "          vf_explained_var: 0.569747269153595\n",
      "          vf_loss: 0.32695897511310046\n",
      "    num_agent_steps_sampled: 403000\n",
      "    num_agent_steps_trained: 403000\n",
      "    num_steps_sampled: 403000\n",
      "    num_steps_trained: 403000\n",
      "  iterations_since_restore: 403\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.24117647058823\n",
      "    ram_util_percent: 60.312941176470595\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03833311213360777\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.752542122162467\n",
      "    mean_inference_ms: 1.5677894367310465\n",
      "    mean_raw_obs_processing_ms: 2.997127673893002\n",
      "  time_since_restore: 10022.38938999176\n",
      "  time_this_iter_s: 59.50844407081604\n",
      "  time_total_s: 10022.38938999176\n",
      "  timers:\n",
      "    learn_throughput: 960.691\n",
      "    learn_time_ms: 1040.917\n",
      "    load_throughput: 54055.673\n",
      "    load_time_ms: 18.499\n",
      "    sample_throughput: 29.094\n",
      "    sample_time_ms: 34371.448\n",
      "    update_time_ms: 2.283\n",
      "  timestamp: 1633715827\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 403000\n",
      "  training_iteration: 403\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   403</td><td style=\"text-align: right;\">         10022.4</td><td style=\"text-align: right;\">403000</td><td style=\"text-align: right;\">    6.69</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            356.61</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 404000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-57-24\n",
      "  done: false\n",
      "  episode_len_mean: 361.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.71\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1038\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.951587892903222\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01162919142881829\n",
      "          policy_loss: -0.035001074771086375\n",
      "          total_loss: -0.008892037636703915\n",
      "          vf_explained_var: 0.8332535028457642\n",
      "          vf_loss: 0.040320383488304086\n",
      "    num_agent_steps_sampled: 404000\n",
      "    num_agent_steps_trained: 404000\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "  iterations_since_restore: 404\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.18800000000001\n",
      "    ram_util_percent: 60.504\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038332469034724585\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.747976821642172\n",
      "    mean_inference_ms: 1.5677676713475035\n",
      "    mean_raw_obs_processing_ms: 3.0284957105166233\n",
      "  time_since_restore: 10039.896091461182\n",
      "  time_this_iter_s: 17.506701469421387\n",
      "  time_total_s: 10039.896091461182\n",
      "  timers:\n",
      "    learn_throughput: 961.444\n",
      "    learn_time_ms: 1040.103\n",
      "    load_throughput: 56632.029\n",
      "    load_time_ms: 17.658\n",
      "    sample_throughput: 34.183\n",
      "    sample_time_ms: 29254.332\n",
      "    update_time_ms: 2.272\n",
      "  timestamp: 1633715844\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 404\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   404</td><td style=\"text-align: right;\">         10039.9</td><td style=\"text-align: right;\">404000</td><td style=\"text-align: right;\">    6.71</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            361.59</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 405000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_17-58-31\n",
      "  done: false\n",
      "  episode_len_mean: 358.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.7\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1041\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6849386188719007\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0110652988392656\n",
      "          policy_loss: -0.09066795044475132\n",
      "          total_loss: -0.0070169651259978615\n",
      "          vf_explained_var: 0.8144615292549133\n",
      "          vf_loss: 0.0954530489527517\n",
      "    num_agent_steps_sampled: 405000\n",
      "    num_agent_steps_trained: 405000\n",
      "    num_steps_sampled: 405000\n",
      "    num_steps_trained: 405000\n",
      "  iterations_since_restore: 405\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.31894736842106\n",
      "    ram_util_percent: 60.058947368421045\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038331826265964423\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.74395174767139\n",
      "    mean_inference_ms: 1.5677466226637387\n",
      "    mean_raw_obs_processing_ms: 3.0581363147170784\n",
      "  time_since_restore: 10106.609053134918\n",
      "  time_this_iter_s: 66.71296167373657\n",
      "  time_total_s: 10106.609053134918\n",
      "  timers:\n",
      "    learn_throughput: 962.739\n",
      "    learn_time_ms: 1038.703\n",
      "    load_throughput: 56911.815\n",
      "    load_time_ms: 17.571\n",
      "    sample_throughput: 32.05\n",
      "    sample_time_ms: 31200.798\n",
      "    update_time_ms: 2.281\n",
      "  timestamp: 1633715911\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 405000\n",
      "  training_iteration: 405\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   405</td><td style=\"text-align: right;\">         10106.6</td><td style=\"text-align: right;\">405000</td><td style=\"text-align: right;\">     6.7</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            358.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 406000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_18-00-27\n",
      "  done: false\n",
      "  episode_len_mean: 345.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.81\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1048\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7165810704231261\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.017235039163420594\n",
      "          policy_loss: 0.18550394765204853\n",
      "          total_loss: 0.3280689977109432\n",
      "          vf_explained_var: 0.8468323349952698\n",
      "          vf_loss: 0.15186927875296938\n",
      "    num_agent_steps_sampled: 406000\n",
      "    num_agent_steps_trained: 406000\n",
      "    num_steps_sampled: 406000\n",
      "    num_steps_trained: 406000\n",
      "  iterations_since_restore: 406\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.95\n",
      "    ram_util_percent: 60.581927710843395\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038330266255213935\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.73674080338516\n",
      "    mean_inference_ms: 1.5677029886963367\n",
      "    mean_raw_obs_processing_ms: 3.1407259863526154\n",
      "  time_since_restore: 10222.776350736618\n",
      "  time_this_iter_s: 116.16729760169983\n",
      "  time_total_s: 10222.776350736618\n",
      "  timers:\n",
      "    learn_throughput: 962.977\n",
      "    learn_time_ms: 1038.446\n",
      "    load_throughput: 56680.397\n",
      "    load_time_ms: 17.643\n",
      "    sample_throughput: 24.457\n",
      "    sample_time_ms: 40888.765\n",
      "    update_time_ms: 2.255\n",
      "  timestamp: 1633716027\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 406000\n",
      "  training_iteration: 406\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   406</td><td style=\"text-align: right;\">         10222.8</td><td style=\"text-align: right;\">406000</td><td style=\"text-align: right;\">    6.81</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            345.29</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 407000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_18-02-47\n",
      "  done: false\n",
      "  episode_len_mean: 321.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 6.92\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1055\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4996302054988013\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013181651949368383\n",
      "          policy_loss: -0.1565324506825871\n",
      "          total_loss: 0.08715519981665744\n",
      "          vf_explained_var: 0.3126491606235504\n",
      "          vf_loss: 0.25267127737816836\n",
      "    num_agent_steps_sampled: 407000\n",
      "    num_agent_steps_trained: 407000\n",
      "    num_steps_sampled: 407000\n",
      "    num_steps_trained: 407000\n",
      "  iterations_since_restore: 407\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.729499999999998\n",
      "    ram_util_percent: 60.15849999999998\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03832868682697049\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.731772567423977\n",
      "    mean_inference_ms: 1.5676649186316087\n",
      "    mean_raw_obs_processing_ms: 3.2368044872644317\n",
      "  time_since_restore: 10362.81384730339\n",
      "  time_this_iter_s: 140.03749656677246\n",
      "  time_total_s: 10362.81384730339\n",
      "  timers:\n",
      "    learn_throughput: 962.368\n",
      "    learn_time_ms: 1039.104\n",
      "    load_throughput: 56640.824\n",
      "    load_time_ms: 17.655\n",
      "    sample_throughput: 18.891\n",
      "    sample_time_ms: 52934.899\n",
      "    update_time_ms: 2.268\n",
      "  timestamp: 1633716167\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 407000\n",
      "  training_iteration: 407\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   407</td><td style=\"text-align: right;\">         10362.8</td><td style=\"text-align: right;\">407000</td><td style=\"text-align: right;\">    6.92</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            321.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_18-05-48\n",
      "  done: false\n",
      "  episode_len_mean: 304.27\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 7.12\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1065\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4842854075961642\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01054035931984981\n",
      "          policy_loss: -0.09714475588666069\n",
      "          total_loss: 0.08588763781719738\n",
      "          vf_explained_var: 0.3841882348060608\n",
      "          vf_loss: 0.1930673650931567\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "  iterations_since_restore: 408\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.718217054263565\n",
      "    ram_util_percent: 60.513178294573635\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03832647591649424\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.72565390304312\n",
      "    mean_inference_ms: 1.5676155583505724\n",
      "    mean_raw_obs_processing_ms: 3.3940505307015214\n",
      "  time_since_restore: 10543.545955181122\n",
      "  time_this_iter_s: 180.73210787773132\n",
      "  time_total_s: 10543.545955181122\n",
      "  timers:\n",
      "    learn_throughput: 962.557\n",
      "    learn_time_ms: 1038.899\n",
      "    load_throughput: 56651.841\n",
      "    load_time_ms: 17.652\n",
      "    sample_throughput: 14.466\n",
      "    sample_time_ms: 69128.082\n",
      "    update_time_ms: 2.258\n",
      "  timestamp: 1633716348\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 408\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   408</td><td style=\"text-align: right;\">         10543.5</td><td style=\"text-align: right;\">408000</td><td style=\"text-align: right;\">    7.12</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            304.27</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 409000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_18-06-26\n",
      "  done: false\n",
      "  episode_len_mean: 300.12\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 7.15\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 1068\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7715070565541586\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.015562889405258358\n",
      "          policy_loss: 0.009558311270342932\n",
      "          total_loss: 0.48839850339831575\n",
      "          vf_explained_var: 0.3502582311630249\n",
      "          vf_loss: 0.4894564071463214\n",
      "    num_agent_steps_sampled: 409000\n",
      "    num_agent_steps_trained: 409000\n",
      "    num_steps_sampled: 409000\n",
      "    num_steps_trained: 409000\n",
      "  iterations_since_restore: 409\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.42830188679246\n",
      "    ram_util_percent: 60.45849056603774\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0383258733244113\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.723823806352858\n",
      "    mean_inference_ms: 1.5676015475549656\n",
      "    mean_raw_obs_processing_ms: 3.440170067134898\n",
      "  time_since_restore: 10580.835258960724\n",
      "  time_this_iter_s: 37.28930377960205\n",
      "  time_total_s: 10580.835258960724\n",
      "  timers:\n",
      "    learn_throughput: 963.357\n",
      "    learn_time_ms: 1038.037\n",
      "    load_throughput: 56512.537\n",
      "    load_time_ms: 17.695\n",
      "    sample_throughput: 14.109\n",
      "    sample_time_ms: 70876.019\n",
      "    update_time_ms: 2.249\n",
      "  timestamp: 1633716386\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 409000\n",
      "  training_iteration: 409\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   409</td><td style=\"text-align: right;\">         10580.8</td><td style=\"text-align: right;\">409000</td><td style=\"text-align: right;\">    7.15</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            300.12</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 410000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_18-07-31\n",
      "  done: false\n",
      "  episode_len_mean: 290.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 7.18\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1072\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5084158658981324\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012258325050510482\n",
      "          policy_loss: -0.12783834147784445\n",
      "          total_loss: 0.015682006627321242\n",
      "          vf_explained_var: 0.9588323831558228\n",
      "          vf_loss: 0.15301300171348783\n",
      "    num_agent_steps_sampled: 410000\n",
      "    num_agent_steps_trained: 410000\n",
      "    num_steps_sampled: 410000\n",
      "    num_steps_trained: 410000\n",
      "  iterations_since_restore: 410\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.374193548387094\n",
      "    ram_util_percent: 60.64623655913981\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03832510708638136\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.72317623749642\n",
      "    mean_inference_ms: 1.567587864494958\n",
      "    mean_raw_obs_processing_ms: 3.505174569549827\n",
      "  time_since_restore: 10646.21719789505\n",
      "  time_this_iter_s: 65.38193893432617\n",
      "  time_total_s: 10646.21719789505\n",
      "  timers:\n",
      "    learn_throughput: 964.816\n",
      "    learn_time_ms: 1036.467\n",
      "    load_throughput: 56425.943\n",
      "    load_time_ms: 17.722\n",
      "    sample_throughput: 13.296\n",
      "    sample_time_ms: 75209.457\n",
      "    update_time_ms: 2.229\n",
      "  timestamp: 1633716451\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 410000\n",
      "  training_iteration: 410\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   410</td><td style=\"text-align: right;\">         10646.2</td><td style=\"text-align: right;\">410000</td><td style=\"text-align: right;\">    7.18</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            290.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 411000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_18-09-54\n",
      "  done: false\n",
      "  episode_len_mean: 270.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 7.37\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1081\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6374191019270155\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013076143478521414\n",
      "          policy_loss: -0.1034759828613864\n",
      "          total_loss: 0.03883657331267993\n",
      "          vf_explained_var: 0.9796387553215027\n",
      "          vf_loss: 0.1527222015377548\n",
      "    num_agent_steps_sampled: 411000\n",
      "    num_agent_steps_trained: 411000\n",
      "    num_steps_sampled: 411000\n",
      "    num_steps_trained: 411000\n",
      "  iterations_since_restore: 411\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.249756097560976\n",
      "    ram_util_percent: 60.496585365853676\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03832336328288634\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.723467060454855\n",
      "    mean_inference_ms: 1.5675654124380172\n",
      "    mean_raw_obs_processing_ms: 3.6699639523767495\n",
      "  time_since_restore: 10789.691032409668\n",
      "  time_this_iter_s: 143.47383451461792\n",
      "  time_total_s: 10789.691032409668\n",
      "  timers:\n",
      "    learn_throughput: 965.016\n",
      "    learn_time_ms: 1036.252\n",
      "    load_throughput: 56489.932\n",
      "    load_time_ms: 17.702\n",
      "    sample_throughput: 11.941\n",
      "    sample_time_ms: 83741.653\n",
      "    update_time_ms: 2.2\n",
      "  timestamp: 1633716594\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 411000\n",
      "  training_iteration: 411\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   411</td><td style=\"text-align: right;\">         10789.7</td><td style=\"text-align: right;\">411000</td><td style=\"text-align: right;\">    7.37</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">             270.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 412000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_18-14-24\n",
      "  done: false\n",
      "  episode_len_mean: 227.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 7.72\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1096\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.97809320009417\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01603357505646596\n",
      "          policy_loss: -0.042377182344595594\n",
      "          total_loss: 0.4077900965594583\n",
      "          vf_explained_var: 0.9452282190322876\n",
      "          vf_loss: 0.45263466007179687\n",
      "    num_agent_steps_sampled: 412000\n",
      "    num_agent_steps_trained: 412000\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "  iterations_since_restore: 412\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.272467532467527\n",
      "    ram_util_percent: 60.496623376623376\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03832078795511202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.729483443519236\n",
      "    mean_inference_ms: 1.5675463006501082\n",
      "    mean_raw_obs_processing_ms: 4.024844762155957\n",
      "  time_since_restore: 11059.172191143036\n",
      "  time_this_iter_s: 269.4811587333679\n",
      "  time_total_s: 11059.172191143036\n",
      "  timers:\n",
      "    learn_throughput: 966.009\n",
      "    learn_time_ms: 1035.187\n",
      "    load_throughput: 56380.131\n",
      "    load_time_ms: 17.737\n",
      "    sample_throughput: 9.211\n",
      "    sample_time_ms: 108570.864\n",
      "    update_time_ms: 2.184\n",
      "  timestamp: 1633716864\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 412\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   412</td><td style=\"text-align: right;\">         11059.2</td><td style=\"text-align: right;\">412000</td><td style=\"text-align: right;\">    7.72</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            227.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 413000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_18-16-37\n",
      "  done: false\n",
      "  episode_len_mean: 208.67\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 7.94\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1104\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6270766615867616\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012051023475252625\n",
      "          policy_loss: -0.0072445067266623175\n",
      "          total_loss: 0.12271466520097521\n",
      "          vf_explained_var: 0.8321728110313416\n",
      "          vf_loss: 0.14073299144705137\n",
      "    num_agent_steps_sampled: 413000\n",
      "    num_agent_steps_trained: 413000\n",
      "    num_steps_sampled: 413000\n",
      "    num_steps_trained: 413000\n",
      "  iterations_since_restore: 413\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.839682539682542\n",
      "    ram_util_percent: 60.712169312169316\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038319628198266294\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.7336633254467\n",
      "    mean_inference_ms: 1.567539133528471\n",
      "    mean_raw_obs_processing_ms: 4.227643188412708\n",
      "  time_since_restore: 11192.175059556961\n",
      "  time_this_iter_s: 133.00286841392517\n",
      "  time_total_s: 11192.175059556961\n",
      "  timers:\n",
      "    learn_throughput: 967.156\n",
      "    learn_time_ms: 1033.959\n",
      "    load_throughput: 56673.58\n",
      "    load_time_ms: 17.645\n",
      "    sample_throughput: 8.627\n",
      "    sample_time_ms: 115921.625\n",
      "    update_time_ms: 2.181\n",
      "  timestamp: 1633716997\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 413000\n",
      "  training_iteration: 413\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   413</td><td style=\"text-align: right;\">         11192.2</td><td style=\"text-align: right;\">413000</td><td style=\"text-align: right;\">    7.94</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            208.67</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 414000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_18-19-15\n",
      "  done: false\n",
      "  episode_len_mean: 196.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 8.02\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1112\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7260015938017104\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013951566935012849\n",
      "          policy_loss: -0.014896022031704585\n",
      "          total_loss: 0.15067578090561762\n",
      "          vf_explained_var: 0.673677384853363\n",
      "          vf_loss: 0.17646796007951102\n",
      "    num_agent_steps_sampled: 414000\n",
      "    num_agent_steps_trained: 414000\n",
      "    num_steps_sampled: 414000\n",
      "    num_steps_trained: 414000\n",
      "  iterations_since_restore: 414\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.010176991150438\n",
      "    ram_util_percent: 60.74955752212389\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03831865844275266\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.739030755413747\n",
      "    mean_inference_ms: 1.5675357703169672\n",
      "    mean_raw_obs_processing_ms: 4.44144857256544\n",
      "  time_since_restore: 11350.520008802414\n",
      "  time_this_iter_s: 158.34494924545288\n",
      "  time_total_s: 11350.520008802414\n",
      "  timers:\n",
      "    learn_throughput: 967.233\n",
      "    learn_time_ms: 1033.877\n",
      "    load_throughput: 54716.068\n",
      "    load_time_ms: 18.276\n",
      "    sample_throughput: 7.692\n",
      "    sample_time_ms: 130004.896\n",
      "    update_time_ms: 2.182\n",
      "  timestamp: 1633717155\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 414000\n",
      "  training_iteration: 414\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   414</td><td style=\"text-align: right;\">         11350.5</td><td style=\"text-align: right;\">414000</td><td style=\"text-align: right;\">    8.02</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            196.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 415000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_18-21-30\n",
      "  done: false\n",
      "  episode_len_mean: 178.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 8.3\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 7\n",
      "  episodes_total: 1119\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6054788178867765\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014003478227576052\n",
      "          policy_loss: -0.09974916825691858\n",
      "          total_loss: 0.24106390265127023\n",
      "          vf_explained_var: 0.838350236415863\n",
      "          vf_loss: 0.3504803203874164\n",
      "    num_agent_steps_sampled: 415000\n",
      "    num_agent_steps_trained: 415000\n",
      "    num_steps_sampled: 415000\n",
      "    num_steps_trained: 415000\n",
      "  iterations_since_restore: 415\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.73523316062176\n",
      "    ram_util_percent: 60.85906735751295\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03831783464075318\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.744347140592883\n",
      "    mean_inference_ms: 1.5675355112715568\n",
      "    mean_raw_obs_processing_ms: 4.640586491963352\n",
      "  time_since_restore: 11485.63008260727\n",
      "  time_this_iter_s: 135.11007380485535\n",
      "  time_total_s: 11485.63008260727\n",
      "  timers:\n",
      "    learn_throughput: 965.587\n",
      "    learn_time_ms: 1035.639\n",
      "    load_throughput: 54552.388\n",
      "    load_time_ms: 18.331\n",
      "    sample_throughput: 7.308\n",
      "    sample_time_ms: 136842.799\n",
      "    update_time_ms: 2.178\n",
      "  timestamp: 1633717290\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 415000\n",
      "  training_iteration: 415\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   415</td><td style=\"text-align: right;\">         11485.6</td><td style=\"text-align: right;\">415000</td><td style=\"text-align: right;\">     8.3</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            178.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_18-24-37\n",
      "  done: false\n",
      "  episode_len_mean: 146.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 8.51\n",
      "  episode_reward_min: -5.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 1131\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5190176321400537\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010091227193148929\n",
      "          policy_loss: -0.0010486279096868303\n",
      "          total_loss: 0.19333688418070474\n",
      "          vf_explained_var: 0.8887879252433777\n",
      "          vf_loss: 0.20497267916798592\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_agent_steps_trained: 416000\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "  iterations_since_restore: 416\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.852434456928837\n",
      "    ram_util_percent: 60.89063670411985\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038316842550953537\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.75676177699991\n",
      "    mean_inference_ms: 1.5675494198953153\n",
      "    mean_raw_obs_processing_ms: 5.025571606502026\n",
      "  time_since_restore: 11672.530667304993\n",
      "  time_this_iter_s: 186.9005846977234\n",
      "  time_total_s: 11672.530667304993\n",
      "  timers:\n",
      "    learn_throughput: 965.884\n",
      "    learn_time_ms: 1035.321\n",
      "    load_throughput: 54676.838\n",
      "    load_time_ms: 18.289\n",
      "    sample_throughput: 6.949\n",
      "    sample_time_ms: 143914.998\n",
      "    update_time_ms: 2.201\n",
      "  timestamp: 1633717477\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 416\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   416</td><td style=\"text-align: right;\">         11672.5</td><td style=\"text-align: right;\">416000</td><td style=\"text-align: right;\">    8.51</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -5</td><td style=\"text-align: right;\">            146.01</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 417000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_18-25-38\n",
      "  done: false\n",
      "  episode_len_mean: 143.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 8.66\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1135\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7393817252582975\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01635850087743313\n",
      "          policy_loss: -0.04871884356770251\n",
      "          total_loss: 0.10461008274513814\n",
      "          vf_explained_var: 0.6193366050720215\n",
      "          vf_loss: 0.1632609889532129\n",
      "    num_agent_steps_sampled: 417000\n",
      "    num_agent_steps_trained: 417000\n",
      "    num_steps_sampled: 417000\n",
      "    num_steps_trained: 417000\n",
      "  iterations_since_restore: 417\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.88390804597701\n",
      "    ram_util_percent: 61.04137931034485\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03831654429585774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.76120722579152\n",
      "    mean_inference_ms: 1.5675567536926596\n",
      "    mean_raw_obs_processing_ms: 5.152717895986997\n",
      "  time_since_restore: 11733.332783937454\n",
      "  time_this_iter_s: 60.80211663246155\n",
      "  time_total_s: 11733.332783937454\n",
      "  timers:\n",
      "    learn_throughput: 964.733\n",
      "    learn_time_ms: 1036.556\n",
      "    load_throughput: 54564.027\n",
      "    load_time_ms: 18.327\n",
      "    sample_throughput: 7.353\n",
      "    sample_time_ms: 135990.189\n",
      "    update_time_ms: 2.186\n",
      "  timestamp: 1633717538\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 417000\n",
      "  training_iteration: 417\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   417</td><td style=\"text-align: right;\">         11733.3</td><td style=\"text-align: right;\">417000</td><td style=\"text-align: right;\">    8.66</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            143.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 418000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_18-28-28\n",
      "  done: false\n",
      "  episode_len_mean: 121.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 8.91\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1144\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.312792248196072\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007812925585737516\n",
      "          policy_loss: -0.09402027461263869\n",
      "          total_loss: 0.06197232630931669\n",
      "          vf_explained_var: 0.9660365581512451\n",
      "          vf_loss: 0.16555673778057098\n",
      "    num_agent_steps_sampled: 418000\n",
      "    num_agent_steps_trained: 418000\n",
      "    num_steps_sampled: 418000\n",
      "    num_steps_trained: 418000\n",
      "  iterations_since_restore: 418\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.090082644628097\n",
      "    ram_util_percent: 60.958264462809915\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038315965604788664\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.77177449162643\n",
      "    mean_inference_ms: 1.5675775528337175\n",
      "    mean_raw_obs_processing_ms: 5.455510341827549\n",
      "  time_since_restore: 11903.275751590729\n",
      "  time_this_iter_s: 169.94296765327454\n",
      "  time_total_s: 11903.275751590729\n",
      "  timers:\n",
      "    learn_throughput: 964.614\n",
      "    learn_time_ms: 1036.684\n",
      "    load_throughput: 54741.777\n",
      "    load_time_ms: 18.268\n",
      "    sample_throughput: 7.412\n",
      "    sample_time_ms: 134911.183\n",
      "    update_time_ms: 2.205\n",
      "  timestamp: 1633717708\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 418000\n",
      "  training_iteration: 418\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   418</td><td style=\"text-align: right;\">         11903.3</td><td style=\"text-align: right;\">418000</td><td style=\"text-align: right;\">    8.91</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">            121.62</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 419000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_18-30-26\n",
      "  done: false\n",
      "  episode_len_mean: 127.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 8.78\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 1152\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.45613946914672854\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.42722622818417\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02201398032508241\n",
      "          policy_loss: -0.19156934916973115\n",
      "          total_loss: 0.24219994693994523\n",
      "          vf_explained_var: 0.6687705516815186\n",
      "          vf_loss: 0.4380001133307815\n",
      "    num_agent_steps_sampled: 419000\n",
      "    num_agent_steps_trained: 419000\n",
      "    num_steps_sampled: 419000\n",
      "    num_steps_trained: 419000\n",
      "  iterations_since_restore: 419\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.295266272189348\n",
      "    ram_util_percent: 61.09230769230767\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0383155296163741\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.78086927401314\n",
      "    mean_inference_ms: 1.5675959257072214\n",
      "    mean_raw_obs_processing_ms: 5.716016742571683\n",
      "  time_since_restore: 12021.256104707718\n",
      "  time_this_iter_s: 117.98035311698914\n",
      "  time_total_s: 12021.256104707718\n",
      "  timers:\n",
      "    learn_throughput: 963.753\n",
      "    learn_time_ms: 1037.61\n",
      "    load_throughput: 54580.216\n",
      "    load_time_ms: 18.322\n",
      "    sample_throughput: 6.994\n",
      "    sample_time_ms: 142979.293\n",
      "    update_time_ms: 2.194\n",
      "  timestamp: 1633717826\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 419000\n",
      "  training_iteration: 419\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   419</td><td style=\"text-align: right;\">         12021.3</td><td style=\"text-align: right;\">419000</td><td style=\"text-align: right;\">    8.78</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -2</td><td style=\"text-align: right;\">             127.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 420000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_18-31-24\n",
      "  done: false\n",
      "  episode_len_mean: 128.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 8.61\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 1156\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6842092037200928\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5738774140675862\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020080204269755954\n",
      "          policy_loss: -0.07753506956828965\n",
      "          total_loss: 0.346898462706142\n",
      "          vf_explained_var: 0.8598333597183228\n",
      "          vf_loss: 0.42643324923184184\n",
      "    num_agent_steps_sampled: 420000\n",
      "    num_agent_steps_trained: 420000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "  iterations_since_restore: 420\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.745783132530118\n",
      "    ram_util_percent: 61.05060240963855\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038315337998974915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.785386060190348\n",
      "    mean_inference_ms: 1.5676053878668046\n",
      "    mean_raw_obs_processing_ms: 5.839655338030123\n",
      "  time_since_restore: 12079.403468847275\n",
      "  time_this_iter_s: 58.147364139556885\n",
      "  time_total_s: 12079.403468847275\n",
      "  timers:\n",
      "    learn_throughput: 962.153\n",
      "    learn_time_ms: 1039.335\n",
      "    load_throughput: 54624.144\n",
      "    load_time_ms: 18.307\n",
      "    sample_throughput: 7.03\n",
      "    sample_time_ms: 142254.014\n",
      "    update_time_ms: 2.292\n",
      "  timestamp: 1633717884\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 420\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   420</td><td style=\"text-align: right;\">         12079.4</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\">    8.61</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">            128.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 421000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_18-42-13\n",
      "  done: false\n",
      "  episode_len_mean: 94.38\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 8.97\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 35\n",
      "  episodes_total: 1191\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0263138055801395\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.6239207181665632\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014148846537667618\n",
      "          policy_loss: -0.007912888957394493\n",
      "          total_loss: 0.20356609635055065\n",
      "          vf_explained_var: 0.9798527956008911\n",
      "          vf_loss: 0.20319703850481247\n",
      "    num_agent_steps_sampled: 421000\n",
      "    num_agent_steps_trained: 421000\n",
      "    num_steps_sampled: 421000\n",
      "    num_steps_trained: 421000\n",
      "  iterations_since_restore: 421\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.837621621621622\n",
      "    ram_util_percent: 61.11643243243243\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0383139590163639\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.818071372428925\n",
      "    mean_inference_ms: 1.5676899854323316\n",
      "    mean_raw_obs_processing_ms: 7.2010291573729\n",
      "  time_since_restore: 12728.157984733582\n",
      "  time_this_iter_s: 648.7545158863068\n",
      "  time_total_s: 12728.157984733582\n",
      "  timers:\n",
      "    learn_throughput: 960.491\n",
      "    learn_time_ms: 1041.135\n",
      "    load_throughput: 54150.446\n",
      "    load_time_ms: 18.467\n",
      "    sample_throughput: 5.187\n",
      "    sample_time_ms: 192780.091\n",
      "    update_time_ms: 2.283\n",
      "  timestamp: 1633718533\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 421000\n",
      "  training_iteration: 421\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   421</td><td style=\"text-align: right;\">         12728.2</td><td style=\"text-align: right;\">421000</td><td style=\"text-align: right;\">    8.97</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             94.38</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 422000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_18-46-01\n",
      "  done: false\n",
      "  episode_len_mean: 90.11\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 8.96\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1204\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0263138055801395\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2158998754289416\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007909021425826722\n",
      "          policy_loss: -0.1567625596291489\n",
      "          total_loss: 0.12962437296907106\n",
      "          vf_explained_var: 0.6933659911155701\n",
      "          vf_loss: 0.2904287954999341\n",
      "    num_agent_steps_sampled: 422000\n",
      "    num_agent_steps_trained: 422000\n",
      "    num_steps_sampled: 422000\n",
      "    num_steps_trained: 422000\n",
      "  iterations_since_restore: 422\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.63476923076923\n",
      "    ram_util_percent: 61.18523076923077\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03831366611602542\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.828068369552682\n",
      "    mean_inference_ms: 1.5677188529414154\n",
      "    mean_raw_obs_processing_ms: 7.672013167769276\n",
      "  time_since_restore: 12955.760956525803\n",
      "  time_this_iter_s: 227.60297179222107\n",
      "  time_total_s: 12955.760956525803\n",
      "  timers:\n",
      "    learn_throughput: 959.347\n",
      "    learn_time_ms: 1042.375\n",
      "    load_throughput: 54313.548\n",
      "    load_time_ms: 18.412\n",
      "    sample_throughput: 5.302\n",
      "    sample_time_ms: 188591.081\n",
      "    update_time_ms: 2.278\n",
      "  timestamp: 1633718761\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 422000\n",
      "  training_iteration: 422\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   422</td><td style=\"text-align: right;\">         12955.8</td><td style=\"text-align: right;\">422000</td><td style=\"text-align: right;\">    8.96</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             90.11</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 423000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_18-50-44\n",
      "  done: false\n",
      "  episode_len_mean: 83.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.03\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1219\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0263138055801395\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0485549403561487\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008548879522897027\n",
      "          policy_loss: 0.02304066920446025\n",
      "          total_loss: 0.24215527052680652\n",
      "          vf_explained_var: 0.9754531979560852\n",
      "          vf_loss: 0.22082631869448555\n",
      "    num_agent_steps_sampled: 423000\n",
      "    num_agent_steps_trained: 423000\n",
      "    num_steps_sampled: 423000\n",
      "    num_steps_trained: 423000\n",
      "  iterations_since_restore: 423\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.931683168316834\n",
      "    ram_util_percent: 61.16881188118813\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0383132579319534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.83789727518667\n",
      "    mean_inference_ms: 1.567749219892715\n",
      "    mean_raw_obs_processing_ms: 8.226147939301296\n",
      "  time_since_restore: 13238.954407691956\n",
      "  time_this_iter_s: 283.19345116615295\n",
      "  time_total_s: 13238.954407691956\n",
      "  timers:\n",
      "    learn_throughput: 959.116\n",
      "    learn_time_ms: 1042.626\n",
      "    load_throughput: 53951.583\n",
      "    load_time_ms: 18.535\n",
      "    sample_throughput: 4.911\n",
      "    sample_time_ms: 203609.742\n",
      "    update_time_ms: 2.293\n",
      "  timestamp: 1633719044\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 423000\n",
      "  training_iteration: 423\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   423</td><td style=\"text-align: right;\">           13239</td><td style=\"text-align: right;\">423000</td><td style=\"text-align: right;\">    9.03</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             83.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_18-54-32\n",
      "  done: false\n",
      "  episode_len_mean: 76.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.19\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1232\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0263138055801395\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2939164340496063\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007599162316884249\n",
      "          policy_loss: -0.15076049491763116\n",
      "          total_loss: 0.13930291291843686\n",
      "          vf_explained_var: 0.7931714653968811\n",
      "          vf_loss: 0.29520344419611827\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "  iterations_since_restore: 424\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.463384615384616\n",
      "    ram_util_percent: 61.3643076923077\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03831297992521364\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.845894544553786\n",
      "    mean_inference_ms: 1.5677755816595798\n",
      "    mean_raw_obs_processing_ms: 8.698636577754744\n",
      "  time_since_restore: 13466.75554561615\n",
      "  time_this_iter_s: 227.80113792419434\n",
      "  time_total_s: 13466.75554561615\n",
      "  timers:\n",
      "    learn_throughput: 958.274\n",
      "    learn_time_ms: 1043.543\n",
      "    load_throughput: 53877.153\n",
      "    load_time_ms: 18.561\n",
      "    sample_throughput: 4.749\n",
      "    sample_time_ms: 210554.405\n",
      "    update_time_ms: 2.308\n",
      "  timestamp: 1633719272\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 424\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   424</td><td style=\"text-align: right;\">         13466.8</td><td style=\"text-align: right;\">424000</td><td style=\"text-align: right;\">    9.19</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             76.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 425000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_19-03-51\n",
      "  done: false\n",
      "  episode_len_mean: 47.42\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.64\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 30\n",
      "  episodes_total: 1262\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0263138055801395\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9022077613406712\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007942282342230437\n",
      "          policy_loss: 0.03286443005005519\n",
      "          total_loss: 0.2740702026420169\n",
      "          vf_explained_var: 0.9712600111961365\n",
      "          vf_loss: 0.24207658047477404\n",
      "    num_agent_steps_sampled: 425000\n",
      "    num_agent_steps_trained: 425000\n",
      "    num_steps_sampled: 425000\n",
      "    num_steps_trained: 425000\n",
      "  iterations_since_restore: 425\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.99185463659148\n",
      "    ram_util_percent: 61.52230576441103\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03831258438942386\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.8596850553723\n",
      "    mean_inference_ms: 1.5678252700896496\n",
      "    mean_raw_obs_processing_ms: 9.918842269551853\n",
      "  time_since_restore: 14025.97373342514\n",
      "  time_this_iter_s: 559.2181878089905\n",
      "  time_total_s: 14025.97373342514\n",
      "  timers:\n",
      "    learn_throughput: 959.67\n",
      "    learn_time_ms: 1042.025\n",
      "    load_throughput: 53787.885\n",
      "    load_time_ms: 18.592\n",
      "    sample_throughput: 3.953\n",
      "    sample_time_ms: 252966.669\n",
      "    update_time_ms: 2.31\n",
      "  timestamp: 1633719831\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 425000\n",
      "  training_iteration: 425\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   425</td><td style=\"text-align: right;\">           14026</td><td style=\"text-align: right;\">425000</td><td style=\"text-align: right;\">    9.64</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">             47.42</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 426000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_19-16-19\n",
      "  done: false\n",
      "  episode_len_mean: 44.28\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.7\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 39\n",
      "  episodes_total: 1301\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0263138055801395\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5839931428432464\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.002569724062694645\n",
      "          policy_loss: -0.2571816358301375\n",
      "          total_loss: -0.21538022889031305\n",
      "          vf_explained_var: 0.9955976009368896\n",
      "          vf_loss: 0.04500399546490775\n",
      "    num_agent_steps_sampled: 426000\n",
      "    num_agent_steps_trained: 426000\n",
      "    num_steps_sampled: 426000\n",
      "    num_steps_trained: 426000\n",
      "  iterations_since_restore: 426\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.815276476101218\n",
      "    ram_util_percent: 61.62389878163075\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0383120823409485\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.87170048808347\n",
      "    mean_inference_ms: 1.567879600563569\n",
      "    mean_raw_obs_processing_ms: 11.594199867791087\n",
      "  time_since_restore: 14773.550785541534\n",
      "  time_this_iter_s: 747.577052116394\n",
      "  time_total_s: 14773.550785541534\n",
      "  timers:\n",
      "    learn_throughput: 959.442\n",
      "    learn_time_ms: 1042.273\n",
      "    load_throughput: 53444.922\n",
      "    load_time_ms: 18.711\n",
      "    sample_throughput: 3.236\n",
      "    sample_time_ms: 309035.367\n",
      "    update_time_ms: 2.315\n",
      "  timestamp: 1633720579\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 426000\n",
      "  training_iteration: 426\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   426</td><td style=\"text-align: right;\">         14773.6</td><td style=\"text-align: right;\">426000</td><td style=\"text-align: right;\">     9.7</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">             44.28</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 427000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_19-17-58\n",
      "  done: false\n",
      "  episode_len_mean: 42.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.65\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 1307\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.3765932771894667\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012683890716551164\n",
      "          policy_loss: -0.05795346779955758\n",
      "          total_loss: 0.13411048222333194\n",
      "          vf_explained_var: 0.8756049275398254\n",
      "          vf_loss: 0.19932105967568026\n",
      "    num_agent_steps_sampled: 427000\n",
      "    num_agent_steps_trained: 427000\n",
      "    num_steps_sampled: 427000\n",
      "    num_steps_trained: 427000\n",
      "  iterations_since_restore: 427\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.73262411347518\n",
      "    ram_util_percent: 61.75248226950354\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038312027218337645\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.873785355366763\n",
      "    mean_inference_ms: 1.5678876614759034\n",
      "    mean_raw_obs_processing_ms: 11.821969750231688\n",
      "  time_since_restore: 14872.507759332657\n",
      "  time_this_iter_s: 98.95697379112244\n",
      "  time_total_s: 14872.507759332657\n",
      "  timers:\n",
      "    learn_throughput: 961.01\n",
      "    learn_time_ms: 1040.572\n",
      "    load_throughput: 53372.017\n",
      "    load_time_ms: 18.736\n",
      "    sample_throughput: 3.196\n",
      "    sample_time_ms: 312852.536\n",
      "    update_time_ms: 2.322\n",
      "  timestamp: 1633720678\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 427000\n",
      "  training_iteration: 427\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   427</td><td style=\"text-align: right;\">         14872.5</td><td style=\"text-align: right;\">427000</td><td style=\"text-align: right;\">    9.65</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">             42.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 428000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_19-23-55\n",
      "  done: false\n",
      "  episode_len_mean: 46.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.53\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1327\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9795435276296404\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007705521945488848\n",
      "          policy_loss: -0.17828210530181726\n",
      "          total_loss: 0.022443459224369793\n",
      "          vf_explained_var: 0.9093360304832458\n",
      "          vf_loss: 0.206566861561603\n",
      "    num_agent_steps_sampled: 428000\n",
      "    num_agent_steps_trained: 428000\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "  iterations_since_restore: 428\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.597260273972605\n",
      "    ram_util_percent: 62.11409001956946\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0383120349088795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.880109879470886\n",
      "    mean_inference_ms: 1.567915989243231\n",
      "    mean_raw_obs_processing_ms: 12.635964067816532\n",
      "  time_since_restore: 15230.291453123093\n",
      "  time_this_iter_s: 357.7836937904358\n",
      "  time_total_s: 15230.291453123093\n",
      "  timers:\n",
      "    learn_throughput: 960.342\n",
      "    learn_time_ms: 1041.296\n",
      "    load_throughput: 53210.529\n",
      "    load_time_ms: 18.793\n",
      "    sample_throughput: 3.015\n",
      "    sample_time_ms: 331635.834\n",
      "    update_time_ms: 2.306\n",
      "  timestamp: 1633721035\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 428\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   428</td><td style=\"text-align: right;\">         15230.3</td><td style=\"text-align: right;\">428000</td><td style=\"text-align: right;\">    9.53</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">             46.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 429000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_19-28-54\n",
      "  done: false\n",
      "  episode_len_mean: 43.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.6\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 1344\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4936184194352893\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012299113893410605\n",
      "          policy_loss: -0.03510694209900167\n",
      "          total_loss: 0.16483328027857674\n",
      "          vf_explained_var: 0.6049861907958984\n",
      "          vf_loss: 0.20856502912938596\n",
      "    num_agent_steps_sampled: 429000\n",
      "    num_agent_steps_trained: 429000\n",
      "    num_steps_sampled: 429000\n",
      "    num_steps_trained: 429000\n",
      "  iterations_since_restore: 429\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.721361502347417\n",
      "    ram_util_percent: 62.36009389671362\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03831206648375903\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.88360822405345\n",
      "    mean_inference_ms: 1.5679376757801324\n",
      "    mean_raw_obs_processing_ms: 13.237781468688876\n",
      "  time_since_restore: 15528.940378427505\n",
      "  time_this_iter_s: 298.64892530441284\n",
      "  time_total_s: 15528.940378427505\n",
      "  timers:\n",
      "    learn_throughput: 961.722\n",
      "    learn_time_ms: 1039.802\n",
      "    load_throughput: 53671.84\n",
      "    load_time_ms: 18.632\n",
      "    sample_throughput: 2.86\n",
      "    sample_time_ms: 349704.33\n",
      "    update_time_ms: 2.316\n",
      "  timestamp: 1633721334\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 429000\n",
      "  training_iteration: 429\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   429</td><td style=\"text-align: right;\">         15528.9</td><td style=\"text-align: right;\">429000</td><td style=\"text-align: right;\">     9.6</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">              43.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 430000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_19-34-11\n",
      "  done: false\n",
      "  episode_len_mean: 50.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.47\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 1361\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2077673256397248\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016088210803084695\n",
      "          policy_loss: 0.1350923948817783\n",
      "          total_loss: 0.42163475503524145\n",
      "          vf_explained_var: 0.8322502970695496\n",
      "          vf_loss: 0.2903642610543304\n",
      "    num_agent_steps_sampled: 430000\n",
      "    num_agent_steps_trained: 430000\n",
      "    num_steps_sampled: 430000\n",
      "    num_steps_trained: 430000\n",
      "  iterations_since_restore: 430\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.64933628318584\n",
      "    ram_util_percent: 62.28761061946903\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038312120597655965\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.887216147617394\n",
      "    mean_inference_ms: 1.5679650393374898\n",
      "    mean_raw_obs_processing_ms: 13.888290045280113\n",
      "  time_since_restore: 15845.533686876297\n",
      "  time_this_iter_s: 316.5933084487915\n",
      "  time_total_s: 15845.533686876297\n",
      "  timers:\n",
      "    learn_throughput: 963.697\n",
      "    learn_time_ms: 1037.67\n",
      "    load_throughput: 53925.71\n",
      "    load_time_ms: 18.544\n",
      "    sample_throughput: 2.663\n",
      "    sample_time_ms: 375551.256\n",
      "    update_time_ms: 2.214\n",
      "  timestamp: 1633721651\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 430000\n",
      "  training_iteration: 430\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   430</td><td style=\"text-align: right;\">         15845.5</td><td style=\"text-align: right;\">430000</td><td style=\"text-align: right;\">    9.47</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             50.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 431000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_19-39-11\n",
      "  done: false\n",
      "  episode_len_mean: 54.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.45\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 1377\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.338613282971912\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008920884400228759\n",
      "          policy_loss: -0.06542806459797754\n",
      "          total_loss: 0.0031263210707240634\n",
      "          vf_explained_var: 0.5497366189956665\n",
      "          vf_loss: 0.07736270535323354\n",
      "    num_agent_steps_sampled: 431000\n",
      "    num_agent_steps_trained: 431000\n",
      "    num_steps_sampled: 431000\n",
      "    num_steps_trained: 431000\n",
      "  iterations_since_restore: 431\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.603729603729604\n",
      "    ram_util_percent: 62.5062937062937\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03831233073621171\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.890317226422493\n",
      "    mean_inference_ms: 1.5679916400329807\n",
      "    mean_raw_obs_processing_ms: 14.347080521552671\n",
      "  time_since_restore: 16146.002428293228\n",
      "  time_this_iter_s: 300.46874141693115\n",
      "  time_total_s: 16146.002428293228\n",
      "  timers:\n",
      "    learn_throughput: 965.129\n",
      "    learn_time_ms: 1036.13\n",
      "    load_throughput: 54266.747\n",
      "    load_time_ms: 18.427\n",
      "    sample_throughput: 2.935\n",
      "    sample_time_ms: 340724.348\n",
      "    update_time_ms: 2.221\n",
      "  timestamp: 1633721951\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 431000\n",
      "  training_iteration: 431\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   431</td><td style=\"text-align: right;\">           16146</td><td style=\"text-align: right;\">431000</td><td style=\"text-align: right;\">    9.45</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             54.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_19-44-14\n",
      "  done: false\n",
      "  episode_len_mean: 62.07\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.27\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 17\n",
      "  episodes_total: 1394\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.064276385307312\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01952054651418421\n",
      "          policy_loss: -0.04418809053798516\n",
      "          total_loss: 0.3164227633840508\n",
      "          vf_explained_var: 0.9114653468132019\n",
      "          vf_loss: 0.3612365235057142\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_agent_steps_trained: 432000\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "  iterations_since_restore: 432\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.890023201856145\n",
      "    ram_util_percent: 62.54408352668214\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03831272227048622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.89557269528578\n",
      "    mean_inference_ms: 1.5680320449406127\n",
      "    mean_raw_obs_processing_ms: 14.918987604464258\n",
      "  time_since_restore: 16448.34968328476\n",
      "  time_this_iter_s: 302.3472549915314\n",
      "  time_total_s: 16448.34968328476\n",
      "  timers:\n",
      "    learn_throughput: 965.198\n",
      "    learn_time_ms: 1036.057\n",
      "    load_throughput: 54108.532\n",
      "    load_time_ms: 18.481\n",
      "    sample_throughput: 2.872\n",
      "    sample_time_ms: 348198.772\n",
      "    update_time_ms: 2.255\n",
      "  timestamp: 1633722254\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 432\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   432</td><td style=\"text-align: right;\">         16448.3</td><td style=\"text-align: right;\">432000</td><td style=\"text-align: right;\">    9.27</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             62.07</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 433000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_19-48-37\n",
      "  done: false\n",
      "  episode_len_mean: 58.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.3\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1409\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0634526332219443\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012966667370698499\n",
      "          policy_loss: -0.11170579906966951\n",
      "          total_loss: 0.16364508424368168\n",
      "          vf_explained_var: 0.6871446967124939\n",
      "          vf_loss: 0.2793314728471968\n",
      "    num_agent_steps_sampled: 433000\n",
      "    num_agent_steps_trained: 433000\n",
      "    num_steps_sampled: 433000\n",
      "    num_steps_trained: 433000\n",
      "  iterations_since_restore: 433\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.0436170212766\n",
      "    ram_util_percent: 62.48962765957447\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03831302751765325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.90033142664687\n",
      "    mean_inference_ms: 1.5680730182571994\n",
      "    mean_raw_obs_processing_ms: 15.474324576749327\n",
      "  time_since_restore: 16711.496958971024\n",
      "  time_this_iter_s: 263.14727568626404\n",
      "  time_total_s: 16711.496958971024\n",
      "  timers:\n",
      "    learn_throughput: 965.835\n",
      "    learn_time_ms: 1035.374\n",
      "    load_throughput: 54089.134\n",
      "    load_time_ms: 18.488\n",
      "    sample_throughput: 2.889\n",
      "    sample_time_ms: 346194.831\n",
      "    update_time_ms: 2.258\n",
      "  timestamp: 1633722517\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 433000\n",
      "  training_iteration: 433\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   433</td><td style=\"text-align: right;\">         16711.5</td><td style=\"text-align: right;\">433000</td><td style=\"text-align: right;\">     9.3</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">             58.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 434000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_19-51-26\n",
      "  done: false\n",
      "  episode_len_mean: 67.23\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.13\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1419\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.623931849002838\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01984209667038886\n",
      "          policy_loss: 0.06369750963317024\n",
      "          total_loss: 0.33667141223947206\n",
      "          vf_explained_var: 0.6073736548423767\n",
      "          vf_loss: 0.27903111336959735\n",
      "    num_agent_steps_sampled: 434000\n",
      "    num_agent_steps_trained: 434000\n",
      "    num_steps_sampled: 434000\n",
      "    num_steps_trained: 434000\n",
      "  iterations_since_restore: 434\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.9206611570248\n",
      "    ram_util_percent: 62.615289256198366\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03831319871191367\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.902550852824486\n",
      "    mean_inference_ms: 1.5681008931009242\n",
      "    mean_raw_obs_processing_ms: 15.803266591983993\n",
      "  time_since_restore: 16881.089654684067\n",
      "  time_this_iter_s: 169.5926957130432\n",
      "  time_total_s: 16881.089654684067\n",
      "  timers:\n",
      "    learn_throughput: 965.177\n",
      "    learn_time_ms: 1036.079\n",
      "    load_throughput: 54265.553\n",
      "    load_time_ms: 18.428\n",
      "    sample_throughput: 2.938\n",
      "    sample_time_ms: 340373.349\n",
      "    update_time_ms: 2.254\n",
      "  timestamp: 1633722686\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 434000\n",
      "  training_iteration: 434\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   434</td><td style=\"text-align: right;\">         16881.1</td><td style=\"text-align: right;\">434000</td><td style=\"text-align: right;\">    9.13</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">             67.23</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 435000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_19-57-31\n",
      "  done: false\n",
      "  episode_len_mean: 59.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.23\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 19\n",
      "  episodes_total: 1438\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9897212074862586\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013874697762891191\n",
      "          policy_loss: -0.207474684715271\n",
      "          total_loss: 0.33190356170137725\n",
      "          vf_explained_var: 0.9211868047714233\n",
      "          vf_loss: 0.5421555642452505\n",
      "    num_agent_steps_sampled: 435000\n",
      "    num_agent_steps_trained: 435000\n",
      "    num_steps_sampled: 435000\n",
      "    num_steps_trained: 435000\n",
      "  iterations_since_restore: 435\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.755\n",
      "    ram_util_percent: 62.60134615384616\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03831367983161126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.909238371420468\n",
      "    mean_inference_ms: 1.5681627413777202\n",
      "    mean_raw_obs_processing_ms: 16.500406294993393\n",
      "  time_since_restore: 17245.27533555031\n",
      "  time_this_iter_s: 364.18568086624146\n",
      "  time_total_s: 17245.27533555031\n",
      "  timers:\n",
      "    learn_throughput: 964.866\n",
      "    learn_time_ms: 1036.413\n",
      "    load_throughput: 54223.671\n",
      "    load_time_ms: 18.442\n",
      "    sample_throughput: 3.117\n",
      "    sample_time_ms: 320869.78\n",
      "    update_time_ms: 2.248\n",
      "  timestamp: 1633723051\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 435000\n",
      "  training_iteration: 435\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   435</td><td style=\"text-align: right;\">         17245.3</td><td style=\"text-align: right;\">435000</td><td style=\"text-align: right;\">    9.23</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">             59.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 436000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_20-01-39\n",
      "  done: false\n",
      "  episode_len_mean: 64.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.09\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1453\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0730046742492252\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0124027967771422\n",
      "          policy_loss: 0.06726069707009527\n",
      "          total_loss: 0.5750918790284131\n",
      "          vf_explained_var: 0.940179705619812\n",
      "          vf_loss: 0.512196656399303\n",
      "    num_agent_steps_sampled: 436000\n",
      "    num_agent_steps_trained: 436000\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "  iterations_since_restore: 436\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.312429378531068\n",
      "    ram_util_percent: 63.092090395480234\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038314153499312546\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.916662926384834\n",
      "    mean_inference_ms: 1.5682140164998015\n",
      "    mean_raw_obs_processing_ms: 17.02301379187885\n",
      "  time_since_restore: 17493.62119793892\n",
      "  time_this_iter_s: 248.34586238861084\n",
      "  time_total_s: 17493.62119793892\n",
      "  timers:\n",
      "    learn_throughput: 964.792\n",
      "    learn_time_ms: 1036.492\n",
      "    load_throughput: 54326.07\n",
      "    load_time_ms: 18.407\n",
      "    sample_throughput: 3.691\n",
      "    sample_time_ms: 270946.66\n",
      "    update_time_ms: 2.232\n",
      "  timestamp: 1633723299\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 436\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   436</td><td style=\"text-align: right;\">         17493.6</td><td style=\"text-align: right;\">436000</td><td style=\"text-align: right;\">    9.09</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">             64.57</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 437000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_20-10-42\n",
      "  done: false\n",
      "  episode_len_mean: 55.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.18\n",
      "  episode_reward_min: -4.0\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 1482\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7820501377185186\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008117704721526191\n",
      "          policy_loss: 0.012277906388044357\n",
      "          total_loss: 0.18182885779274835\n",
      "          vf_explained_var: 0.5608119368553162\n",
      "          vf_loss: 0.1732057976639933\n",
      "    num_agent_steps_sampled: 437000\n",
      "    num_agent_steps_trained: 437000\n",
      "    num_steps_sampled: 437000\n",
      "    num_steps_trained: 437000\n",
      "  iterations_since_restore: 437\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.153806451612905\n",
      "    ram_util_percent: 63.06954838709679\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03831496476727095\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.932129872425335\n",
      "    mean_inference_ms: 1.5683134803144565\n",
      "    mean_raw_obs_processing_ms: 18.131281597716594\n",
      "  time_since_restore: 18036.69264984131\n",
      "  time_this_iter_s: 543.0714519023895\n",
      "  time_total_s: 18036.69264984131\n",
      "  timers:\n",
      "    learn_throughput: 963.779\n",
      "    learn_time_ms: 1037.582\n",
      "    load_throughput: 54537.705\n",
      "    load_time_ms: 18.336\n",
      "    sample_throughput: 3.171\n",
      "    sample_time_ms: 315357.055\n",
      "    update_time_ms: 2.229\n",
      "  timestamp: 1633723842\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 437000\n",
      "  training_iteration: 437\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   437</td><td style=\"text-align: right;\">         18036.7</td><td style=\"text-align: right;\">437000</td><td style=\"text-align: right;\">    9.18</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -4</td><td style=\"text-align: right;\">             55.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 438000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_20-18-49\n",
      "  done: false\n",
      "  episode_len_mean: 50.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.41\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 1508\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0839086754454508\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016315550959625055\n",
      "          policy_loss: -0.03561833699544271\n",
      "          total_loss: 0.3592434090148244\n",
      "          vf_explained_var: 0.5805529952049255\n",
      "          vf_loss: 0.3973283891048696\n",
      "    num_agent_steps_sampled: 438000\n",
      "    num_agent_steps_trained: 438000\n",
      "    num_steps_sampled: 438000\n",
      "    num_steps_trained: 438000\n",
      "  iterations_since_restore: 438\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.094827586206897\n",
      "    ram_util_percent: 63.27859195402298\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03831555947983379\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.943226353353253\n",
      "    mean_inference_ms: 1.5683930779600461\n",
      "    mean_raw_obs_processing_ms: 19.143961059605328\n",
      "  time_since_restore: 18524.164414167404\n",
      "  time_this_iter_s: 487.4717643260956\n",
      "  time_total_s: 18524.164414167404\n",
      "  timers:\n",
      "    learn_throughput: 964.116\n",
      "    learn_time_ms: 1037.22\n",
      "    load_throughput: 54258.253\n",
      "    load_time_ms: 18.43\n",
      "    sample_throughput: 3.046\n",
      "    sample_time_ms: 328326.131\n",
      "    update_time_ms: 2.233\n",
      "  timestamp: 1633724329\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 438000\n",
      "  training_iteration: 438\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   438</td><td style=\"text-align: right;\">         18524.2</td><td style=\"text-align: right;\">438000</td><td style=\"text-align: right;\">    9.41</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">             50.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 439000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_20-27-33\n",
      "  done: false\n",
      "  episode_len_mean: 43.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.57\n",
      "  episode_reward_min: -3.0\n",
      "  episodes_this_iter: 28\n",
      "  episodes_total: 1536\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.0949523667494456\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01588294696402204\n",
      "          policy_loss: 0.024060288319985072\n",
      "          total_loss: 0.2811321049928665\n",
      "          vf_explained_var: 0.7413131594657898\n",
      "          vf_loss: 0.2598708973990546\n",
      "    num_agent_steps_sampled: 439000\n",
      "    num_agent_steps_trained: 439000\n",
      "    num_steps_sampled: 439000\n",
      "    num_steps_trained: 439000\n",
      "  iterations_since_restore: 439\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.168808567603747\n",
      "    ram_util_percent: 63.37831325301205\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03831625049629618\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.9539788388914\n",
      "    mean_inference_ms: 1.5684693143557535\n",
      "    mean_raw_obs_processing_ms: 20.266959735697355\n",
      "  time_since_restore: 19047.805407762527\n",
      "  time_this_iter_s: 523.6409935951233\n",
      "  time_total_s: 19047.805407762527\n",
      "  timers:\n",
      "    learn_throughput: 964.707\n",
      "    learn_time_ms: 1036.585\n",
      "    load_throughput: 53864.975\n",
      "    load_time_ms: 18.565\n",
      "    sample_throughput: 2.85\n",
      "    sample_time_ms: 350825.82\n",
      "    update_time_ms: 2.23\n",
      "  timestamp: 1633724853\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 439000\n",
      "  training_iteration: 439\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   439</td><td style=\"text-align: right;\">         19047.8</td><td style=\"text-align: right;\">439000</td><td style=\"text-align: right;\">    9.57</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -3</td><td style=\"text-align: right;\">                43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_20-31-10\n",
      "  done: false\n",
      "  episode_len_mean: 43.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.54\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_total: 1549\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8330875822239452\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008763198590859705\n",
      "          policy_loss: -0.05022023994889524\n",
      "          total_loss: 0.33526170137855743\n",
      "          vf_explained_var: 0.9590905904769897\n",
      "          vf_loss: 0.38931591196192633\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_agent_steps_trained: 440000\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "  iterations_since_restore: 440\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.469032258064516\n",
      "    ram_util_percent: 63.592580645161306\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03831662631903125\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.96046382822502\n",
      "    mean_inference_ms: 1.5685052494730514\n",
      "    mean_raw_obs_processing_ms: 20.745987988800586\n",
      "  time_since_restore: 19265.095419168472\n",
      "  time_this_iter_s: 217.29001140594482\n",
      "  time_total_s: 19265.095419168472\n",
      "  timers:\n",
      "    learn_throughput: 962.61\n",
      "    learn_time_ms: 1038.842\n",
      "    load_throughput: 53733.999\n",
      "    load_time_ms: 18.61\n",
      "    sample_throughput: 2.933\n",
      "    sample_time_ms: 340893.171\n",
      "    update_time_ms: 2.247\n",
      "  timestamp: 1633725070\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 440\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   440</td><td style=\"text-align: right;\">         19265.1</td><td style=\"text-align: right;\">440000</td><td style=\"text-align: right;\">    9.54</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             43.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 441000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_20-34-02\n",
      "  done: false\n",
      "  episode_len_mean: 43.94\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.5\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_total: 1558\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.2346592638227674\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.018221831455976716\n",
      "          policy_loss: -0.054160619030396144\n",
      "          total_loss: 0.42375589782993\n",
      "          vf_explained_var: 0.4349954426288605\n",
      "          vf_loss: 0.48091244250535964\n",
      "    num_agent_steps_sampled: 441000\n",
      "    num_agent_steps_trained: 441000\n",
      "    num_steps_sampled: 441000\n",
      "    num_steps_trained: 441000\n",
      "  iterations_since_restore: 441\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.24081632653061\n",
      "    ram_util_percent: 63.683673469387756\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03831696169225841\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.965038161719647\n",
      "    mean_inference_ms: 1.5685315970578575\n",
      "    mean_raw_obs_processing_ms: 21.039391993476798\n",
      "  time_since_restore: 19436.425635576248\n",
      "  time_this_iter_s: 171.33021640777588\n",
      "  time_total_s: 19436.425635576248\n",
      "  timers:\n",
      "    learn_throughput: 960.92\n",
      "    learn_time_ms: 1040.669\n",
      "    load_throughput: 53792.783\n",
      "    load_time_ms: 18.59\n",
      "    sample_throughput: 3.049\n",
      "    sample_time_ms: 327977.517\n",
      "    update_time_ms: 2.247\n",
      "  timestamp: 1633725242\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 441000\n",
      "  training_iteration: 441\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   441</td><td style=\"text-align: right;\">         19436.4</td><td style=\"text-align: right;\">441000</td><td style=\"text-align: right;\">     9.5</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             43.94</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 442000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_20-44-40\n",
      "  done: false\n",
      "  episode_len_mean: 41.21\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.54\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 34\n",
      "  episodes_total: 1592\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5131569027900698\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.4782712426450517\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0032250333323269862\n",
      "          policy_loss: -0.17364733583397335\n",
      "          total_loss: 0.035012715227074095\n",
      "          vf_explained_var: 0.9825040102005005\n",
      "          vf_loss: 0.21178781458487114\n",
      "    num_agent_steps_sampled: 442000\n",
      "    num_agent_steps_trained: 442000\n",
      "    num_steps_sampled: 442000\n",
      "    num_steps_trained: 442000\n",
      "  iterations_since_restore: 442\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.938748627881452\n",
      "    ram_util_percent: 63.47870472008782\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038318534620527804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.98272107650978\n",
      "    mean_inference_ms: 1.5686424881617642\n",
      "    mean_raw_obs_processing_ms: 22.32884079187279\n",
      "  time_since_restore: 20074.897300958633\n",
      "  time_this_iter_s: 638.4716653823853\n",
      "  time_total_s: 20074.897300958633\n",
      "  timers:\n",
      "    learn_throughput: 961.473\n",
      "    learn_time_ms: 1040.071\n",
      "    load_throughput: 53494.136\n",
      "    load_time_ms: 18.694\n",
      "    sample_throughput: 2.766\n",
      "    sample_time_ms: 361590.441\n",
      "    update_time_ms: 2.222\n",
      "  timestamp: 1633725880\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 442000\n",
      "  training_iteration: 442\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   442</td><td style=\"text-align: right;\">         20074.9</td><td style=\"text-align: right;\">442000</td><td style=\"text-align: right;\">    9.54</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             41.21</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 443000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_20-55-50\n",
      "  done: false\n",
      "  episode_len_mean: 41.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.59\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 36\n",
      "  episodes_total: 1628\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2565784513950349\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.7005860752529568\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013543921584192967\n",
      "          policy_loss: -0.05791283133957121\n",
      "          total_loss: 0.02394189462065697\n",
      "          vf_explained_var: 0.9935194849967957\n",
      "          vf_loss: 0.08538550262649854\n",
      "    num_agent_steps_sampled: 443000\n",
      "    num_agent_steps_trained: 443000\n",
      "    num_steps_sampled: 443000\n",
      "    num_steps_trained: 443000\n",
      "  iterations_since_restore: 443\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.781066945606693\n",
      "    ram_util_percent: 63.73744769874475\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038320613395315645\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.00016524677182\n",
      "    mean_inference_ms: 1.5687630577971095\n",
      "    mean_raw_obs_processing_ms: 23.72476337785062\n",
      "  time_since_restore: 20744.842297315598\n",
      "  time_this_iter_s: 669.9449963569641\n",
      "  time_total_s: 20744.842297315598\n",
      "  timers:\n",
      "    learn_throughput: 960.508\n",
      "    learn_time_ms: 1041.116\n",
      "    load_throughput: 54114.396\n",
      "    load_time_ms: 18.479\n",
      "    sample_throughput: 2.486\n",
      "    sample_time_ms: 402269.361\n",
      "    update_time_ms: 2.216\n",
      "  timestamp: 1633726550\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 443000\n",
      "  training_iteration: 443\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   443</td><td style=\"text-align: right;\">         20744.8</td><td style=\"text-align: right;\">443000</td><td style=\"text-align: right;\">    9.59</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             41.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 444000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_21-11-43\n",
      "  done: false\n",
      "  episode_len_mean: 25.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.95\n",
      "  episode_reward_min: 5.0\n",
      "  episodes_this_iter: 50\n",
      "  episodes_total: 1678\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2565784513950349\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.40432808101177214\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0072814849158108035\n",
      "          policy_loss: -0.05465913189368116\n",
      "          total_loss: -0.03705448324067725\n",
      "          vf_explained_var: 0.997933030128479\n",
      "          vf_loss: 0.019779654861324363\n",
      "    num_agent_steps_sampled: 444000\n",
      "    num_agent_steps_trained: 444000\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "  iterations_since_restore: 444\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.610669610007356\n",
      "    ram_util_percent: 63.88020603384842\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03832332901171104\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.0086696113348\n",
      "    mean_inference_ms: 1.5689089777636456\n",
      "    mean_raw_obs_processing_ms: 25.936597253700555\n",
      "  time_since_restore: 21697.23351764679\n",
      "  time_this_iter_s: 952.391220331192\n",
      "  time_total_s: 21697.23351764679\n",
      "  timers:\n",
      "    learn_throughput: 961.852\n",
      "    learn_time_ms: 1039.661\n",
      "    load_throughput: 54814.749\n",
      "    load_time_ms: 18.243\n",
      "    sample_throughput: 2.081\n",
      "    sample_time_ms: 480550.867\n",
      "    update_time_ms: 2.218\n",
      "  timestamp: 1633727503\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 444\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   444</td><td style=\"text-align: right;\">         21697.2</td><td style=\"text-align: right;\">444000</td><td style=\"text-align: right;\">    9.95</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">              25.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 445000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_21-18-58\n",
      "  done: false\n",
      "  episode_len_mean: 24.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.95\n",
      "  episode_reward_min: 5.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 1701\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2565784513950349\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9529898948139615\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012488734223232158\n",
      "          policy_loss: -0.04593011596136623\n",
      "          total_loss: 0.18107065856456755\n",
      "          vf_explained_var: 0.8822134137153625\n",
      "          vf_loss: 0.2333263285872009\n",
      "    num_agent_steps_sampled: 445000\n",
      "    num_agent_steps_trained: 445000\n",
      "    num_steps_sampled: 445000\n",
      "    num_steps_trained: 445000\n",
      "  iterations_since_restore: 445\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.349597423510467\n",
      "    ram_util_percent: 64.24766505636072\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038324088540981883\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.01125340572761\n",
      "    mean_inference_ms: 1.5689735003120076\n",
      "    mean_raw_obs_processing_ms: 26.800133954563947\n",
      "  time_since_restore: 22132.47774195671\n",
      "  time_this_iter_s: 435.24422430992126\n",
      "  time_total_s: 22132.47774195671\n",
      "  timers:\n",
      "    learn_throughput: 962.019\n",
      "    learn_time_ms: 1039.48\n",
      "    load_throughput: 54678.121\n",
      "    load_time_ms: 18.289\n",
      "    sample_throughput: 2.051\n",
      "    sample_time_ms: 487656.834\n",
      "    update_time_ms: 2.239\n",
      "  timestamp: 1633727938\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 445000\n",
      "  training_iteration: 445\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   445</td><td style=\"text-align: right;\">         22132.5</td><td style=\"text-align: right;\">445000</td><td style=\"text-align: right;\">    9.95</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">              24.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 446000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_21-22-07\n",
      "  done: false\n",
      "  episode_len_mean: 33.5\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.87\n",
      "  episode_reward_min: 5.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_total: 1712\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2565784513950349\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.261567813820309\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01938983019369432\n",
      "          policy_loss: -0.057127623053060635\n",
      "          total_loss: 0.17456127719746695\n",
      "          vf_explained_var: 0.878809928894043\n",
      "          vf_loss: 0.23932956515087023\n",
      "    num_agent_steps_sampled: 446000\n",
      "    num_agent_steps_trained: 446000\n",
      "    num_steps_sampled: 446000\n",
      "    num_steps_trained: 446000\n",
      "  iterations_since_restore: 446\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.591449814126396\n",
      "    ram_util_percent: 64.51152416356877\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03832453549456117\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.012966967981882\n",
      "    mean_inference_ms: 1.5690059996135879\n",
      "    mean_raw_obs_processing_ms: 27.152510715439245\n",
      "  time_since_restore: 22321.09996533394\n",
      "  time_this_iter_s: 188.62222337722778\n",
      "  time_total_s: 22321.09996533394\n",
      "  timers:\n",
      "    learn_throughput: 961.182\n",
      "    learn_time_ms: 1040.385\n",
      "    load_throughput: 54966.392\n",
      "    load_time_ms: 18.193\n",
      "    sample_throughput: 2.076\n",
      "    sample_time_ms: 481683.694\n",
      "    update_time_ms: 2.226\n",
      "  timestamp: 1633728127\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 446000\n",
      "  training_iteration: 446\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   446</td><td style=\"text-align: right;\">         22321.1</td><td style=\"text-align: right;\">446000</td><td style=\"text-align: right;\">    9.87</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   5</td><td style=\"text-align: right;\">              33.5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 447000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_21-32-06\n",
      "  done: false\n",
      "  episode_len_mean: 36.82\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.77\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 32\n",
      "  episodes_total: 1744\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2565784513950349\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8187974459595151\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.019272237508322996\n",
      "          policy_loss: -0.03200320394502746\n",
      "          total_loss: 0.419122522086319\n",
      "          vf_explained_var: 0.9256473779678345\n",
      "          vf_loss: 0.454368868470192\n",
      "    num_agent_steps_sampled: 447000\n",
      "    num_agent_steps_trained: 447000\n",
      "    num_steps_sampled: 447000\n",
      "    num_steps_trained: 447000\n",
      "  iterations_since_restore: 447\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.976023391812866\n",
      "    ram_util_percent: 64.6825730994152\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03832603129714224\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.020916426029085\n",
      "    mean_inference_ms: 1.5691166372915841\n",
      "    mean_raw_obs_processing_ms: 28.241425023096898\n",
      "  time_since_restore: 22920.30270934105\n",
      "  time_this_iter_s: 599.2027440071106\n",
      "  time_total_s: 22920.30270934105\n",
      "  timers:\n",
      "    learn_throughput: 962.027\n",
      "    learn_time_ms: 1039.471\n",
      "    load_throughput: 54532.883\n",
      "    load_time_ms: 18.338\n",
      "    sample_throughput: 2.052\n",
      "    sample_time_ms: 487297.592\n",
      "    update_time_ms: 2.225\n",
      "  timestamp: 1633728726\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 447000\n",
      "  training_iteration: 447\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   447</td><td style=\"text-align: right;\">         22920.3</td><td style=\"text-align: right;\">447000</td><td style=\"text-align: right;\">    9.77</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             36.82</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_21-45-12\n",
      "  done: false\n",
      "  episode_len_mean: 37.44\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.77\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 41\n",
      "  episodes_total: 1785\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2565784513950349\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.501581965221299\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003353590960274487\n",
      "          policy_loss: -0.1451002188026905\n",
      "          total_loss: -0.12742593155966864\n",
      "          vf_explained_var: 0.998026967048645\n",
      "          vf_loss: 0.021829647053447036\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_agent_steps_trained: 448000\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "  iterations_since_restore: 448\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.80677966101695\n",
      "    ram_util_percent: 64.5264049955397\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03832792384539911\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.03257352612297\n",
      "    mean_inference_ms: 1.5692698712423947\n",
      "    mean_raw_obs_processing_ms: 29.828534643097846\n",
      "  time_since_restore: 23706.199193239212\n",
      "  time_this_iter_s: 785.8964838981628\n",
      "  time_total_s: 23706.199193239212\n",
      "  timers:\n",
      "    learn_throughput: 963.284\n",
      "    learn_time_ms: 1038.116\n",
      "    load_throughput: 55833.157\n",
      "    load_time_ms: 17.911\n",
      "    sample_throughput: 1.934\n",
      "    sample_time_ms: 517141.656\n",
      "    update_time_ms: 2.235\n",
      "  timestamp: 1633729512\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 448\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   448</td><td style=\"text-align: right;\">         23706.2</td><td style=\"text-align: right;\">448000</td><td style=\"text-align: right;\">    9.77</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   0</td><td style=\"text-align: right;\">             37.44</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 449000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_21-48-04\n",
      "  done: false\n",
      "  episode_len_mean: 42.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.65\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1795\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.12828922569751744\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4282671637005275\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.02492389443724152\n",
      "          policy_loss: -0.046922378490368524\n",
      "          total_loss: 0.39581886637541985\n",
      "          vf_explained_var: 0.7037784457206726\n",
      "          vf_loss: 0.45382645212941697\n",
      "    num_agent_steps_sampled: 449000\n",
      "    num_agent_steps_trained: 449000\n",
      "    num_steps_sampled: 449000\n",
      "    num_steps_trained: 449000\n",
      "  iterations_since_restore: 449\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.11016260162602\n",
      "    ram_util_percent: 65.01544715447156\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038328551647531434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.03588474566388\n",
      "    mean_inference_ms: 1.5693066486588676\n",
      "    mean_raw_obs_processing_ms: 30.16969152612879\n",
      "  time_since_restore: 23878.23291826248\n",
      "  time_this_iter_s: 172.03372502326965\n",
      "  time_total_s: 23878.23291826248\n",
      "  timers:\n",
      "    learn_throughput: 961.27\n",
      "    learn_time_ms: 1040.291\n",
      "    load_throughput: 55963.9\n",
      "    load_time_ms: 17.869\n",
      "    sample_throughput: 2.075\n",
      "    sample_time_ms: 481978.806\n",
      "    update_time_ms: 2.258\n",
      "  timestamp: 1633729684\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 449000\n",
      "  training_iteration: 449\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   449</td><td style=\"text-align: right;\">         23878.2</td><td style=\"text-align: right;\">449000</td><td style=\"text-align: right;\">    9.65</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             42.39</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 450000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_21-55-38\n",
      "  done: false\n",
      "  episode_len_mean: 35.3\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.75\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 1819\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19243383854627605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9536621077193155\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016824275512770093\n",
      "          policy_loss: 0.017854134655661054\n",
      "          total_loss: 0.26898889595435727\n",
      "          vf_explained_var: 0.8184055089950562\n",
      "          vf_loss: 0.2574338257312775\n",
      "    num_agent_steps_sampled: 450000\n",
      "    num_agent_steps_trained: 450000\n",
      "    num_steps_sampled: 450000\n",
      "    num_steps_trained: 450000\n",
      "  iterations_since_restore: 450\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.458705701078582\n",
      "    ram_util_percent: 65.06841294298921\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038329946439624465\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.043607095168987\n",
      "    mean_inference_ms: 1.5693957887113215\n",
      "    mean_raw_obs_processing_ms: 31.060740947947387\n",
      "  time_since_restore: 24332.862228631973\n",
      "  time_this_iter_s: 454.6293103694916\n",
      "  time_total_s: 24332.862228631973\n",
      "  timers:\n",
      "    learn_throughput: 960.777\n",
      "    learn_time_ms: 1040.825\n",
      "    load_throughput: 55267.621\n",
      "    load_time_ms: 18.094\n",
      "    sample_throughput: 1.977\n",
      "    sample_time_ms: 505711.94\n",
      "    update_time_ms: 2.254\n",
      "  timestamp: 1633730138\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 450000\n",
      "  training_iteration: 450\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   450</td><td style=\"text-align: right;\">         24332.9</td><td style=\"text-align: right;\">450000</td><td style=\"text-align: right;\">    9.75</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">              35.3</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 451000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_22-09-19\n",
      "  done: false\n",
      "  episode_len_mean: 36.22\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.74\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 43\n",
      "  episodes_total: 1862\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19243383854627605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.5290845271613863\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00878110389454596\n",
      "          policy_loss: -0.028665608747137916\n",
      "          total_loss: 0.21068333180414306\n",
      "          vf_explained_var: 0.9763010740280151\n",
      "          vf_loss: 0.24295000301467048\n",
      "    num_agent_steps_sampled: 451000\n",
      "    num_agent_steps_trained: 451000\n",
      "    num_steps_sampled: 451000\n",
      "    num_steps_trained: 451000\n",
      "  iterations_since_restore: 451\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.77941929974381\n",
      "    ram_util_percent: 65.15747224594365\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03833210398587594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.05577232865815\n",
      "    mean_inference_ms: 1.5695498042960705\n",
      "    mean_raw_obs_processing_ms: 32.68674240968665\n",
      "  time_since_restore: 25153.311795949936\n",
      "  time_this_iter_s: 820.4495673179626\n",
      "  time_total_s: 25153.311795949936\n",
      "  timers:\n",
      "    learn_throughput: 962.013\n",
      "    learn_time_ms: 1039.487\n",
      "    load_throughput: 55537.512\n",
      "    load_time_ms: 18.006\n",
      "    sample_throughput: 1.752\n",
      "    sample_time_ms: 570625.275\n",
      "    update_time_ms: 2.254\n",
      "  timestamp: 1633730959\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 451000\n",
      "  training_iteration: 451\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   451</td><td style=\"text-align: right;\">         25153.3</td><td style=\"text-align: right;\">451000</td><td style=\"text-align: right;\">    9.74</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             36.22</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 452000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_22-14-23\n",
      "  done: false\n",
      "  episode_len_mean: 40.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.66\n",
      "  episode_reward_min: -1.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 1878\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19243383854627605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.9384575873613358\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01162034722699106\n",
      "          policy_loss: -0.05990577878223525\n",
      "          total_loss: 0.456799240079191\n",
      "          vf_explained_var: 0.913115918636322\n",
      "          vf_loss: 0.5238534470399221\n",
      "    num_agent_steps_sampled: 452000\n",
      "    num_agent_steps_trained: 452000\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "  iterations_since_restore: 452\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.866974595842954\n",
      "    ram_util_percent: 65.170207852194\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03833310587878494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.0605337567778\n",
      "    mean_inference_ms: 1.5696116612909676\n",
      "    mean_raw_obs_processing_ms: 33.22685000198977\n",
      "  time_since_restore: 25457.30463886261\n",
      "  time_this_iter_s: 303.99284291267395\n",
      "  time_total_s: 25457.30463886261\n",
      "  timers:\n",
      "    learn_throughput: 960.664\n",
      "    learn_time_ms: 1040.947\n",
      "    load_throughput: 56067.586\n",
      "    load_time_ms: 17.836\n",
      "    sample_throughput: 1.862\n",
      "    sample_time_ms: 537176.142\n",
      "    update_time_ms: 2.26\n",
      "  timestamp: 1633731263\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 452\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   452</td><td style=\"text-align: right;\">         25457.3</td><td style=\"text-align: right;\">452000</td><td style=\"text-align: right;\">    9.66</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">             40.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c4eb6_00000:\n",
      "  agent_timesteps_total: 453000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-08_22-19-03\n",
      "  done: false\n",
      "  episode_len_mean: 35.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 9.74\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 1893\n",
      "  experiment_id: 85625a43911a48d5a508c61fb7ca3640\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19243383854627605\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 0.8339942104286617\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01307731742886163\n",
      "          policy_loss: -0.1722606495850616\n",
      "          total_loss: -0.003991273128324085\n",
      "          vf_explained_var: 0.6649593710899353\n",
      "          vf_loss: 0.17409280232257313\n",
      "    num_agent_steps_sampled: 453000\n",
      "    num_agent_steps_trained: 453000\n",
      "    num_steps_sampled: 453000\n",
      "    num_steps_trained: 453000\n",
      "  iterations_since_restore: 453\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.81075\n",
      "    ram_util_percent: 65.416\n",
      "  pid: 213\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03833414685945552\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.06379608909008\n",
      "    mean_inference_ms: 1.5696767165342758\n",
      "    mean_raw_obs_processing_ms: 33.787314331941715\n",
      "  time_since_restore: 25737.520041942596\n",
      "  time_this_iter_s: 280.2154030799866\n",
      "  time_total_s: 25737.520041942596\n",
      "  timers:\n",
      "    learn_throughput: 961.465\n",
      "    learn_time_ms: 1040.08\n",
      "    load_throughput: 55660.593\n",
      "    load_time_ms: 17.966\n",
      "    sample_throughput: 2.007\n",
      "    sample_time_ms: 498203.928\n",
      "    update_time_ms: 2.276\n",
      "  timestamp: 1633731543\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 453000\n",
      "  training_iteration: 453\n",
      "  trial_id: c4eb6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/8.85 GiB heap, 0.0/4.42 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-08_15-09-44<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc            </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c4eb6_00000</td><td>RUNNING </td><td>192.168.3.5:213</td><td style=\"text-align: right;\">   453</td><td style=\"text-align: right;\">         25737.5</td><td style=\"text-align: right;\">453000</td><td style=\"text-align: right;\">    9.74</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   2</td><td style=\"text-align: right;\">             35.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=210)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "2021-10-08 22:22:47,412\tWARNING tune.py:519 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2021-10-08 22:22:47,412\tWARNING tune.py:519 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2021-10-08 22:22:47,738\tWARNING tune.py:519 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "analysis = tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 1,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO C32 pretrained (AngelaCNN) (3 noops after placement)\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
