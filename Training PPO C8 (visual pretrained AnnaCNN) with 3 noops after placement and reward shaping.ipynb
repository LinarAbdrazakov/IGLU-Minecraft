{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *\n",
    "from wrappers_2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9fd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4, padding=0),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(64, 512, kernel_size=2, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        features_dim = 512\n",
    "        self.encoder = VisualEncoder()\n",
    "        self.encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AnnaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.action_head = nn.Linear(features_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(features_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.encoder.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        if self.use_cuda:\n",
    "            obs.cuda()\n",
    "            \n",
    "        features = self.encoder(obs)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def reward(self, rew):\n",
    "        if rew == 0:\n",
    "            rew = -0.1\n",
    "        return rew\n",
    "    \n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C8']))\n",
    "    env = PovOnlyWrapper(env)\n",
    "    env = SelectAndPlace(env)\n",
    "    env = Discretization(env, flat_action_space('human-level'))\n",
    "    env = RewardWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n",
      "2021-10-09 21:38:54,838\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-10-09 21:38:54,853\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to auto resume run with id 5d62f_00000 but id 4d34e_00000 is set.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=258828)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=258828)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO C8 pretrained (AnnaCNN) (3 noops after placement and reward shaping)</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/4d34e_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/4d34e_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20211009_213855-4d34e_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=258828)\u001b[0m 2021-10-09 21:38:58,243\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=258828)\u001b[0m 2021-10-09 21:38:58,243\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=258822)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=258822)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=258828)\u001b[0m 2021-10-09 21:39:03,962\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=258822)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=258822)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=258822)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=258822)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=258822)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=258822)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-40-05\n",
      "  done: false\n",
      "  episode_len_mean: 484.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -45.20000000000037\n",
      "  episode_reward_mean: -48.40000000000042\n",
      "  episode_reward_min: -51.60000000000046\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6374062498410542\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011904515929222962\n",
      "          policy_loss: -0.0018852401110861037\n",
      "          total_loss: 0.21260175696677633\n",
      "          vf_explained_var: 0.5787085890769958\n",
      "          vf_loss: 0.2284801604019271\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.513483146067415\n",
      "    ram_util_percent: 84.26966292134831\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03910588694142771\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 59.503839208886816\n",
      "    mean_inference_ms: 1.4390230893374205\n",
      "    mean_raw_obs_processing_ms: 0.1631171315104573\n",
      "  time_since_restore: 61.92158269882202\n",
      "  time_this_iter_s: 61.92158269882202\n",
      "  time_total_s: 61.92158269882202\n",
      "  timers:\n",
      "    learn_throughput: 1555.106\n",
      "    learn_time_ms: 643.043\n",
      "    load_throughput: 57903.584\n",
      "    load_time_ms: 17.27\n",
      "    sample_throughput: 16.325\n",
      "    sample_time_ms: 61256.439\n",
      "    update_time_ms: 2.126\n",
      "  timestamp: 1633815605\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         61.9216</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">   -48.4</td><td style=\"text-align: right;\">               -45.2</td><td style=\"text-align: right;\">               -51.6</td><td style=\"text-align: right;\">               484</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-40-20\n",
      "  done: false\n",
      "  episode_len_mean: 517.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -45.20000000000037\n",
      "  episode_reward_mean: -51.700000000000465\n",
      "  episode_reward_min: -58.30000000000056\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 3\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6620309193929037\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01803415500471033\n",
      "          policy_loss: 0.019109862463341818\n",
      "          total_loss: 0.23480405290093687\n",
      "          vf_explained_var: 0.18970385193824768\n",
      "          vf_loss: 0.228707674311267\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.93000000000001\n",
      "    ram_util_percent: 89.38500000000002\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.038735689650766895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 51.58258853142096\n",
      "    mean_inference_ms: 1.419848750731025\n",
      "    mean_raw_obs_processing_ms: 0.15949796539397806\n",
      "  time_since_restore: 76.06137800216675\n",
      "  time_this_iter_s: 14.139795303344727\n",
      "  time_total_s: 76.06137800216675\n",
      "  timers:\n",
      "    learn_throughput: 1558.606\n",
      "    learn_time_ms: 641.599\n",
      "    load_throughput: 96200.736\n",
      "    load_time_ms: 10.395\n",
      "    sample_throughput: 26.757\n",
      "    sample_time_ms: 37373.961\n",
      "    update_time_ms: 2.16\n",
      "  timestamp: 1633815620\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         76.0614</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">   -51.7</td><td style=\"text-align: right;\">               -45.2</td><td style=\"text-align: right;\">               -58.3</td><td style=\"text-align: right;\">               517</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-40-33\n",
      "  done: false\n",
      "  episode_len_mean: 534.6\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -45.20000000000037\n",
      "  episode_reward_mean: -53.46000000000049\n",
      "  episode_reward_min: -58.700000000000564\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 5\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5666171855396696\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013362808227218852\n",
      "          policy_loss: 0.019572347692317433\n",
      "          total_loss: 0.2517342766539918\n",
      "          vf_explained_var: 0.4106024503707886\n",
      "          vf_loss: 0.24515554122626781\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.315000000000005\n",
      "    ram_util_percent: 89.475\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03819306315725649\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 42.02203349948701\n",
      "    mean_inference_ms: 1.395834203056541\n",
      "    mean_raw_obs_processing_ms: 0.15864865674303824\n",
      "  time_since_restore: 89.844797372818\n",
      "  time_this_iter_s: 13.783419370651245\n",
      "  time_total_s: 89.844797372818\n",
      "  timers:\n",
      "    learn_throughput: 1532.891\n",
      "    learn_time_ms: 652.362\n",
      "    load_throughput: 124383.78\n",
      "    load_time_ms: 8.04\n",
      "    sample_throughput: 34.149\n",
      "    sample_time_ms: 29283.294\n",
      "    update_time_ms: 1.991\n",
      "  timestamp: 1633815633\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         89.8448</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">  -53.46</td><td style=\"text-align: right;\">               -45.2</td><td style=\"text-align: right;\">               -58.7</td><td style=\"text-align: right;\">             534.6</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-40-48\n",
      "  done: false\n",
      "  episode_len_mean: 538.4285714285714\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -45.20000000000037\n",
      "  episode_reward_mean: -53.842857142857646\n",
      "  episode_reward_min: -58.700000000000564\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 7\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6130048897531297\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011417587975743496\n",
      "          policy_loss: -0.0015001221663422054\n",
      "          total_loss: 0.2973063816626867\n",
      "          vf_explained_var: 0.23403385281562805\n",
      "          vf_loss: 0.31265303124156263\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.25238095238095\n",
      "    ram_util_percent: 89.51904761904763\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03789679206589878\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 36.858528041119605\n",
      "    mean_inference_ms: 1.3819494603037774\n",
      "    mean_raw_obs_processing_ms: 0.15956657375511898\n",
      "  time_since_restore: 104.7847900390625\n",
      "  time_this_iter_s: 14.939992666244507\n",
      "  time_total_s: 104.7847900390625\n",
      "  timers:\n",
      "    learn_throughput: 1544.557\n",
      "    learn_time_ms: 647.435\n",
      "    load_throughput: 145365.519\n",
      "    load_time_ms: 6.879\n",
      "    sample_throughput: 39.158\n",
      "    sample_time_ms: 25537.418\n",
      "    update_time_ms: 1.907\n",
      "  timestamp: 1633815648\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.9/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         104.785</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-53.8429</td><td style=\"text-align: right;\">               -45.2</td><td style=\"text-align: right;\">               -58.7</td><td style=\"text-align: right;\">           538.429</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-41-03\n",
      "  done: false\n",
      "  episode_len_mean: 540.2222222222222\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -45.20000000000037\n",
      "  episode_reward_mean: -54.02222222222272\n",
      "  episode_reward_min: -58.700000000000564\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 9\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7277319894896612\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011258051009762484\n",
      "          policy_loss: 0.12729045020209417\n",
      "          total_loss: 0.3761722773313522\n",
      "          vf_explained_var: 0.28124526143074036\n",
      "          vf_loss: 0.2639075417899423\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.29\n",
      "    ram_util_percent: 89.52999999999999\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037691948999058485\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 33.458457970611704\n",
      "    mean_inference_ms: 1.3722442041455551\n",
      "    mean_raw_obs_processing_ms: 0.16166637920496177\n",
      "  time_since_restore: 118.98098087310791\n",
      "  time_this_iter_s: 14.19619083404541\n",
      "  time_total_s: 118.98098087310791\n",
      "  timers:\n",
      "    learn_throughput: 1551.784\n",
      "    learn_time_ms: 644.42\n",
      "    load_throughput: 163355.04\n",
      "    load_time_ms: 6.122\n",
      "    sample_throughput: 43.213\n",
      "    sample_time_ms: 23141.214\n",
      "    update_time_ms: 1.876\n",
      "  timestamp: 1633815663\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         118.981</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">-54.0222</td><td style=\"text-align: right;\">               -45.2</td><td style=\"text-align: right;\">               -58.7</td><td style=\"text-align: right;\">           540.222</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-41-20\n",
      "  done: false\n",
      "  episode_len_mean: 527.4545454545455\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -45.20000000000037\n",
      "  episode_reward_mean: -52.745454545455026\n",
      "  episode_reward_min: -58.700000000000564\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 11\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.769703167014652\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011410914931694964\n",
      "          policy_loss: -0.010947428312566546\n",
      "          total_loss: 0.24913705297642283\n",
      "          vf_explained_var: 0.33329471945762634\n",
      "          vf_loss: 0.27549933230297435\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.33599999999999\n",
      "    ram_util_percent: 89.75200000000001\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037549181405263296\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 31.102550820179896\n",
      "    mean_inference_ms: 1.3656717616690095\n",
      "    mean_raw_obs_processing_ms: 0.16309872747001283\n",
      "  time_since_restore: 136.40868401527405\n",
      "  time_this_iter_s: 17.427703142166138\n",
      "  time_total_s: 136.40868401527405\n",
      "  timers:\n",
      "    learn_throughput: 1557.34\n",
      "    learn_time_ms: 642.121\n",
      "    load_throughput: 122066.423\n",
      "    load_time_ms: 8.192\n",
      "    sample_throughput: 45.29\n",
      "    sample_time_ms: 22080.052\n",
      "    update_time_ms: 1.835\n",
      "  timestamp: 1633815680\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         136.409</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">-52.7455</td><td style=\"text-align: right;\">               -45.2</td><td style=\"text-align: right;\">               -58.7</td><td style=\"text-align: right;\">           527.455</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-41-35\n",
      "  done: false\n",
      "  episode_len_mean: 522.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -45.20000000000037\n",
      "  episode_reward_mean: -52.200000000000465\n",
      "  episode_reward_min: -58.700000000000564\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 13\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6403181711832682\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007259975367006098\n",
      "          policy_loss: 0.06299657391177284\n",
      "          total_loss: 0.3950923330254025\n",
      "          vf_explained_var: 0.41784006357192993\n",
      "          vf_loss: 0.34704694603052405\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 7000\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 7000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.25909090909091\n",
      "    ram_util_percent: 89.78636363636362\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037434456809524716\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 29.30764365770531\n",
      "    mean_inference_ms: 1.3603444188711045\n",
      "    mean_raw_obs_processing_ms: 0.16467717042160285\n",
      "  time_since_restore: 151.67806267738342\n",
      "  time_this_iter_s: 15.269378662109375\n",
      "  time_total_s: 151.67806267738342\n",
      "  timers:\n",
      "    learn_throughput: 1559.006\n",
      "    learn_time_ms: 641.434\n",
      "    load_throughput: 133549.219\n",
      "    load_time_ms: 7.488\n",
      "    sample_throughput: 47.585\n",
      "    sample_time_ms: 21014.964\n",
      "    update_time_ms: 1.828\n",
      "  timestamp: 1633815695\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         151.678</td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\">   -52.2</td><td style=\"text-align: right;\">               -45.2</td><td style=\"text-align: right;\">               -58.7</td><td style=\"text-align: right;\">               522</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-41-48\n",
      "  done: false\n",
      "  episode_len_mean: 527.5333333333333\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -45.20000000000037\n",
      "  episode_reward_mean: -52.753333333333806\n",
      "  episode_reward_min: -58.700000000000564\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 15\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.704326374000973\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011227955352923816\n",
      "          policy_loss: 0.09704650574260287\n",
      "          total_loss: 0.43954161124096974\n",
      "          vf_explained_var: 0.3781976103782654\n",
      "          vf_loss: 0.35729277847955626\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.689473684210526\n",
      "    ram_util_percent: 89.76315789473685\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037350134935433627\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.847063712111563\n",
      "    mean_inference_ms: 1.355904694044449\n",
      "    mean_raw_obs_processing_ms: 0.16613234471351618\n",
      "  time_since_restore: 164.65083479881287\n",
      "  time_this_iter_s: 12.972772121429443\n",
      "  time_total_s: 164.65083479881287\n",
      "  timers:\n",
      "    learn_throughput: 1560.712\n",
      "    learn_time_ms: 640.733\n",
      "    load_throughput: 143596.903\n",
      "    load_time_ms: 6.964\n",
      "    sample_throughput: 50.178\n",
      "    sample_time_ms: 19929.226\n",
      "    update_time_ms: 1.823\n",
      "  timestamp: 1633815708\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         164.651</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-52.7533</td><td style=\"text-align: right;\">               -45.2</td><td style=\"text-align: right;\">               -58.7</td><td style=\"text-align: right;\">           527.533</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-42-02\n",
      "  done: false\n",
      "  episode_len_mean: 532.125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -45.20000000000037\n",
      "  episode_reward_mean: -53.21250000000049\n",
      "  episode_reward_min: -60.100000000000584\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 16\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7570066266589695\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0075257509468616805\n",
      "          policy_loss: -0.06456692959699366\n",
      "          total_loss: 0.30471791649858154\n",
      "          vf_explained_var: 0.3997000455856323\n",
      "          vf_loss: 0.3853497669307722\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 9000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.79473684210526\n",
      "    ram_util_percent: 89.70526315789475\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037312235491473125\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 27.205456616361616\n",
      "    mean_inference_ms: 1.3538551521228723\n",
      "    mean_raw_obs_processing_ms: 0.1664748117868082\n",
      "  time_since_restore: 178.20905351638794\n",
      "  time_this_iter_s: 13.558218717575073\n",
      "  time_total_s: 178.20905351638794\n",
      "  timers:\n",
      "    learn_throughput: 1559.276\n",
      "    learn_time_ms: 641.323\n",
      "    load_throughput: 152046.498\n",
      "    load_time_ms: 6.577\n",
      "    sample_throughput: 52.223\n",
      "    sample_time_ms: 19148.715\n",
      "    update_time_ms: 1.798\n",
      "  timestamp: 1633815722\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         178.209</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">-53.2125</td><td style=\"text-align: right;\">               -45.2</td><td style=\"text-align: right;\">               -60.1</td><td style=\"text-align: right;\">           532.125</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-42-14\n",
      "  done: false\n",
      "  episode_len_mean: 536.6111111111111\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -45.20000000000037\n",
      "  episode_reward_mean: -53.66111111111161\n",
      "  episode_reward_min: -62.300000000000615\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 18\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6769349733988443\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009709388017812633\n",
      "          policy_loss: -0.09002938369909923\n",
      "          total_loss: 0.7142201367351744\n",
      "          vf_explained_var: 0.3765750825405121\n",
      "          vf_loss: 0.8190769890116321\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.738888888888894\n",
      "    ram_util_percent: 89.78333333333333\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03724298954209938\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 26.056035454715715\n",
      "    mean_inference_ms: 1.3500721844560135\n",
      "    mean_raw_obs_processing_ms: 0.16764158998002363\n",
      "  time_since_restore: 190.81687760353088\n",
      "  time_this_iter_s: 12.607824087142944\n",
      "  time_total_s: 190.81687760353088\n",
      "  timers:\n",
      "    learn_throughput: 1557.544\n",
      "    learn_time_ms: 642.036\n",
      "    load_throughput: 160306.984\n",
      "    load_time_ms: 6.238\n",
      "    sample_throughput: 54.262\n",
      "    sample_time_ms: 18429.043\n",
      "    update_time_ms: 1.787\n",
      "  timestamp: 1633815734\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         190.817</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">-53.6611</td><td style=\"text-align: right;\">               -45.2</td><td style=\"text-align: right;\">               -62.3</td><td style=\"text-align: right;\">           536.611</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 11000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-42-27\n",
      "  done: false\n",
      "  episode_len_mean: 543.4\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -45.20000000000037\n",
      "  episode_reward_mean: -54.34000000000051\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 20\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.4617929763264126\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.003940579199421753\n",
      "          policy_loss: 0.09973888910479016\n",
      "          total_loss: 0.5756603595283296\n",
      "          vf_explained_var: 0.32582178711891174\n",
      "          vf_loss: 0.4897512865977155\n",
      "    num_agent_steps_sampled: 11000\n",
      "    num_agent_steps_trained: 11000\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 11000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.76666666666667\n",
      "    ram_util_percent: 89.83888888888889\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03718300925204075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.07781718420561\n",
      "    mean_inference_ms: 1.3467801888527582\n",
      "    mean_raw_obs_processing_ms: 0.1687346184596345\n",
      "  time_since_restore: 203.42968273162842\n",
      "  time_this_iter_s: 12.612805128097534\n",
      "  time_total_s: 203.42968273162842\n",
      "  timers:\n",
      "    learn_throughput: 1555.301\n",
      "    learn_time_ms: 642.963\n",
      "    load_throughput: 207237.674\n",
      "    load_time_ms: 4.825\n",
      "    sample_throughput: 74.081\n",
      "    sample_time_ms: 13498.711\n",
      "    update_time_ms: 1.739\n",
      "  timestamp: 1633815747\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">          203.43</td><td style=\"text-align: right;\">11000</td><td style=\"text-align: right;\">  -54.34</td><td style=\"text-align: right;\">               -45.2</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">             543.4</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-42-39\n",
      "  done: false\n",
      "  episode_len_mean: 545.3181818181819\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -45.20000000000037\n",
      "  episode_reward_mean: -54.53181818181869\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 22\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.09999999999999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6589543210135567\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.025686176814379753\n",
      "          policy_loss: 0.09353899508714676\n",
      "          total_loss: 0.5687202600969209\n",
      "          vf_explained_var: 0.37338143587112427\n",
      "          vf_loss: 0.4892021916496257\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.65\n",
      "    ram_util_percent: 89.88125\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03713074558974525\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 24.22468501419977\n",
      "    mean_inference_ms: 1.3438421320476492\n",
      "    mean_raw_obs_processing_ms: 0.1693005677092543\n",
      "  time_since_restore: 214.87032890319824\n",
      "  time_this_iter_s: 11.440646171569824\n",
      "  time_total_s: 214.87032890319824\n",
      "  timers:\n",
      "    learn_throughput: 1554.453\n",
      "    learn_time_ms: 643.313\n",
      "    load_throughput: 208893.205\n",
      "    load_time_ms: 4.787\n",
      "    sample_throughput: 75.594\n",
      "    sample_time_ms: 13228.511\n",
      "    update_time_ms: 1.694\n",
      "  timestamp: 1633815759\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">          214.87</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-54.5318</td><td style=\"text-align: right;\">               -45.2</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           545.318</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 13000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-42-52\n",
      "  done: false\n",
      "  episode_len_mean: 547.9130434782609\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -45.20000000000037\n",
      "  episode_reward_mean: -54.7913043478266\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 23\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9412229710155062\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.07891329814396315\n",
      "          policy_loss: 0.11812745134035746\n",
      "          total_loss: 0.2619482696470287\n",
      "          vf_explained_var: 0.5541654229164124\n",
      "          vf_loss: 0.1513960467858447\n",
      "    num_agent_steps_sampled: 13000\n",
      "    num_agent_steps_trained: 13000\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 13000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.015\n",
      "    ram_util_percent: 89.93500000000002\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710722928818549\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.839220456271963\n",
      "    mean_inference_ms: 1.3425372717958586\n",
      "    mean_raw_obs_processing_ms: 0.1693149070240411\n",
      "  time_since_restore: 228.33591270446777\n",
      "  time_this_iter_s: 13.465583801269531\n",
      "  time_total_s: 228.33591270446777\n",
      "  timers:\n",
      "    learn_throughput: 1563.428\n",
      "    learn_time_ms: 639.62\n",
      "    load_throughput: 209589.446\n",
      "    load_time_ms: 4.771\n",
      "    sample_throughput: 75.755\n",
      "    sample_time_ms: 13200.45\n",
      "    update_time_ms: 1.687\n",
      "  timestamp: 1633815772\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         228.336</td><td style=\"text-align: right;\">13000</td><td style=\"text-align: right;\">-54.7913</td><td style=\"text-align: right;\">               -45.2</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           547.913</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 14000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-43-08\n",
      "  done: false\n",
      "  episode_len_mean: 542.96\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -45.20000000000037\n",
      "  episode_reward_mean: -54.296000000000504\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 25\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2250000000000001\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8534856849246555\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020184519999214008\n",
      "          policy_loss: -0.16709880199697283\n",
      "          total_loss: 0.4188656525479423\n",
      "          vf_explained_var: 0.615522027015686\n",
      "          vf_loss: 0.5999577907638417\n",
      "    num_agent_steps_sampled: 14000\n",
      "    num_agent_steps_trained: 14000\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.88260869565217\n",
      "    ram_util_percent: 90.02173913043477\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037066907218451385\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 23.152820755483454\n",
      "    mean_inference_ms: 1.3403023004469898\n",
      "    mean_raw_obs_processing_ms: 0.16912075029031687\n",
      "  time_since_restore: 244.5103485584259\n",
      "  time_this_iter_s: 16.17443585395813\n",
      "  time_total_s: 244.5103485584259\n",
      "  timers:\n",
      "    learn_throughput: 1551.782\n",
      "    learn_time_ms: 644.42\n",
      "    load_throughput: 189899.217\n",
      "    load_time_ms: 5.266\n",
      "    sample_throughput: 75.083\n",
      "    sample_time_ms: 13318.592\n",
      "    update_time_ms: 1.683\n",
      "  timestamp: 1633815788\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">          244.51</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\"> -54.296</td><td style=\"text-align: right;\">               -45.2</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">            542.96</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-43-26\n",
      "  done: false\n",
      "  episode_len_mean: 530.8214285714286\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -36.300000000000246\n",
      "  episode_reward_mean: -53.082142857143346\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 28\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8966564959949916\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012317307921302164\n",
      "          policy_loss: 0.010679596289992332\n",
      "          total_loss: 0.5422705931795968\n",
      "          vf_explained_var: 0.7506063580513\n",
      "          vf_loss: 0.5464004711972342\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_agent_steps_trained: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.371999999999986\n",
      "    ram_util_percent: 90.20800000000001\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037024329132127165\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 22.309250484858868\n",
      "    mean_inference_ms: 1.3378228407663213\n",
      "    mean_raw_obs_processing_ms: 0.16886504308581105\n",
      "  time_since_restore: 262.3899586200714\n",
      "  time_this_iter_s: 17.879610061645508\n",
      "  time_total_s: 262.3899586200714\n",
      "  timers:\n",
      "    learn_throughput: 1531.435\n",
      "    learn_time_ms: 652.982\n",
      "    load_throughput: 162497.491\n",
      "    load_time_ms: 6.154\n",
      "    sample_throughput: 73.113\n",
      "    sample_time_ms: 13677.464\n",
      "    update_time_ms: 1.694\n",
      "  timestamp: 1633815806\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">          262.39</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">-53.0821</td><td style=\"text-align: right;\">               -36.3</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           530.821</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=258822)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=258822)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-44-01\n",
      "  done: false\n",
      "  episode_len_mean: 524.3333333333334\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -36.300000000000246\n",
      "  episode_reward_mean: -52.43333333333381\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 30\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9640616801049975\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0087817882562713\n",
      "          policy_loss: -0.04746115571922726\n",
      "          total_loss: 0.4191859797471099\n",
      "          vf_explained_var: 0.787265419960022\n",
      "          vf_loss: 0.4833238970074389\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.925490196078435\n",
      "    ram_util_percent: 89.32745098039213\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037009270006432865\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.842274094681105\n",
      "    mean_inference_ms: 1.3366659528300495\n",
      "    mean_raw_obs_processing_ms: 0.24083731927055485\n",
      "  time_since_restore: 297.6517074108124\n",
      "  time_this_iter_s: 35.26174879074097\n",
      "  time_total_s: 297.6517074108124\n",
      "  timers:\n",
      "    learn_throughput: 1527.349\n",
      "    learn_time_ms: 654.729\n",
      "    load_throughput: 168964.373\n",
      "    load_time_ms: 5.918\n",
      "    sample_throughput: 64.686\n",
      "    sample_time_ms: 15459.362\n",
      "    update_time_ms: 1.703\n",
      "  timestamp: 1633815841\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         297.652</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-52.4333</td><td style=\"text-align: right;\">               -36.3</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           524.333</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 17000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-44-21\n",
      "  done: false\n",
      "  episode_len_mean: 522.78125\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -36.300000000000246\n",
      "  episode_reward_mean: -52.478125000000475\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 32\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.160564568307665\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01928802825954262\n",
      "          policy_loss: -0.09622615033553707\n",
      "          total_loss: 0.9063408523797989\n",
      "          vf_explained_var: 0.6060976982116699\n",
      "          vf_loss: 1.017662951350212\n",
      "    num_agent_steps_sampled: 17000\n",
      "    num_agent_steps_trained: 17000\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 17000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.0037037037037\n",
      "    ram_util_percent: 90.27777777777777\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.036998830669797034\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.43994021867845\n",
      "    mean_inference_ms: 1.3358014257093533\n",
      "    mean_raw_obs_processing_ms: 0.2998592174331947\n",
      "  time_since_restore: 317.00911569595337\n",
      "  time_this_iter_s: 19.35740828514099\n",
      "  time_total_s: 317.00911569595337\n",
      "  timers:\n",
      "    learn_throughput: 1515.042\n",
      "    learn_time_ms: 660.048\n",
      "    load_throughput: 135910.851\n",
      "    load_time_ms: 7.358\n",
      "    sample_throughput: 63.046\n",
      "    sample_time_ms: 15861.376\n",
      "    update_time_ms: 1.697\n",
      "  timestamp: 1633815861\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         317.009</td><td style=\"text-align: right;\">17000</td><td style=\"text-align: right;\">-52.4781</td><td style=\"text-align: right;\">               -36.3</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           522.781</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-44-38\n",
      "  done: false\n",
      "  episode_len_mean: 522.5882352941177\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -36.300000000000246\n",
      "  episode_reward_mean: -52.658823529412246\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 34\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3374999999999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1023019499248927\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020649540136417916\n",
      "          policy_loss: -0.0032476163572735255\n",
      "          total_loss: 0.9231698777940538\n",
      "          vf_explained_var: 0.4732872545719147\n",
      "          vf_loss: 0.9404713047875298\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.091666666666676\n",
      "    ram_util_percent: 90.45\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03699470581586343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 21.082072074870634\n",
      "    mean_inference_ms: 1.3351725058209267\n",
      "    mean_raw_obs_processing_ms: 0.34867261726319293\n",
      "  time_since_restore: 333.8037211894989\n",
      "  time_this_iter_s: 16.794605493545532\n",
      "  time_total_s: 333.8037211894989\n",
      "  timers:\n",
      "    learn_throughput: 1514.072\n",
      "    learn_time_ms: 660.471\n",
      "    load_throughput: 128773.79\n",
      "    load_time_ms: 7.766\n",
      "    sample_throughput: 61.566\n",
      "    sample_time_ms: 16242.751\n",
      "    update_time_ms: 1.683\n",
      "  timestamp: 1633815878\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         333.804</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">-52.6588</td><td style=\"text-align: right;\">               -36.3</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           522.588</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 19000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-44-59\n",
      "  done: false\n",
      "  episode_len_mean: 517.9166666666666\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -36.300000000000246\n",
      "  episode_reward_mean: -52.19166666666714\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 36\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.102501567204793\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013442778299188794\n",
      "          policy_loss: -0.131264272166623\n",
      "          total_loss: 0.6592761879165967\n",
      "          vf_explained_var: 0.674710750579834\n",
      "          vf_loss: 0.8047600693172878\n",
      "    num_agent_steps_sampled: 19000\n",
      "    num_agent_steps_trained: 19000\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 19000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.04193548387097\n",
      "    ram_util_percent: 90.11290322580646\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0369931527078471\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.77581576915318\n",
      "    mean_inference_ms: 1.3347142717400577\n",
      "    mean_raw_obs_processing_ms: 0.389257335820407\n",
      "  time_since_restore: 355.47614884376526\n",
      "  time_this_iter_s: 21.672427654266357\n",
      "  time_total_s: 355.47614884376526\n",
      "  timers:\n",
      "    learn_throughput: 1514.088\n",
      "    learn_time_ms: 660.464\n",
      "    load_throughput: 108646.933\n",
      "    load_time_ms: 9.204\n",
      "    sample_throughput: 58.642\n",
      "    sample_time_ms: 17052.701\n",
      "    update_time_ms: 1.694\n",
      "  timestamp: 1633815899\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 19\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         355.476</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">-52.1917</td><td style=\"text-align: right;\">               -36.3</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           517.917</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-45-18\n",
      "  done: false\n",
      "  episode_len_mean: 510.7435897435897\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -36.300000000000246\n",
      "  episode_reward_mean: -51.46153846153892\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 39\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.50625\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9263595408863492\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004200479659693259\n",
      "          policy_loss: 0.0093581047323015\n",
      "          total_loss: 0.5124841400318676\n",
      "          vf_explained_var: 0.8380680680274963\n",
      "          vf_loss: 0.5202631426768171\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.00714285714286\n",
      "    ram_util_percent: 89.83928571428571\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03699477503945699\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.38031948790312\n",
      "    mean_inference_ms: 1.3342287378978044\n",
      "    mean_raw_obs_processing_ms: 0.4389327910253851\n",
      "  time_since_restore: 374.6463782787323\n",
      "  time_this_iter_s: 19.17022943496704\n",
      "  time_total_s: 374.6463782787323\n",
      "  timers:\n",
      "    learn_throughput: 1510.813\n",
      "    learn_time_ms: 661.895\n",
      "    load_throughput: 94160.285\n",
      "    load_time_ms: 10.62\n",
      "    sample_throughput: 56.478\n",
      "    sample_time_ms: 17706.059\n",
      "    update_time_ms: 1.701\n",
      "  timestamp: 1633815918\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 20\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         374.646</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-51.4615</td><td style=\"text-align: right;\">               -36.3</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           510.744</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 21000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-45-36\n",
      "  done: false\n",
      "  episode_len_mean: 510.5853658536585\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -36.300000000000246\n",
      "  episode_reward_mean: -51.50243902439071\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 41\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.2353843424055313\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01688800767602048\n",
      "          policy_loss: 0.032173023621241255\n",
      "          total_loss: 0.549167098932796\n",
      "          vf_explained_var: 0.5079111456871033\n",
      "          vf_loss: 0.5350731456445323\n",
      "    num_agent_steps_sampled: 21000\n",
      "    num_agent_steps_trained: 21000\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 21000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.74799999999999\n",
      "    ram_util_percent: 90.184\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03700157827968117\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 20.148010080524344\n",
      "    mean_inference_ms: 1.3340840638983387\n",
      "    mean_raw_obs_processing_ms: 0.46597358557949603\n",
      "  time_since_restore: 392.233181476593\n",
      "  time_this_iter_s: 17.586803197860718\n",
      "  time_total_s: 392.233181476593\n",
      "  timers:\n",
      "    learn_throughput: 1508.276\n",
      "    learn_time_ms: 663.009\n",
      "    load_throughput: 84807.942\n",
      "    load_time_ms: 11.791\n",
      "    sample_throughput: 54.942\n",
      "    sample_time_ms: 18201.156\n",
      "    update_time_ms: 1.707\n",
      "  timestamp: 1633815936\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         392.233</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\">-51.5024</td><td style=\"text-align: right;\">               -36.3</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           510.585</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 22000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-45-55\n",
      "  done: false\n",
      "  episode_len_mean: 508.93023255813955\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -36.300000000000246\n",
      "  episode_reward_mean: -51.28372093023302\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 43\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.106798301802741\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010835478036245311\n",
      "          policy_loss: -0.008011911229954826\n",
      "          total_loss: 0.548278480850988\n",
      "          vf_explained_var: 0.6041936874389648\n",
      "          vf_loss: 0.5746156500859393\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 22000\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.49615384615384\n",
      "    ram_util_percent: 90.20384615384614\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03701152271161046\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.938971676087714\n",
      "    mean_inference_ms: 1.3340365510416776\n",
      "    mean_raw_obs_processing_ms: 0.4887334807773161\n",
      "  time_since_restore: 410.90877962112427\n",
      "  time_this_iter_s: 18.67559814453125\n",
      "  time_total_s: 410.90877962112427\n",
      "  timers:\n",
      "    learn_throughput: 1505.904\n",
      "    learn_time_ms: 664.053\n",
      "    load_throughput: 75800.857\n",
      "    load_time_ms: 13.192\n",
      "    sample_throughput: 52.848\n",
      "    sample_time_ms: 18922.196\n",
      "    update_time_ms: 1.703\n",
      "  timestamp: 1633815955\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 22\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         410.909</td><td style=\"text-align: right;\">22000</td><td style=\"text-align: right;\">-51.2837</td><td style=\"text-align: right;\">               -36.3</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">            508.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 23000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-46-11\n",
      "  done: false\n",
      "  episode_len_mean: 508.8888888888889\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -36.300000000000246\n",
      "  episode_reward_mean: -51.47333333333379\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 45\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.228655155499776\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014543435001720518\n",
      "          policy_loss: 0.07323275605837504\n",
      "          total_loss: 0.920251762535837\n",
      "          vf_explained_var: 0.4346863925457001\n",
      "          vf_loss: 0.8656242529551188\n",
      "    num_agent_steps_sampled: 23000\n",
      "    num_agent_steps_trained: 23000\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 23000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.23333333333333\n",
      "    ram_util_percent: 90.17916666666667\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03702152547904825\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.74594784370059\n",
      "    mean_inference_ms: 1.3340241364349659\n",
      "    mean_raw_obs_processing_ms: 0.5079547557147867\n",
      "  time_since_restore: 427.4821879863739\n",
      "  time_this_iter_s: 16.573408365249634\n",
      "  time_total_s: 427.4821879863739\n",
      "  timers:\n",
      "    learn_throughput: 1502.535\n",
      "    learn_time_ms: 665.542\n",
      "    load_throughput: 73378.563\n",
      "    load_time_ms: 13.628\n",
      "    sample_throughput: 51.999\n",
      "    sample_time_ms: 19231.038\n",
      "    update_time_ms: 1.703\n",
      "  timestamp: 1633815971\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 23\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         427.482</td><td style=\"text-align: right;\">23000</td><td style=\"text-align: right;\">-51.4733</td><td style=\"text-align: right;\">               -36.3</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           508.889</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-46-30\n",
      "  done: false\n",
      "  episode_len_mean: 505.72340425531917\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -36.300000000000246\n",
      "  episode_reward_mean: -51.13191489361747\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 47\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.10178224907981\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008844130696868745\n",
      "          policy_loss: -0.013350406040747961\n",
      "          total_loss: 0.6259875532653597\n",
      "          vf_explained_var: 0.7506561279296875\n",
      "          vf_loss: 0.6581171169877053\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.351851851851855\n",
      "    ram_util_percent: 90.15925925925923\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037029802123799645\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.571504307560907\n",
      "    mean_inference_ms: 1.3340194972648622\n",
      "    mean_raw_obs_processing_ms: 0.524158362700413\n",
      "  time_since_restore: 446.5234925746918\n",
      "  time_this_iter_s: 19.04130458831787\n",
      "  time_total_s: 446.5234925746918\n",
      "  timers:\n",
      "    learn_throughput: 1506.558\n",
      "    learn_time_ms: 663.765\n",
      "    load_throughput: 68834.009\n",
      "    load_time_ms: 14.528\n",
      "    sample_throughput: 51.233\n",
      "    sample_time_ms: 19518.581\n",
      "    update_time_ms: 1.709\n",
      "  timestamp: 1633815990\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.0/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         446.523</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">-51.1319</td><td style=\"text-align: right;\">               -36.3</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           505.723</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 25000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-46-49\n",
      "  done: false\n",
      "  episode_len_mean: 503.7959183673469\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -36.300000000000246\n",
      "  episode_reward_mean: -50.90000000000045\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 49\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.253125\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.054393830564287\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.020211616021907453\n",
      "          policy_loss: -0.0449271146621969\n",
      "          total_loss: 0.6154619554678599\n",
      "          vf_explained_var: 0.783899188041687\n",
      "          vf_loss: 0.6758169332726134\n",
      "    num_agent_steps_sampled: 25000\n",
      "    num_agent_steps_trained: 25000\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 25000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.199999999999996\n",
      "    ram_util_percent: 90.31153846153846\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03703743763817475\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.411790214594884\n",
      "    mean_inference_ms: 1.3340137718980858\n",
      "    mean_raw_obs_processing_ms: 0.5378221966634912\n",
      "  time_since_restore: 464.67478680610657\n",
      "  time_this_iter_s: 18.151294231414795\n",
      "  time_total_s: 464.67478680610657\n",
      "  timers:\n",
      "    learn_throughput: 1523.917\n",
      "    learn_time_ms: 656.204\n",
      "    load_throughput: 66519.237\n",
      "    load_time_ms: 15.033\n",
      "    sample_throughput: 51.144\n",
      "    sample_time_ms: 19552.826\n",
      "    update_time_ms: 1.681\n",
      "  timestamp: 1633816009\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 25\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         464.675</td><td style=\"text-align: right;\">25000</td><td style=\"text-align: right;\">   -50.9</td><td style=\"text-align: right;\">               -36.3</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           503.796</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 26000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-47-06\n",
      "  done: false\n",
      "  episode_len_mean: 502.84313725490193\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -36.300000000000246\n",
      "  episode_reward_mean: -50.79411764705927\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 51\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0699371735254926\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014924769289720126\n",
      "          policy_loss: -0.0762119311425421\n",
      "          total_loss: 0.6686217622624503\n",
      "          vf_explained_var: 0.6756674647331238\n",
      "          vf_loss: 0.7598663098282284\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 26000\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.888000000000005\n",
      "    ram_util_percent: 90.49600000000002\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03704476200674617\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.26409407875034\n",
      "    mean_inference_ms: 1.3340188841987914\n",
      "    mean_raw_obs_processing_ms: 0.5493591123011513\n",
      "  time_since_restore: 482.3000247478485\n",
      "  time_this_iter_s: 17.625237941741943\n",
      "  time_total_s: 482.3000247478485\n",
      "  timers:\n",
      "    learn_throughput: 1505.936\n",
      "    learn_time_ms: 664.039\n",
      "    load_throughput: 70199.319\n",
      "    load_time_ms: 14.245\n",
      "    sample_throughput: 56.236\n",
      "    sample_time_ms: 17782.05\n",
      "    update_time_ms: 1.677\n",
      "  timestamp: 1633816026\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 26\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">           482.3</td><td style=\"text-align: right;\">26000</td><td style=\"text-align: right;\">-50.7941</td><td style=\"text-align: right;\">               -36.3</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           502.843</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 27000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-47-28\n",
      "  done: false\n",
      "  episode_len_mean: 497.462962962963\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -35.70000000000024\n",
      "  episode_reward_mean: -50.187037037037484\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 54\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9458345068825615\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00931491688109034\n",
      "          policy_loss: 0.1026957134405772\n",
      "          total_loss: 0.5781308208902677\n",
      "          vf_explained_var: 0.5016123652458191\n",
      "          vf_loss: 0.4913566998309559\n",
      "    num_agent_steps_sampled: 27000\n",
      "    num_agent_steps_trained: 27000\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 27000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.851612903225806\n",
      "    ram_util_percent: 90.83548387096776\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03705669564082171\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 19.070705477807273\n",
      "    mean_inference_ms: 1.3341094988797677\n",
      "    mean_raw_obs_processing_ms: 0.5637412199169947\n",
      "  time_since_restore: 503.95320105552673\n",
      "  time_this_iter_s: 21.653176307678223\n",
      "  time_total_s: 503.95320105552673\n",
      "  timers:\n",
      "    learn_throughput: 1517.053\n",
      "    learn_time_ms: 659.173\n",
      "    load_throughput: 70614.626\n",
      "    load_time_ms: 14.161\n",
      "    sample_throughput: 55.504\n",
      "    sample_time_ms: 18016.613\n",
      "    update_time_ms: 1.671\n",
      "  timestamp: 1633816048\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 27\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         503.953</td><td style=\"text-align: right;\">27000</td><td style=\"text-align: right;\"> -50.187</td><td style=\"text-align: right;\">               -35.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           497.463</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-47-51\n",
      "  done: false\n",
      "  episode_len_mean: 490.7719298245614\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -35.00000000000023\n",
      "  episode_reward_mean: -49.49473684210569\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 57\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9411522560649448\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011518302966845716\n",
      "          policy_loss: 0.005967400802506341\n",
      "          total_loss: 0.6680805901686351\n",
      "          vf_explained_var: 0.4502255320549011\n",
      "          vf_loss: 0.6771513478623497\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.16666666666666\n",
      "    ram_util_percent: 90.66363636363637\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03706752459515642\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.906493997321217\n",
      "    mean_inference_ms: 1.3342067678228886\n",
      "    mean_raw_obs_processing_ms: 0.5754135638045851\n",
      "  time_since_restore: 526.706910610199\n",
      "  time_this_iter_s: 22.75370955467224\n",
      "  time_total_s: 526.706910610199\n",
      "  timers:\n",
      "    learn_throughput: 1508.97\n",
      "    learn_time_ms: 662.704\n",
      "    load_throughput: 66315.623\n",
      "    load_time_ms: 15.079\n",
      "    sample_throughput: 53.74\n",
      "    sample_time_ms: 18608.044\n",
      "    update_time_ms: 1.681\n",
      "  timestamp: 1633816071\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 28\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/15.6 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         526.707</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">-49.4947</td><td style=\"text-align: right;\">                 -35</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           490.772</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=258822)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=258822)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 29000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-48-31\n",
      "  done: false\n",
      "  episode_len_mean: 483.23333333333335\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -48.72000000000042\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 60\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9043640004263984\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010205370808529936\n",
      "          policy_loss: -0.009177690082126193\n",
      "          total_loss: 0.7675500459141201\n",
      "          vf_explained_var: 0.5658093094825745\n",
      "          vf_loss: 0.7918965222934882\n",
      "    num_agent_steps_sampled: 29000\n",
      "    num_agent_steps_trained: 29000\n",
      "    num_steps_sampled: 29000\n",
      "    num_steps_trained: 29000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.76724137931034\n",
      "    ram_util_percent: 83.67413793103448\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03707722542787737\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.767377458019048\n",
      "    mean_inference_ms: 1.334309775986346\n",
      "    mean_raw_obs_processing_ms: 0.6144473841825985\n",
      "  time_since_restore: 567.1028492450714\n",
      "  time_this_iter_s: 40.39593863487244\n",
      "  time_total_s: 567.1028492450714\n",
      "  timers:\n",
      "    learn_throughput: 1508.171\n",
      "    learn_time_ms: 663.055\n",
      "    load_throughput: 65962.281\n",
      "    load_time_ms: 15.16\n",
      "    sample_throughput: 48.828\n",
      "    sample_time_ms: 20479.966\n",
      "    update_time_ms: 1.683\n",
      "  timestamp: 1633816111\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 29\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         567.103</td><td style=\"text-align: right;\">29000</td><td style=\"text-align: right;\">  -48.72</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           483.233</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-48-55\n",
      "  done: false\n",
      "  episode_len_mean: 479.03225806451616\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -48.28709677419396\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 62\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9573316706551447\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00982629216524155\n",
      "          policy_loss: -0.11412738396061792\n",
      "          total_loss: 0.653442473212878\n",
      "          vf_explained_var: 0.5345725417137146\n",
      "          vf_loss: 0.7834122449159622\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 30000\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.932352941176475\n",
      "    ram_util_percent: 73.47058823529412\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037083313573864574\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.688314677935324\n",
      "    mean_inference_ms: 1.3344446401586942\n",
      "    mean_raw_obs_processing_ms: 0.6370665278338159\n",
      "  time_since_restore: 591.2824878692627\n",
      "  time_this_iter_s: 24.179638624191284\n",
      "  time_total_s: 591.2824878692627\n",
      "  timers:\n",
      "    learn_throughput: 1508.604\n",
      "    learn_time_ms: 662.865\n",
      "    load_throughput: 68735.644\n",
      "    load_time_ms: 14.548\n",
      "    sample_throughput: 47.661\n",
      "    sample_time_ms: 20981.73\n",
      "    update_time_ms: 1.666\n",
      "  timestamp: 1633816135\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 30\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         591.282</td><td style=\"text-align: right;\">30000</td><td style=\"text-align: right;\">-48.2871</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           479.032</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 31000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-49-17\n",
      "  done: false\n",
      "  episode_len_mean: 474.3538461538462\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -47.80153846153887\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 65\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9874671207533943\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00811639943051811\n",
      "          policy_loss: 0.06259272752536668\n",
      "          total_loss: 0.8514562868409686\n",
      "          vf_explained_var: 0.4475015103816986\n",
      "          vf_loss: 0.8056565299216244\n",
      "    num_agent_steps_sampled: 31000\n",
      "    num_agent_steps_trained: 31000\n",
      "    num_steps_sampled: 31000\n",
      "    num_steps_trained: 31000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.370000000000005\n",
      "    ram_util_percent: 73.59666666666665\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709092487656011\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.58273877541654\n",
      "    mean_inference_ms: 1.3346379094001048\n",
      "    mean_raw_obs_processing_ms: 0.6666822990911431\n",
      "  time_since_restore: 612.4633333683014\n",
      "  time_this_iter_s: 21.180845499038696\n",
      "  time_total_s: 612.4633333683014\n",
      "  timers:\n",
      "    learn_throughput: 1512.422\n",
      "    learn_time_ms: 661.191\n",
      "    load_throughput: 67668.946\n",
      "    load_time_ms: 14.778\n",
      "    sample_throughput: 46.855\n",
      "    sample_time_ms: 21342.561\n",
      "    update_time_ms: 1.658\n",
      "  timestamp: 1633816157\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 31\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         612.463</td><td style=\"text-align: right;\">31000</td><td style=\"text-align: right;\">-47.8015</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           474.354</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-49-38\n",
      "  done: false\n",
      "  episode_len_mean: 469.3970588235294\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -47.28970588235333\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 68\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9932178139686585\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011011475211836668\n",
      "          policy_loss: 0.021861753861109414\n",
      "          total_loss: 0.9240744481484096\n",
      "          vf_explained_var: -0.0689140036702156\n",
      "          vf_loss: 0.917963960270087\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.24516129032257\n",
      "    ram_util_percent: 73.63225806451611\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709755286909162\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.49038189023858\n",
      "    mean_inference_ms: 1.3348133004103702\n",
      "    mean_raw_obs_processing_ms: 0.6921541102045291\n",
      "  time_since_restore: 633.957615852356\n",
      "  time_this_iter_s: 21.494282484054565\n",
      "  time_total_s: 633.957615852356\n",
      "  timers:\n",
      "    learn_throughput: 1508.821\n",
      "    learn_time_ms: 662.769\n",
      "    load_throughput: 67766.47\n",
      "    load_time_ms: 14.757\n",
      "    sample_throughput: 46.247\n",
      "    sample_time_ms: 21622.875\n",
      "    update_time_ms: 1.663\n",
      "  timestamp: 1633816178\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 32\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         633.958</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">-47.2897</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           469.397</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 33000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-50-00\n",
      "  done: false\n",
      "  episode_len_mean: 464.76056338028167\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -46.811267605634185\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 71\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9691729876730177\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01191372856258314\n",
      "          policy_loss: 0.013771491911676195\n",
      "          total_loss: 0.7221578114562565\n",
      "          vf_explained_var: 0.5251137614250183\n",
      "          vf_loss: 0.7235545557406213\n",
      "    num_agent_steps_sampled: 33000\n",
      "    num_agent_steps_trained: 33000\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 33000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.584375\n",
      "    ram_util_percent: 73.70937500000001\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710322917381716\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.410533478945162\n",
      "    mean_inference_ms: 1.3349739677260524\n",
      "    mean_raw_obs_processing_ms: 0.7141071880056317\n",
      "  time_since_restore: 656.389217376709\n",
      "  time_this_iter_s: 22.431601524353027\n",
      "  time_total_s: 656.389217376709\n",
      "  timers:\n",
      "    learn_throughput: 1505.832\n",
      "    learn_time_ms: 664.085\n",
      "    load_throughput: 63616.858\n",
      "    load_time_ms: 15.719\n",
      "    sample_throughput: 45.032\n",
      "    sample_time_ms: 22206.407\n",
      "    update_time_ms: 1.669\n",
      "  timestamp: 1633816200\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 33\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         656.389</td><td style=\"text-align: right;\">33000</td><td style=\"text-align: right;\">-46.8113</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           464.761</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 34000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-50-22\n",
      "  done: false\n",
      "  episode_len_mean: 462.26027397260276\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -46.552054794520934\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 73\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9500460863113402\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012578289635201086\n",
      "          policy_loss: -0.06122422019640605\n",
      "          total_loss: 0.8698786520295673\n",
      "          vf_explained_var: 0.4047825336456299\n",
      "          vf_loss: 0.9458275084694227\n",
      "    num_agent_steps_sampled: 34000\n",
      "    num_agent_steps_trained: 34000\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 34000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.24516129032258\n",
      "    ram_util_percent: 73.89032258064519\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710640514085376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.363055013465658\n",
      "    mean_inference_ms: 1.3350694271643002\n",
      "    mean_raw_obs_processing_ms: 0.7268812025792543\n",
      "  time_since_restore: 677.8345515727997\n",
      "  time_this_iter_s: 21.4453341960907\n",
      "  time_total_s: 677.8345515727997\n",
      "  timers:\n",
      "    learn_throughput: 1505.39\n",
      "    learn_time_ms: 664.28\n",
      "    load_throughput: 63775.594\n",
      "    load_time_ms: 15.68\n",
      "    sample_throughput: 44.55\n",
      "    sample_time_ms: 22446.662\n",
      "    update_time_ms: 1.671\n",
      "  timestamp: 1633816222\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 34\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         677.835</td><td style=\"text-align: right;\">34000</td><td style=\"text-align: right;\">-46.5521</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">            462.26</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 35000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-50-48\n",
      "  done: false\n",
      "  episode_len_mean: 457.13157894736844\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -46.02631578947407\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 76\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7983195490307278\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009092985089838615\n",
      "          policy_loss: -0.07037640313307444\n",
      "          total_loss: 1.0627915302912394\n",
      "          vf_explained_var: 0.5226301550865173\n",
      "          vf_loss: 1.1476986255910662\n",
      "    num_agent_steps_sampled: 35000\n",
      "    num_agent_steps_trained: 35000\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 35000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.8054054054054\n",
      "    ram_util_percent: 74.20540540540537\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03711148232782898\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.304311828588474\n",
      "    mean_inference_ms: 1.3352282849476202\n",
      "    mean_raw_obs_processing_ms: 0.7436475978177869\n",
      "  time_since_restore: 703.7070055007935\n",
      "  time_this_iter_s: 25.872453927993774\n",
      "  time_total_s: 703.7070055007935\n",
      "  timers:\n",
      "    learn_throughput: 1500.072\n",
      "    learn_time_ms: 666.635\n",
      "    load_throughput: 64215.174\n",
      "    load_time_ms: 15.573\n",
      "    sample_throughput: 43.073\n",
      "    sample_time_ms: 23216.519\n",
      "    update_time_ms: 1.682\n",
      "  timestamp: 1633816248\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 35\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         703.707</td><td style=\"text-align: right;\">35000</td><td style=\"text-align: right;\">-46.0263</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           457.132</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-51-12\n",
      "  done: false\n",
      "  episode_len_mean: 451.9240506329114\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -45.49367088607632\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 79\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6760740253660413\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0063294320572967735\n",
      "          policy_loss: -0.09740730979376369\n",
      "          total_loss: 0.5504085911644829\n",
      "          vf_explained_var: 0.8195704817771912\n",
      "          vf_loss: 0.6621734400590261\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.07352941176471\n",
      "    ram_util_percent: 74.21176470588235\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03711587764521919\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.255086951060836\n",
      "    mean_inference_ms: 1.3353941428478595\n",
      "    mean_raw_obs_processing_ms: 0.7581126141875008\n",
      "  time_since_restore: 727.6353900432587\n",
      "  time_this_iter_s: 23.92838454246521\n",
      "  time_total_s: 727.6353900432587\n",
      "  timers:\n",
      "    learn_throughput: 1519.892\n",
      "    learn_time_ms: 657.941\n",
      "    load_throughput: 60836.83\n",
      "    load_time_ms: 16.437\n",
      "    sample_throughput: 41.92\n",
      "    sample_time_ms: 23854.685\n",
      "    update_time_ms: 1.693\n",
      "  timestamp: 1633816272\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 36\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         727.635</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">-45.4937</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           451.924</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 37000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-51-34\n",
      "  done: false\n",
      "  episode_len_mean: 448.8658536585366\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -45.17682926829305\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 82\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8350398725933499\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01097473731536197\n",
      "          policy_loss: -0.04011894538998604\n",
      "          total_loss: 0.39322917229599424\n",
      "          vf_explained_var: 0.7785560488700867\n",
      "          vf_loss: 0.44753154582447474\n",
      "    num_agent_steps_sampled: 37000\n",
      "    num_agent_steps_trained: 37000\n",
      "    num_steps_sampled: 37000\n",
      "    num_steps_trained: 37000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.3375\n",
      "    ram_util_percent: 74.2875\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03711981973977066\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.212443551422055\n",
      "    mean_inference_ms: 1.3355494323971855\n",
      "    mean_raw_obs_processing_ms: 0.7706018435922962\n",
      "  time_since_restore: 749.9437835216522\n",
      "  time_this_iter_s: 22.308393478393555\n",
      "  time_total_s: 749.9437835216522\n",
      "  timers:\n",
      "    learn_throughput: 1515.173\n",
      "    learn_time_ms: 659.991\n",
      "    load_throughput: 60874.091\n",
      "    load_time_ms: 16.427\n",
      "    sample_throughput: 41.809\n",
      "    sample_time_ms: 23918.14\n",
      "    update_time_ms: 1.7\n",
      "  timestamp: 1633816294\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 37\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         749.944</td><td style=\"text-align: right;\">37000</td><td style=\"text-align: right;\">-45.1768</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           448.866</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 38000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-52-00\n",
      "  done: false\n",
      "  episode_len_mean: 444.50588235294117\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -44.73058823529448\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 85\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8151248627238803\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010426505586619329\n",
      "          policy_loss: 0.013093307945463392\n",
      "          total_loss: 0.4724164120025105\n",
      "          vf_explained_var: 0.8405457139015198\n",
      "          vf_loss: 0.4735155345665084\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 38000\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.31666666666667\n",
      "    ram_util_percent: 74.36666666666665\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03712322997797153\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.17852639835239\n",
      "    mean_inference_ms: 1.3356965320275334\n",
      "    mean_raw_obs_processing_ms: 0.7813476685994958\n",
      "  time_since_restore: 775.4353039264679\n",
      "  time_this_iter_s: 25.491520404815674\n",
      "  time_total_s: 775.4353039264679\n",
      "  timers:\n",
      "    learn_throughput: 1521.438\n",
      "    learn_time_ms: 657.273\n",
      "    load_throughput: 60771.073\n",
      "    load_time_ms: 16.455\n",
      "    sample_throughput: 41.331\n",
      "    sample_time_ms: 24194.639\n",
      "    update_time_ms: 1.686\n",
      "  timestamp: 1633816320\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 38\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         775.435</td><td style=\"text-align: right;\">38000</td><td style=\"text-align: right;\">-44.7306</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           444.506</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 39000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-52-24\n",
      "  done: false\n",
      "  episode_len_mean: 440.34090909090907\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -44.30454545454581\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 88\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8008758889304266\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008408555288075013\n",
      "          policy_loss: -0.04895925753646427\n",
      "          total_loss: 0.8659470114443037\n",
      "          vf_explained_var: 0.578240156173706\n",
      "          vf_loss: 0.9297224031554328\n",
      "    num_agent_steps_sampled: 39000\n",
      "    num_agent_steps_trained: 39000\n",
      "    num_steps_sampled: 39000\n",
      "    num_steps_trained: 39000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.35428571428571\n",
      "    ram_util_percent: 74.39714285714288\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0371265243894139\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.151120890861133\n",
      "    mean_inference_ms: 1.3358379274393881\n",
      "    mean_raw_obs_processing_ms: 0.7905773729855305\n",
      "  time_since_restore: 799.744633436203\n",
      "  time_this_iter_s: 24.309329509735107\n",
      "  time_total_s: 799.744633436203\n",
      "  timers:\n",
      "    learn_throughput: 1523.204\n",
      "    learn_time_ms: 656.511\n",
      "    load_throughput: 60599.858\n",
      "    load_time_ms: 16.502\n",
      "    sample_throughput: 44.274\n",
      "    sample_time_ms: 22586.683\n",
      "    update_time_ms: 1.678\n",
      "  timestamp: 1633816344\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 39\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         799.745</td><td style=\"text-align: right;\">39000</td><td style=\"text-align: right;\">-44.3045</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           440.341</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=258822)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=258822)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-53-05\n",
      "  done: false\n",
      "  episode_len_mean: 436.5054945054945\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -43.91208791208826\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 91\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.737001609802246\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007267331520050351\n",
      "          policy_loss: -0.07009933408763673\n",
      "          total_loss: 0.6155373142825232\n",
      "          vf_explained_var: 0.6677569150924683\n",
      "          vf_loss: 0.7002473437123828\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.97966101694916\n",
      "    ram_util_percent: 74.34237288135594\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037129158408043365\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.129785283199528\n",
      "    mean_inference_ms: 1.3359693739298926\n",
      "    mean_raw_obs_processing_ms: 0.8120421661552771\n",
      "  time_since_restore: 840.9986639022827\n",
      "  time_this_iter_s: 41.25403046607971\n",
      "  time_total_s: 840.9986639022827\n",
      "  timers:\n",
      "    learn_throughput: 1526.498\n",
      "    learn_time_ms: 655.094\n",
      "    load_throughput: 58597.67\n",
      "    load_time_ms: 17.066\n",
      "    sample_throughput: 41.161\n",
      "    sample_time_ms: 24294.971\n",
      "    update_time_ms: 1.686\n",
      "  timestamp: 1633816385\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 40\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         840.999</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">-43.9121</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           436.505</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 41000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-53-32\n",
      "  done: false\n",
      "  episode_len_mean: 431.2315789473684\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -43.37368421052666\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 95\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.5262902683681911\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0052532847997676865\n",
      "          policy_loss: -0.03928265016939905\n",
      "          total_loss: 0.4347560207049052\n",
      "          vf_explained_var: 0.8780453205108643\n",
      "          vf_loss: 0.4873069680399365\n",
      "    num_agent_steps_sampled: 41000\n",
      "    num_agent_steps_trained: 41000\n",
      "    num_steps_sampled: 41000\n",
      "    num_steps_trained: 41000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.57105263157895\n",
      "    ram_util_percent: 74.26842105263157\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03713164186384004\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.11029759718882\n",
      "    mean_inference_ms: 1.3361361578998443\n",
      "    mean_raw_obs_processing_ms: 0.8373086671253979\n",
      "  time_since_restore: 867.5029954910278\n",
      "  time_this_iter_s: 26.504331588745117\n",
      "  time_total_s: 867.5029954910278\n",
      "  timers:\n",
      "    learn_throughput: 1524.186\n",
      "    learn_time_ms: 656.088\n",
      "    load_throughput: 58388.678\n",
      "    load_time_ms: 17.127\n",
      "    sample_throughput: 40.28\n",
      "    sample_time_ms: 24826.287\n",
      "    update_time_ms: 1.686\n",
      "  timestamp: 1633816412\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 41000\n",
      "  training_iteration: 41\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         867.503</td><td style=\"text-align: right;\">41000</td><td style=\"text-align: right;\">-43.3737</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           431.232</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 42000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-53-56\n",
      "  done: false\n",
      "  episode_len_mean: 428.2448979591837\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -43.06734693877585\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 98\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.7853638423813714\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01617741837293922\n",
      "          policy_loss: 0.01998936790559027\n",
      "          total_loss: 0.25848158357871903\n",
      "          vf_explained_var: 0.9446802735328674\n",
      "          vf_loss: 0.2502034905884001\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 42000\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.17941176470589\n",
      "    ram_util_percent: 73.86470588235294\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03713304500905331\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 18.09976651811157\n",
      "    mean_inference_ms: 1.3362518342259588\n",
      "    mean_raw_obs_processing_ms: 0.8540078683234547\n",
      "  time_since_restore: 891.6064462661743\n",
      "  time_this_iter_s: 24.103450775146484\n",
      "  time_total_s: 891.6064462661743\n",
      "  timers:\n",
      "    learn_throughput: 1526.791\n",
      "    learn_time_ms: 654.968\n",
      "    load_throughput: 58444.247\n",
      "    load_time_ms: 17.11\n",
      "    sample_throughput: 39.859\n",
      "    sample_time_ms: 25088.327\n",
      "    update_time_ms: 1.69\n",
      "  timestamp: 1633816436\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 42\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         891.606</td><td style=\"text-align: right;\">42000</td><td style=\"text-align: right;\">-43.0673</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">           428.245</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 43000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-54-22\n",
      "  done: false\n",
      "  episode_len_mean: 423.63\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -42.60100000000033\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 101\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.6356724964247809\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008558793435173071\n",
      "          policy_loss: 0.0613114879363113\n",
      "          total_loss: 0.4004164681666427\n",
      "          vf_explained_var: 0.876855194568634\n",
      "          vf_loss: 0.35221203598711226\n",
      "    num_agent_steps_sampled: 43000\n",
      "    num_agent_steps_trained: 43000\n",
      "    num_steps_sampled: 43000\n",
      "    num_steps_trained: 43000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.32972972972973\n",
      "    ram_util_percent: 73.60540540540539\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03711430316169594\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.679680852029072\n",
      "    mean_inference_ms: 1.3353348036922315\n",
      "    mean_raw_obs_processing_ms: 0.8759378599921481\n",
      "  time_since_restore: 917.2961947917938\n",
      "  time_this_iter_s: 25.689748525619507\n",
      "  time_total_s: 917.2961947917938\n",
      "  timers:\n",
      "    learn_throughput: 1531.436\n",
      "    learn_time_ms: 652.982\n",
      "    load_throughput: 58238.935\n",
      "    load_time_ms: 17.171\n",
      "    sample_throughput: 39.345\n",
      "    sample_time_ms: 25416.032\n",
      "    update_time_ms: 1.713\n",
      "  timestamp: 1633816462\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 43000\n",
      "  training_iteration: 43\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         917.296</td><td style=\"text-align: right;\">43000</td><td style=\"text-align: right;\"> -42.601</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">            423.63</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-54-45\n",
      "  done: false\n",
      "  episode_len_mean: 419.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -42.14000000000032\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 104\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3796875000000002\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8803703917397394\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.030427328131540025\n",
      "          policy_loss: -0.034681281157665785\n",
      "          total_loss: 0.1063942385216554\n",
      "          vf_explained_var: 0.976069986820221\n",
      "          vf_loss: 0.14832634689907234\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.6\n",
      "    ram_util_percent: 73.80000000000001\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708413890666215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.98987438840227\n",
      "    mean_inference_ms: 1.3337248475294283\n",
      "    mean_raw_obs_processing_ms: 0.9110650275896625\n",
      "  time_since_restore: 941.1297843456268\n",
      "  time_this_iter_s: 23.833589553833008\n",
      "  time_total_s: 941.1297843456268\n",
      "  timers:\n",
      "    learn_throughput: 1533.909\n",
      "    learn_time_ms: 651.929\n",
      "    load_throughput: 58180.93\n",
      "    load_time_ms: 17.188\n",
      "    sample_throughput: 38.977\n",
      "    sample_time_ms: 25655.864\n",
      "    update_time_ms: 1.715\n",
      "  timestamp: 1633816485\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 44\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">          941.13</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  -42.14</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">            419.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 45000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-55-08\n",
      "  done: false\n",
      "  episode_len_mean: 415.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -41.751000000000325\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 106\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8246644934018452\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01563561029974877\n",
      "          policy_loss: 0.00910200575987498\n",
      "          total_loss: 0.2106970423211654\n",
      "          vf_explained_var: 0.9415456056594849\n",
      "          vf_loss: 0.21093671491576566\n",
      "    num_agent_steps_sampled: 45000\n",
      "    num_agent_steps_trained: 45000\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 45000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.3875\n",
      "    ram_util_percent: 73.9125\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037081703928939946\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.834411108283764\n",
      "    mean_inference_ms: 1.333450032551179\n",
      "    mean_raw_obs_processing_ms: 0.933916495022314\n",
      "  time_since_restore: 964.0620291233063\n",
      "  time_this_iter_s: 22.932244777679443\n",
      "  time_total_s: 964.0620291233063\n",
      "  timers:\n",
      "    learn_throughput: 1539.458\n",
      "    learn_time_ms: 649.579\n",
      "    load_throughput: 57902.864\n",
      "    load_time_ms: 17.27\n",
      "    sample_throughput: 39.426\n",
      "    sample_time_ms: 25364.082\n",
      "    update_time_ms: 1.712\n",
      "  timestamp: 1633816508\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 45\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         964.062</td><td style=\"text-align: right;\">45000</td><td style=\"text-align: right;\"> -41.751</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">            415.13</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 46000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-55-29\n",
      "  done: false\n",
      "  episode_len_mean: 411.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -41.34300000000031\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 109\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.039301155673133\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010096572644339118\n",
      "          policy_loss: -0.08162537415822348\n",
      "          total_loss: 0.25781638564334974\n",
      "          vf_explained_var: 0.7590982913970947\n",
      "          vf_loss: 0.3540844584504763\n",
      "    num_agent_steps_sampled: 46000\n",
      "    num_agent_steps_trained: 46000\n",
      "    num_steps_sampled: 46000\n",
      "    num_steps_trained: 46000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.28666666666666\n",
      "    ram_util_percent: 74.07333333333332\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708470000511163\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.70519419631833\n",
      "    mean_inference_ms: 1.333400590180295\n",
      "    mean_raw_obs_processing_ms: 0.9672566340252489\n",
      "  time_since_restore: 984.6310913562775\n",
      "  time_this_iter_s: 20.56906223297119\n",
      "  time_total_s: 984.6310913562775\n",
      "  timers:\n",
      "    learn_throughput: 1536.489\n",
      "    learn_time_ms: 650.834\n",
      "    load_throughput: 57959.034\n",
      "    load_time_ms: 17.254\n",
      "    sample_throughput: 39.957\n",
      "    sample_time_ms: 25026.935\n",
      "    update_time_ms: 1.709\n",
      "  timestamp: 1633816529\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 46\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         984.631</td><td style=\"text-align: right;\">46000</td><td style=\"text-align: right;\"> -41.343</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">            411.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 47000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-55-49\n",
      "  done: false\n",
      "  episode_len_mean: 410.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -41.255000000000315\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 111\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9652636978361342\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008119518024797361\n",
      "          policy_loss: 0.11208021168907484\n",
      "          total_loss: 0.30656674719519084\n",
      "          vf_explained_var: 0.9170020818710327\n",
      "          vf_loss: 0.2095148532754845\n",
      "    num_agent_steps_sampled: 47000\n",
      "    num_agent_steps_trained: 47000\n",
      "    num_steps_sampled: 47000\n",
      "    num_steps_trained: 47000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.24827586206896\n",
      "    ram_util_percent: 74.26896551724138\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037089154646828215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.656214667160675\n",
      "    mean_inference_ms: 1.333468860626199\n",
      "    mean_raw_obs_processing_ms: 0.988950460926931\n",
      "  time_since_restore: 1005.0915615558624\n",
      "  time_this_iter_s: 20.46047019958496\n",
      "  time_total_s: 1005.0915615558624\n",
      "  timers:\n",
      "    learn_throughput: 1539.457\n",
      "    learn_time_ms: 649.58\n",
      "    load_throughput: 57933.896\n",
      "    load_time_ms: 17.261\n",
      "    sample_throughput: 40.252\n",
      "    sample_time_ms: 24843.398\n",
      "    update_time_ms: 1.707\n",
      "  timestamp: 1633816549\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47000\n",
      "  training_iteration: 47\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         1005.09</td><td style=\"text-align: right;\">47000</td><td style=\"text-align: right;\"> -41.255</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">            410.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-56-08\n",
      "  done: false\n",
      "  episode_len_mean: 409.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -41.20800000000032\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 113\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1435962292883133\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008334116767951254\n",
      "          policy_loss: 0.07305906431542503\n",
      "          total_loss: 0.5442634800242053\n",
      "          vf_explained_var: 0.26417356729507446\n",
      "          vf_loss: 0.48789383934603797\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.670370370370364\n",
      "    ram_util_percent: 74.38888888888889\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03709561835153491\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.62781753649122\n",
      "    mean_inference_ms: 1.333635367017444\n",
      "    mean_raw_obs_processing_ms: 1.010106093437892\n",
      "  time_since_restore: 1023.6323070526123\n",
      "  time_this_iter_s: 18.540745496749878\n",
      "  time_total_s: 1023.6323070526123\n",
      "  timers:\n",
      "    learn_throughput: 1536.159\n",
      "    learn_time_ms: 650.974\n",
      "    load_throughput: 57594.525\n",
      "    load_time_ms: 17.363\n",
      "    sample_throughput: 41.413\n",
      "    sample_time_ms: 24146.815\n",
      "    update_time_ms: 1.713\n",
      "  timestamp: 1633816568\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 48\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         1023.63</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> -41.208</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">             409.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 49000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-56-26\n",
      "  done: false\n",
      "  episode_len_mean: 407.75\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -41.01300000000031\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 115\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1908050960964625\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01012051038408208\n",
      "          policy_loss: -0.05652474938995308\n",
      "          total_loss: 0.4658586450748973\n",
      "          vf_explained_var: 0.777675986289978\n",
      "          vf_loss: 0.5385275040235784\n",
      "    num_agent_steps_sampled: 49000\n",
      "    num_agent_steps_trained: 49000\n",
      "    num_steps_sampled: 49000\n",
      "    num_steps_trained: 49000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.47200000000001\n",
      "    ram_util_percent: 74.284\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710171851570974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.619980863983695\n",
      "    mean_inference_ms: 1.3338764324569707\n",
      "    mean_raw_obs_processing_ms: 1.0307784210992716\n",
      "  time_since_restore: 1041.203096628189\n",
      "  time_this_iter_s: 17.570789575576782\n",
      "  time_total_s: 1041.203096628189\n",
      "  timers:\n",
      "    learn_throughput: 1538.284\n",
      "    learn_time_ms: 650.075\n",
      "    load_throughput: 52761.461\n",
      "    load_time_ms: 18.953\n",
      "    sample_throughput: 42.604\n",
      "    sample_time_ms: 23472.25\n",
      "    update_time_ms: 1.735\n",
      "  timestamp: 1633816586\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49000\n",
      "  training_iteration: 49\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">          1041.2</td><td style=\"text-align: right;\">49000</td><td style=\"text-align: right;\"> -41.013</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">            407.75</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 50000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-56-43\n",
      "  done: false\n",
      "  episode_len_mean: 406.45\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -40.88300000000031\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 117\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1861447042889064\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00847109909474963\n",
      "          policy_loss: -0.06938267631663217\n",
      "          total_loss: 0.5775346640083525\n",
      "          vf_explained_var: 0.6911901831626892\n",
      "          vf_loss: 0.663954238138265\n",
      "    num_agent_steps_sampled: 50000\n",
      "    num_agent_steps_trained: 50000\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.5625\n",
      "    ram_util_percent: 74.42083333333333\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037109375835112965\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.633513449234584\n",
      "    mean_inference_ms: 1.3342256283247624\n",
      "    mean_raw_obs_processing_ms: 1.0510558643023808\n",
      "  time_since_restore: 1058.1880142688751\n",
      "  time_this_iter_s: 16.984917640686035\n",
      "  time_total_s: 1058.1880142688751\n",
      "  timers:\n",
      "    learn_throughput: 1537.217\n",
      "    learn_time_ms: 650.526\n",
      "    load_throughput: 55428.522\n",
      "    load_time_ms: 18.041\n",
      "    sample_throughput: 47.515\n",
      "    sample_time_ms: 21045.769\n",
      "    update_time_ms: 1.745\n",
      "  timestamp: 1633816603\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 50\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         1058.19</td><td style=\"text-align: right;\">50000</td><td style=\"text-align: right;\"> -40.883</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">            406.45</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 51000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-56-59\n",
      "  done: false\n",
      "  episode_len_mean: 404.14\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -40.652000000000314\n",
      "  episode_reward_min: -64.00000000000064\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 119\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1581727849112617\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00578170648519117\n",
      "          policy_loss: -0.12790673275788625\n",
      "          total_loss: 0.5679368125067816\n",
      "          vf_explained_var: 0.09014052152633667\n",
      "          vf_loss: 0.7141324101222886\n",
      "    num_agent_steps_sampled: 51000\n",
      "    num_agent_steps_trained: 51000\n",
      "    num_steps_sampled: 51000\n",
      "    num_steps_trained: 51000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.65833333333333\n",
      "    ram_util_percent: 74.4375\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037117858510951215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.658780843070257\n",
      "    mean_inference_ms: 1.3346315493357375\n",
      "    mean_raw_obs_processing_ms: 1.0708550161099561\n",
      "  time_since_restore: 1074.863367319107\n",
      "  time_this_iter_s: 16.675353050231934\n",
      "  time_total_s: 1074.863367319107\n",
      "  timers:\n",
      "    learn_throughput: 1541.691\n",
      "    learn_time_ms: 648.638\n",
      "    load_throughput: 57164.056\n",
      "    load_time_ms: 17.494\n",
      "    sample_throughput: 49.837\n",
      "    sample_time_ms: 20065.297\n",
      "    update_time_ms: 1.753\n",
      "  timestamp: 1633816619\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 51000\n",
      "  training_iteration: 51\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         1074.86</td><td style=\"text-align: right;\">51000</td><td style=\"text-align: right;\"> -40.652</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -64</td><td style=\"text-align: right;\">            404.14</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=258822)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=258822)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-57-34\n",
      "  done: false\n",
      "  episode_len_mean: 401.02\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -40.3400000000003\n",
      "  episode_reward_min: -62.00000000000055\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 121\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.147787425253126\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007125164759164903\n",
      "          policy_loss: -0.06005838844511244\n",
      "          total_loss: 0.7154328465461731\n",
      "          vf_explained_var: 0.3778996765613556\n",
      "          vf_loss: 0.7929111086659961\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.6530612244898\n",
      "    ram_util_percent: 74.41836734693878\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03712710440044463\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.694986408605317\n",
      "    mean_inference_ms: 1.3350871285347878\n",
      "    mean_raw_obs_processing_ms: 1.0966091625867818\n",
      "  time_since_restore: 1109.4477417469025\n",
      "  time_this_iter_s: 34.58437442779541\n",
      "  time_total_s: 1109.4477417469025\n",
      "  timers:\n",
      "    learn_throughput: 1543.695\n",
      "    learn_time_ms: 647.796\n",
      "    load_throughput: 57151.36\n",
      "    load_time_ms: 17.497\n",
      "    sample_throughput: 47.361\n",
      "    sample_time_ms: 21114.229\n",
      "    update_time_ms: 1.727\n",
      "  timestamp: 1633816654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 52\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         1109.45</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  -40.34</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -62</td><td style=\"text-align: right;\">            401.02</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 53000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-57-50\n",
      "  done: false\n",
      "  episode_len_mean: 398.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -40.0940000000003\n",
      "  episode_reward_min: -62.00000000000055\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 124\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.131542082627614\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012687394822211021\n",
      "          policy_loss: -0.025143651829825506\n",
      "          total_loss: 0.8133208925525347\n",
      "          vf_explained_var: 0.5456615686416626\n",
      "          vf_loss: 0.8525541102720632\n",
      "    num_agent_steps_sampled: 53000\n",
      "    num_agent_steps_trained: 53000\n",
      "    num_steps_sampled: 53000\n",
      "    num_steps_trained: 53000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.362500000000004\n",
      "    ram_util_percent: 74.47916666666667\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03714152623804316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.763687007882382\n",
      "    mean_inference_ms: 1.3358138154156893\n",
      "    mean_raw_obs_processing_ms: 1.134724822859826\n",
      "  time_since_restore: 1125.9854092597961\n",
      "  time_this_iter_s: 16.537667512893677\n",
      "  time_total_s: 1125.9854092597961\n",
      "  timers:\n",
      "    learn_throughput: 1543.687\n",
      "    learn_time_ms: 647.8\n",
      "    load_throughput: 60867.289\n",
      "    load_time_ms: 16.429\n",
      "    sample_throughput: 49.505\n",
      "    sample_time_ms: 20200.11\n",
      "    update_time_ms: 1.71\n",
      "  timestamp: 1633816670\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 53000\n",
      "  training_iteration: 53\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         1125.99</td><td style=\"text-align: right;\">53000</td><td style=\"text-align: right;\"> -40.094</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -62</td><td style=\"text-align: right;\">            398.79</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 54000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-58-06\n",
      "  done: false\n",
      "  episode_len_mean: 399.03\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -40.1180000000003\n",
      "  episode_reward_min: -62.00000000000055\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 125\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0822729931937323\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008244365119240468\n",
      "          policy_loss: -0.14581242174737982\n",
      "          total_loss: 0.3512969901578294\n",
      "          vf_explained_var: 0.6676865220069885\n",
      "          vf_loss: 0.5132367073661751\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 54000\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 54000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.709090909090904\n",
      "    ram_util_percent: 74.43181818181819\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03714623577871795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.787551440286805\n",
      "    mean_inference_ms: 1.336049224310214\n",
      "    mean_raw_obs_processing_ms: 1.147219287926256\n",
      "  time_since_restore: 1141.4644277095795\n",
      "  time_this_iter_s: 15.479018449783325\n",
      "  time_total_s: 1141.4644277095795\n",
      "  timers:\n",
      "    learn_throughput: 1542.916\n",
      "    learn_time_ms: 648.123\n",
      "    load_throughput: 66328.417\n",
      "    load_time_ms: 15.076\n",
      "    sample_throughput: 51.638\n",
      "    sample_time_ms: 19365.691\n",
      "    update_time_ms: 1.707\n",
      "  timestamp: 1633816686\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 54\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         1141.46</td><td style=\"text-align: right;\">54000</td><td style=\"text-align: right;\"> -40.118</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -62</td><td style=\"text-align: right;\">            399.03</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 55000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-58-23\n",
      "  done: false\n",
      "  episode_len_mean: 400.88\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -40.30300000000031\n",
      "  episode_reward_min: -62.00000000000055\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 128\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.075615378220876\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.006908033913039022\n",
      "          policy_loss: 0.03954378763834635\n",
      "          total_loss: 0.621994104112188\n",
      "          vf_explained_var: 0.6130428910255432\n",
      "          vf_loss: 0.5992721346517403\n",
      "    num_agent_steps_sampled: 55000\n",
      "    num_agent_steps_trained: 55000\n",
      "    num_steps_sampled: 55000\n",
      "    num_steps_trained: 55000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.55416666666667\n",
      "    ram_util_percent: 74.4\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03715816951375337\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.8568250590466\n",
      "    mean_inference_ms: 1.336670892901785\n",
      "    mean_raw_obs_processing_ms: 1.1840352977475392\n",
      "  time_since_restore: 1158.1918280124664\n",
      "  time_this_iter_s: 16.727400302886963\n",
      "  time_total_s: 1158.1918280124664\n",
      "  timers:\n",
      "    learn_throughput: 1544.942\n",
      "    learn_time_ms: 647.274\n",
      "    load_throughput: 70083.897\n",
      "    load_time_ms: 14.269\n",
      "    sample_throughput: 53.342\n",
      "    sample_time_ms: 18746.838\n",
      "    update_time_ms: 1.699\n",
      "  timestamp: 1633816703\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55000\n",
      "  training_iteration: 55\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         1158.19</td><td style=\"text-align: right;\">55000</td><td style=\"text-align: right;\"> -40.303</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -62</td><td style=\"text-align: right;\">            400.88</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-58-38\n",
      "  done: false\n",
      "  episode_len_mean: 402.43\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -40.458000000000304\n",
      "  episode_reward_min: -62.00000000000055\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 130\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8092089388105603\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009829429845432043\n",
      "          policy_loss: 0.11363618593249056\n",
      "          total_loss: 0.25625798896782925\n",
      "          vf_explained_var: 0.8377797603607178\n",
      "          vf_loss: 0.15511572683850924\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.76818181818181\n",
      "    ram_util_percent: 74.4409090909091\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03716331933629823\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.90096696185805\n",
      "    mean_inference_ms: 1.3370135029779353\n",
      "    mean_raw_obs_processing_ms: 1.1864986794492427\n",
      "  time_since_restore: 1173.6434569358826\n",
      "  time_this_iter_s: 15.451628923416138\n",
      "  time_total_s: 1173.6434569358826\n",
      "  timers:\n",
      "    learn_throughput: 1549.262\n",
      "    learn_time_ms: 645.469\n",
      "    load_throughput: 77527.292\n",
      "    load_time_ms: 12.899\n",
      "    sample_throughput: 54.83\n",
      "    sample_time_ms: 18238.276\n",
      "    update_time_ms: 1.692\n",
      "  timestamp: 1633816718\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 56\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         1173.64</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\"> -40.458</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -62</td><td style=\"text-align: right;\">            402.43</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 57000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-58-56\n",
      "  done: false\n",
      "  episode_len_mean: 401.83\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -40.334000000000295\n",
      "  episode_reward_min: -62.00000000000055\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 132\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.123471956782871\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.005531791952262862\n",
      "          policy_loss: 0.08648552331659529\n",
      "          total_loss: 0.7289005551073287\n",
      "          vf_explained_var: -0.34066012501716614\n",
      "          vf_loss: 0.6604992133047846\n",
      "    num_agent_steps_sampled: 57000\n",
      "    num_agent_steps_trained: 57000\n",
      "    num_steps_sampled: 57000\n",
      "    num_steps_trained: 57000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.476000000000006\n",
      "    ram_util_percent: 74.612\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03716768990818124\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.942252229601383\n",
      "    mean_inference_ms: 1.3373115311971315\n",
      "    mean_raw_obs_processing_ms: 1.1897956134471714\n",
      "  time_since_restore: 1190.9553215503693\n",
      "  time_this_iter_s: 17.311864614486694\n",
      "  time_total_s: 1190.9553215503693\n",
      "  timers:\n",
      "    learn_throughput: 1546.999\n",
      "    learn_time_ms: 646.413\n",
      "    load_throughput: 83808.306\n",
      "    load_time_ms: 11.932\n",
      "    sample_throughput: 55.793\n",
      "    sample_time_ms: 17923.45\n",
      "    update_time_ms: 1.685\n",
      "  timestamp: 1633816736\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 57000\n",
      "  training_iteration: 57\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         1190.96</td><td style=\"text-align: right;\">57000</td><td style=\"text-align: right;\"> -40.334</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -62</td><td style=\"text-align: right;\">            401.83</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 58000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-59-12\n",
      "  done: false\n",
      "  episode_len_mean: 400.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -40.1690000000003\n",
      "  episode_reward_min: -62.00000000000055\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 134\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5695312499999998\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1491546829541526\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004358949599387035\n",
      "          policy_loss: 0.09965233537885877\n",
      "          total_loss: 0.6599284920427534\n",
      "          vf_explained_var: -0.21627317368984222\n",
      "          vf_loss: 0.5792851437607573\n",
      "    num_agent_steps_sampled: 58000\n",
      "    num_agent_steps_trained: 58000\n",
      "    num_steps_sampled: 58000\n",
      "    num_steps_trained: 58000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.75416666666666\n",
      "    ram_util_percent: 74.53750000000001\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037170431170377896\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 16.98351758256256\n",
      "    mean_inference_ms: 1.3375605424238797\n",
      "    mean_raw_obs_processing_ms: 1.1937878356425393\n",
      "  time_since_restore: 1207.743050813675\n",
      "  time_this_iter_s: 16.787729263305664\n",
      "  time_total_s: 1207.743050813675\n",
      "  timers:\n",
      "    learn_throughput: 1549.022\n",
      "    learn_time_ms: 645.569\n",
      "    load_throughput: 92090.415\n",
      "    load_time_ms: 10.859\n",
      "    sample_throughput: 56.338\n",
      "    sample_time_ms: 17750.065\n",
      "    update_time_ms: 1.682\n",
      "  timestamp: 1633816752\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 58000\n",
      "  training_iteration: 58\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         1207.74</td><td style=\"text-align: right;\">58000</td><td style=\"text-align: right;\"> -40.169</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -62</td><td style=\"text-align: right;\">             400.9</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 59000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-59-29\n",
      "  done: false\n",
      "  episode_len_mean: 401.36\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -40.2070000000003\n",
      "  episode_reward_min: -62.00000000000055\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 136\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.15542604525884\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010967853677526607\n",
      "          policy_loss: 0.019697845230499902\n",
      "          total_loss: 0.653761335545116\n",
      "          vf_explained_var: -0.1710469126701355\n",
      "          vf_loss: 0.6524944908089108\n",
      "    num_agent_steps_sampled: 59000\n",
      "    num_agent_steps_trained: 59000\n",
      "    num_steps_sampled: 59000\n",
      "    num_steps_trained: 59000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.475\n",
      "    ram_util_percent: 74.5125\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03717233739772427\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.019649171531466\n",
      "    mean_inference_ms: 1.3377698979728325\n",
      "    mean_raw_obs_processing_ms: 1.198389306521469\n",
      "  time_since_restore: 1224.815356016159\n",
      "  time_this_iter_s: 17.07230520248413\n",
      "  time_total_s: 1224.815356016159\n",
      "  timers:\n",
      "    learn_throughput: 1541.582\n",
      "    learn_time_ms: 648.684\n",
      "    load_throughput: 123997.186\n",
      "    load_time_ms: 8.065\n",
      "    sample_throughput: 56.497\n",
      "    sample_time_ms: 17699.917\n",
      "    update_time_ms: 1.669\n",
      "  timestamp: 1633816769\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 59000\n",
      "  training_iteration: 59\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         1224.82</td><td style=\"text-align: right;\">59000</td><td style=\"text-align: right;\"> -40.207</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -62</td><td style=\"text-align: right;\">            401.36</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_21-59-48\n",
      "  done: false\n",
      "  episode_len_mean: 401.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -40.256000000000306\n",
      "  episode_reward_min: -62.00000000000055\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 138\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.034890209303962\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008560083596067712\n",
      "          policy_loss: -0.08711452649699317\n",
      "          total_loss: 1.2408103578620486\n",
      "          vf_explained_var: -0.03783407062292099\n",
      "          vf_loss: 1.3458361848360962\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.59615384615385\n",
      "    ram_util_percent: 74.52307692307691\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03717326447275693\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.053992297929888\n",
      "    mean_inference_ms: 1.3379475956457185\n",
      "    mean_raw_obs_processing_ms: 1.2034849422453704\n",
      "  time_since_restore: 1242.9292051792145\n",
      "  time_this_iter_s: 18.11384916305542\n",
      "  time_total_s: 1242.9292051792145\n",
      "  timers:\n",
      "    learn_throughput: 1542.019\n",
      "    learn_time_ms: 648.501\n",
      "    load_throughput: 110428.729\n",
      "    load_time_ms: 9.056\n",
      "    sample_throughput: 56.142\n",
      "    sample_time_ms: 17812.019\n",
      "    update_time_ms: 1.654\n",
      "  timestamp: 1633816788\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 60\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         1242.93</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\"> -40.256</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -62</td><td style=\"text-align: right;\">            401.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 61000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-00-06\n",
      "  done: false\n",
      "  episode_len_mean: 400.65\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -40.0980000000003\n",
      "  episode_reward_min: -62.00000000000055\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 141\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9878830605083042\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008583159674769513\n",
      "          policy_loss: 0.015368676516744825\n",
      "          total_loss: 1.2919688549306658\n",
      "          vf_explained_var: 0.13307806849479675\n",
      "          vf_loss: 1.294034821457333\n",
      "    num_agent_steps_sampled: 61000\n",
      "    num_agent_steps_trained: 61000\n",
      "    num_steps_sampled: 61000\n",
      "    num_steps_trained: 61000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.66400000000001\n",
      "    ram_util_percent: 74.57199999999999\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03717204026070159\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.10505775240882\n",
      "    mean_inference_ms: 1.3381525200002533\n",
      "    mean_raw_obs_processing_ms: 1.2114144287216355\n",
      "  time_since_restore: 1260.9316220283508\n",
      "  time_this_iter_s: 18.002416849136353\n",
      "  time_total_s: 1260.9316220283508\n",
      "  timers:\n",
      "    learn_throughput: 1540.802\n",
      "    learn_time_ms: 649.013\n",
      "    load_throughput: 104371.23\n",
      "    load_time_ms: 9.581\n",
      "    sample_throughput: 55.73\n",
      "    sample_time_ms: 17943.702\n",
      "    update_time_ms: 1.64\n",
      "  timestamp: 1633816806\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 61000\n",
      "  training_iteration: 61\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         1260.93</td><td style=\"text-align: right;\">61000</td><td style=\"text-align: right;\"> -40.098</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -62</td><td style=\"text-align: right;\">            400.65</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 62000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-00-25\n",
      "  done: false\n",
      "  episode_len_mean: 399.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -39.98100000000029\n",
      "  episode_reward_min: -62.00000000000055\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 143\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.037040620379978\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009103510756029342\n",
      "          policy_loss: 0.07812299985024664\n",
      "          total_loss: 0.5956764355301857\n",
      "          vf_explained_var: 0.5151048302650452\n",
      "          vf_loss: 0.5353314687601394\n",
      "    num_agent_steps_sampled: 62000\n",
      "    num_agent_steps_trained: 62000\n",
      "    num_steps_sampled: 62000\n",
      "    num_steps_trained: 62000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.146428571428565\n",
      "    ram_util_percent: 74.52142857142857\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03716865075026296\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.138406933854544\n",
      "    mean_inference_ms: 1.3382309352731045\n",
      "    mean_raw_obs_processing_ms: 1.2173736749231174\n",
      "  time_since_restore: 1280.17972946167\n",
      "  time_this_iter_s: 19.248107433319092\n",
      "  time_total_s: 1280.17972946167\n",
      "  timers:\n",
      "    learn_throughput: 1541.894\n",
      "    learn_time_ms: 648.553\n",
      "    load_throughput: 103784.982\n",
      "    load_time_ms: 9.635\n",
      "    sample_throughput: 60.937\n",
      "    sample_time_ms: 16410.488\n",
      "    update_time_ms: 1.648\n",
      "  timestamp: 1633816825\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 62000\n",
      "  training_iteration: 62\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         1280.18</td><td style=\"text-align: right;\">62000</td><td style=\"text-align: right;\"> -39.981</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">                 -62</td><td style=\"text-align: right;\">            399.34</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 63000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-00-43\n",
      "  done: false\n",
      "  episode_len_mean: 397.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -39.749000000000294\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 145\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0628636439641315\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010821728522433608\n",
      "          policy_loss: -0.0858194856180085\n",
      "          total_loss: 1.2004126409689586\n",
      "          vf_explained_var: 0.288672536611557\n",
      "          vf_loss: 1.3037790900303259\n",
      "    num_agent_steps_sampled: 63000\n",
      "    num_agent_steps_trained: 63000\n",
      "    num_steps_sampled: 63000\n",
      "    num_steps_trained: 63000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.483999999999995\n",
      "    ram_util_percent: 74.536\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0371647726654709\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.17234317644457\n",
      "    mean_inference_ms: 1.3382926343641444\n",
      "    mean_raw_obs_processing_ms: 1.2236615153038743\n",
      "  time_since_restore: 1297.9086818695068\n",
      "  time_this_iter_s: 17.728952407836914\n",
      "  time_total_s: 1297.9086818695068\n",
      "  timers:\n",
      "    learn_throughput: 1541.061\n",
      "    learn_time_ms: 648.903\n",
      "    load_throughput: 99143.232\n",
      "    load_time_ms: 10.086\n",
      "    sample_throughput: 60.5\n",
      "    sample_time_ms: 16528.796\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1633816843\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63000\n",
      "  training_iteration: 63\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         1297.91</td><td style=\"text-align: right;\">63000</td><td style=\"text-align: right;\"> -39.749</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            397.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-01-01\n",
      "  done: false\n",
      "  episode_len_mean: 398.06\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -39.758000000000294\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 147\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.052139268981086\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014457054935789977\n",
      "          policy_loss: -0.12988973491721684\n",
      "          total_loss: 1.0641609986623128\n",
      "          vf_explained_var: 0.14465342462062836\n",
      "          vf_loss: 1.210455266634623\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.71538461538462\n",
      "    ram_util_percent: 74.59615384615384\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03716136115942853\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.20482251954026\n",
      "    mean_inference_ms: 1.3383494059134773\n",
      "    mean_raw_obs_processing_ms: 1.2302564457452327\n",
      "  time_since_restore: 1315.9938275814056\n",
      "  time_this_iter_s: 18.085145711898804\n",
      "  time_total_s: 1315.9938275814056\n",
      "  timers:\n",
      "    learn_throughput: 1540.444\n",
      "    learn_time_ms: 649.163\n",
      "    load_throughput: 87562.113\n",
      "    load_time_ms: 11.42\n",
      "    sample_throughput: 59.567\n",
      "    sample_time_ms: 16787.814\n",
      "    update_time_ms: 1.639\n",
      "  timestamp: 1633816861\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 64\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         1315.99</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\"> -39.758</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            398.06</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=258822)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=258822)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 65000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-01-37\n",
      "  done: false\n",
      "  episode_len_mean: 396.17\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -39.5720000000003\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 150\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0689600083563064\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009594358430689611\n",
      "          policy_loss: 0.023103589316209157\n",
      "          total_loss: 1.057578941517406\n",
      "          vf_explained_var: 0.4577123820781708\n",
      "          vf_loss: 1.0524328203664886\n",
      "    num_agent_steps_sampled: 65000\n",
      "    num_agent_steps_trained: 65000\n",
      "    num_steps_sampled: 65000\n",
      "    num_steps_trained: 65000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.08627450980392\n",
      "    ram_util_percent: 74.4686274509804\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037156091588751244\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.253049397440552\n",
      "    mean_inference_ms: 1.3384312402153982\n",
      "    mean_raw_obs_processing_ms: 1.2486340844738932\n",
      "  time_since_restore: 1351.8551330566406\n",
      "  time_this_iter_s: 35.861305475234985\n",
      "  time_total_s: 1351.8551330566406\n",
      "  timers:\n",
      "    learn_throughput: 1534.593\n",
      "    learn_time_ms: 651.639\n",
      "    load_throughput: 81574.424\n",
      "    load_time_ms: 12.259\n",
      "    sample_throughput: 53.482\n",
      "    sample_time_ms: 18697.942\n",
      "    update_time_ms: 1.646\n",
      "  timestamp: 1633816897\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 65000\n",
      "  training_iteration: 65\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         1351.86</td><td style=\"text-align: right;\">65000</td><td style=\"text-align: right;\"> -39.572</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            396.17</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 66000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-01-56\n",
      "  done: false\n",
      "  episode_len_mean: 395.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -39.5640000000003\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 152\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9513062371148004\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010777636335551193\n",
      "          policy_loss: -0.06571590469943153\n",
      "          total_loss: 0.8590329286538892\n",
      "          vf_explained_var: 0.585188090801239\n",
      "          vf_loss: 0.9411927934322092\n",
      "    num_agent_steps_sampled: 66000\n",
      "    num_agent_steps_trained: 66000\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 66000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.27857142857143\n",
      "    ram_util_percent: 74.38571428571429\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03715218237409528\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.28397650457275\n",
      "    mean_inference_ms: 1.338466018517861\n",
      "    mean_raw_obs_processing_ms: 1.2610823097659145\n",
      "  time_since_restore: 1371.2667903900146\n",
      "  time_this_iter_s: 19.411657333374023\n",
      "  time_total_s: 1371.2667903900146\n",
      "  timers:\n",
      "    learn_throughput: 1528.31\n",
      "    learn_time_ms: 654.318\n",
      "    load_throughput: 73384.469\n",
      "    load_time_ms: 13.627\n",
      "    sample_throughput: 52.384\n",
      "    sample_time_ms: 19089.888\n",
      "    update_time_ms: 1.641\n",
      "  timestamp: 1633816916\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 66\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         1371.27</td><td style=\"text-align: right;\">66000</td><td style=\"text-align: right;\"> -39.564</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            395.87</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 67000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-02-14\n",
      "  done: false\n",
      "  episode_len_mean: 397.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -39.70100000000029\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 155\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9186053660180833\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007645272145573046\n",
      "          policy_loss: -0.0020075139072206283\n",
      "          total_loss: 1.0141999145348868\n",
      "          vf_explained_var: 0.0696999654173851\n",
      "          vf_loss: 1.0332163698143428\n",
      "    num_agent_steps_sampled: 67000\n",
      "    num_agent_steps_trained: 67000\n",
      "    num_steps_sampled: 67000\n",
      "    num_steps_trained: 67000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.55\n",
      "    ram_util_percent: 74.51538461538462\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03714568669627958\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.326036906620846\n",
      "    mean_inference_ms: 1.3384888961447559\n",
      "    mean_raw_obs_processing_ms: 1.2797698021402455\n",
      "  time_since_restore: 1389.4279725551605\n",
      "  time_this_iter_s: 18.161182165145874\n",
      "  time_total_s: 1389.4279725551605\n",
      "  timers:\n",
      "    learn_throughput: 1534.005\n",
      "    learn_time_ms: 651.888\n",
      "    load_throughput: 68720.216\n",
      "    load_time_ms: 14.552\n",
      "    sample_throughput: 52.148\n",
      "    sample_time_ms: 19176.335\n",
      "    update_time_ms: 1.637\n",
      "  timestamp: 1633816934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 67000\n",
      "  training_iteration: 67\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         1389.43</td><td style=\"text-align: right;\">67000</td><td style=\"text-align: right;\"> -39.701</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            397.24</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-02-32\n",
      "  done: false\n",
      "  episode_len_mean: 399.05\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -39.8820000000003\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 157\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.1004348516464235\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010303660046732412\n",
      "          policy_loss: 0.09761639502313402\n",
      "          total_loss: 0.6457213870353169\n",
      "          vf_explained_var: 0.7358344793319702\n",
      "          vf_loss: 0.5661752249010735\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.672000000000004\n",
      "    ram_util_percent: 74.44\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037141178967470795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.351244625273935\n",
      "    mean_inference_ms: 1.338497545654509\n",
      "    mean_raw_obs_processing_ms: 1.292152762836958\n",
      "  time_since_restore: 1406.823090314865\n",
      "  time_this_iter_s: 17.39511775970459\n",
      "  time_total_s: 1406.823090314865\n",
      "  timers:\n",
      "    learn_throughput: 1535.672\n",
      "    learn_time_ms: 651.181\n",
      "    load_throughput: 68723.931\n",
      "    load_time_ms: 14.551\n",
      "    sample_throughput: 51.981\n",
      "    sample_time_ms: 19237.783\n",
      "    update_time_ms: 1.638\n",
      "  timestamp: 1633816952\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 68\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         1406.82</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\"> -39.882</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            399.05</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 69000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-02-50\n",
      "  done: false\n",
      "  episode_len_mean: 399.81\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -27.700000000000124\n",
      "  episode_reward_mean: -39.958000000000304\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 159\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.161374558342828\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008907546313044624\n",
      "          policy_loss: -0.08045660571919547\n",
      "          total_loss: 1.0796540922588773\n",
      "          vf_explained_var: 0.17223265767097473\n",
      "          vf_loss: 1.1791878948609034\n",
      "    num_agent_steps_sampled: 69000\n",
      "    num_agent_steps_trained: 69000\n",
      "    num_steps_sampled: 69000\n",
      "    num_steps_trained: 69000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.6888888888889\n",
      "    ram_util_percent: 74.33703703703704\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037136688867085106\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.37280685533739\n",
      "    mean_inference_ms: 1.3384997183857188\n",
      "    mean_raw_obs_processing_ms: 1.29275998883591\n",
      "  time_since_restore: 1425.6496803760529\n",
      "  time_this_iter_s: 18.826590061187744\n",
      "  time_total_s: 1425.6496803760529\n",
      "  timers:\n",
      "    learn_throughput: 1538.259\n",
      "    learn_time_ms: 650.086\n",
      "    load_throughput: 64329.125\n",
      "    load_time_ms: 15.545\n",
      "    sample_throughput: 51.511\n",
      "    sample_time_ms: 19413.31\n",
      "    update_time_ms: 1.643\n",
      "  timestamp: 1633816970\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 69000\n",
      "  training_iteration: 69\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         1425.65</td><td style=\"text-align: right;\">69000</td><td style=\"text-align: right;\"> -39.958</td><td style=\"text-align: right;\">               -27.7</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            399.81</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 70000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-03-09\n",
      "  done: false\n",
      "  episode_len_mean: 402.76\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -28.20000000000013\n",
      "  episode_reward_mean: -40.253000000000306\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 162\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0817906313472325\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010252013754585064\n",
      "          policy_loss: 0.008621711035569508\n",
      "          total_loss: 1.1062114854653677\n",
      "          vf_explained_var: 0.020898794755339622\n",
      "          vf_loss: 1.1154882530371348\n",
      "    num_agent_steps_sampled: 70000\n",
      "    num_agent_steps_trained: 70000\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.98846153846154\n",
      "    ram_util_percent: 74.4423076923077\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0371298141310807\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.40083276978634\n",
      "    mean_inference_ms: 1.338458588135247\n",
      "    mean_raw_obs_processing_ms: 1.2939607587705455\n",
      "  time_since_restore: 1444.0117325782776\n",
      "  time_this_iter_s: 18.36205220222473\n",
      "  time_total_s: 1444.0117325782776\n",
      "  timers:\n",
      "    learn_throughput: 1534.491\n",
      "    learn_time_ms: 651.682\n",
      "    load_throughput: 65108.017\n",
      "    load_time_ms: 15.359\n",
      "    sample_throughput: 51.449\n",
      "    sample_time_ms: 19436.717\n",
      "    update_time_ms: 1.649\n",
      "  timestamp: 1633816989\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 70\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         1444.01</td><td style=\"text-align: right;\">70000</td><td style=\"text-align: right;\"> -40.253</td><td style=\"text-align: right;\">               -28.2</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            402.76</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 71000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-03-27\n",
      "  done: false\n",
      "  episode_len_mean: 403.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -28.20000000000013\n",
      "  episode_reward_mean: -40.351000000000305\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 164\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.0151816606521606\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007628681778541161\n",
      "          policy_loss: 0.10580744900637203\n",
      "          total_loss: 0.666815181904369\n",
      "          vf_explained_var: 0.40879881381988525\n",
      "          vf_loss: 0.578987159828345\n",
      "    num_agent_steps_sampled: 71000\n",
      "    num_agent_steps_trained: 71000\n",
      "    num_steps_sampled: 71000\n",
      "    num_steps_trained: 71000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.48461538461538\n",
      "    ram_util_percent: 74.59230769230768\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0371254910797357\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.41623134571781\n",
      "    mean_inference_ms: 1.3384134719405771\n",
      "    mean_raw_obs_processing_ms: 1.2954297262460435\n",
      "  time_since_restore: 1462.3132283687592\n",
      "  time_this_iter_s: 18.301495790481567\n",
      "  time_total_s: 1462.3132283687592\n",
      "  timers:\n",
      "    learn_throughput: 1531.605\n",
      "    learn_time_ms: 652.91\n",
      "    load_throughput: 68346.315\n",
      "    load_time_ms: 14.631\n",
      "    sample_throughput: 51.371\n",
      "    sample_time_ms: 19466.111\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1633817007\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 71000\n",
      "  training_iteration: 71\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         1462.31</td><td style=\"text-align: right;\">71000</td><td style=\"text-align: right;\"> -40.351</td><td style=\"text-align: right;\">               -28.2</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            403.74</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-03-46\n",
      "  done: false\n",
      "  episode_len_mean: 404.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -28.20000000000013\n",
      "  episode_reward_mean: -40.47000000000031\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 166\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.055369422170851\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014707580410842963\n",
      "          policy_loss: -0.10647326757510503\n",
      "          total_loss: 0.8879651231898202\n",
      "          vf_explained_var: 0.48361650109291077\n",
      "          vf_loss: 1.0108038663864136\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.62592592592593\n",
      "    ram_util_percent: 74.70370370370371\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03712117426285148\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.43063897326182\n",
      "    mean_inference_ms: 1.3383669729552607\n",
      "    mean_raw_obs_processing_ms: 1.296910820447174\n",
      "  time_since_restore: 1481.3222901821136\n",
      "  time_this_iter_s: 19.009061813354492\n",
      "  time_total_s: 1481.3222901821136\n",
      "  timers:\n",
      "    learn_throughput: 1528.54\n",
      "    learn_time_ms: 654.219\n",
      "    load_throughput: 68445.132\n",
      "    load_time_ms: 14.61\n",
      "    sample_throughput: 51.438\n",
      "    sample_time_ms: 19440.927\n",
      "    update_time_ms: 1.654\n",
      "  timestamp: 1633817026\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 72\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         1481.32</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  -40.47</td><td style=\"text-align: right;\">               -28.2</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            404.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 73000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-04-05\n",
      "  done: false\n",
      "  episode_len_mean: 406.64\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -28.20000000000013\n",
      "  episode_reward_mean: -40.64100000000031\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 168\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9609473360909355\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01518699582587764\n",
      "          policy_loss: -0.1415975605448087\n",
      "          total_loss: 1.0906041201617982\n",
      "          vf_explained_var: 0.1009545549750328\n",
      "          vf_loss: 1.2474864184028573\n",
      "    num_agent_steps_sampled: 73000\n",
      "    num_agent_steps_trained: 73000\n",
      "    num_steps_sampled: 73000\n",
      "    num_steps_trained: 73000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.46071428571428\n",
      "    ram_util_percent: 74.83214285714287\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03711691743955496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.4440885615037\n",
      "    mean_inference_ms: 1.3383221892343522\n",
      "    mean_raw_obs_processing_ms: 1.2984132286308678\n",
      "  time_since_restore: 1500.437665462494\n",
      "  time_this_iter_s: 19.11537528038025\n",
      "  time_total_s: 1500.437665462494\n",
      "  timers:\n",
      "    learn_throughput: 1529.833\n",
      "    learn_time_ms: 653.666\n",
      "    load_throughput: 65864.499\n",
      "    load_time_ms: 15.183\n",
      "    sample_throughput: 51.074\n",
      "    sample_time_ms: 19579.555\n",
      "    update_time_ms: 1.653\n",
      "  timestamp: 1633817045\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 73000\n",
      "  training_iteration: 73\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         1500.44</td><td style=\"text-align: right;\">73000</td><td style=\"text-align: right;\"> -40.641</td><td style=\"text-align: right;\">               -28.2</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            406.64</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 74000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-04-23\n",
      "  done: false\n",
      "  episode_len_mean: 409.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -28.20000000000013\n",
      "  episode_reward_mean: -40.91800000000031\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 171\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9527940246793958\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0070379757483983955\n",
      "          policy_loss: 0.012872003846698336\n",
      "          total_loss: 1.0691691163513395\n",
      "          vf_explained_var: 0.562126100063324\n",
      "          vf_loss: 1.0738208825389544\n",
      "    num_agent_steps_sampled: 74000\n",
      "    num_agent_steps_trained: 74000\n",
      "    num_steps_sampled: 74000\n",
      "    num_steps_trained: 74000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.708\n",
      "    ram_util_percent: 74.836\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037110604732896964\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.460135792122994\n",
      "    mean_inference_ms: 1.338253089897197\n",
      "    mean_raw_obs_processing_ms: 1.301185632874528\n",
      "  time_since_restore: 1517.9357206821442\n",
      "  time_this_iter_s: 17.49805521965027\n",
      "  time_total_s: 1517.9357206821442\n",
      "  timers:\n",
      "    learn_throughput: 1530.552\n",
      "    learn_time_ms: 653.359\n",
      "    load_throughput: 66100.748\n",
      "    load_time_ms: 15.128\n",
      "    sample_throughput: 51.226\n",
      "    sample_time_ms: 19521.208\n",
      "    update_time_ms: 1.651\n",
      "  timestamp: 1633817063\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 74000\n",
      "  training_iteration: 74\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         1517.94</td><td style=\"text-align: right;\">74000</td><td style=\"text-align: right;\"> -40.918</td><td style=\"text-align: right;\">               -28.2</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            409.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 75000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-04-42\n",
      "  done: false\n",
      "  episode_len_mean: 410.73\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -28.20000000000013\n",
      "  episode_reward_mean: -41.05000000000031\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 173\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9631200671195983\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.007649669556396511\n",
      "          policy_loss: 0.08134256402651469\n",
      "          total_loss: 0.6777900310026275\n",
      "          vf_explained_var: 0.16140763461589813\n",
      "          vf_loss: 0.6139003054549297\n",
      "    num_agent_steps_sampled: 75000\n",
      "    num_agent_steps_trained: 75000\n",
      "    num_steps_sampled: 75000\n",
      "    num_steps_trained: 75000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.853846153846156\n",
      "    ram_util_percent: 74.8423076923077\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03710662243548767\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.469130152513852\n",
      "    mean_inference_ms: 1.3382104360935876\n",
      "    mean_raw_obs_processing_ms: 1.3033563197523401\n",
      "  time_since_restore: 1536.684476852417\n",
      "  time_this_iter_s: 18.748756170272827\n",
      "  time_total_s: 1536.684476852417\n",
      "  timers:\n",
      "    learn_throughput: 1533.145\n",
      "    learn_time_ms: 652.254\n",
      "    load_throughput: 66384.582\n",
      "    load_time_ms: 15.064\n",
      "    sample_throughput: 56.145\n",
      "    sample_time_ms: 17811.114\n",
      "    update_time_ms: 1.652\n",
      "  timestamp: 1633817082\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75000\n",
      "  training_iteration: 75\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         1536.68</td><td style=\"text-align: right;\">75000</td><td style=\"text-align: right;\">  -41.05</td><td style=\"text-align: right;\">               -28.2</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            410.73</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-05-00\n",
      "  done: false\n",
      "  episode_len_mean: 412.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -28.20000000000013\n",
      "  episode_reward_mean: -41.27000000000031\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 175\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.918579720126258\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008056658340915214\n",
      "          policy_loss: -0.0524515536096361\n",
      "          total_loss: 1.0901817076736027\n",
      "          vf_explained_var: 0.27545472979545593\n",
      "          vf_loss: 1.1595248156123692\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.94615384615385\n",
      "    ram_util_percent: 74.85\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037102380524954286\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.47387214685988\n",
      "    mean_inference_ms: 1.3381573545306742\n",
      "    mean_raw_obs_processing_ms: 1.305805973317207\n",
      "  time_since_restore: 1554.899109840393\n",
      "  time_this_iter_s: 18.214632987976074\n",
      "  time_total_s: 1554.899109840393\n",
      "  timers:\n",
      "    learn_throughput: 1534.976\n",
      "    learn_time_ms: 651.476\n",
      "    load_throughput: 66586.295\n",
      "    load_time_ms: 15.018\n",
      "    sample_throughput: 56.522\n",
      "    sample_time_ms: 17692.252\n",
      "    update_time_ms: 1.656\n",
      "  timestamp: 1633817100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 76\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">          1554.9</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  -41.27</td><td style=\"text-align: right;\">               -28.2</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            412.93</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 77000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-05-18\n",
      "  done: false\n",
      "  episode_len_mean: 415.16\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -28.20000000000013\n",
      "  episode_reward_mean: -41.49300000000033\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 177\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9143878724839953\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.0071142194659304415\n",
      "          policy_loss: -0.13642967409557766\n",
      "          total_loss: 0.9418503734800551\n",
      "          vf_explained_var: 0.5150964260101318\n",
      "          vf_loss: 1.0953980321685473\n",
      "    num_agent_steps_sampled: 77000\n",
      "    num_agent_steps_trained: 77000\n",
      "    num_steps_sampled: 77000\n",
      "    num_steps_trained: 77000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.77307692307693\n",
      "    ram_util_percent: 74.80384615384615\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037098201245343056\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.476860049333823\n",
      "    mean_inference_ms: 1.3380983237686883\n",
      "    mean_raw_obs_processing_ms: 1.3082427218619532\n",
      "  time_since_restore: 1572.608370065689\n",
      "  time_this_iter_s: 17.70926022529602\n",
      "  time_total_s: 1572.608370065689\n",
      "  timers:\n",
      "    learn_throughput: 1535.824\n",
      "    learn_time_ms: 651.116\n",
      "    load_throughput: 70969.731\n",
      "    load_time_ms: 14.091\n",
      "    sample_throughput: 56.663\n",
      "    sample_time_ms: 17648.108\n",
      "    update_time_ms: 1.663\n",
      "  timestamp: 1633817118\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 77000\n",
      "  training_iteration: 77\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         1572.61</td><td style=\"text-align: right;\">77000</td><td style=\"text-align: right;\"> -41.493</td><td style=\"text-align: right;\">               -28.2</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            415.16</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=258822)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=258822)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 78000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-05-52\n",
      "  done: false\n",
      "  episode_len_mean: 418.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -28.20000000000013\n",
      "  episode_reward_mean: -41.777000000000314\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 180\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656249999999\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8984206901656258\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.004528923469725073\n",
      "          policy_loss: 0.049490080773830415\n",
      "          total_loss: 1.0835717734363344\n",
      "          vf_explained_var: 0.5526365041732788\n",
      "          vf_loss: 1.0517762175036802\n",
      "    num_agent_steps_sampled: 78000\n",
      "    num_agent_steps_trained: 78000\n",
      "    num_steps_sampled: 78000\n",
      "    num_steps_trained: 78000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.23673469387755\n",
      "    ram_util_percent: 74.89591836734694\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370919288673843\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.47797873729905\n",
      "    mean_inference_ms: 1.338000984575122\n",
      "    mean_raw_obs_processing_ms: 1.3186552516173182\n",
      "  time_since_restore: 1607.459760427475\n",
      "  time_this_iter_s: 34.85139036178589\n",
      "  time_total_s: 1607.459760427475\n",
      "  timers:\n",
      "    learn_throughput: 1534.504\n",
      "    learn_time_ms: 651.676\n",
      "    load_throughput: 66378.278\n",
      "    load_time_ms: 15.065\n",
      "    sample_throughput: 51.567\n",
      "    sample_time_ms: 19392.18\n",
      "    update_time_ms: 1.665\n",
      "  timestamp: 1633817152\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 78000\n",
      "  training_iteration: 78\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         1607.46</td><td style=\"text-align: right;\">78000</td><td style=\"text-align: right;\"> -41.777</td><td style=\"text-align: right;\">               -28.2</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">               418</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 79000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-06-13\n",
      "  done: false\n",
      "  episode_len_mean: 419.47\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -28.20000000000013\n",
      "  episode_reward_mean: -41.924000000000326\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 182\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.971814849641588\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01308568899825359\n",
      "          policy_loss: -0.0460116962591807\n",
      "          total_loss: 1.001903235912323\n",
      "          vf_explained_var: 0.29148826003074646\n",
      "          vf_loss: 1.065769895994001\n",
      "    num_agent_steps_sampled: 79000\n",
      "    num_agent_steps_trained: 79000\n",
      "    num_steps_sampled: 79000\n",
      "    num_steps_trained: 79000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.43\n",
      "    ram_util_percent: 74.82000000000001\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037087838565552006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.478030470576286\n",
      "    mean_inference_ms: 1.3379373075548582\n",
      "    mean_raw_obs_processing_ms: 1.325601263271825\n",
      "  time_since_restore: 1628.3133018016815\n",
      "  time_this_iter_s: 20.853541374206543\n",
      "  time_total_s: 1628.3133018016815\n",
      "  timers:\n",
      "    learn_throughput: 1536.534\n",
      "    learn_time_ms: 650.815\n",
      "    load_throughput: 66373.551\n",
      "    load_time_ms: 15.066\n",
      "    sample_throughput: 51.032\n",
      "    sample_time_ms: 19595.729\n",
      "    update_time_ms: 1.653\n",
      "  timestamp: 1633817173\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79000\n",
      "  training_iteration: 79\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.6/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         1628.31</td><td style=\"text-align: right;\">79000</td><td style=\"text-align: right;\"> -41.924</td><td style=\"text-align: right;\">               -28.2</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            419.47</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-06-33\n",
      "  done: false\n",
      "  episode_len_mean: 421.41\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -28.20000000000013\n",
      "  episode_reward_mean: -42.11800000000033\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 184\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9881000757217406\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.014037733210479865\n",
      "          policy_loss: -0.10595210376713012\n",
      "          total_loss: 0.6724559259083536\n",
      "          vf_explained_var: 0.5548824667930603\n",
      "          vf_loss: 0.796290300703711\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.14999999999999\n",
      "    ram_util_percent: 74.83928571428571\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03708394290184267\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.47489249001656\n",
      "    mean_inference_ms: 1.3378742391433571\n",
      "    mean_raw_obs_processing_ms: 1.3327174260152734\n",
      "  time_since_restore: 1647.876315832138\n",
      "  time_this_iter_s: 19.563014030456543\n",
      "  time_total_s: 1647.876315832138\n",
      "  timers:\n",
      "    learn_throughput: 1535.552\n",
      "    learn_time_ms: 651.232\n",
      "    load_throughput: 65961.14\n",
      "    load_time_ms: 15.16\n",
      "    sample_throughput: 50.722\n",
      "    sample_time_ms: 19715.329\n",
      "    update_time_ms: 1.643\n",
      "  timestamp: 1633817193\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 80\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         1647.88</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> -42.118</td><td style=\"text-align: right;\">               -28.2</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            421.41</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 81000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-06-53\n",
      "  done: false\n",
      "  episode_len_mean: 423.97\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -28.20000000000013\n",
      "  episode_reward_mean: -42.37400000000034\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 187\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.942355247338613\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.013673070268591481\n",
      "          policy_loss: -0.019775384995672437\n",
      "          total_loss: 1.1126318246126174\n",
      "          vf_explained_var: 0.3141019940376282\n",
      "          vf_loss: 1.149883963747157\n",
      "    num_agent_steps_sampled: 81000\n",
      "    num_agent_steps_trained: 81000\n",
      "    num_steps_sampled: 81000\n",
      "    num_steps_trained: 81000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.548275862068955\n",
      "    ram_util_percent: 74.91034482758621\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03707788020826491\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.46806445347367\n",
      "    mean_inference_ms: 1.3377784885340256\n",
      "    mean_raw_obs_processing_ms: 1.3433920850459173\n",
      "  time_since_restore: 1668.2297768592834\n",
      "  time_this_iter_s: 20.353461027145386\n",
      "  time_total_s: 1668.2297768592834\n",
      "  timers:\n",
      "    learn_throughput: 1521.04\n",
      "    learn_time_ms: 657.445\n",
      "    load_throughput: 63247.336\n",
      "    load_time_ms: 15.811\n",
      "    sample_throughput: 50.217\n",
      "    sample_time_ms: 19913.612\n",
      "    update_time_ms: 1.688\n",
      "  timestamp: 1633817213\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 81000\n",
      "  training_iteration: 81\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         1668.23</td><td style=\"text-align: right;\">81000</td><td style=\"text-align: right;\"> -42.374</td><td style=\"text-align: right;\">               -28.2</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            423.97</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 82000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-07-15\n",
      "  done: false\n",
      "  episode_len_mean: 425.58\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -28.20000000000013\n",
      "  episode_reward_mean: -42.535000000000345\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 189\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.9124355726771884\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.016038637460927024\n",
      "          policy_loss: -0.12778589361243778\n",
      "          total_loss: 0.9803591691785388\n",
      "          vf_explained_var: 0.43339967727661133\n",
      "          vf_loss: 1.1249857889281378\n",
      "    num_agent_steps_sampled: 82000\n",
      "    num_agent_steps_trained: 82000\n",
      "    num_steps_sampled: 82000\n",
      "    num_steps_trained: 82000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.08387096774193\n",
      "    ram_util_percent: 75.34193548387097\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0370745175685431\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.461918956040808\n",
      "    mean_inference_ms: 1.3377281209291128\n",
      "    mean_raw_obs_processing_ms: 1.3464645013874363\n",
      "  time_since_restore: 1689.8456752300262\n",
      "  time_this_iter_s: 21.615898370742798\n",
      "  time_total_s: 1689.8456752300262\n",
      "  timers:\n",
      "    learn_throughput: 1508.988\n",
      "    learn_time_ms: 662.696\n",
      "    load_throughput: 63474.949\n",
      "    load_time_ms: 15.754\n",
      "    sample_throughput: 49.581\n",
      "    sample_time_ms: 20169.004\n",
      "    update_time_ms: 1.716\n",
      "  timestamp: 1633817235\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 82000\n",
      "  training_iteration: 82\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         1689.85</td><td style=\"text-align: right;\">82000</td><td style=\"text-align: right;\"> -42.535</td><td style=\"text-align: right;\">               -28.2</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            425.58</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 83000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-07-34\n",
      "  done: false\n",
      "  episode_len_mean: 428.19\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -28.20000000000013\n",
      "  episode_reward_mean: -42.79600000000034\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 192\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.895238881640964\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009300669661712746\n",
      "          policy_loss: 0.046633350849151614\n",
      "          total_loss: 1.1614609407054053\n",
      "          vf_explained_var: 0.4194941520690918\n",
      "          vf_loss: 1.132455710735586\n",
      "    num_agent_steps_sampled: 83000\n",
      "    num_agent_steps_trained: 83000\n",
      "    num_steps_sampled: 83000\n",
      "    num_steps_trained: 83000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.660714285714285\n",
      "    ram_util_percent: 75.27142857142857\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03706981748813389\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.44908732255991\n",
      "    mean_inference_ms: 1.3376534625704508\n",
      "    mean_raw_obs_processing_ms: 1.345095160357689\n",
      "  time_since_restore: 1709.1450262069702\n",
      "  time_this_iter_s: 19.29935097694397\n",
      "  time_total_s: 1709.1450262069702\n",
      "  timers:\n",
      "    learn_throughput: 1507.792\n",
      "    learn_time_ms: 663.221\n",
      "    load_throughput: 63799.167\n",
      "    load_time_ms: 15.674\n",
      "    sample_throughput: 49.537\n",
      "    sample_time_ms: 20186.964\n",
      "    update_time_ms: 1.721\n",
      "  timestamp: 1633817254\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 83000\n",
      "  training_iteration: 83\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         1709.15</td><td style=\"text-align: right;\">83000</td><td style=\"text-align: right;\"> -42.796</td><td style=\"text-align: right;\">               -28.2</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            428.19</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-07-53\n",
      "  done: false\n",
      "  episode_len_mean: 430.31\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -28.20000000000013\n",
      "  episode_reward_mean: -43.00800000000034\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 194\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8891247325473361\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01034087901932828\n",
      "          policy_loss: -0.081502808464898\n",
      "          total_loss: 1.0523974395460554\n",
      "          vf_explained_var: 0.08321187645196915\n",
      "          vf_loss: 1.1513191238873535\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.88148148148149\n",
      "    ram_util_percent: 75.23333333333333\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03706684292511511\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.438262909403424\n",
      "    mean_inference_ms: 1.3376018128894451\n",
      "    mean_raw_obs_processing_ms: 1.3442857766450118\n",
      "  time_since_restore: 1728.1137838363647\n",
      "  time_this_iter_s: 18.96875762939453\n",
      "  time_total_s: 1728.1137838363647\n",
      "  timers:\n",
      "    learn_throughput: 1507.62\n",
      "    learn_time_ms: 663.297\n",
      "    load_throughput: 63694.822\n",
      "    load_time_ms: 15.7\n",
      "    sample_throughput: 49.179\n",
      "    sample_time_ms: 20333.93\n",
      "    update_time_ms: 1.72\n",
      "  timestamp: 1633817273\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 84\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         1728.11</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\"> -43.008</td><td style=\"text-align: right;\">               -28.2</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            430.31</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 85000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-08-12\n",
      "  done: false\n",
      "  episode_len_mean: 433.55\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -29.300000000000146\n",
      "  episode_reward_mean: -43.332000000000356\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 197\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8781017422676087\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009669664204027543\n",
      "          policy_loss: 0.009170784221755133\n",
      "          total_loss: 1.1180203275548086\n",
      "          vf_explained_var: -0.043351005762815475\n",
      "          vf_loss: 1.1262537668148676\n",
      "    num_agent_steps_sampled: 85000\n",
      "    num_agent_steps_trained: 85000\n",
      "    num_steps_sampled: 85000\n",
      "    num_steps_trained: 85000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.666666666666664\n",
      "    ram_util_percent: 75.48518518518519\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03706245603160804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.41982351229619\n",
      "    mean_inference_ms: 1.337525194042758\n",
      "    mean_raw_obs_processing_ms: 1.3432461553161845\n",
      "  time_since_restore: 1746.9174156188965\n",
      "  time_this_iter_s: 18.80363178253174\n",
      "  time_total_s: 1746.9174156188965\n",
      "  timers:\n",
      "    learn_throughput: 1504.547\n",
      "    learn_time_ms: 664.652\n",
      "    load_throughput: 63551.409\n",
      "    load_time_ms: 15.735\n",
      "    sample_throughput: 49.169\n",
      "    sample_time_ms: 20338.041\n",
      "    update_time_ms: 1.714\n",
      "  timestamp: 1633817292\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 85000\n",
      "  training_iteration: 85\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         1746.92</td><td style=\"text-align: right;\">85000</td><td style=\"text-align: right;\"> -43.332</td><td style=\"text-align: right;\">               -29.3</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            433.55</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 86000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-08-31\n",
      "  done: false\n",
      "  episode_len_mean: 435.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -30.700000000000166\n",
      "  episode_reward_mean: -43.525000000000354\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 199\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.879926500055525\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009627042105578711\n",
      "          policy_loss: 0.054338582936260434\n",
      "          total_loss: 0.6161935482588079\n",
      "          vf_explained_var: 0.2281894087791443\n",
      "          vf_loss: 0.5792835054712163\n",
      "    num_agent_steps_sampled: 86000\n",
      "    num_agent_steps_trained: 86000\n",
      "    num_steps_sampled: 86000\n",
      "    num_steps_trained: 86000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.67037037037038\n",
      "    ram_util_percent: 75.5037037037037\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.037059693638782566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.405447182020584\n",
      "    mean_inference_ms: 1.3374724222877712\n",
      "    mean_raw_obs_processing_ms: 1.3427520285609555\n",
      "  time_since_restore: 1765.8358635902405\n",
      "  time_this_iter_s: 18.918447971343994\n",
      "  time_total_s: 1765.8358635902405\n",
      "  timers:\n",
      "    learn_throughput: 1504.121\n",
      "    learn_time_ms: 664.84\n",
      "    load_throughput: 63006.96\n",
      "    load_time_ms: 15.871\n",
      "    sample_throughput: 49.0\n",
      "    sample_time_ms: 20408.082\n",
      "    update_time_ms: 1.719\n",
      "  timestamp: 1633817311\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 86000\n",
      "  training_iteration: 86\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         1765.84</td><td style=\"text-align: right;\">86000</td><td style=\"text-align: right;\"> -43.525</td><td style=\"text-align: right;\">               -30.7</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            435.48</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_4d34e_00000:\n",
      "  agent_timesteps_total: 87000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-09_22-08-50\n",
      "  done: false\n",
      "  episode_len_mean: 437.85\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -32.100000000000186\n",
      "  episode_reward_mean: -43.76200000000036\n",
      "  episode_reward_min: -54.1000000000005\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 201\n",
      "  experiment_id: 1667dda7b06542909a1096e12654b23d\n",
      "  hostname: linar-Z390-GAMING-X\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.14238281249999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 1.8463498089048598\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.012046026045272745\n",
      "          policy_loss: -0.09771091938018799\n",
      "          total_loss: 1.0890718814399507\n",
      "          vf_explained_var: 0.3711315095424652\n",
      "          vf_loss: 1.2035311463806364\n",
      "    num_agent_steps_sampled: 87000\n",
      "    num_agent_steps_trained: 87000\n",
      "    num_steps_sampled: 87000\n",
      "    num_steps_trained: 87000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.3.5\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.629629629629626\n",
      "    ram_util_percent: 75.54444444444444\n",
      "  pid: 258828\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03705701031911928\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 17.38964451504322\n",
      "    mean_inference_ms: 1.337417322098657\n",
      "    mean_raw_obs_processing_ms: 1.3422675391669645\n",
      "  time_since_restore: 1784.7009134292603\n",
      "  time_this_iter_s: 18.865049839019775\n",
      "  time_total_s: 1784.7009134292603\n",
      "  timers:\n",
      "    learn_throughput: 1502.578\n",
      "    learn_time_ms: 665.523\n",
      "    load_throughput: 59299.29\n",
      "    load_time_ms: 16.864\n",
      "    sample_throughput: 48.728\n",
      "    sample_time_ms: 20522.202\n",
      "    update_time_ms: 1.712\n",
      "  timestamp: 1633817330\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 87000\n",
      "  training_iteration: 87\n",
      "  trial_id: 4d34e_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.8/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/8 CPUs, 1.0/1 GPUs, 0.0/5.77 GiB heap, 0.0/2.89 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /root/ray_results/PPO_2021-10-09_21-38-54<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_4d34e_00000</td><td>RUNNING </td><td>192.168.3.5:258828</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">          1784.7</td><td style=\"text-align: right;\">87000</td><td style=\"text-align: right;\"> -43.762</td><td style=\"text-align: right;\">               -32.1</td><td style=\"text-align: right;\">               -54.1</td><td style=\"text-align: right;\">            437.85</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "analysis = tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 1,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO C8 pretrained (AnnaCNN) (3 noops after placement and reward shaping)\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
