{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d79e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "#from models import VisualEncoder\n",
    "from train import *\n",
    "from wrappers_2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9fd218",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=2, stride=2, padding=0),  \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=2, stride=2, padding=0), \n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(), \n",
    "            nn.Conv2d(128, 256, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.cnn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7deb70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import one_hot\n",
    "\n",
    "class MyModelClass(TorchModelV2, nn.Module):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs, model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "        visual_features_dim = 512\n",
    "        target_features_dim = 9 * 11 * 11 \n",
    "        self.visual_encoder = VisualEncoder()\n",
    "        self.visual_encoder.load_state_dict(\n",
    "            torch.load(\"/IGLU-Minecraft/models/AngelaCNN/encoder_weigths.pth\", map_location=torch.device('cpu'))\n",
    "        )\n",
    "        self.target_encoder = nn.Sequential(\n",
    "            nn.Conv3d(7, 1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        policy_hidden_dim = 256 \n",
    "        self.policy_network = nn.Sequential(\n",
    "            nn.Linear(visual_features_dim + target_features_dim, 1024),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(512, policy_hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "            nn.ELU(),\n",
    "            #nn.Linear(policy_hidden_dim, policy_hidden_dim),\n",
    "            #nn.ELU(),\n",
    "        )\n",
    "        self.action_head = nn.Linear(policy_hidden_dim, action_space.n)\n",
    "        self.value_head = nn.Linear(policy_hidden_dim, 1)\n",
    "        self.last_value = None\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.visual_encoder.cuda()\n",
    "            self.target_encoder.cuda()\n",
    "            self.policy_network.cuda()\n",
    "            self.action_head.cuda()\n",
    "            self.value_head.cuda()\n",
    "        \n",
    "    @override(TorchModelV2)\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        obs = input_dict['obs']\n",
    "        pov = obs['pov'].permute(0, 3, 1, 2).float() / 255.0\n",
    "        target = one_hot(obs['target_grid'].long(), num_classes=7).permute(0, 4, 1, 2, 3).float()\n",
    "        if self.use_cuda:\n",
    "            pov.cuda()\n",
    "            target.cuda()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            visual_features = self.visual_encoder(pov)\n",
    "            \n",
    "        target_features = self.target_encoder(target)\n",
    "        target_features = target_features.reshape(target_features.shape[0], -1)\n",
    "        features = torch.cat([visual_features, target_features], dim=1)\n",
    "        features = self.policy_network(features)\n",
    "        action = self.action_head(features)\n",
    "        self.last_value = self.value_head(features).squeeze(1)\n",
    "        return action, state\n",
    "    \n",
    "    @override(TorchModelV2)\n",
    "    def value_function(self):\n",
    "        assert self.last_value is not None, \"must call forward() first\"\n",
    "        return self.last_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579b418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_torch_model\", MyModelClass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc09c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualObservationWrapper(ObsWrapper):\n",
    "    def __init__(self, env, include_target=False):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = {   \n",
    "            'pov': gym.spaces.Box(low=0, high=255, shape=(64, 64, 3)),\n",
    "            'inventory': gym.spaces.Box(low=0.0, high=20.0, shape=(6,)),\n",
    "            'compass': gym.spaces.Box(low=-180.0, high=180.0, shape=(1,))\n",
    "        }\n",
    "        if include_target:\n",
    "            self.observation_space['target_grid'] = \\\n",
    "                gym.spaces.Box(low=0, high=6, shape=(9, 11, 11))\n",
    "        self.observation_space = gym.spaces.Dict(self.observation_space)\n",
    "\n",
    "    def observation(self, obs, reward=None, done=None, info=None):\n",
    "        if info is not None:\n",
    "            if 'target_grid' in info:\n",
    "                target_grid = info['target_grid']\n",
    "                del info['target_grid']\n",
    "            else:\n",
    "                logger.error(f'info: {info}')\n",
    "                if hasattr(self.unwrapped, 'should_reset'):\n",
    "                    self.unwrapped.should_reset(True)\n",
    "                target_grid = self.env.unwrapped.tasks.current.target_grid\n",
    "        else:\n",
    "            target_grid = self.env.unwrapped.tasks.current.target_grid\n",
    "        return {\n",
    "            'pov': obs['pov'].astype(np.float32),\n",
    "            'inventory': obs['inventory'],\n",
    "            'compass': np.array([obs['compass']['angle'].item()]),\n",
    "            'target_grid': target_grid\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b86a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "class RewardWrapper(gym.RewardWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def reward(self, rew):\n",
    "        if rew == 0:\n",
    "            rew = -0.01\n",
    "        if abs(rew) == 1:\n",
    "            rew /= 10\n",
    "        return rew\n",
    "    \n",
    "def env_creator(env_config):\n",
    "    env = gym.make('IGLUSilentBuilder-v0', max_steps=1000)\n",
    "    env.update_taskset(TaskSet(preset=['C3', 'C17', 'C32']))\n",
    "    #env = PovOnlyWrapper(env)\n",
    "    env = VisualObservationWrapper(env, include_target=True)\n",
    "    env = SelectAndPlace(env)\n",
    "    env = Discretization(env, flat_action_space('human-level'))\n",
    "    env = RewardWrapper(env)\n",
    "    return env\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"my_env\", env_creator)\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0adede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/py37/lib/python3.7/site-packages/ray/_private/services.py:238: UserWarning: Not all Ray Dashboard dependencies were found. To use the dashboard please install Ray using `pip install ray[default]`. To disable this message, set RAY_DISABLE_IMPORT_WARNING env var to '1'.\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 17.7/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/1 GPUs, 0.0/62.69 GiB heap, 0.0/30.86 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/PPO_2021-11-05_20-27-12<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2555_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-05 20:27:13,090\tINFO wandb.py:170 -- Already logged into W&B.\n",
      "2021-11-05 20:27:13,129\tERROR syncer.py:72 -- Log sync requires rsync to be installed.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlinar\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to auto resume run with id 5f885_00000 but id c2555_00000 is set.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.6 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(pid=382971)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=382971)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">PPO MultiTask (C3, C17, C32) pretrained (AngelaCNN) (3 noops after placement) r: -0.01 div10</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/linar/IGLU-Minecraft/runs/c2555_00000\" target=\"_blank\">https://wandb.ai/linar/IGLU-Minecraft/runs/c2555_00000</a><br/>\n",
       "                Run data is saved locally in <code>/IGLU-Minecraft/wandb/run-20211105_202713-c2555_00000</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=382971)\u001b[0m 2021-11-05 20:27:17,995\tINFO ppo.py:159 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=382971)\u001b[0m 2021-11-05 20:27:17,995\tINFO trainer.py:728 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=382968)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=382968)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=382971)\u001b[0m 2021-11-05 20:27:25,806\tINFO trainable.py:109 -- Trainable.setup took 11.331 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=382971)\u001b[0m 2021-11-05 20:27:25,807\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=382968)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=382968)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=382968)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=382968)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\u001b[2m\u001b[36m(pid=382968)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/runpy.py:125: RuntimeWarning: 'minerl_patched.utils.process_watcher' found in sys.modules after import of package 'minerl_patched.utils', but prior to execution of 'minerl_patched.utils.process_watcher'; this may result in unpredictable behaviour\n",
      "\u001b[2m\u001b[36m(pid=382968)\u001b[0m   warn(RuntimeWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2555_00000:\n",
      "  agent_timesteps_total: 1000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-05_20-28-55\n",
      "  done: false\n",
      "  episode_len_mean: 411.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -4.139999999999956\n",
      "  episode_reward_mean: -4.154999999999957\n",
      "  episode_reward_min: -4.169999999999957\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 2\n",
      "  experiment_id: ff870b931ac0466ba6c548d9d0717a19\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.878750189145406\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009509325984693628\n",
      "          policy_loss: 0.046141261772976984\n",
      "          total_loss: 0.020941973477602006\n",
      "          vf_explained_var: 0.08785516023635864\n",
      "          vf_loss: 0.0016863471168714265\n",
      "    num_agent_steps_sampled: 1000\n",
      "    num_agent_steps_trained: 1000\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.040625\n",
      "    ram_util_percent: 19.707812500000003\n",
      "  pid: 382971\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06976113333687797\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 83.49428286442867\n",
      "    mean_inference_ms: 4.250652187473172\n",
      "    mean_raw_obs_processing_ms: 0.47205068491079233\n",
      "  time_since_restore: 89.40200400352478\n",
      "  time_this_iter_s: 89.40200400352478\n",
      "  time_total_s: 89.40200400352478\n",
      "  timers:\n",
      "    learn_throughput: 1110.898\n",
      "    learn_time_ms: 900.173\n",
      "    load_throughput: 36140.657\n",
      "    load_time_ms: 27.67\n",
      "    sample_throughput: 11.305\n",
      "    sample_time_ms: 88459.122\n",
      "    update_time_ms: 6.921\n",
      "  timestamp: 1636144135\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: c2555_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.8/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/62.69 GiB heap, 0.0/30.86 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/PPO_2021-11-05_20-27-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2555_00000</td><td>RUNNING </td><td>192.168.1.96:382971</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          89.402</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">  -4.155</td><td style=\"text-align: right;\">               -4.14</td><td style=\"text-align: right;\">               -4.17</td><td style=\"text-align: right;\">               411</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2555_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-05_20-29-31\n",
      "  done: false\n",
      "  episode_len_mean: 399.8\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.2100000000000022\n",
      "  episode_reward_mean: -3.5679999999999694\n",
      "  episode_reward_min: -4.979999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 5\n",
      "  experiment_id: ff870b931ac0466ba6c548d9d0717a19\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.869719534450107\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010928736950149285\n",
      "          policy_loss: -0.08635586665736304\n",
      "          total_loss: 0.09094753488898277\n",
      "          vf_explained_var: -0.1483485996723175\n",
      "          vf_loss: 0.2038148485744993\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 2000\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.27450980392157\n",
      "    ram_util_percent: 20.701960784313723\n",
      "  pid: 382971\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06873365287835095\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 67.57073650690889\n",
      "    mean_inference_ms: 4.195350338733655\n",
      "    mean_raw_obs_processing_ms: 0.47365584060940014\n",
      "  time_since_restore: 125.3386402130127\n",
      "  time_this_iter_s: 35.936636209487915\n",
      "  time_total_s: 125.3386402130127\n",
      "  timers:\n",
      "    learn_throughput: 1154.201\n",
      "    learn_time_ms: 866.4\n",
      "    load_throughput: 32434.658\n",
      "    load_time_ms: 30.831\n",
      "    sample_throughput: 16.192\n",
      "    sample_time_ms: 61757.596\n",
      "    update_time_ms: 6.178\n",
      "  timestamp: 1636144171\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: c2555_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.8/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/62.69 GiB heap, 0.0/30.86 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/PPO_2021-11-05_20-27-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2555_00000</td><td>RUNNING </td><td>192.168.1.96:382971</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         125.339</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">  -3.568</td><td style=\"text-align: right;\">               -1.21</td><td style=\"text-align: right;\">               -4.98</td><td style=\"text-align: right;\">             399.8</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=382968)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=382968)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2555_00000:\n",
      "  agent_timesteps_total: 3000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-05_20-30-46\n",
      "  done: false\n",
      "  episode_len_mean: 394.85714285714283\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0600000000000063\n",
      "  episode_reward_mean: -2.6871428571428324\n",
      "  episode_reward_min: -4.979999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 7\n",
      "  experiment_id: ff870b931ac0466ba6c548d9d0717a19\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8639131599002416\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.00800819651705233\n",
      "          policy_loss: -0.12533319724930658\n",
      "          total_loss: -0.0022743696139918435\n",
      "          vf_explained_var: 0.1485067754983902\n",
      "          vf_loss: 0.15009631680117713\n",
      "    num_agent_steps_sampled: 3000\n",
      "    num_agent_steps_trained: 3000\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.62685185185185\n",
      "    ram_util_percent: 20.61944444444444\n",
      "  pid: 382971\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06838386647062622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 61.862655432772094\n",
      "    mean_inference_ms: 4.179019915730319\n",
      "    mean_raw_obs_processing_ms: 4.362379370071946\n",
      "  time_since_restore: 200.70242190361023\n",
      "  time_this_iter_s: 75.36378169059753\n",
      "  time_total_s: 200.70242190361023\n",
      "  timers:\n",
      "    learn_throughput: 1131.122\n",
      "    learn_time_ms: 884.078\n",
      "    load_throughput: 33385.368\n",
      "    load_time_ms: 29.953\n",
      "    sample_throughput: 15.158\n",
      "    sample_time_ms: 65971.928\n",
      "    update_time_ms: 6.27\n",
      "  timestamp: 1636144246\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: c2555_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.8/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/62.69 GiB heap, 0.0/30.86 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/PPO_2021-11-05_20-27-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2555_00000</td><td>RUNNING </td><td>192.168.1.96:382971</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         200.702</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">-2.68714</td><td style=\"text-align: right;\">                2.06</td><td style=\"text-align: right;\">               -4.98</td><td style=\"text-align: right;\">           394.857</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2555_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-05_20-31-22\n",
      "  done: false\n",
      "  episode_len_mean: 399.7\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0600000000000063\n",
      "  episode_reward_mean: -2.9199999999999746\n",
      "  episode_reward_min: -4.979999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 10\n",
      "  experiment_id: ff870b931ac0466ba6c548d9d0717a19\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.8478457000520496\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008916679019131004\n",
      "          policy_loss: -0.04861029527253575\n",
      "          total_loss: 0.021084851523240408\n",
      "          vf_explained_var: 0.11147531867027283\n",
      "          vf_loss: 0.09639026907065676\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.09019607843137\n",
      "    ram_util_percent: 20.866666666666667\n",
      "  pid: 382971\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06804553278338514\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 56.2843459702245\n",
      "    mean_inference_ms: 4.160355801292766\n",
      "    mean_raw_obs_processing_ms: 6.25330133621752\n",
      "  time_since_restore: 236.47010803222656\n",
      "  time_this_iter_s: 35.76768612861633\n",
      "  time_total_s: 236.47010803222656\n",
      "  timers:\n",
      "    learn_throughput: 1141.596\n",
      "    learn_time_ms: 875.966\n",
      "    load_throughput: 33873.726\n",
      "    load_time_ms: 29.521\n",
      "    sample_throughput: 17.183\n",
      "    sample_time_ms: 58197.63\n",
      "    update_time_ms: 6.299\n",
      "  timestamp: 1636144282\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: c2555_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/62.69 GiB heap, 0.0/30.86 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/PPO_2021-11-05_20-27-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2555_00000</td><td>RUNNING </td><td>192.168.1.96:382971</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">          236.47</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">   -2.92</td><td style=\"text-align: right;\">                2.06</td><td style=\"text-align: right;\">               -4.98</td><td style=\"text-align: right;\">             399.7</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2555_00000:\n",
      "  agent_timesteps_total: 5000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-05_20-31-56\n",
      "  done: false\n",
      "  episode_len_mean: 401.4166666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0600000000000063\n",
      "  episode_reward_mean: -2.4199999999999773\n",
      "  episode_reward_min: -4.979999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 12\n",
      "  experiment_id: ff870b931ac0466ba6c548d9d0717a19\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.835487882296244\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008582832141086044\n",
      "          policy_loss: 0.07007198515865538\n",
      "          total_loss: 0.27668222954703703\n",
      "          vf_explained_var: 0.2568071484565735\n",
      "          vf_loss: 0.23324856099983057\n",
      "    num_agent_steps_sampled: 5000\n",
      "    num_agent_steps_trained: 5000\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.1875\n",
      "    ram_util_percent: 20.920833333333334\n",
      "  pid: 382971\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06787366215390177\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 53.61425714759309\n",
      "    mean_inference_ms: 4.152267553171208\n",
      "    mean_raw_obs_processing_ms: 6.645404715070189\n",
      "  time_since_restore: 270.2802166938782\n",
      "  time_this_iter_s: 33.81010866165161\n",
      "  time_total_s: 270.2802166938782\n",
      "  timers:\n",
      "    learn_throughput: 1126.262\n",
      "    learn_time_ms: 887.893\n",
      "    load_throughput: 34004.488\n",
      "    load_time_ms: 29.408\n",
      "    sample_throughput: 18.824\n",
      "    sample_time_ms: 53123.748\n",
      "    update_time_ms: 6.288\n",
      "  timestamp: 1636144316\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: c2555_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/62.69 GiB heap, 0.0/30.86 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/PPO_2021-11-05_20-27-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2555_00000</td><td>RUNNING </td><td>192.168.1.96:382971</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          270.28</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">   -2.42</td><td style=\"text-align: right;\">                2.06</td><td style=\"text-align: right;\">               -4.98</td><td style=\"text-align: right;\">           401.417</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2555_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-05_20-32-30\n",
      "  done: false\n",
      "  episode_len_mean: 404.35714285714283\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0600000000000063\n",
      "  episode_reward_mean: -2.2499999999999756\n",
      "  episode_reward_min: -4.979999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 14\n",
      "  experiment_id: ff870b931ac0466ba6c548d9d0717a19\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.818758993678623\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.009373819079898817\n",
      "          policy_loss: 0.06912527639004919\n",
      "          total_loss: 0.15560172374049822\n",
      "          vf_explained_var: 0.366378515958786\n",
      "          vf_loss: 0.11278927152355513\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 6000\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.45918367346939\n",
      "    ram_util_percent: 20.944897959183674\n",
      "  pid: 382971\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06778778152634342\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 51.428208213679675\n",
      "    mean_inference_ms: 4.149019457931679\n",
      "    mean_raw_obs_processing_ms: 6.72941488545523\n",
      "  time_since_restore: 304.4777879714966\n",
      "  time_this_iter_s: 34.19757127761841\n",
      "  time_total_s: 304.4777879714966\n",
      "  timers:\n",
      "    learn_throughput: 1127.126\n",
      "    learn_time_ms: 887.212\n",
      "    load_throughput: 33310.069\n",
      "    load_time_ms: 30.021\n",
      "    sample_throughput: 20.075\n",
      "    sample_time_ms: 49814.335\n",
      "    update_time_ms: 6.343\n",
      "  timestamp: 1636144350\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: c2555_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/62.69 GiB heap, 0.0/30.86 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/PPO_2021-11-05_20-27-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2555_00000</td><td>RUNNING </td><td>192.168.1.96:382971</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         304.478</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">   -2.25</td><td style=\"text-align: right;\">                2.06</td><td style=\"text-align: right;\">               -4.98</td><td style=\"text-align: right;\">           404.357</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2555_00000:\n",
      "  agent_timesteps_total: 7000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-05_20-33-06\n",
      "  done: false\n",
      "  episode_len_mean: 401.2352941176471\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0600000000000063\n",
      "  episode_reward_mean: -2.2264705882352716\n",
      "  episode_reward_min: -4.979999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 17\n",
      "  experiment_id: ff870b931ac0466ba6c548d9d0717a19\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.798145898183187\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010994973299824542\n",
      "          policy_loss: -0.07510843964086639\n",
      "          total_loss: -0.023549755497111215\n",
      "          vf_explained_var: 0.3355380892753601\n",
      "          vf_loss: 0.07734114856769642\n",
      "    num_agent_steps_sampled: 7000\n",
      "    num_agent_steps_trained: 7000\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 7000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.75961538461539\n",
      "    ram_util_percent: 20.89423076923077\n",
      "  pid: 382971\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0676947078044761\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 48.91615488597764\n",
      "    mean_inference_ms: 4.146328048927181\n",
      "    mean_raw_obs_processing_ms: 6.6465557885967\n",
      "  time_since_restore: 340.5023281574249\n",
      "  time_this_iter_s: 36.024540185928345\n",
      "  time_total_s: 340.5023281574249\n",
      "  timers:\n",
      "    learn_throughput: 1143.272\n",
      "    learn_time_ms: 874.682\n",
      "    load_throughput: 33864.046\n",
      "    load_time_ms: 29.53\n",
      "    sample_throughput: 20.954\n",
      "    sample_time_ms: 47724.435\n",
      "    update_time_ms: 6.28\n",
      "  timestamp: 1636144386\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: c2555_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.9/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/62.69 GiB heap, 0.0/30.86 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/PPO_2021-11-05_20-27-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2555_00000</td><td>RUNNING </td><td>192.168.1.96:382971</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         340.502</td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\">-2.22647</td><td style=\"text-align: right;\">                2.06</td><td style=\"text-align: right;\">               -4.98</td><td style=\"text-align: right;\">           401.235</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2555_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-05_20-33-41\n",
      "  done: false\n",
      "  episode_len_mean: 400.8421052631579\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0600000000000063\n",
      "  episode_reward_mean: -2.0594736842105057\n",
      "  episode_reward_min: -4.979999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 19\n",
      "  experiment_id: ff870b931ac0466ba6c548d9d0717a19\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7989277601242066\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008321602485222412\n",
      "          policy_loss: -0.08513370272186067\n",
      "          total_loss: 0.10303599459843503\n",
      "          vf_explained_var: 0.23065054416656494\n",
      "          vf_loss: 0.21449464874135124\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.856\n",
      "    ram_util_percent: 20.814\n",
      "  pid: 382971\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06766298515234308\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 47.582643595849056\n",
      "    mean_inference_ms: 4.146890491095779\n",
      "    mean_raw_obs_processing_ms: 6.528670460402293\n",
      "  time_since_restore: 375.78819155693054\n",
      "  time_this_iter_s: 35.285863399505615\n",
      "  time_total_s: 375.78819155693054\n",
      "  timers:\n",
      "    learn_throughput: 1153.764\n",
      "    learn_time_ms: 866.729\n",
      "    load_throughput: 33988.602\n",
      "    load_time_ms: 29.422\n",
      "    sample_throughput: 21.71\n",
      "    sample_time_ms: 46062.586\n",
      "    update_time_ms: 6.152\n",
      "  timestamp: 1636144421\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  trial_id: c2555_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.9/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/62.69 GiB heap, 0.0/30.86 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/PPO_2021-11-05_20-27-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2555_00000</td><td>RUNNING </td><td>192.168.1.96:382971</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         375.788</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-2.05947</td><td style=\"text-align: right;\">                2.06</td><td style=\"text-align: right;\">               -4.98</td><td style=\"text-align: right;\">           400.842</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2555_00000:\n",
      "  agent_timesteps_total: 9000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-05_20-34-16\n",
      "  done: false\n",
      "  episode_len_mean: 400.09090909090907\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0600000000000063\n",
      "  episode_reward_mean: -2.0963636363636176\n",
      "  episode_reward_min: -4.979999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 22\n",
      "  experiment_id: ff870b931ac0466ba6c548d9d0717a19\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.778216176562839\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011919210666138862\n",
      "          policy_loss: -0.005463524659474691\n",
      "          total_loss: 0.11884123952024513\n",
      "          vf_explained_var: -0.03294960409402847\n",
      "          vf_loss: 0.14970308239054347\n",
      "    num_agent_steps_sampled: 9000\n",
      "    num_agent_steps_trained: 9000\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 9000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.964\n",
      "    ram_util_percent: 20.804000000000002\n",
      "  pid: 382971\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06765242018333353\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 45.92883859557029\n",
      "    mean_inference_ms: 4.1476247976529255\n",
      "    mean_raw_obs_processing_ms: 6.3145971515391475\n",
      "  time_since_restore: 410.5767002105713\n",
      "  time_this_iter_s: 34.78850865364075\n",
      "  time_total_s: 410.5767002105713\n",
      "  timers:\n",
      "    learn_throughput: 1144.14\n",
      "    learn_time_ms: 874.019\n",
      "    load_throughput: 34065.874\n",
      "    load_time_ms: 29.355\n",
      "    sample_throughput: 22.371\n",
      "    sample_time_ms: 44701.337\n",
      "    update_time_ms: 6.23\n",
      "  timestamp: 1636144456\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: c2555_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.9/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/62.69 GiB heap, 0.0/30.86 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/PPO_2021-11-05_20-27-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2555_00000</td><td>RUNNING </td><td>192.168.1.96:382971</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         410.577</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">-2.09636</td><td style=\"text-align: right;\">                2.06</td><td style=\"text-align: right;\">               -4.98</td><td style=\"text-align: right;\">           400.091</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2555_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-05_20-34-50\n",
      "  done: false\n",
      "  episode_len_mean: 401.5416666666667\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0600000000000063\n",
      "  episode_reward_mean: -2.1504166666666475\n",
      "  episode_reward_min: -4.979999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 24\n",
      "  experiment_id: ff870b931ac0466ba6c548d9d0717a19\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.750260093477037\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011243702691025328\n",
      "          policy_loss: -0.04557857612768809\n",
      "          total_loss: 0.0479824673384428\n",
      "          vf_explained_var: 0.5034226179122925\n",
      "          vf_loss: 0.11881490502920415\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 10000\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.276595744680854\n",
      "    ram_util_percent: 20.797872340425524\n",
      "  pid: 382971\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06763894110504322\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 44.9924463107158\n",
      "    mean_inference_ms: 4.1481347593783156\n",
      "    mean_raw_obs_processing_ms: 6.163354311349235\n",
      "  time_since_restore: 444.0091407299042\n",
      "  time_this_iter_s: 33.432440519332886\n",
      "  time_total_s: 444.0091407299042\n",
      "  timers:\n",
      "    learn_throughput: 1140.796\n",
      "    learn_time_ms: 876.581\n",
      "    load_throughput: 34343.537\n",
      "    load_time_ms: 29.118\n",
      "    sample_throughput: 22.999\n",
      "    sample_time_ms: 43480.011\n",
      "    update_time_ms: 6.239\n",
      "  timestamp: 1636144490\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: c2555_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.9/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/62.69 GiB heap, 0.0/30.86 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/PPO_2021-11-05_20-27-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2555_00000</td><td>RUNNING </td><td>192.168.1.96:382971</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         444.009</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">-2.15042</td><td style=\"text-align: right;\">                2.06</td><td style=\"text-align: right;\">               -4.98</td><td style=\"text-align: right;\">           401.542</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2555_00000:\n",
      "  agent_timesteps_total: 11000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-05_20-35-26\n",
      "  done: false\n",
      "  episode_len_mean: 401.8888888888889\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0600000000000063\n",
      "  episode_reward_mean: -2.212222222222204\n",
      "  episode_reward_min: -4.979999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 27\n",
      "  experiment_id: ff870b931ac0466ba6c548d9d0717a19\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.766965937614441\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008916482369115325\n",
      "          policy_loss: 0.018910453882482317\n",
      "          total_loss: 0.1863621167010731\n",
      "          vf_explained_var: 0.30861523747444153\n",
      "          vf_loss: 0.1933380252785153\n",
      "    num_agent_steps_sampled: 11000\n",
      "    num_agent_steps_trained: 11000\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 11000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.861538461538466\n",
      "    ram_util_percent: 20.728846153846156\n",
      "  pid: 382971\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06763274328672894\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 43.80054661823405\n",
      "    mean_inference_ms: 4.150821276056727\n",
      "    mean_raw_obs_processing_ms: 5.937535357839507\n",
      "  time_since_restore: 479.83158445358276\n",
      "  time_this_iter_s: 35.82244372367859\n",
      "  time_total_s: 479.83158445358276\n",
      "  timers:\n",
      "    learn_throughput: 1147.483\n",
      "    learn_time_ms: 871.472\n",
      "    load_throughput: 34137.751\n",
      "    load_time_ms: 29.293\n",
      "    sample_throughput: 26.228\n",
      "    sample_time_ms: 38126.642\n",
      "    update_time_ms: 6.373\n",
      "  timestamp: 1636144526\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  trial_id: c2555_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.8/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/62.69 GiB heap, 0.0/30.86 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/PPO_2021-11-05_20-27-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2555_00000</td><td>RUNNING </td><td>192.168.1.96:382971</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         479.832</td><td style=\"text-align: right;\">11000</td><td style=\"text-align: right;\">-2.21222</td><td style=\"text-align: right;\">                2.06</td><td style=\"text-align: right;\">               -4.98</td><td style=\"text-align: right;\">           401.889</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2555_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-05_20-36-00\n",
      "  done: false\n",
      "  episode_len_mean: 401.7586206896552\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0600000000000063\n",
      "  episode_reward_mean: -2.1358620689654995\n",
      "  episode_reward_min: -4.979999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 29\n",
      "  experiment_id: ff870b931ac0466ba6c548d9d0717a19\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7743186288409762\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010717347678773682\n",
      "          policy_loss: -0.09303775102727943\n",
      "          total_loss: 0.06297180698149735\n",
      "          vf_explained_var: 0.2833046615123749\n",
      "          vf_loss: 0.18160927432278792\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.210204081632654\n",
      "    ram_util_percent: 20.73877551020408\n",
      "  pid: 382971\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0676317073475006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 43.1104322017137\n",
      "    mean_inference_ms: 4.153191628990324\n",
      "    mean_raw_obs_processing_ms: 5.791417974849788\n",
      "  time_since_restore: 514.2301168441772\n",
      "  time_this_iter_s: 34.39853239059448\n",
      "  time_total_s: 514.2301168441772\n",
      "  timers:\n",
      "    learn_throughput: 1137.616\n",
      "    learn_time_ms: 879.031\n",
      "    load_throughput: 34661.521\n",
      "    load_time_ms: 28.85\n",
      "    sample_throughput: 26.34\n",
      "    sample_time_ms: 37965.556\n",
      "    update_time_ms: 6.469\n",
      "  timestamp: 1636144560\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: c2555_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.9/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/62.69 GiB heap, 0.0/30.86 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/PPO_2021-11-05_20-27-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2555_00000</td><td>RUNNING </td><td>192.168.1.96:382971</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">          514.23</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-2.13586</td><td style=\"text-align: right;\">                2.06</td><td style=\"text-align: right;\">               -4.98</td><td style=\"text-align: right;\">           401.759</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2555_00000:\n",
      "  agent_timesteps_total: 13000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-05_20-36-36\n",
      "  done: false\n",
      "  episode_len_mean: 402.46875\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0600000000000063\n",
      "  episode_reward_mean: -2.0365624999999805\n",
      "  episode_reward_min: -4.979999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 32\n",
      "  experiment_id: ff870b931ac0466ba6c548d9d0717a19\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.744903972413805\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010858998144247718\n",
      "          policy_loss: -0.06344245200355848\n",
      "          total_loss: 0.09839994926005602\n",
      "          vf_explained_var: 0.6244847774505615\n",
      "          vf_loss: 0.1871196443008052\n",
      "    num_agent_steps_sampled: 13000\n",
      "    num_agent_steps_trained: 13000\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 13000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.260000000000005\n",
      "    ram_util_percent: 20.72\n",
      "  pid: 382971\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06763049718584649\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 42.20749379323882\n",
      "    mean_inference_ms: 4.156448358818347\n",
      "    mean_raw_obs_processing_ms: 5.582315010618334\n",
      "  time_since_restore: 549.67236161232\n",
      "  time_this_iter_s: 35.4422447681427\n",
      "  time_total_s: 549.67236161232\n",
      "  timers:\n",
      "    learn_throughput: 1142.424\n",
      "    learn_time_ms: 875.332\n",
      "    load_throughput: 34791.94\n",
      "    load_time_ms: 28.742\n",
      "    sample_throughput: 29.432\n",
      "    sample_time_ms: 33977.143\n",
      "    update_time_ms: 6.634\n",
      "  timestamp: 1636144596\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  trial_id: c2555_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.8/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/62.69 GiB heap, 0.0/30.86 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/PPO_2021-11-05_20-27-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2555_00000</td><td>RUNNING </td><td>192.168.1.96:382971</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         549.672</td><td style=\"text-align: right;\">13000</td><td style=\"text-align: right;\">-2.03656</td><td style=\"text-align: right;\">                2.06</td><td style=\"text-align: right;\">               -4.98</td><td style=\"text-align: right;\">           402.469</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2555_00000:\n",
      "  agent_timesteps_total: 14000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-05_20-37-13\n",
      "  done: false\n",
      "  episode_len_mean: 401.61764705882354\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 2.0600000000000063\n",
      "  episode_reward_mean: -2.1114705882352744\n",
      "  episode_reward_min: -4.979999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 34\n",
      "  experiment_id: ff870b931ac0466ba6c548d9d0717a19\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7772578345404733\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.011247855246502562\n",
      "          policy_loss: -0.025533474940392705\n",
      "          total_loss: 0.06885179264677896\n",
      "          vf_explained_var: 0.5300604701042175\n",
      "          vf_loss: 0.11990827259918053\n",
      "    num_agent_steps_sampled: 14000\n",
      "    num_agent_steps_trained: 14000\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.212962962962955\n",
      "    ram_util_percent: 20.703703703703702\n",
      "  pid: 382971\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0676316774027136\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 41.68610001260153\n",
      "    mean_inference_ms: 4.158664391292622\n",
      "    mean_raw_obs_processing_ms: 5.450075720631327\n",
      "  time_since_restore: 586.9686555862427\n",
      "  time_this_iter_s: 37.29629397392273\n",
      "  time_total_s: 586.9686555862427\n",
      "  timers:\n",
      "    learn_throughput: 1138.679\n",
      "    learn_time_ms: 878.211\n",
      "    load_throughput: 35018.573\n",
      "    load_time_ms: 28.556\n",
      "    sample_throughput: 29.302\n",
      "    sample_time_ms: 34127.037\n",
      "    update_time_ms: 6.817\n",
      "  timestamp: 1636144633\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  trial_id: c2555_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.8/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/62.69 GiB heap, 0.0/30.86 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/PPO_2021-11-05_20-27-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2555_00000</td><td>RUNNING </td><td>192.168.1.96:382971</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         586.969</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\">-2.11147</td><td style=\"text-align: right;\">                2.06</td><td style=\"text-align: right;\">               -4.98</td><td style=\"text-align: right;\">           401.618</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=382968)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=382968)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2555_00000:\n",
      "  agent_timesteps_total: 15000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-05_20-38-26\n",
      "  done: false\n",
      "  episode_len_mean: 391.60526315789474\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.33\n",
      "  episode_reward_mean: -1.9265789473684014\n",
      "  episode_reward_min: -4.979999999999956\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 38\n",
      "  experiment_id: ff870b931ac0466ba6c548d9d0717a19\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7573852247662014\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.008852936254648194\n",
      "          policy_loss: 0.025173151327504053\n",
      "          total_loss: 0.13699780946804418\n",
      "          vf_explained_var: 0.45000430941581726\n",
      "          vf_loss: 0.13762792299191157\n",
      "    num_agent_steps_sampled: 15000\n",
      "    num_agent_steps_trained: 15000\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 15000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.45961538461539\n",
      "    ram_util_percent: 20.674038461538462\n",
      "  pid: 382971\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06765691647825367\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 40.788042358270694\n",
      "    mean_inference_ms: 4.164068770935109\n",
      "    mean_raw_obs_processing_ms: 5.464843859894579\n",
      "  time_since_restore: 660.1379897594452\n",
      "  time_this_iter_s: 73.16933417320251\n",
      "  time_total_s: 660.1379897594452\n",
      "  timers:\n",
      "    learn_throughput: 1146.04\n",
      "    learn_time_ms: 872.57\n",
      "    load_throughput: 35065.62\n",
      "    load_time_ms: 28.518\n",
      "    sample_throughput: 26.268\n",
      "    sample_time_ms: 38069.118\n",
      "    update_time_ms: 6.654\n",
      "  timestamp: 1636144706\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: c2555_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 22.8/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/62.69 GiB heap, 0.0/30.86 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/PPO_2021-11-05_20-27-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2555_00000</td><td>RUNNING </td><td>192.168.1.96:382971</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         660.138</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">-1.92658</td><td style=\"text-align: right;\">                5.33</td><td style=\"text-align: right;\">               -4.98</td><td style=\"text-align: right;\">           391.605</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=382968)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=382968)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2555_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-05_20-40-29\n",
      "  done: false\n",
      "  episode_len_mean: 371.2093023255814\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.33\n",
      "  episode_reward_mean: -1.4644186046511445\n",
      "  episode_reward_min: -4.979999999999956\n",
      "  episodes_this_iter: 5\n",
      "  episodes_total: 43\n",
      "  experiment_id: ff870b931ac0466ba6c548d9d0717a19\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.7185783359739517\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.01313044895265002\n",
      "          policy_loss: -0.1678030978060431\n",
      "          total_loss: 0.16328392285439702\n",
      "          vf_explained_var: 0.11948759108781815\n",
      "          vf_loss: 0.35564670852488944\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.27613636363637\n",
      "    ram_util_percent: 20.856818181818184\n",
      "  pid: 382971\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06770420291188747\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 39.8994377862221\n",
      "    mean_inference_ms: 4.169934630724412\n",
      "    mean_raw_obs_processing_ms: 6.054704174698911\n",
      "  time_since_restore: 783.3316178321838\n",
      "  time_this_iter_s: 123.19362807273865\n",
      "  time_total_s: 783.3316178321838\n",
      "  timers:\n",
      "    learn_throughput: 1135.103\n",
      "    learn_time_ms: 880.977\n",
      "    load_throughput: 35756.092\n",
      "    load_time_ms: 27.967\n",
      "    sample_throughput: 21.295\n",
      "    sample_time_ms: 46960.367\n",
      "    update_time_ms: 6.907\n",
      "  timestamp: 1636144829\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  trial_id: c2555_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/62.69 GiB heap, 0.0/30.86 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/PPO_2021-11-05_20-27-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2555_00000</td><td>RUNNING </td><td>192.168.1.96:382971</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         783.332</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-1.46442</td><td style=\"text-align: right;\">                5.33</td><td style=\"text-align: right;\">               -4.98</td><td style=\"text-align: right;\">           371.209</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2555_00000:\n",
      "  agent_timesteps_total: 17000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-05_20-41-06\n",
      "  done: false\n",
      "  episode_len_mean: 372.97777777777776\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.33\n",
      "  episode_reward_mean: -1.4337777777777596\n",
      "  episode_reward_min: -4.979999999999956\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 45\n",
      "  experiment_id: ff870b931ac0466ba6c548d9d0717a19\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.716971156332228\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010306969684661462\n",
      "          policy_loss: -0.00444153282377455\n",
      "          total_loss: 0.058341307938098906\n",
      "          vf_explained_var: 0.3316081762313843\n",
      "          vf_loss: 0.08789115924802092\n",
      "    num_agent_steps_sampled: 17000\n",
      "    num_agent_steps_trained: 17000\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 17000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.40576923076924\n",
      "    ram_util_percent: 20.999999999999996\n",
      "  pid: 382971\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06772668266856222\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 39.592167350171025\n",
      "    mean_inference_ms: 4.171946833422158\n",
      "    mean_raw_obs_processing_ms: 6.227385660605304\n",
      "  time_since_restore: 819.5053148269653\n",
      "  time_this_iter_s: 36.173696994781494\n",
      "  time_total_s: 819.5053148269653\n",
      "  timers:\n",
      "    learn_throughput: 1112.533\n",
      "    learn_time_ms: 898.85\n",
      "    load_throughput: 35711.644\n",
      "    load_time_ms: 28.002\n",
      "    sample_throughput: 21.296\n",
      "    sample_time_ms: 46957.28\n",
      "    update_time_ms: 6.978\n",
      "  timestamp: 1636144866\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  trial_id: c2555_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.1/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/62.69 GiB heap, 0.0/30.86 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/PPO_2021-11-05_20-27-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2555_00000</td><td>RUNNING </td><td>192.168.1.96:382971</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         819.505</td><td style=\"text-align: right;\">17000</td><td style=\"text-align: right;\">-1.43378</td><td style=\"text-align: right;\">                5.33</td><td style=\"text-align: right;\">               -4.98</td><td style=\"text-align: right;\">           372.978</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=382968)\u001b[0m /root/miniconda/envs/py37/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "\u001b[2m\u001b[36m(pid=382968)\u001b[0m   warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_my_env_c2555_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-05_20-42-16\n",
      "  done: false\n",
      "  episode_len_mean: 368.375\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 5.33\n",
      "  episode_reward_mean: -1.2806249999999808\n",
      "  episode_reward_min: -4.979999999999956\n",
      "  episodes_this_iter: 3\n",
      "  episodes_total: 48\n",
      "  experiment_id: ff870b931ac0466ba6c548d9d0717a19\n",
      "  hostname: cdsserver\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.19999999999999996\n",
      "          cur_lr: 5.000000000000001e-05\n",
      "          entropy: 2.6692831092410616\n",
      "          entropy_coeff: 0.009999999999999998\n",
      "          kl: 0.010933409736867574\n",
      "          policy_loss: -0.10132481687598759\n",
      "          total_loss: 0.07588327959593799\n",
      "          vf_explained_var: 0.17513208091259003\n",
      "          vf_loss: 0.20171424320174589\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 18000\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.1.96\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.257\n",
      "    ram_util_percent: 20.825000000000003\n",
      "  pid: 382971\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06776195308656016\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 39.16637025877501\n",
      "    mean_inference_ms: 4.174667746338582\n",
      "    mean_raw_obs_processing_ms: 6.5494438349078505\n",
      "  time_since_restore: 889.933515548706\n",
      "  time_this_iter_s: 70.42820072174072\n",
      "  time_total_s: 889.933515548706\n",
      "  timers:\n",
      "    learn_throughput: 1101.224\n",
      "    learn_time_ms: 908.08\n",
      "    load_throughput: 35823.798\n",
      "    load_time_ms: 27.914\n",
      "    sample_throughput: 19.817\n",
      "    sample_time_ms: 50462.394\n",
      "    update_time_ms: 7.112\n",
      "  timestamp: 1636144936\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: c2555_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 23.0/110.1 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/16 CPUs, 1.0/1 GPUs, 0.0/62.69 GiB heap, 0.0/30.86 GiB objects (0.0/1.0 accelerator_type:G)<br>Result logdir: /IGLU-Minecraft/checkpoints/PPO_2021-11-05_20-27-12<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_my_env_c2555_00000</td><td>RUNNING </td><td>192.168.1.96:382971</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         889.934</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">-1.28062</td><td style=\"text-align: right;\">                5.33</td><td style=\"text-align: right;\">               -4.98</td><td style=\"text-align: right;\">           368.375</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ray.tune.integration.wandb import WandbLogger\n",
    "\n",
    "analysis = tune.run(PPOTrainer, \n",
    "         config={\n",
    "             \"env\": \"my_env\", \n",
    "             \"framework\": \"torch\",\n",
    "             \"num_gpus\": 1,\n",
    "             \"num_workers\": 1,\n",
    "             \"sgd_minibatch_size\": 256,\n",
    "             \"clip_param\": 0.2,\n",
    "             \"entropy_coeff\": 0.01,\n",
    "             \"lambda\": 0.95,\n",
    "             \"train_batch_size\": 1000,\n",
    "             #\"gamma\": 0.99,\n",
    "             \"model\": {\n",
    "                    # Specify our custom model from above.\n",
    "                    \"custom_model\": \"my_torch_model\",\n",
    "                    # Extra kwargs to be passed to your model's c'tor.\n",
    "                    \"custom_model_config\": {},\n",
    "              },\n",
    "             \"logger_config\": {\n",
    "                  \"wandb\": {\n",
    "                      \"project\": \"IGLU-Minecraft\",\n",
    "                      \"name\": \"PPO MultiTask (C3, C17, C32) pretrained (AngelaCNN) (3 noops after placement) r: -0.01 div10\"\n",
    "                  }\n",
    "              }\n",
    "\n",
    "        },\n",
    "        loggers=[WandbLogger],\n",
    "        local_dir=\"/IGLU-Minecraft/checkpoints/\",\n",
    "        keep_checkpoints_num=50,\n",
    "        checkpoint_freq=5,\n",
    "        checkpoint_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967c7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408a7d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
