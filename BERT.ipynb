{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2947d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a19f85",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "410d3f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model=12):\n",
    "        super().__init__()\n",
    "        self.d_model= d_model\n",
    "        if self.d_model % 6 != 0:\n",
    "            raise ValueError(\"d_models must be divedable on 6!\")\n",
    "\n",
    "        pe = np.zeros((9, 11, 11, d_model))\n",
    "\n",
    "        for pos_x in range(9):\n",
    "            pe[pos_x,:,:,0:d_model//3:2] = np.sin(0.33 * pos_x / 10_000 ** (6*np.arange(d_model//6)/d_model))\n",
    "            pe[pos_x,:,:,1:d_model//3:2] = np.cos(0.33 * pos_x / 10_000 ** (6*np.arange(d_model//6)/d_model))\n",
    "\n",
    "        for pos_y in range(11):\n",
    "            pe[:,pos_y,:,d_model//3:2*d_model//3:2] = np.sin(0.33 * pos_y / 10_000 ** (6*np.arange(d_model//6)/d_model))\n",
    "            pe[:,pos_y,:,1+d_model//3:2*d_model//3:2] = np.cos(0.33 * pos_y / 10_000 ** (6*np.arange(d_model//6)/d_model))\n",
    "\n",
    "        for pos_z in range(11):\n",
    "            pe[:,:,pos_z,2*d_model//3::2] = np.sin(0.33 * pos_z / 10_000 ** (6*np.arange(d_model//6)/d_model))\n",
    "            pe[:,:,pos_z,1+2*d_model//3::2] = np.cos(0.33 * pos_z / 10_000 ** (6*np.arange(d_model//6)/d_model))\n",
    "            \n",
    "        pe = pe.reshape(9 * 11 * 11, d_model)\n",
    "        pe = torch.tensor(pe).float()\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x * math.sqrt(self.d_model // 3) # is it actualy needed?\n",
    "        num_tokens = x.shape[1]\n",
    "        aux_tokens = num_tokens - 9 * 11 * 11\n",
    "        \n",
    "        x = x + torch.cat([self.pe, torch.zeros((aux_tokens, self.d_model))], dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b3220f",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b039338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dim_feedforward=64, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, num_heads, dropout=0.0, batch_first=True)\n",
    "        # Implementation of feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoder(d_model=d_model)\n",
    "        self.activation = activation()\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # Forward with prenormalization\n",
    "        src2 = self.norm1(src)\n",
    "        q = k = self.pos_encoder(src2)\n",
    "        src2 = self.self_attn(q, k, value=src2)[0]\n",
    "        src = src + src2\n",
    "        src2 = self.norm2(src)\n",
    "        src2 = self.linear2(self.activation(self.linear1(src2)))\n",
    "        src = src + src2\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b90d879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1139, 12])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.rand(1, 9 * 11 * 11 + 50, 12)\n",
    "encoder = TransformerEncoderLayer(d_model=12, num_heads=4)\n",
    "out = encoder(src)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f2581ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def _get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebc25137",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder_layer, num_layers, norm=None):\n",
    "        super().__init__()\n",
    "        self.layers = _get_clones(encoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src):\n",
    "        output = src\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            output = layer(output)\n",
    "        \n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39bb998f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1139, 6])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.rand(1, 9 * 11 * 11 + 50, 6)\n",
    "encoder_layer = TransformerEncoderLayer(d_model=6, num_heads=1, dim_feedforward=1024)\n",
    "encoder = TransformerEncoder(encoder_layer, 1)\n",
    "out = encoder(src)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf305ee",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57e1df69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13510"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in encoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb024a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f29cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea0dcfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dac045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bbe26c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
